@article{2018-61201-03020181201,
Abstract = {We present the Chinese Lexical Database (CLD): a large-scale lexical database for simplified Chinese. The CLD provides a wealth of lexical information for 3913 one-character words, 34,233 two-character words, 7143 three-character words, and 3355 four-character words, and is publicly available through http://www.chineselexicaldatabase.com. For each of the 48,644 words in the CLD, we provide a wide range of categorical predictors, as well as an extensive set of frequency measures, complexity measures, neighborhood density measures, orthography-phonology consistency measures, and information-theoretic measures. We evaluate the explanatory power of the lexical variables in the CLD in the context of experimental data through analyses of lexical decision latencies for one-character, two-character, three-character and four-character words, as well as word naming latencies for one-character and two-character words. The results of these analyses are discussed. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
Author = {Sun, Ching Chu and  Hendrix, Peter and  Ma, Jianqiang and  Baayen, Rolf Harald},
ISSN = {1554-351X, 1554-3528},
Journal = {Behavior Research Methods},
Keywords = {Mandarin Chinese, simplified Chinese, Chinese lexical database, word naming, lexical decision, Databases, Language, Lexical Decision, Naming, Words (Phonetic Units)},
Number = {6},
Pages = {2606 - 2629},
Title = {Chinese lexical database (CLD): A large-scale lexical database for simplified Mandarin Chinese},
URL = {ching-chu.sun@uni-tuebingen.de, peter.hendrix@gmail.com, jianqiang.ma@uni-tuebingen.de, harald.baayen@uni-tuebingen.de},
Volume = {50},
Year = {2018},
}


@article{2018-61201-01020181201,
Abstract = {In this article, we present StimulStat—a lexical database for the Russian language in the form of a web application. The database contains more than 52,000 of the most frequent Russian lemmas and more than 1.7 million word forms derived from them. These lemmas and forms are characterized according to more than 70 properties that were demonstrated to be relevant for psycholinguistic research, including frequency, length, phonological and grammatical properties, orthographic and phonological neighborhood frequency and size, grammatical ambiguity, homonymy and polysemy. Some properties were retrieved from various dictionaries and are presented collectively in a searchable form for the first time, the others were computed specifically for the database. The database can be accessed freely at http://stimul.cognitivestudies.ru. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
Author = {Alexeeva, Svetlana and  Slioussar, Natalia and  Chernova, Daria},
ISSN = {1554-351X, 1554-3528},
Journal = {Behavior Research Methods},
Keywords = {lexical database, Russian, lemmas, forms, psycholinguistic research, Databases, Experimentation, Language, Psycholinguistics, Words (Phonetic Units)},
Number = {6},
Pages = {2305 - 2315},
Title = {StimulStat: A lexical database for Russian},
URL = {mail@s-alexeeva.ru, ORCID: 0000-0002-8540-6178, ORCID: 0000-0003-1706-6439},
Volume = {50},
Year = {2018},
}


@article{2018-38143-01020180801,
Abstract = {In this article, we present Procura-PALavras (P-PAL), a Web-based interface for a new European Portuguese (EP) lexical database. Based on a contemporary printed corpus of over 227 million words, P-PAL provides a broad range of word attributes and statistics, including several measures of word frequency (e.g., raw counts, per-million word frequency, logarithmic Zipf scale), morpho-syntactic information (e.g., parts of speech [PoSs], grammatical gender and number, dominant PoS, and frequency and relative frequency of the dominant PoS), as well as several lexical and sublexical orthographic (e.g., number of letters; consonant–vowel orthographic structure; density and frequency of orthographic neighbors; orthographic Levenshtein distance; orthographic uniqueness point; orthographic syllabification; and trigram, bigram, and letter type and token frequencies), and phonological measures (e.g., pronunciation, number of phonemes, stress, density and frequency of phonological neighbors, transposed and phonographic neighbors, syllabification, and biphone and phone type and token frequencies) for ~53,000 lemmatized and ~208,000 nonlemmatized EP word forms. To obtain these metrics, researchers can choose between two word queries in the application: (i) analyze words previously selected for specific attributes and/or lexical and sublexical characteristics, or (ii) generate word lists that meet word requirements defined by the user in the menu of analyses. For the measures it provides and the flexibility it allows, P-PAL will be a key resource to support research in all cognitive areas that use EP verbal stimuli. P-PAL is freely available at http://p-pal.di.uminho.pt/tools. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
Author = {Soares, Ana Paula and  Iriarte, Álvaro and  de Almeida, José João and  Simões, Alberto and  Costa, Ana and  Machado, João and  França, Patrícia and  Comesaña, Montserrat and  Rauber, Andreia and  Rato, Anabela and  Perea, Manuel},
ISSN = {1554-351X, 1554-3528},
Journal = {Behavior Research Methods},
Keywords = {Lexical databases, Word frequency, Orthographic word statistics, Phonological word statistics, European Portuguese, Databases, Human Computer Interaction, Language, Phonology, Word Frequency, Foreign Languages, Grammar, Orthography, Statistics},
Number = {4},
Pages = {1461 - 1481},
Title = {Procura-PALavras (P-PAL): A Web-based interface for a new European Portuguese lexical database},
URL = {asoares@psi.uminho.pt},
Volume = {50},
Year = {2018},
}


@article{2017-19296-02920170401,
Abstract = {ASL-LEX is a lexical database that catalogues information about nearly 1,000 signs in American Sign Language (ASL). It includes the following information: subjective frequency ratings from 25–31 deaf signers, iconicity ratings from 21–37 hearing non-signers, videoclip duration, sign length (onset and offset), grammatical class, and whether the sign is initialized, a fingerspelled loan sign, or a compound. Information about English translations is available for a subset of signs (e.g., alternate translations, translation consistency). In addition, phonological properties (sign type, selected fingers, flexion, major and minor location, and movement) were coded and used to generate sub-lexical frequency and neighborhood density estimates. ASL-LEX is intended for use by researchers, educators, and students who are interested in the properties of the ASL lexicon. An interactive website where the database can be browsed and downloaded is available at http://asl-lex.org . (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
Author = {Caselli, Naomi K. and  Sehyr, Zed Sevcikova and  Cohen-Goldberg, Ariel M. and  Emmorey, Karen},
ISSN = {1554-351X, 1554-3528},
Journal = {Behavior Research Methods},
Keywords = {Sign language, Lexical database, Subjective frequency, Iconicity, Neighborhood density, Deaf, Language Proficiency, Lexical Decision, Sign Language},
Number = {2},
Pages = {784 - 801},
Title = {ASL-LEX: A lexical database of American Sign Language},
URL = {nkc@bu.edu, kemmorey@mail.sdsu.edu},
Volume = {49},
Year = {2017},
}


@article{2017-45571-03020171001,
Abstract = {This article presents K-SPAN (Korean Surface Phonetics and Neighborhoods), a database of surface phonetic forms and several measures of phonological neighborhood density for 63,836 Korean words. Currently publicly available Korean corpora are limited by the fact that they only provide orthographic representations in Hangeul, which is problematic since phonetic forms in Korean cannot be reliably predicted from orthographic forms. We describe the method used to derive the surface phonetic forms from a publicly available orthographic corpus of Korean, and report on several statistics calculated using this database; namely, segment unigram frequencies, which are compared to previously reported results, along with segment-based and syllable-based neighborhood density statistics for three types of representation: an 'orthographic' form, which is a quasi-phonological representation, a 'conservative' form, which maintains all known contrasts, and a 'modern' form, which represents the pronunciation of contemporary Seoul Korean. These representations are rendered in an ASCII-encoded scheme, which allows users to query the corpus without having to read Korean orthography, and permits the calculation of a wide range of phonological measures. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
Author = {Holliday, Jeffrey J. and  Turnbull, Rory and  Eychenne, Julien},
ISSN = {1554-351X, 1554-3528},
Journal = {Behavior Research Methods},
Keywords = {Korean, Phonological neighborhood density, Lexicon, Lexical database, Databases, Methodology, Orthography, Statistical Analysis, Mental Lexicon, Cognitive Science, Language, Phonology},
Number = {5},
Pages = {1939 - 1950},
Title = {K-SPAN: A lexical database of Korean surface phonetic forms and phonological neighborhood density statistics},
URL = {holliday@korea.ac.kr, rory.turnbull@ens.fr, jeychenne@hufs.ac.kr},
Volume = {49},
Year = {2017},
}


@article{2017-08981-00120170223,
Abstract = {Databases containing lexical properties on any given orthography are crucial for psycholinguistic research. In the last ten years, a number of lexical databases have been developed for Greek. However, these lack important part-of-speech information. Furthermore, the need for alternative procedures for calculating syllabic measurements and stress information, as well as combination of several metrics to investigate linguistic properties of the Greek language are highlighted. To address these issues, we present a new extensive lexical database of Modern Greek (GreekLex 2) with part-of-speech information for each word and accurate syllabification and orthographic information predictive of stress, as well as several measurements of word similarity and phonetic information. The addition of detailed statistical information about Greek part-of-speech, syllabification, and stress neighbourhood allowed novel analyses of stress distribution within different grammatical categories and syllabic lengths to be carried out. Results showed that the statistical preponderance of stress position on the pre-final syllable that is reported for Greek language is dependent upon grammatical category. Additionally, analyses showed that a proportion higher than 90% of the tokens in the database would be stressed correctly solely by relying on stress neighbourhood information. The database and the scripts for orthographic and phonological syllabification as well as phonetic transcription are available at http://www.psychology.nottingham. ac.uk/greeklex/. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
Author = {Kyparissiadis, Antonios and  van Heuven, Walter J. B. and  Pitchford, Nicola J. and  Ledgeway, Timothy},
ISSN = {1932-6203},
Journal = {PLoS ONE},
Keywords = {GreekLex 2, lexical databases, speech, syllables, phonology, stress information, Databases, Lexical Access, Oral Communication, Phonology, Syllables, Information, Stress},
Number = {2},
Title = {GreekLex 2: A comprehensive lexical database with part-of-speech, syllabic, phonological, and stress information},
URL = {Antonios.Kyparissiadis@nottingham.ac.uk},
Volume = {12},
Year = {2017},
}


@article{2016-16578-01020160301,
Abstract = {The LSE-Sign database is a free online tool for selecting Spanish Sign Language stimulus materials to be used in experiments. It contains 2,400 individual signs taken from a recent standardized LSE dictionary, and a further 2,700 related nonsigns. Each entry is coded for a wide range of grammatical, phonological, and articulatory information, including handshape, location, movement, and non-manual elements. The database is accessible via a graphically based search facility which is highly flexible both in terms of the search options available and the way the results are displayed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Gutierrez-Sigut, Eva and  Costello, Brendan and  Baus, Cristina and  Carreiras, Manuel},
ISSN = {1554-351X, 1554-3528},
Journal = {Behavior Research Methods},
Keywords = {Sign language, Lexical database, Spanish Sign Language (LSE lengua de signos española), Stimulus material, Databases, Grammar, Lexical Access, Phonology, Sign Language, Stimulus Parameters},
Number = {1},
Pages = {123 - 137},
Title = {LSE-Sign: A lexical database for Spanish Sign Language},
URL = {b.costello@bcbl.eu, ORCID: 0000-0002-1713-1483},
Volume = {48},
Year = {2016},
}


@article{2015-13542-00420150401,
Abstract = {All words have properties linked to form, meaning and usage patterns which influence how easily they are accessed from the mental lexicon in language production, perception and comprehension. Examples of such properties are imageability, phonological and morphological complexity, word class, argument structure, frequency of use and age of acquisition. Due to linguistic and cultural variation the properties and the values associated with them differ across languages. Hence, for research as well as clinical purposes, language specific information on lexical properties is needed. To meet this need, an electronically searchable lexical database with more than 1600 Norwegian words coded for more than 12 different properties has been established. This article presents the content and structure of the database as well as the search options available in the interface. Finally, it briefly describes some of the ways in which the database can be used in research, clinical practice and teaching. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Lind, Marianne and  Simonsen, Hanne Gram and  Hansen, Pernille and  Holm, Elisabeth and  Mevik, Bjørn-Helge},
ISSN = {0269-9206, 1464-5076},
Journal = {Clinical Linguistics & Phonetics},
Keywords = {Age of acquisition, frequency, imageability, language processing, phonological neighbourhood density, Adult, Child, Child, Preschool, Databases, Factual, Humans, Infant, Language, Language Development, Language Disorders, Norway, Phonetics, Research, Semantics, Speech-Language Pathology, User-Computer Interface, Verbal Learning, Vocabulary, Clinicians, Language, Phonological Awareness, Comprehension, Databases},
Number = {4},
Pages = {276 - 290},
Title = {Norwegian Words: A lexical database for clinicians and researchers},
URL = {marianne.lind@statped.no},
Volume = {29},
Year = {2015},
}


@article{2014-07832-02120140301,
Abstract = {In this article, we introduce ESCOLEX, the first European Portuguese children’s lexical database with grade-level-adjusted word frequency statistics. Computed from a 3.2-million-word corpus, ESCOLEX provides 48,381 word forms extracted from 171 elementary and middle school textbooks for 6- to 11-year-old children attending the first six grades in the Portuguese educational system. Like other children’s grade-level databases (e.g., Carroll, Davies, & Richman, 1971; Corral, Ferrero, & Goikoetxea, Behavior Research Methods, 41, 1009–1017, 2009; Lété, Sprenger-Charolles, & Colé, Behavior Research Methods, Instruments, & Computers, 36, 156–166, 2004; Zeno, Ivens, Millard, Duvvuri, 1995), ESCOLEX provides four frequency indices for each grade: overall word frequency (F), index of dispersion across the selected textbooks (D), estimated frequency per million words (U), and standard frequency index (SFI). It also provides a new measure, contextual diversity (CD). In addition, the number of letters in the word and its part(s) of speech, number of syllables, syllable structure, and adult frequencies taken from P-PAL (a European Portuguese corpus-based lexical database; Soares, Comesaña, Iriarte, Almeida, Simões, Costa, …, Machado, 2010; Soares, Iriarte, Almeida, Simões, Costa, França, …, Comesaña, in press) are provided. ESCOLEX will be a useful tool both for researchers interested in language processing and development and for professionals in need of verbal materials adjusted to children’s developmental stages. ESCOLEX can be downloaded along with this article or from http://p-pal.di.uminho.pt/about/databases. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Soares, Ana Paula and  Medeiros, José Carlos and  Simões, Alberto and  Machado, João and  Costa, Ana and  Iriarte, Álvaro and  de Almeida, José João and  Pinheiro, Ana P. and  Comesaña, Montserrat},
ISSN = {1554-351X, 1554-3528},
Journal = {Behavior Research Methods},
Keywords = {ESCOLEX, lexical database, textbooks, word frequency, language processing, grade level, teaching programs, Child, Computer-Assisted Instruction, Databases, Factual, Humans, Information Literacy, Language, Portugal, Reading, Schools, Textbooks as Topic, Vocabulary, Language Development, Lexical Access, Teaching Methods, Word Frequency, Databases, Grade Level, Textbooks},
Number = {1},
Pages = {240 - 253},
Title = {Escolex: A grade-level lexical database from European Portuguese elementary to middle school textbooks},
URL = {asoares@psi.uminho.pt, ORCID: 0000-0001-6961-2660, ORCID: 0000-0002-7981-3682, ORCID: 0000-0003-2547-7684},
Volume = {46},
Year = {2014},
}


@article{2015-47357-00120150701,
Abstract = {In this article we introduce childLex, an online database for children’s print language in German. childLex is based on a large corpus of children’s books and textbooks used in school that comprises ca. 8 million words. It includes linguistic norms on the lexical, super-lexical, and sub-lexical level. Such norms are urgently needed in psychological research for the design of age-adequate stimulus and training materials. Separate norms are available for three age groups: 6–8 years (1st and 2nd grade), 9–10 years (3rd and 4th grade), and 11–12 years (5th and 6th grade). Moreover, childLex is directly comparable to the DWDS corpus, a large corpus for adults’ print language. Here, we describe how childLex was collected and analyzed and compare frequency norms on the lexical, super-lexical, and sub-lexical level with frequency norms obtained from the DWDS corpus. Results show moderate levels of correspondence in the high-frequency range and on the sub-lexical level, but strong discrepancies for medium- and low-frequency elements on the lexical and super-lexical level. Finally, we illustrate which variables are available in childLex and how they can be retrieved using a web interface. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Schroeder, Sascha and  Würzner, Kay-Michael and  Heister, Julian and  Geyken, Alexander and  Kliegl, Reinhold},
ISSN = {0033-3042, 2190-6238},
Journal = {Psychologische Rundschau},
Keywords = {child language, linguistic database, lexical variables, Childhood Development, Databases, Linguistics, Lexical Access, Age Differences},
Number = {3},
Pages = {155 - 165},
Title = {childLex—Eine lexikalische Datenbank zur Schriftsprache für Kinder im Deutschen = childLex—A Lexical Database for Print Language for Children in German},
URL = {sascha.schroeder@mpib-berlin.mpg.de, ORCID: 0000-0002-0180-8488},
Volume = {66},
Year = {2015},
}


@article{2011-01460-00120110101,
Abstract = {The lexical database dlexDB supplies in form of an online database frequency-based norms of numerous processrelated word properties for psychological and linguistic research. These values include well known variables such as printed frequency of word form and lemma as documented also in CELEX (Baayen, Piepenbrock und Gulikers, 1995). In addition, we compute new values like frequencies based on syllables, and morphemes as well as frequencies of character chains, and multiple word combinations. The statistics are based on the Kernkorpus des Digitalen Wörterbuchs der deutschen Sprache (DWDS) with over 100 million running words. We illustrate the validity of these norms with new results about fixation durations in sentence reading. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Heister, Julian and  Würzner, Kay-Michael and  Bubenzer, Johannes and  Pohl, Edmund and  Hanneforth, Thomas and  Geyken, Alexander and  Kliegl, Reinhold},
ISSN = {0033-3042, 2190-6238},
Journal = {Psychologische Rundschau},
Keywords = {lexical database, corpus linguistics, dlexDB, reading, Databases, Psycholinguistics},
Number = {1},
Pages = {10 - 20},
Title = {dlexDB—Eine lexikalische Datenbank für die psychologische und linguistische Forschung = dlexDB—A lexical database for the psychological and linguistic research},
URL = {kliegl@uni-potsdam.de, ORCID: 0000-0002-0180-8488},
Volume = {62},
Year = {2011},
}


@article{2011-00095-01320100501,
Abstract = {In this article, we present a new lexical database for Modern Standard Arabic: Aralex. Based on a contemporary text corpus of 40 million words, Aralex provides information about (1) the token frequencies of roots and word patterns, (2) the type frequency, or family size, of roots and word patterns, and (3) the frequency of bigrams, trigrams in orthographic forms, roots, and word patterns. Aralex will be a useful tool for studying the cognitive processing of Arabic through the selection of stimuli on the basis of precise frequency counts. Researchers can use it as a source of information on natural language processing, and it may serve an educational purpose by providing basic vocabulary lists. Aralex is distributed under a GNU-like license, allowing people to interrogate it freely online or to download it from www.mrc-cbu.cam.ac.uk:8081/aralex.online/login.jsp. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Boudelaa, Sami and  Marslen-Wilson, William D.},
ISSN = {1554-351X, 1554-3528},
Journal = {Behavior Research Methods},
Keywords = {Aralex, lexical database, Modern Standard Arabic, cognitive processes, Arabs, Databases, Factual, Humans, Internet, Language, Psycholinguistics, Software, Vocabulary, Cognitive Processes, Databases, Language, Lexical Access, Words (Phonetic Units)},
Number = {2},
Pages = {481 - 487},
Title = {Aralex: A lexical database for Modern Standard Arabic},
URL = {sami.boudelaa@mrc-cbu.cam.ac.uk, ORCID: 0000-0003-0690-6308},
Volume = {42},
Year = {2010},
}


@article{2008-10371-01520080801,
Abstract = {To conduct experimental investigations into the orthographic processing of Modern Greek, information is needed about the lexical properties known to influence visual word recognition. In this article we introduce GreekLex, a lexical database for Modern Greek, which presents collectively for the first time a series of orthographic measures that can be used for psycholinguistic research. GreekLex consists of 35,304 Modern Greek words ranging in length from 1 to 22 letters, and for each word includes the following statistical information: word length, word-form frequency, lemma frequency, neighborhood density and frequency, transposition neighbors, and addition and deletion neighbors. Furthermore, type and token frequency measures of single letters and bigrams derived from the database are also available. The complete database can be accessed and downloaded freely from www.psychology.nottingham.ac.uk/GreekLex. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Ktori, Maria and  van Heuven, Walter J. B. and  Pitchford, Nicola J.},
ISSN = {1554-351X, 1554-3528},
Journal = {Behavior Research Methods},
Keywords = {GreekLex, lexical database, modern Greek, experimental investigations, orthographic processing, visual word recognition, psycholinguistics, Databases, Factual, Greece, Humans, Language, Vocabulary, Cognitive Processes, Orthography, Psycholinguistics, Word Recognition, Lexical Access, Visual Perception},
Number = {3},
Pages = {773 - 783},
Title = {GreekLex: A lexical database of modern Greek},
URL = {lpxmk2@nottingham.ac.uk, ORCID: 0000-0003-3183-4449},
Volume = {40},
Year = {2008},
}


@article{2006-22961-00620071201,
Abstract = {Both classroom instruction and lexical database development stand to benefit from applied research on sign language, which takes into consideration American Sign Language rules, pedagogical issues, and teacher characteristics. In this study of technical science signs, teachers' experience with signing and, especially, knowledge of content, were found to be essential for the identification of signs appropriate for instruction. The results of this study also indicate a need for a systematic approach to examine both sign selection and its impact on learning by deaf students. Recommendations are made for the development of lexical databases and areas of research for optimizing the use of sign language in instruction. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Lang, Harry G. and  Hupper, Mary LaPorta and  Monte, Denise A. and  Brown, Scott W. and  Babb, Ivar and  Scheifele, Pete M.},
ISSN = {1081-4159, 1465-7325},
Journal = {Journal of Deaf Studies and Deaf Education},
Keywords = {technical signs, science, lexical database development, classroom instruction, sign language, Databases as Topic, Deafness, Humans, Science, Semantics, Sign Language, Symbolism, Teaching, Vocabulary, Databases, Deaf, Lexical Access, Sign Language, Teaching Methods, Sciences, Teaching},
Number = {1},
Pages = {65 - 79},
Title = {A study of technical signs in science: Implications for lexical database development},
URL = {harry.lang@rit.edu},
Volume = {12},
Year = {2007},
}


@article{2009-21653-00420091101,
Abstract = {The LEXIN database offers psycholinguistic indexes of the 13,184 different words (types) computed from 178,839 occurrences of these words (tokens) contained in a corpus of 134 beginning readers widely used in Spain. This database provides four statistical indicators: F (overall word frequency), D (index of dispersion across selected readers), U (estimated frequency per million words), and SFI (standard frequency index). It also gives information about the number of letters, syntactic category, and syllabic structure of the words included. To facilitate comparisons, LEXIN provides data from LEXESP’s (Sebastián-Gallés, Martí, Cuetos, & Carreiras, 2000), Alameda and Cuetos’s (1995), and Martínez and García’s (2004) Spanish adult psycholinguistic frequency databases. Access to the LEXIN database is facilitated by a computer program. The LEXIN program allows for the creation of word lists by letting the user specify searching criteria. LEXIN can be useful for researchers in cognitive psychology, particularly in the areas of psycholinguistics and education. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Corral, Silvia and  Ferrero, Marta and  Goikoetxea, Edurne},
ISSN = {0743-3808},
Journal = {Behavior Research Methods, Instruments & Computers},
Keywords = {lexical database, Spanish readers, kindergarten readers, first-grade readers, syntactic category, psycholinguistic indexes, LEXIN, Computer Software, Databases, Psycholinguistics, Reading Development, Vocabulary, Elementary School Students, Kindergarten Students, Phonology, Syntax, Words (Phonetic Units)},
Number = {4},
Pages = {1009 - 1017},
Title = {LEXIN: A lexical database from Spanish kindergarten and first-grade readers},
URL = {edurne.goikoetxea@deusto.es},
Volume = {41},
Year = {2009},
}


@article{2004-22444-01620040801,
Abstract = {In this article, we present a new lexical database for French: Lexique. In addition to classical word information such as gender, number, and grammatical category, Lexique includes a series of interesting new characteristics. First, word frequencies are based on two cues: a contemporary corpus of texts and the number of Web pages containing the word. Second, the database is split into a graphemic table with all the relevant frequencies, a table structured around lemmas (particularly interesting for the study of the inflectional family), and a table about surface frequency cues. Third, Lexique is distributed under a GNU-like license, allowing people to contribute to it. Finally, a metasearch engine, Open Lexique, has been developed so that new databases can be added very easily to the existing ones. Lexique can either be downloaded or interrogated freely from http://www.lexique.org. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {New, Boris and  Pallier, Christophe and  Brysbaert, Marc and  Ferrand, Ludovic},
ISSN = {0743-3808},
Journal = {Behavior Research Methods, Instruments & Computers},
Keywords = {Lexique, French lexical database, word frequency, Automatic Data Processing, Databases as Topic, Humans, Language, Psycholinguistics, Vocabulary, Databases, Word Frequency, Words (Phonetic Units)},
Number = {3},
Pages = {516 - 524},
Title = {Lexique 2: A new French lexical database},
URL = {boris.new@univ-paris5.fr},
Volume = {36},
Year = {2004},
}


@article{2003-04099-00520021001,
Abstract = {Several studies on auditory word recognition indicate that word processing is influenced by phonological similarity with other words. We describe a lexical database, VoColex, which provides several statistical indexes of phonological similarity between French words. Phonological similarity is computed according to two distinct principles. According to the first principle, phonologically similar words share initial phonemes with the target word. According to the second principle, phonological neighbours correspond to any words which can be derived from the target by a single phoneme change (substitution, addition, or deletion) whatever the position of the modified phoneme. The statistical data provided by VoCoLex allow the control and the empirical manipulation of various measures of phonological similarity, as well as quantitative descriptions of the auditory lexicon. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Dufour, Sophie and  Peereman, Ronald and  Pallier, Christophe and  Radeau, Monique},
ISSN = {0003-5033, 1955-2580},
Journal = {L'Année Psychologique},
Keywords = {phonological similarity, lexical database, French words, auditory word recognition, word processing, phoneme change, auditory lexicon, Phonemes, Word Processing, Word Recognition, Words (Phonetic Units)},
Number = {4},
Pages = {725 - 746},
Title = {VoCoLex: Une base de données lexicales sur les similaritiés phonologiques entre les mots français = VoCoLex: A lexical database on phonological similarity between French words},
URL = {Sophie@leadserv.u-bourgogne.fr},
Volume = {102},
Year = {2002},
}


@article{1999-05841-02419990501,
Abstract = {During the last 20 years, psycholinguistic research has identified many variables that influence reading and spelling processes. In this article, the authors describe a new computerized lexical database, LEXOP, which provides quantitative descriptors about the relations between orthography and phonology for French monosyllabic words. Three main classes of variables are considered: consistency of print-to-sound and sound-to-print associations, frequency of orthography–phonology correspondences, and word neighborhood characteristics. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Peereman, Ronald and  Content, Alain},
ISSN = {0743-3808},
Journal = {Behavior Research Methods, Instruments & Computers},
Keywords = {computerized lexical database which provides quantitative descriptors about the relations between orthography & phonology for French monosyllabic words, Databases as Topic, Dictionaries as Topic, France, Language, Psycholinguistics, Statistics as Topic, Computer Applications, Databases, Human Computer Interaction, Orthography, Phonology, Computer Software, Words (Phonetic Units)},
Number = {2},
Pages = {376 - 379},
Title = {LEXOP: A lexical database providing orthography–phonology statistics for French monosyllabic words},
Volume = {31},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=psyh&AN=1999-05841-024&site=ehost-live&scope=site},
Year = {1999},
}


@article{2004-15015-01720040201,
Abstract = {This article presents MANULEX, a Web-accessible database that provides grade-level word frequency lists of nonlemmatized and lemmatized words (48,886 and 23,812 entries, respectively) computed from the 1.9 million words taken from 54 French elementary school readers. Word frequencies are provided for four levels: first grade (G1), second grade (G2), third to fifth grades (G3-5), and all grades (Gl-5). The frequencies were computed following the methods described by Carroll, Davies, and Richman (1971) and Zeno, Ivenz, Millard, and Duvvuri (1995), with four statistics at each level (F, overall word frequency; D, index of dispersion across the selected readers; U, estimated frequency per million words; and SFI, standard frequency index). The database also provides the number of letters in the word and syntactic category information. MANULEX is intended to be a useful tool for studying language development through the selection of stimuli based on precise frequency norms. Researchers in artificial intelligence can also use it as a source of information on natural language processing to simulate written language acquisition in children. Finally, it may serve an educational purpose by providing basic vocabulary lists. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Lété, Bernard and  Sprenger-Charolles, Liliane and  Colé, Pascale},
ISSN = {0743-3808},
Journal = {Behavior Research Methods, Instruments & Computers},
Keywords = {grade-level lexical database, elementary school readers, Web-accessible database, MANULEX, word frequency, nonlemmatized words, lemmatized words, Adolescent, Child, Databases, Factual, Humans, Information Dissemination, Information Storage and Retrieval, Internet, Language, Language Development, Psycholinguistics, Reading, Reference Values, Vocabulary, Databases, Elementary School Students, Lexical Access, Reading Ability, Word Frequency, Grade Level},
Number = {1},
Pages = {156 - 166},
Title = {MANULEX: A grade-level lexical database from French elementary school readers},
URL = {lete@inrp.fr},
Volume = {36},
Year = {2004},
}


@article{2000-14325-01120001101,
Abstract = {This article presents a computerized database of words for use in experimental research in cognitive psychology and psycholinguistics. The data are based on the oral vocabulary of 200 Spanish-speaking children aged from 11.16–49.16 mo. The database includes 15,428 Spanish words (tokens) and comprises 1,259 different words (types). It provides information about age of acquisition, orthography, grammar, semantics, and frequency. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Piñeiro, Armando and  Manzano, Mayra},
ISSN = {0743-3808},
Journal = {Behavior Research Methods, Instruments & Computers},
Keywords = {lexical database for Spanish-speaking children aged 1–4 yrs, Child, Child, Preschool, Cognitive Science, Cuba, Databases as Topic, Female, Humans, Infant, Language Development, Male, Psycholinguistics, Semantics, Databases, Verbal Stimuli, Words (Phonetic Units), Lexical Access},
Number = {4},
Pages = {616 - 628},
Title = {A lexical database for Spanish-speaking children},
Volume = {32},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=psyh&AN=2000-14325-011&site=ehost-live&scope=site},
Year = {2000},
}


@article{2017-36245-00120171201,
Abstract = {Plagiarism takes place when we use any person’s work without giving due acknowledgment. There are several fields where the text similarity is involved like web document retrieval, information mining, and searching related articles. Several approaches have been introduced for detecting plagiarism in the text documents based on the syntactic structure of the text, string similarity, fingerprinting, semantic meaning underlying the text, etc. The basic limitation of plagiarism detection systems these days is that they fail to detect tough cases of plagiarism. The proposed plagiarism detection approach is the hybrid of semantic and syntactic similarity between the text documents. This novel approach exploits linguistic information sources non-linearly using the lexical database for finding the relatedness between text documents. The proposed approach uses semantic knowledge to perform cognitive-inspired computing. The framework is capable of detecting intelligent plagiarism cases like a verbatim copy, paraphrasing, rewording in a sentence, and sentence transformation. The approach has been evaluated on the standard PAN-PC-11 dataset. The experiments show that our technique has outperformed other strong baseline techniques in terms of precision, recall, F-measure, and plagiarism detection (PlagDet) score. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
Author = {Sahi, Mansi and  Gupta, Vishal},
ISSN = {1866-9956, 1866-9964},
Journal = {Cognitive Computation},
Keywords = {Information sources, Semantic relatedness, Lexical database, Databases, Semantics, Websites},
Number = {6},
Pages = {852 - 867},
Title = {A novel technique for detecting plagiarism in documents exploiting information sources},
URL = {vishal_gupta100@yahoo.co.in},
Volume = {9},
Year = {2017},
}


@masterthesis{2016-31156-07220160101,
Abstract = {Our understanding of the mental lexicon, the way meaning is extracted from word forms, is almost entirely built on data from spoken languages. While there is much work demonstrating that in many ways the linguistic structure and psychological mechanisms for processing signed language and spoken language processing are the same, less is known about the signed language mental lexicon. In this dissertation, I examine the structure of the American Sign Language mental lexicon, and the ways meaning can be extracted from the manual/visual signal. In the third chapter of this dissertation I ask whether a single cognitive architecture might explain diverse behavioral patterns in signed and spoken language. Chen and Mirman (2012) presented a computational model of word processing that unified opposite effects of neighborhood density in speech production, perception, and written word recognition. Carreiras et al. (2008) demonstrate that neighborhood density effects in Spanish Sign Language (LSE) also vary depending on whether the neighbors share the same handshape or location. We present a spreading activation architecture that borrows the principles proposed by Chen and Mirman (2012), and show that if this architecture is elaborated to incorporate relatively minor facts about either 1) the time course of sign perception or 2) the frequency of sub-lexical units in sign languages, it produces data that match the experimental findings from sign languages. This work serves as a proof of concept that a single cognitive architecture could underlie both sign and word recognition. In the second chapter I present ASL-LEX, a lexical database for ASL that catalogues more than forty properties about almost 1,000 signs. The database includes, for example, information about each sign's iconicity, phonological make-up, and neighborhood density. I use this information to better understand the structure of the ASL lexicon, the distribution of each of these properties, and the relationships between these properties. This lexical database is the largest and most comprehensive database of ASL, and can be used by researchers to develop experiments and by educators to identify and support vocabulary development. In the fourth chapter, I use ASL-LEX to develop a tightly-controlled study of sign perception. I ask whether neighborhood density and sub-lexical frequency play a role in sign perception, and if the mechanisms of sign perception are affected by early language experience. Eighty deaf participants with varying early language backgrounds completed a lexical decision task. I find that neighborhood density inhibits sign perception in people with low early ASL exposure, but has no effect in people with high early ASL exposure. Location frequency inhibits sign perception in all people, but the effect is stronger in people with low early ASL exposure. This suggests that impoverished access to ASL early in life has lasting consequences for sign perception. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Caselli, Naomi K.},
Keywords = {language, sign language, lexicon, word forms, Language, Sign Language, Word Recognition, Mental Lexicon},
School = {ProQuest Information & Learning},
Title = {Language deprivation and the American sign language lexicon},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=psyh&AN=2016-31156-072&site=ehost-live&scope=site},
Year = {2016},
}


@article{2015-13542-00120150401,
Abstract = {The editorial discusses articles in this issue of Clinical Linguistics & Phonetics. The papers included in this issue cover the main areas of the speech and language sciences: phonetics, phonology, lexis, syntax and semantics. The work reported here underlines the importance of considering head stabilization in ultrasound studies. Lili Yeh, Bill Wells, Joy Stackhouse and Marcin Szczerbinski investigated phonological awareness in children acquiring Mandarin, especially of constituent parts of the syllable. However, as well as looking at the development of awareness, the authors were able to use their data to compare the merits of different models of the syllable in Mandarin. Marianne Lind and colleagues Hanna Simonsen, Pernille Hansen, Elisabeth Holm, and Bjørn- Helge Mevik provide a lexical database for clinicians and clinical linguists working with Norwegian. Jodi Tommerdahl and Cynthia Kilpatrick describe a study of child directed speech. Their particular area of interest is test–retest reliability, and they looked at frequency of morphosyntactic productions in 17 mothers talking to their children. Yalda Kazemi, Thomas Klee and Helen Stringer examined language sample measures for Persian-speaking children and their diagnostic accuracy in distinguishing language impaired from normally developing speakers. The final paper in this issue is by Seung-yun Yan and Diane Sidtis and explores hemispheric specialization for common and proper nouns. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Ball, Martin J. and  Müller, Nicole},
ISSN = {0269-9206, 1464-5076},
Journal = {Clinical Linguistics & Phonetics},
Keywords = {language sciences, diagnostic accuracy, lexical database, phonological awareness, Child, Congresses as Topic, Humans, Linguistics, Phonetics, Societies, Scientific, Language, Lexical Access, Phonological Awareness, Sciences},
Number = {4},
Pages = {247 - 248},
Title = {Editorial},
URL = {martin.j.ball@liu.se},
Volume = {29},
Year = {2015},
}


@article{2017-26094-00520170701,
Abstract = {Cognitive mechanisms for sign language lexical access are fairly unknown. This study investigated whether phonological similarity facilitates lexical retrieval in sign languages using measures from a new lexical database for American Sign Language. Additionally, it aimed to determine which similarity metric best fits the present data in order to inform theories of how phonological similarity is constructed within the lexicon and to aid in the operationalization of phonological similarity in sign language. Sign repetition latencies and accuracy were obtained when native signers were asked to reproduce a sign displayed on a computer screen. Results indicated that, as predicted, phonological similarity facilitated repetition latencies and accuracy as long as there were no strict constraints on the type of sublexical features that overlapped. The data converged to suggest that one similarity measure, MaxD, defined as the overlap of any 4 sublexical features, likely best represents mechanisms of phonological similarity in the mental lexicon. Together, these data suggest that lexical access in sign language is facilitated by phonologically similar lexical representations in memory and the optimal operationalization is defined as liberal constraints on overlap of 4 out of 5 sublexical features—similar to the majority of extant definitions in the literature. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
Author = {Williams, Joshua T. and  Stone, Adam and  Newman, Sharlene D.},
ISSN = {1081-4159, 1465-7325},
Journal = {Journal of Deaf Studies and Deaf Education},
Keywords = {sign language, phonological similarity, lexical access, Lexical Access, Sign Language, Phonological Awareness},
Number = {3},
Pages = {303 - 315},
Title = {Operationalization of sign language phonological similarity and its effects on lexical access},
URL = {willjota@indiana.edu, ORCID: 0000-0002-2026-309X},
Volume = {22},
Year = {2017},
}


@masterthesis{2013-99130-55520130101,
Abstract = {Learning vocabulary and understanding texts present difficulty for language learners due to, among other things, the high degree of lexical ambiguity. By developing an intelligent tutoring system, this dissertation examines whether automatically providing enriched sense-specific information is effective for vocabulary learning and reading comprehension of second language learners. The system developed in this study contributes to an extended understanding of how NLP techniques can be applied more effectively in an educational environment. The system allows learners to upload texts and click on any content word in order to obtain sense-appropriate lexical information for unfamiliar or unknown words during reading. The system consists of three components: (1) the system manager controls the interaction among each learner, the NLP server, and the lexical database; (2) the NLP server converts a raw input text to a linguistically-analyzed text; (3) the lexical database is used to provide a sense-appropriate definition and example sentences of a word to the learner. To obtain the sense-appropriate information, the system first performs word sense disambiguation (WSD) on the input text. Pointing to appropriate examples tuned for language learners, however, is complicated by the fact that the database of examples is from one repository (COBUILD), while automatic WSD systems generally rely on senses from another (WordNet). The lexical database, then, is indexed by WordNet senses, each of which points to an appropriate corresponding COBUILD sense. The fact that every sense inventory has its own standards of sense distinction poses a serious problem in integrating these inventories into one. To redirect an input WordNet sense to a corresponding COBUILD sense, thus, a word sense alignment algorithm was developed, following a heuristic of favoring flatter alignment structures. With this system, an empirical study was conducted with 60 intermediate learners of English as a second language to examine whether this system can lead learners to improve their vocabulary acquisition and reading comprehension. The findings show that learners demonstrated higher performance when receiving sense-specific information. Furthermore, the qualitative examination of the effect of automatic system errors show that, although learners showed learning regardless of the appropriateness of lexical information, they still showed relatively greater learning when given appropriate lexical information. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Eom, Soojeong},
Keywords = {automatic presentation, lexical information, intelligent learning system, multimedia, vocabulary, Foreign Language Learning, Intelligent Tutoring Systems, Vocabulary, Multimedia, Neurolinguistic Programming},
School = {ProQuest Information & Learning},
Title = {Automatic presentation of sense-specific lexical information in an intelligent learning system},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=psyh&AN=2013-99130-555&site=ehost-live&scope=site},
Year = {2013},
}


@article{2017-18797-00620170401,
Abstract = {This article analyses how a set of psycholinguistic factors may account for children’s lexical development. Age of acquisition is compared to a measure of lexical development based on vocabulary size rather than age, and robust regression models are used to assess the individual and joint effects of word class, frequency, imageability and phonological neighbourhood density on Norwegian children’s early lexical development. The Norwegian Communicative Development Inventories (CDI) norms were used to calculate each CDI word’s age of acquisition and vocabulary size of acquisition. Lexical properties were downloaded from the lexical database Norwegian Words, supplemented with data on frequency in adult and child-directed speech. Age of acquisition correlated highly with vocabulary size of acquisition, but the new measure was more evenly distributed and more sensitive to lexical effects. Frequency in child-directed speech was the most important predictor of lexical development, followed by imageability, which seems to account for the dominance of nominals over predicates in Norwegian. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
Author = {Hansen, Pernille},
ISSN = {0142-7237, 1740-2344},
Journal = {First Language},
Keywords = {Acquisition, age of acquisition, CDI, frequency, imageability, lexical development, phonological neighbourhood density, psycholinguistics, vocabulary size, word class, Form Classes (Language), Language Development, Lexical Access, Phonological Awareness, Words (Phonetic Units), Childhood Development, Psycholinguistics},
Number = {2},
Pages = {205 - 225},
Title = {What makes a word easy to acquire? The effects of word class, frequency, imageability and phonological neighbourhood density on lexical development},
URL = {pernille.hansen@iln.uio.no},
Volume = {37},
Year = {2017},
}


@article{2018-20002-00620170101,
Abstract = {The CELEX lexical database (Baayen, Piepenbrock & van Rijn 1995) was developed in the 1990s, providing a database of the syntactic, morphological, phonological and orthographic forms of between 50,000 and 125,000 words of Dutch, English and German. This database was used as the basis for the development of the PolyLex lexicons, which included syntactic, morphological and phonological information for around 3,000 words of Dutch, English and German. Orthographic information was subsequently added in the PolyOrth project. The PolyOrth project was based on the assumption that the underlying, lexical phonological forms could be used to derive the surface orthographic forms by means of a combination of phoneme-grapheme mappings and sets of autonomous spelling rules for each language. One of the complications encountered during the project was the fact that the phonological forms in CELEX were not always genuinely underlying forms which made deriving the orthographic forms tricky. This paper discusses the nature and status of underlying phonological forms, their relation to orthography and the issues of finding this information in databases. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
Author = {Cahill, Lynne},
ISSN = {1387-6732, 1570-6001},
Journal = {Written Language and Literacy},
Keywords = {phoneme-grapheme mappings, lexical databases, lexicons, underlying phonology, lexical phonology, post-lexical phonology, Databases, Handwriting, Phonemes, Phonology, Linguistics, Lexical Access},
Number = {1},
Pages = {104 - 127},
Title = {What are the 'phonemes' in phoneme-grapheme mappings? A perspective on the use of databases for lexicon development},
URL = {L.J.Cahill@sussex.ac.uk},
Volume = {20},
Year = {2017},
}


@inbook{2016-61176-01620170101,
Abstract = {WordNet, a large lexical database of English, was conceived as a model of human semantic organization. Evidence from timing experiments, association norms, and distributional properties of words supported a semantic network model in which words are interlinked via a small number of lexical and conceptual relations. Its large coverage and unique structure, which allows automatic systems to detect and quantify semantic relatedness among words, soon made WordNet an invaluable tool for natural language processing tasks. Information retrieval, document summarization, and machine translation crucially require word sense discrimination and disambiguation. Wordnets have been built in dozens of languages and for specific technical sublanguages, and the number of applications in research, language technology and pedagogy has grown. Although WordNet’s central focus has shifted from its psycholinguistic origins, its design, based on theories about the structure of the human mental lexicon, is validated as a sound approach to representing the meanings of words. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
Author = {Fellbaum, Christiane and  Chipman, Susan E. F.},
Booktitle = {The Oxford handbook of cognitive science.},
ISBN = {978-0-19-984219-3, 978-0-19-984417-3},
Pages = {301 - 313},
Publisher = {Oxford University Press},
Title = {WordNet: An electronic lexical resource},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=psyh&AN=2016-61176-016&site=ehost-live&scope=site},
Year = {2017},
}


@article{2015-58841-00120160301,
Abstract = {What determines consonant doubling in English? This question is pursued by using a large lexical database to establish systematic correlations between spelling, phonology and morphology. The main insights are: Consonant doubling is most regular at morpheme boundaries. It can be described in graphemic terms alone, i.e. without reference to phonology. In monomorphemic words, consonant doubling depends mostly on the word ending. Certain endings correlate with double consonants (e.g. <er> as in <summer>), while others correlate with single consonants (e.g. <it> as in <visit>). What is more, it is the graphemic form of the word ending that determines the presence or absence of double consonants: The word endings <-ic> and <-ick>, for example, are homophonous, but the former almost always occurs with single consonants (e.g. <panic>), the latter with double consonants (e.g. <derrick>). That makes graphemic word endings peculiar entities: Like suffixes, they are recurring and they have distributional properties—but unlike suffixes, they have no morphosyntactic or semantic function. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
Author = {Berg, Kristian},
ISSN = {0922-4777, 1573-0905},
Journal = {Reading and Writing: An Interdisciplinary Journal},
Keywords = {Double consonants, English, Word ending, Inflection, Derivation, Consonants, Morphology (Language), Phonology, Spelling},
Number = {3},
Pages = {453 - 474},
Title = {Double consonants in English: Graphemic, morphological, prosodic and etymological determinants},
URL = {kristian.berg@uni-oldenburg.de, ORCID: 0000-0003-4059-3106},
Volume = {29},
Year = {2016},
}


@article{2015-11703-00520150401,
Abstract = {We examined the potential advantage of the lexical databases using subtitles and present SUBTLEX-PT, a new lexical database for 132,710 Portuguese words obtained from a 78 million corpus based on film and television series subtitles, offering word frequency and contextual diversity measures. Additionally we validated SUBTLEX-PT with a lexical decision study involving 1920 Portuguese words (and 1920 nonwords) with different lengths in letters (M = 6.89, SD = 2.10) and syllables (M = 2.99, SD = 0.94). Multiple regression analyses on latency and accuracy data were conducted to compare the proportion of variance explained by the Portuguese subtitle word frequency measures with that accounted by the recent written-word frequency database (Procura-PALavras; P-PAL; Soares, Iriarte, et al., 2014). As its international counterparts, SUBTLEX-PT explains approximately 15% more of the variance in the lexical decision performance of young adults than the P-PAL database. Moreover, in line with recent studies, contextual diversity accounted for approximately 2% more of the variance in participants' reading performance than the raw frequency counts obtained from subtitles. SUBTLEX-PT is freely available for research purposes (at http://p-pal.di.uminho.pt/about/databases). (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
Author = {Soares, Ana Paula and  Machado, João and  Costa, Ana and  Iriarte, Álvaro and  Simões, Alberto and  de Almeida, José João and  Comesaña, Montserrat and  Perea, Manuel},
ISSN = {1747-0218, 1747-0226},
Journal = {The Quarterly Journal of Experimental Psychology},
Keywords = {Word frequency, Contextual diversity, Subtitles, Portuguese, Adolescent, Decision Making, Female, Humans, Male, Motion Perception, Photic Stimulation, Portugal, Psychomotor Performance, Reaction Time, Reading, Regression Analysis, Semantics, Vocabulary, Young Adult, Assistive Technology, Films, Lexical Decision, Text Structure, Television, Word Frequency},
Number = {4},
Pages = {680 - 696},
Title = {On the advantages of word frequency and contextual diversity measures extracted from subtitles: The case of Portuguese},
URL = {asoares@psi.uminho.pt, ORCID: 0000-0002-4047-3799, ORCID: 0000-0001-6961-2660, ORCID: 0000-0003-2547-7684},
Volume = {68},
Year = {2015},
}


@article{2015-09766-00420150301,
Abstract = {This paper predicts the mutual intelligibility of 15 Chinese dialects from multiple objective distance measures. Empirical mutual intelligibility measures were obtained from functional intelligibility tests at the sentence level from 15 listeners for each of 15 Chinese dialects. We computed various proximity measures on the basis of shared phonemes and tones in the sound inventories of the 15 dialects. Next, Levenshtein (string-edit) distance measures were computed on the 764 common syllabic units (zi in Pinyin, i.e., a meaningful character or morpheme with a complete transcription of segments and tone) shared by the same 15 Chinese dialects in the Dialect Sound Database of Modern Chinese (compiled by the Chinese Academy of Social Sciences). Unweighted and perceptually weighted Levenshtein distance measures were computed. We also included objective similarity measures of phonological correspondence, based on the Zihui character list and of lexical affinity, based on the Cihui cross-dialect lexical database with all cognate and non-cognate expressions of 905 core concepts) that have been published by Cheng (1997). The best single predictor of mutual intelligibility between a pair of dialects was the percentage of cognates shared between them (r² = .548). Including all predictors afforded a highly accurate prediction of mutual intelligibility (R² = .874). A very reasonable prediction is afforded if we just add the lexical frequency of finals (syllable rhymes) shared by a pair of dialects (R² = .611). (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Tang, Chaoju and  van Heuven, Vincent J.},
ISSN = {0024-3949, 1613-396X},
Journal = {Linguistics},
Keywords = {mutual intelligibility, opinion test, functional test, word recognition, Chinese dialects, Levenshtein distance, tonal distance, lexical affinity, phonological affinity, Dialect, Linguistics, Phonology, Speech Perception, Word Recognition},
Number = {2},
Pages = {285 - 312},
Title = {Predicting mutual intelligibility of Chinese dialects from multiple objective linguistic distance measures},
URL = {1006946669@qq.com, v.j.j.p.van.heuven@hum.leidenuniv.nl},
Volume = {53},
Year = {2015},
}


@article{2009-00421-00820090101,
Abstract = {We report work on adding semantic role labels to the Chinese Treebank, a corpus already annotated with phrase structures. The work involves locating all verbs and their nominalizations in the corpus, and semi-automatically adding semantic role labels to their arguments, which are constituents in a parse tree. Although the same procedure is followed, different issues arise in the annotation of verbs and nominalized predicates. For verbs, identifying their arguments is generally straightforward given their syntactic structure in the Chinese Treebank as they tend to occupy well-defined syntactic positions. Our discussion focuses on the syntactic variations in the realization of the arguments as well as our approach to annotating dislocated and discontinuous arguments. In comparison, identifying the arguments for nominalized predicates is more challenging and we discuss criteria and procedures for distinguishing arguments from non-arguments. In particular we focus on the role of support verbs as well as the relevance of event/result distinctions in the annotation of the predicate-argument structure of nominalized predicates. We also present our approach to taking advantage of the syntactic structure in the Chinese Treebank to bootstrap the predicate-argument structure annotation of verbs. Finally, we discuss the creation of a lexical database of frame files and its role in guiding predicate argument annotation. Procedures for ensuring annotation consistency and inter-annotator agreement evaluation results are also presented. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Xue, Nianwen and  Martha, Palmer},
ISSN = {1351-3249, 1469-8110},
Journal = {Natural Language Engineering},
Keywords = {Chinese Treebank, semantic roles, phrase structures, verbs, lexical database, Databases, Lexical Access, Phrases, Semantics, Verbs},
Number = {1},
Pages = {143 - 172},
Title = {Adding semantic roles to the Chinese Treebank},
URL = {Nianwen.Xue@Colorado.EDU},
Volume = {15},
Year = {2009},
}


@article{2008-15532-00520080101,
Abstract = {Considering the popularity of the Internet, an automatic interactive feedback system for E-learning websites is becoming increasingly desirable. However, computers still have problems understanding natural languages, especially the Chinese language, firstly because the Chinese language has no space to segment lexical entries (its segmentation method is more difficult than that of English) and secondly because of the lack of a complete grammar in the Chinese language, making parsing more difficult and complicated. Building an automated Chinese feedback system for special application domains could solve these problems. This paper proposes an interactive feedback mechanism in a virtual campus that can parse, understand and respond to Chinese sentences. This mechanism utilizes a specific lexical database according to the particular application. In this way, a virtual campus website can implement a special application domain that chooses the proper response in a user friendly, accurate and timely manner. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Chen, Jui-Fa and  Lin, Wei-Chuan and  Jian, Chih-Yu and  Hung, Ching-Chung},
ISSN = {1539-3100, 1539-3119},
Journal = {International Journal of Distance Education Technologies},
Keywords = {chinese interactive feedback system, e-learning websites, virtual campus, specific lexical database, user friendly, Campuses, Databases, Systems Design, Virtual Reality, Websites, Computers, Feedback, Internet},
Number = {4},
Pages = {62 - 90},
Title = {A Chinese interactive feedback system for a virtual campus},
Volume = {6},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=psyh&AN=2008-15532-005&site=ehost-live&scope=site},
Year = {2008},
}


@article{2014-30503-02020140901,
Abstract = {In this article, we present the first open-access lexical database that provides phonological representations for 120,000 Italian word forms. Each of these also includes syllable boundaries and stress markings and a comprehensive range of lexical statistics. Using data derived from this lexicon, we have also generated a set of derived databases and provided estimates of positional frequency use for Italian phonemes, syllables, syllable onsets and codas, and character and phoneme bigrams. These databases are freely available from phonitalia.org. This article describes the methods, content, and summarizing statistics for these databases. In a first application of this database, we also demonstrate how the distribution of phonological substitution errors made by Italian aphasic patients is related to phoneme frequency. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Goslin, Jeremy and  Galluzzi, Claudia and  Romani, Cristina},
ISSN = {1554-351X, 1554-3528},
Journal = {Behavior Research Methods},
Keywords = {Phonological lexicon, Lexical statistics, Aphasic errors, Databases, Phonology, Statistics, Vocabulary, Words (Phonetic Units)},
Number = {3},
Pages = {872 - 886},
Title = {PhonItalia: A phonological lexicon for Italian},
URL = {jeremy.goslin@plymouth.ac.uk},
Volume = {46},
Year = {2014},
}


@article{2014-08495-00920140401,
Abstract = {Cross-language comparisons can provide important constraints on our understanding of how people read aloud. French is an interesting case because it differs from most other writing systems in that it uses a large number of multi-letter vowel graphemes and consonants that are systematically silent (i.e., do not map to any lexical phonology; e.g., trop ). Here, we developed a French version of the Connectionist Dual Process Model of Reading Aloud (CDP++) that can handle multisyllabic stimuli (up to three syllables) and has a large-scale lexicon of more than 100,000 words. We tested the model on extant data and an additional experiment examining the reading aloud of nonwords with potentially silent letters. The results from the extant data showed that the model was able to capture a number of important psycholinguistic effects in the literature and explained between 52% and 67% of the item-specific variance in two large databases. The results of the silent-letter experiment showed that, contrary to what would be predicted on the basis of lexical database statistics, people generally pronounce 'silent' consonants in nonwords. We show that the French CDP++ model faithfully predicted this effect because it implements a linear mapping between orthography and phonology. These findings highlight the theoretical and practical significance of using computational models to help determine the processes and representations that underlie skilled reading. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
Author = {Perry, Conrad and  Ziegler, Johannes C. and  Zorzi, Marco},
ISSN = {0749-596X, 1096-0821},
Journal = {Journal of Memory and Language},
Keywords = {reading aloud, dual process models, lexical phonology, letters, psycholinguistic effects, Lexical Access, Oral Reading, Phonology, Dual Process Models, Letters (Alphabet), Psycholinguistics},
Pages = {98 - 115},
Title = {When silent letters say more than a thousand words: An implementation and evaluation of CDP++ in French},
URL = {ConradPerry@gmail.com, ORCID: 0000-0002-4651-6390},
Volume = {72},
Year = {2014},
}


@article{2014-44358-00220140101,
Abstract = {The architecture of writing systems metaphor has special relevance for understanding the structural nature of the Japanese writing system, and, more specifically, for appreciating how the 2,136 kanji of the jō-yō-kanji- hyō/ ‘List of characters for general use’ function as the core building blocks in the orthographic representation of a considerable proportion of the Japanese lexicon. In seeking to illuminate the multiple layers of internal structure within Japanese kanji, the Japanese lexicon, and the Japanese writing system, the paper draws on insights and observations gained from an ongoing project to construct a large-scale Japanese lexical database system. Reflecting structural distinctions within the database, the paper consists of three main sections addressing the different structural levels of kanji components, jōyō kanji, and the lexicon. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Joyce, Terry and  Masuda, Hisashi and  Ogawa, Taeko},
ISSN = {1387-6732, 1570-6001},
Journal = {Written Language and Literacy},
Keywords = {Japanese writing system, building blocks, jōyō kanji, components, orthographic structure, database, Metaphor, Orthography, Writing Skills, Written Language, Databases},
Number = {2},
Pages = {173 - 194},
Title = {Jōyō kanji as core building blocks of the Japanese writing system: Some observations from database construction},
URL = {terry@tama.ac.jp},
Volume = {17},
Year = {2014},
}


@article{2018-60750-00120181112,
Abstract = {Presents a study which aims to investigate SPALEX, a Spanish lexical decision database by focusing on native Spanish speakers at a global scale and with a vast amount of words, to provide a useful tool for researchers exploring the acquisition and processing of this language in native and foreign contexts. SPALEX contains data from a Spanish crowd-sourced lexical decision mega study. The authors collected the data through an online platform from May 12th, 2014 to December 19th, 2017. The majority of the data was acquired during the first month of the experiment, when an advertising campaign was done in order to attract the public’s attention. Participants also had the option of publishing their results via social networks, which led to attract more participants in a snow-ball sampling fashion. Additionally, the database contains information on participants that voluntarily provided information about their gender, age, country of origin, education level, handedness, native language, and best foreign language. In each experimental session, participants responded to 70 words and 30 non-words presented randomly and without repetition. Accuracy in SPALEX is expressed as 1 for correct answers and 0 for incorrect answers. Based on participants’ responses, the authors calculated percentage known, a measure of the percentage of participants that know a particular word. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
Author = {Aguasvivas, Jose Armando and  Carreiras, Manuel and  Brysbaert, Marc and  Mandera, Paweł and  Keuleers, Emmanuel and  Duñabeitia, Jon Andoni},
ISSN = {1664-1078},
Journal = {Frontiers in Psychology},
Keywords = {megastudies, lexical decision, vocabulary knowledge, online assessments, lexical database, Data Collection, Databases, Linguistics, Lexical Decision, Vocabulary},
Title = {SPALEX: A Spanish lexical decision database from a massive online data collection},
URL = {jdunabeitia@nebrija.es},
Volume = {9},
Year = {2018},
}


@article{2012-32188-01120130101,
Abstract = {The substantial increase of social networks and their combination with mobile devices make rigorous analysis of the outcomes of such system of paramount importance for intelligence gathering and decision making purposes. Since the introduction of Twitter system in 2006, tweeting emerged as an efficient open social network that attracted interest from various research/commercial and military communities. This paper investigates the current software architecture of Twitter system and put forward a new architecture dedicated for semantic and spatial analysis of Twitter data. Especially, Twitter Streaming API was used as a basis for tweet collection data stored in MySQL like database. While Lucene system together with WordNet lexical database linked to advanced natural language processing and PostGIS platform were used to ensure semantic and spatial analysis of the collected data. A functional diversity approach was implemented to enforce fault tolerance for the data collection part where its performances were evaluated through comparison with alternative approaches. The proposal enables the discovery of spatial patterns within geo-located Twitter and can provide the user or operator with useful unforeseen elements. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
Author = {Oussalah, M. and  Bhat, F. and  Challis, K. and  Schnier, T.},
ISSN = {0950-7051, 1872-7409},
Journal = {Knowledge-Based Systems},
Keywords = {software architecture, Twitter collections, geolocation services, human machine systems, databases, decision making, Computer Software, Databases, Decision Making, Human Factors Engineering, Online Social Networks, Architecture},
Pages = {105 - 120},
Title = {A software architecture for Twitter collection, search and geolocation services},
URL = {M.Oussalah@bham.ac.uk},
Volume = {37},
Year = {2013},
}


@article{1997-42754-01119971201,
Abstract = {A lexical modeling methodology was employed to examine how the distribution of phonemic patterns in the lexicon constrains lexical equivalence under conditions of reduced phonetic distinctiveness experienced by speechreaders. The technique involved selection of a phonemically transcribed machine-readable lexical database; definition of transcription rules based on measures of phonetic similarity; application of the transcription rules to a lexical database and formation of lexical equivalence classes; and computation of 3 metrics to examine the transcribed lexicon. The metric percent words unique demonstrated that distribution of words in the language preserves lexical uniqueness across a wide range in the number of potentially available phonemic distinctions. Expected class size demonstrated that if at least 12 phonemic equivalence classes were available, any given word would be highly similar to only a few other words. Percent information extracted provided evidence that high-frequency words tend not to reside in the same lexical equivalence classes as other high-frequency words. The steepness of the functions obtained for each metric shows that small increments in the number of visually perceptible phonemic distinctions can result in substantial changes in lexical uniqueness. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Auer, Edward T. Jr. and  Bernstein, Lynne E.},
ISSN = {0001-4966},
Journal = {Journal of the Acoustical Society of America},
Keywords = {reduced phonetic distinctiveness, phonemic patterns in lexicon & lexical uniqueness using computational model, speechreaders, Automatic Data Processing, Humans, Lipreading, Models, Theoretical, Phonetics, Speech Perception, Vocabulary, Lipreading, Mathematical Modeling, Phonemes, Speech Perception, Words (Phonetic Units)},
Number = {6},
Pages = {3704 - 3710},
Title = {Speechreading and the structure of the lexicon: Computationally modeling the effects of reduced phonetic distinctiveness on lexical uniqueness},
Volume = {102},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=psyh&AN=1997-42754-011&site=ehost-live&scope=site},
Year = {1997},
}


@article{2012-17396-00120120701,
Abstract = {The objective of our study is to introduce a fully automated, computational linguistic technique to quantify semantic relations between words generated on a standard semantic verbal fluency test and to determine its cognitive and clinical correlates. Cognitive differences between patients with Alzheimer’s disease and mild cognitive impairment are evident in their performance on the semantic verbal fluency test. In addition to the semantic verbal fluency test score, several other performance characteristics sensitive to disease status and predictive of future cognitive decline have been defined in terms of words generated from semantically related categories (clustering) and shifting between categories (switching). However, the traditional assessment of clustering and switching has been performed manually in a qualitative fashion resulting in subjective scoring with limited reproducibility and scalability. Our approach uses word definitions and hierarchical relations between the words in WordNet®, a large electronic lexical database, to quantify the degree of semantic similarity and relatedness between words. We investigated the novel semantic fluency indices of mean cumulative similarity and relatedness between all pairs of words regardless of their order, and mean sequential similarity and relatedness between pairs of adjacent words in a sample of patients with clinically diagnosed probable (n = 55) or possible (n = 27) Alzheimer’s disease or mild cognitive impairment (n = 31). The semantic fluency indices differed significantly between the diagnostic groups, and were strongly associated with neuropsychological tests of executive function, as well as the rate of global cognitive decline. Our results suggest that word meanings and relations between words shared across individuals and computationally modeled via WordNet and large text corpora provide the necessary context to account for the variability in language-based behavior and relate it to cognitive dysfunction observed in mild cognitive impairment and Alzheimer’s disease. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
Author = {Pakhomov, Serguei V. S. and  Hemmy, Laura S. and  Lim, Kelvin O.},
ISSN = {0028-3932, 1873-3514},
Journal = {Neuropsychologia},
Keywords = {semantic indices, cognitive function, verbal fluency, linguistics, cognitive impairment, Aged, Alzheimer Disease, Analysis of Variance, Cognition, Cognition Disorders, Disease Progression, Executive Function, Female, Humans, Male, Mild Cognitive Impairment, Neuropsychological Tests, Psycholinguistics, Psychomotor Performance, Semantics, Verbal Behavior, Alzheimer's Disease, Cognitive Ability, Semantics, Verbal Fluency, Cognitive Impairment, Linguistics},
Number = {9},
Pages = {2165 - 2175},
Title = {Automated semantic indices related to cognitive function and rate of cognitive decline},
URL = {pakh0002@umn.edu},
Volume = {50},
Year = {2012},
}


@article{2008-16607-00620081001,
Abstract = {The semantic annotation of texts with senses from a computational lexicon is a complex and often subjective task. As a matter of fact, the fine granularity of the WordNet sense inventory [Fellbaum, Christiane (ed.). 1998. WordNet: An Electronic Lexical Database MIT Press], a de facto standard within the research community, is one of the main causes of a low inter-tagger agreement ranging between 70% and 80% and the disappointing performance of automated fine-grained disambiguation systems (around 65% state of the art in the Senseval-3 English all-words task). In order to improve the performance of both manual and automated sense taggers, either we change the sense inventory (e.g. adopting a new dictionary or clustering WordNet senses) or we aim at resolving the disagreements between annotators by dealing with the fineness of sense distinctions. The former approach is not viable in the short term, as wide-coverage resources are not publicly available and no large-scale reliable clustering of WordNet senses has been released to date. The latter approach requires the ability to distinguish between subtle or misleading sense distinctions. In this paper, we propose the use of structural semantic interconnections—a specific kind of lexical chains—for the adjudication of disagreed sense assignments to words in context. The approach relies on the exploitation of the lexicon structure as a support to smooth possible divergencies between sense annotators and foster coherent choices. We perform a twofold experimental evaluation of the approach applied to manual annotations from the SemCor corpus, and automatic annotations from the Senseval-3 English all-words competition. Both sets of experiments and results are entirely novel: structural adjudication allows to improve the state-of-the-art performance in all-words disambiguation by 3.3 points (achieving a 68.5% Fl-score) and attains figures around 80% precision and 60% recall in the adjudication of disagreements from human annotators. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Navigli, Roberto},
ISSN = {1351-3249, 1469-8110},
Journal = {Natural Language Engineering},
Keywords = {structural approach, automatic adjudication, word sense disagreements, semantic annotation, text structure, computational lexicon, Connotations, Lexical Access, Semantics, Text Structure, Word Meaning, Adjudication},
Number = {4},
Pages = {547 - 573},
Title = {A structural approach to the automatic adjudication of word sense disagreements},
URL = {navigli@di.uniroma1.it},
Volume = {14},
Year = {2008},
}


@article{2008-04225-00720080301,
Abstract = {Semantic intrusions are inappropriate responses frequently observed in patients with Alzheimer's disease. They belong to the same category as the words to be remembered, but their prototypic value remains largely unexplored. The prototype is the most representative word in a particular lexical category. The prototypic value is measured according to different criteria: written and oral lexical frequency, frequency of use, degree of typicality, degree of familiarity and rank of quotation. The objective of the study was to evaluate the prototypic value of intrusions produced by 17 Alzheimer's patients with mild to severe dementia, during the cued recall of the Grober & Buschke procedure (RL/RI 16 items). The prototypic value was compared to the categorial norms provided by 1) 17 control subjects and 2) the lexical database 'Lexique 3'. The results show that intrusions had a significantly higher prototypic value than targeted items. The prototypic value increased with the progression of the disease, and according to the evaluation criteria used. Thus with the criteria 'frequency of use', 'degree of typicality' and 'degree of familiarity,' the prototypic value increased exponentially with the severity of dementia. In contrast, in spite of the development of the pathology, the prototypic value decreased when assessed by the criteria of 'rank of quotation', and 'lexical frequency' (oral and written). In conclusion, the qualitative analysis of the prototypic value of intrusion errors in Alzheimers opens up new clinical and methodological considerations. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Canolle, Marianne and  Messaoudi, Mouloud and  Ayoub, Bronia and  Descours, Irène and  Bocquet, Patrick and  Gely-Nargeot, Marie-Christine and  Touchon, Jacques},
ISSN = {1760-1703},
Journal = {Psychologie & NeuroPsychiatrie du Vieillissement},
Keywords = {prototypic value, semantic intrusion errors, Alzheimers disease, dementia, cued recall, Aged, Aged, 80 and over, Alzheimer Disease, Databases, Factual, Female, Humans, Male, Middle Aged, Semantics, Speech, Alzheimer's Disease, Dementia, Errors, Semantics, Cued Recall},
Number = {1},
Pages = {67 - 79},
Title = {Valeur prototypique des intrusions sémantiques dans la maladie d'Alzheimer = Prototypic value of semantic intrusion errors in Alzheimer's disease},
URL = {marianne.canolle@ccl.aphp.fr},
Volume = {6},
Year = {2008},
}


@article{2007-14785-03320070801,
Abstract = {Many recent studies have demonstrated the influence of sublexical frequency measures on language processing, or called for controlling sublexical measures when selecting stimulus material for psycholinguistic studies (Aichert & Ziegler, 2005). The present study discusses which measures should be controlled for in what kind of study, and presents orthographic and phonological syllable, dual unit (bigram and biphoneme) and single unit (letter and phoneme) type and token frequency measures derived from the lemma and word form corpora of the CELEX lexical database (Baayen, Piepenbrock, & Gulikers, 1995). Additionally, we present the SUBLEX software as an adaptive tool for calculating sublexical frequency measures and discuss possible future applications. The measures and the software can be downloaded at www.psychonomic.org. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Hofmann, Markus J. and  Stenneken, Prisca and  Conrad, Markus and  Jacobs, Arthur M.},
ISSN = {1554-351X, 1554-3528},
Journal = {Behavior Research Methods},
Keywords = {sublixal frequency, orthographic units, phonological units, language processing, psycholinguistics, SUBLEX software, Humans, Linguistics, Periodicity, Phonetics, Vocabulary, Orthography, Phonology, Psycholinguistics, Syllables},
Number = {3},
Pages = {620 - 629},
Title = {Sublixal frequency measures for orthographic and phonological units in German},
URL = {mhof@zedat.fu-berlin.de, ORCID: 0000-0003-1335-437X},
Volume = {39},
Year = {2007},
}


@article{2008-08100-00120070701,
Abstract = {Collaborative and content-based filtering are the recommendation techniques most widely adopted to date. Traditional collaborative approaches compute a similarity value between the current user and each other user by taking into account their rating style, that is the set of ratings given on the same items. Based on the ratings of the most similar users, commonly referred to as neighbors, collaborative algorithms compute recommendations for the current user. The problem with this approach is that the similarity value is only computable if users have common rated items. The main contribution of this work is a possible solution to overcome this limitation. We propose a new content-collaborative hybrid recommender which computes similarities between users relying on their content-based profiles, in which user preferences are stored, instead of comparing their rating styles. In more detail, user profiles are clustered to discover current user neighbors. Content-based user profiles play a key role in the proposed hybrid recommender. Traditional keyword-based approaches to user profiling are unable to capture the semantics of user interests. A distinctive feature of our work is the integration of linguistic knowledge in the process of learning semantic user profiles representing user interests in a more effective way, compared to classical keyword-based profiles, due to a sense-based indexing. Semantic profiles are obtained by integrating machine learning algorithms for text categorization, namely a naïve Bayes approach and a relevance feedback method, with a word sense disambiguation strategy based exclusively on the lexical knowledge stored in the WordNet lexical database. Experiments carried out on a content-based extension of the EachMovie dataset show an improvement of the accuracy of sense-based profiles with respect to keyword-based ones, when coping with the task of classifying movies as interesting (or not) for the current user. An experimental session has been also performed in order to evaluate the proposed hybrid recommender system. The results highlight the improvement in the predictive accuracy of collaborative recommendations obtained by selecting like-minded users according to user profiles. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Degemmis, Marco and  Lops, Pasquale and  Semeraro, Giovanni},
ISSN = {0924-1868, 1573-1391},
Journal = {User Modeling and User-Adapted Interaction},
Keywords = {content-collaborative recommender, WordNet-based user profiles, neighborhood formation, content-based filtering, Algorithms, Collaboration, Linguistics, Machine Learning, Semantics},
Number = {3},
Pages = {217 - 255},
Title = {A content-collaborative recommender that exploits WordNet-based user profiles for neighborhood formation},
URL = {degemmis@di.uniba.it, lops@di.uniba.it, semeraro@di.uniba.it, ORCID: 0000-0001-6883-1853},
Volume = {17},
Year = {2007},
}


@article{2006-12184-00220060901,
Abstract = {The complexity of Chinese orthography has hindered the progress of research in Chinese to the same level of sophistication of that in alphabetic languages such as English. Also, there has been no publicly available resource concerning the decomposition of Chinese characters, which is essential in any attempt to model the cognitive processes of Chinese character recognition. Here we report our construction and analysis of a Chinese lexical database containing the most frequent phonetic compounds decomposed into semantic and phonetic radicals according to Chinese etymology. Each radical was further decomposed into basic stroke patterns according to a Chinese transcription system, Cangjie (Chu, 1979 Laboratory of chu Bong-Foo Retrieved August 25, 2004, from http://www.cbflabs.com/). Other information such as pronunciation and character frequency were also incorporated. We examine the distribution of different types of character, the information skew in phonetic compounds, the relations between subcharacter orthographic units and the pronunciation of the entire character, and the processing implications of these phenomena in terms of universal psycholinguistic principles. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Hsiao, Janet Hui-wen and  Shillcock, Richard},
ISSN = {0090-6905, 1573-6555},
Journal = {Journal of Psycholinguistic Research},
Keywords = {Chinese orthography, Chinese character recognition, lexical-semantic etymology, phonetic compounds, pronunciation, language processing, psycholinguistics, Databases as Topic, Discrimination Learning, Handwriting, Humans, Language, Orientation, Pattern Recognition, Visual, Phonetics, Psycholinguistics, Reading, Semantics, Writing, Chinese Cultural Groups, Letters (Alphabet), Written Language, Cognitive Processes, Etymology, Morphology (Language), Orthography, Phonology, Pronunciation, Psycholinguistics, Semantics},
Number = {5},
Pages = {405 - 426},
Title = {Analysis of a Chinese Phonetic Compound Database: Implications for Orthographic Processing},
URL = {jhsiao@cs.ucsd.edu},
Volume = {35},
Year = {2006},
}


@article{2006-11337-00320060101,
Abstract = {The Dutch spelling system, like other European spelling systems, represents a certain balance between preserving the spelling of morphemes (the morphological principle) and obeying letter-to-sound regularities (the phonological principle). We present experimental results with artificial learners that show a competition effect between the two principles: adhering more to one principle leads to more violations of the other. The artificial learners, memory-based learning algorithms, are trained (1) to convert written words to their phonemic counterparts and (2) to analyze written words on their morphological composition, based on data extracted from the CELEX lexical database. As an exception to the competition effect we show that introducing the schwa as a letter in the spelling system causes both morphology and phonology to be learnt better by the artificial learners. In general we argue that artificial learning studies are a tool in obtaining objective measurements on a spelling system that may be of help in spelling reform processes. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {van den Bosch, Antal},
ISSN = {1387-6732, 1570-6001},
Journal = {Written Language and Literacy},
Keywords = {Dutch spelling space, phonology, morphology, phonemes, artificial learners, algorithms, learning, Learning, Morphology, Phonology, Spelling, Algorithms},
Number = {1},
Pages = {25 - 44},
Title = {Spelling space: A computational test bed for phonological and morphological changes in Dutch spelling},
URL = {Antal.vdnBosch@uvt.nl},
Volume = {9},
Year = {2006},
}


@article{2004-22444-01920040801,
Abstract = {On the basis of calculations using the latest lexical database produced by Amano and Kondo (2000), the fourth edition of a Web-accessible database of characteristics of the 1,945 basic Japanese kanji was produced by including the mathematical concepts of entropy, redundancy, and symmetry and by replacing selected indexes found in previous editions (Tamaoka, Kirsner, Yanase, Miyaoka, & Kawakami, 2002). The kanji database in the fourth edition introduces seven new figures for kanji characteristics: (1) printed frequency, (2) lexical productivity, (3) accumulative lexical productivity, (4) symmetry for lexical productivity, (5) entropy, (6) redundancy, and (7) numbers of meanings for On-readings and Kun-readings. The file of the fourth edition of the kanji database may be downloaded from the Psychonomic Society Web archive, http://www.psychonomics.org/archive/ (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Tamaoka, Katsuo and  Makioka, Shogo},
ISSN = {0743-3808},
Journal = {Behavior Research Methods, Instruments & Computers},
Keywords = {Web-accessible database, Japanese kanji, Databases as Topic, Humans, Internet, Language, Linguistics, Models, Statistical, Databases, Internet, Language, Words (Phonetic Units)},
Number = {3},
Pages = {548 - 558},
Title = {New figures for a Web-accessible database of the 1,945 basic Japanese kanji, fourth edition},
URL = {ktamaoka@hiroshima-u.ac.jp},
Volume = {36},
Year = {2004},
}


@article{2004-22444-00820040801,
Abstract = {WordNet, an electronic dictionary (or lexical database), is a valuable resource for computational and cognitive scientists. Recent work on the computing of semantic distances among nodes (synsets) in WordNet has made it possible to build a large database of semantic distances for use in selecting word pairs for psychological research. The database now contains nearly 50,000 pairs of words that have values for semantic distance, associative strength, and similarity based on co-occurrence. Semantic distance was found to correlate weakly with these other measures but to correlate more strongly with another measure of semantic relatedness, featural similarity. Hierarchical clustering analysis suggested that the knowledge structure underlying semantic distance is similar in gross form to that underlying featural similarity. In experiments in which semantic similarity ratings were used, human participants were able to discriminate semantic distance. Thus, semantic distance as derived from WordNet appears distinct from other measures of word pair relatedness and is psychologically functional. This database may be downloaded from www.psychonomic.org/archive/. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Maki, William S. and  McKinley, Lauren N. and  Thompson, Amber G.},
ISSN = {0743-3808},
Journal = {Behavior Research Methods, Instruments & Computers},
Keywords = {semantic distance norms, electronic dictionary, WordNet, Cognitive Science, Dictionaries as Topic, Humans, Reference Values, Semantics, Databases, Semantics, Statistical Norms, Words (Phonetic Units)},
Number = {3},
Pages = {421 - 431},
Title = {Semantic distance norms computed from an electronic dictionary (WordNet)},
URL = {bill.maki@ttu.edu},
Volume = {36},
Year = {2004},
}


@article{2004-10766-00520040301,
Abstract = {This study reports two experiments assessing the spelling performance of French first graders after 3 months and after 9 months of literacy instruction. The participants were asked to spell high and low frequency irregular words (Experiment 1) and pseudowords, some of which had lexical neighbours (Experiment 2). The lexical database which children had been exposed to was strictly controlled. Both a frequency effect in word spelling accuracy and an analogy effect in pseudoword spelling were obtained after only 3 months of reading instruction. The results suggest that children establish specific orthographic knowledge from the very beginning of literacy acquisition. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Martinet, Catherine and  Valdois, Sylviane and  Fayol, Michel},
ISSN = {0010-0277, 1873-7838},
Journal = {Cognition},
Keywords = {orthographic knowledge, literacy acquisition, literacy instruction, spelling performance, French first graders, Cognition, Female, Humans, Male, Verbal Learning, Vocabulary, Language Arts Education, Language Development, Lexical Access, Orthography, Spelling, Literacy, Primary School Students},
Number = {2},
Pages = {B11 - B22},
Title = {Lexical orthographic knowledge develops from the beginning of literacy acquisition},
URL = {catherine.martinet@pse.unige.ch},
Volume = {91},
Year = {2004},
}


@article{2003-03926-04320030201,
Abstract = {Presents an errata to an original article by N. Shibahara and T. V. Kondo entitled 'Variables affecting naming for Japanese Kanji: A reanalysis of Yamazaki, et al. (1997)' which appeared in Perceptual and Motor Skills, (2002), 95, pp. 741-745. The first footnote is incomplete. The complete information is provided. (The following abstract of this article originally appeared in record [rid]2003-04086-006[/rid].) M. Yamazaki, A. W. Ellis, C. M. Morrison, and M. A. L. Lambon Ralph in 1997 (see record [rid]1997-05966-008[/rid]) demonstrated that written and spoken age-of-acquisitions had a stronger effect on the naming latency of single Kanji words than any other variable including familiarity. The present study was designed to reanalyze M. Yamazaki, et al.'s data, using the ratings of written and spoken age-of-acquisitions and visual and auditory familiarities taken from the Nippon Telephone and Telegram Corporation lexical database. This analysis showed that visual familiarity exerted a stronger independent effect on naming latency than two types of age-of-acquisitions. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Shibahara, N. and  Kondo, T. V.},
ISSN = {0031-5125, 1558-688X},
Journal = {Perceptual and Motor Skills},
Keywords = {naming latency, visual familiarity, Japanese Kanji, age of acquisition, auditory familiarity, writing development, speech development, Naming, Speech Development, Word Recognition, Words (Phonetic Units), Written Language},
Number = {1},
Title = {Variables affecting naming latency for Japanese Kanji: A reanalysis of Yamazaki, et al (1997): Erratum},
URL = {n.shibahara@ucl.ac.uk},
Volume = {96},
Year = {2003},
}


@article{2003-04086-00620021201,
Abstract = {M. Yamazaki, A. W. Ellis, C. M. Morrison, and M. A. Lambon Ralph in 1997 (see record [rid]1997-05966-008[/rid]) demonstrated that written and spoken age-of-acquisitions had a stronger effect on the naming latency of single Kanji words than any other variable including familiarity. The present study was designed to reanalyze M. Yamazaki, et al.'s data, using the ratings of written and spoken age-of-acquisitions and visual and auditory familiarities taken from the Nippon Telephone and Telegram Corporation lexical database. This analysis showed that visual familiarity exerted a stronger independent effect on naming latency than two types of age-of-acquisitions. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Shibahara, Naoki and  Kondo, Tadahisa},
ISSN = {0031-5125, 1558-688X},
Journal = {Perceptual and Motor Skills},
Keywords = {naming latency, visual familiarity, Japanese Kanji, age of acquisition, auditory familiarity, writing development, speech development, Naming, Speech Development, Word Recognition, Words (Phonetic Units), Written Language},
Number = {3, Pt 1},
Pages = {741 - 745},
Title = {Variables affecting naming latency for Japanese Kanji: A re-analysis of Yamazaki, et al (1997)},
URL = {n.shibahara@ucl.ac.uk},
Volume = {95},
Year = {2002},
}


@article{2001-11497-00120010901,
Abstract = {Results of the noun–verb pair comprehension and production tests from the Test Battery for Auslan Morphology and Syntax (A. Schembri et al., 2000) are presented, reanalyzed, and compared to data from 2 other cases dealing with noun–verb pairs: the Auslan lexical database and a comparison of Auslan and American Sign Language (ASL) signs. The data confirm the existence of formationally related noun–verb pairs in Auslan in which the verb displays a single movement and the noun displays a repeated movement. The data also suggest that the best exemplars of noun–verb pairs of this type in Auslan form a distinct set of iconic (mimetic) signs archetypically based on inherently reversible actions (such as opening and shutting). This strong iconic link perhaps explains why the derivational process appears to be of limited productivity, though it does appear to have 'spread' to a number of signs that appear to have no such iconicity. There appears to be considerable variability in the use of the derivational markings, particularly in connected discourse, even for signs of the 'open and shut' variety. Overall, the derivational process is apparently still closely linked to an iconic base, is incipient in the grammar of Auslan, and is best described as only partially grammaticalized. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Johnston, Trevor},
ISSN = {1081-4159, 1465-7325},
Journal = {Journal of Deaf Studies and Deaf Education},
Keywords = {verb-noun pair comprehension & production, Auslan, American Sign Language, iconicity of signs, grammaticalized signs, Grammar, Sign Language, Nouns, Verbs},
Number = {4},
Pages = {235 - 257},
Title = {Nouns and verbs in Australian sign language: An open and shut case?},
URL = {rctaj@alinga.newcastle.edu.au},
Volume = {6},
Year = {2001},
}


@article{1997-06868-00119970101,
Abstract = {Studied implementational choices in pronunciation by analogy (PbA) for English to assess its ultimate suitability - both as a model of the human process of reading aloud, and as a component of a text-to-speech (TTS) system. The variables studied were the specific lexical database used as the basis of the analogy process, the way of ranking/scoring candidate pronunciations, and the effect of manual vs automatic alignment of letters and phonemes. When tested with short (monosyllabic) pseudowords, the lowest error rate achieved was 14.3%. This suggests that the current PbA systems are at best poor models of pseudoword pronunciation by humans. When tested with lexical words temporarily removed from the dictionary, the best performance obtained was 93.5% phonemes correct for a 16,280-word dictionary. This was superior to the 25.7% words correct obtained using a set of popular letter-to-sound rules, indicating considerable scope for analogy methods to be exploited in future TTS systems. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Damper, R. I. and  Eastmond, J. F. G.},
ISSN = {0023-8309, 1756-6053},
Journal = {Language and Speech},
Keywords = {basic implementational choices, performance of pronunciation by analogy as model of human process of reading aloud & component of text-to-speech systems, Analogy, Automated Speech Recognition, Models, Oral Reading, Pronunciation, Computer Simulation, Lexical Access, Reasoning},
Number = {1},
Pages = {1 - 23},
Title = {Pronunciation by analogy: Impact of implementational choices on performance},
Volume = {40},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=psyh&AN=1997-06868-001&site=ehost-live&scope=site},
Year = {1997},
}


@article{1994-31765-00119931201,
Abstract = {Describes a synthesis-by-analogy system which is a model of novel-word pronunciation by humans. It uses analogy in both orthographic and phonological domains and is applied to the pronunciation of novel words in British English and German. A major part of this cross-language study concerned the impact of implementational choices on performance, where this was defined as the ability of the system to produce pronunciations in line with those given by humans. The size and content of the lexical database on which any analogy system must be based were also considered. The better performing implementations produced useful results for both British English and German. However, best results for each of the 2 languages were obtained from different implementations. The system described is also a psychological model of reading aloud. (German & French abstracts) (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
Author = {Sullivan, K. P. H. and  Damper, R. I.},
ISSN = {0167-6393, 1872-7182},
Journal = {Speech Communication},
Keywords = {implementation choice, synthesis by analogy computer system modeling of novel word pronunciation in British English vs German, conference presentation, Computer Simulation, Pronunciation, Synthetic Speech, Words (Phonetic Units)},
Number = {3-4},
Pages = {441 - 452},
Title = {Novel-word pronunciation: A cross-language study},
Volume = {13},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=psyh&AN=1994-31765-001&site=ehost-live&scope=site},
Year = {1993},
}


@article{2017-35600-00320160101,
Abstract = {It has been frequently noted in the literature that content words need to consist of at least three letters; this observation is commonly dubbed 'three letter rule.' However, a survey of the CELEX database (Baayen et al. 1995) shows that there are (nearly) no content words in English and German that begin with two or more consonant letters and end in a single vowel letter. Words such as [bruː] are not spelt < bru > but < brew > with an additional letter. These findings cannot be accounted for by the three letter rule but they are explicable within a supra-segmental theory of graphematics that includes graphematic feet and graphematic weight: a well-formed graphematic word consists of at least one graphematic foot that in turn consists of at least one heavy graphematic syllable. This paper offers a data-based survey in order to answer the question whether there is a suprasegmental minimality constraint for monosyllabic graphematic words in English and German. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
Author = {Evertz, Martin},
ISSN = {1387-6732, 1570-6001},
Journal = {Written Language and Literacy},
Keywords = {Graphematic hierarchy, graphematic foot, graphematic syllable, graphematic weight, minimal words, lexical database, CELEX, English, German, Language, Lexical Access, Word Associations, Foreign Language Translation, Letters (Alphabet), Vowels},
Number = {2},
Pages = {189 - 211},
Title = {Minimal graphematic words in English and German: Lexical evidence for a theory of graphematic feet},
URL = {martin.evertz@uni-koeln.de},
Volume = {19},
Year = {2016},
}


@article{1992-22350-00119911201,
Abstract = {Discusses principles of lexical semantics developed in the course of building an on-line lexical database. The approach is relational rather than componential. The fundamental semantic relation is synonymy, which is required in order to define the lexicalized concepts that words can be used to express. Other semantic relations between these concepts are then described. No single set of semantic relations or organizational structure is adequate for the entire lexicon: nouns, adjectives, and verbs each have their own semantic relations and their own organization determined by the role they must play in the construction of linguistic messages. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Miller, George A. and  Fellbaum, Christiane},
ISSN = {0010-0277, 1873-7838},
Journal = {Cognition},
Keywords = {principles of lexical semantic memory & semantic relations & networks of English, Attention, Cognition, Communication, Concept Formation, Databases, Bibliographic, Humans, Online Systems, Psycholinguistics, Reading, Semantics, Linguistics, Semantic Memory},
Number = {1-3},
Pages = {197 - 229},
Title = {Semantic networks of English},
Volume = {41},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=psyh&AN=1992-22350-001&site=ehost-live&scope=site},
Year = {1991},
}


@masterthesis{2002-95003-10820020201,
Abstract = {This thesis investigates the perceptual categories associated with contrasting pitch accents in Tokyo Japanese. Various aspects of pitch movements throughout the words are systematically varied in order to determine what aspects of the pitch contour affect categorization of words on the basis of accent. In addition, this thesis also investigates the effect on categorization of loss of pitch due to lack of voicing in various parts of the word. The model determined by these studies reveals two more-or-less orthogonal perceptual dimensions; pitch alignment with speech segments determines accent location and the amount of pitch drop determines accent presence. This study also investigates how to quantify the distinctive function of pitch accent in a way which incorporates the frequency of the contrasting items, as well as the peculiar category structure of accents. This model was applied in the analysis of a large-scale lexical database, revealing many irregularities in the distribution and use of accents. Comparing this quantification of the lexical use of accent with the perceptual experiments shows that accent-location detection is functionally more fundamental than accent-presence detection in short, 2-mora words. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Kitahara, Mafuyu},
Keywords = {category structure, pitch accent, Tokyo Japanese, Speech Characteristics, Speech Pitch},
School = {ProQuest Information & Learning},
Title = {Category structure and function of pitch accent in Tokyo Japanese},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=psyh&AN=2002-95003-108&site=ehost-live&scope=site},
Year = {2002},
}


@article{2007-06890-00220070101,
Abstract = {Age of acquisition of a word and familiarity are important factors for the lexical processing, in production as in perception. To help developing research on the mechanisms underlying the lexical processing in French, a lexical data base was built for a corpus of 1225 monosyllabic and disyllabic French words. This article describes the method used to collect the data, the rough information obtained with the survey, explains the method that was used to process the rough information, describes the content of the lexical data base CHACQFAM, obtained after the rough data has been processed, and describes the validation procedure of its content. CHACQFAM is made freely available to researchers in an electronic format, from the website 'http://psycholinguistique.unige.ch/'. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Lachaud, Christian Michel},
ISSN = {0003-5033, 1955-2580},
Journal = {L'Année Psychologique},
Keywords = {age of acquisition, familiarity, monosyllabic French words, bisyllabic French words, lexical processing, lexical database, Databases, Familiarity, Learning, Syllables, Words (Phonetic Units), Cognitive Processes},
Number = {1},
Pages = {39 - 63},
Title = {CHACQFAM: Une base de données renseignant l'âge d'acquisition estimé et la familiarité pour 1225 mots monosyllabiques et bisyllabiques du français = CHACQFAM: A lexical data base for the estimated age of acquisition and familiarity of 1225 monosyllabic and bisyllabic French words},
URL = {Christian.Lachaud@pse.unige.ch},
Volume = {107},
Year = {2007},
}

