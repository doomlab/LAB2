---
title: "Support Vector Machine Results"
author: "Erin M. Buchanan"
date: "2/19/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Libraries and Data

```{r}
library(reticulate)
library(reshape)
library(ggplot2, quietly = T)

##here::here() should return the folder of the entire Rproject
master <- read.csv(paste0(here::here(),"/02.data/output_data/training_data_no_stem.csv"), stringsAsFactors = F)

set.seed(439489043)
sample_rows <- sample(1:nrow(master), 
                          floor(nrow(master)*.9), 
                          replace = FALSE)

training <- master[sample_rows, ]
dev_testing <- master[-sample_rows, ]
testing <- read.csv(paste0(here::here(),"/02.data/output_data/test_data_new_no_stem.csv"), stringsAsFactors = F)

#sample sizes 
nrow(training)
nrow(dev_testing)
nrow(testing)
```

## SVM Analysis

### Python Libraries and Data

```{python}
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import CountVectorizer
from gensim import corpora
from gensim.models import LsiModel
from gensim.models.coherencemodel import CoherenceModel
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
import gensim
from gensim.models import LdaModel
from gensim.models import Word2Vec
from gensim.models.fasttext import FastText
from sklearn.svm import LinearSVC

master = r.master
training = r.training
dev_testing = r.dev_testing
testing = r.testing

training_text = training["text"]
training_answer = training["class"]

dev_text = dev_testing["text"]
dev_answer = dev_testing["class"]

testing_text = testing["text"]
testing_answer = testing["class"]
```

### Create Blank SVM Model

```{python}
svm = LinearSVC(penalty='l2', C=1, random_state=42)
```

### Bag of Words 

There are approximately 20,000 words in this data. We will start with the top 100 words and increase in units of 100 for matrix size up to 4000 words (which accounts for approximately .05% of word occurrence ~ 10 mentions across all articles).

```{python eval = F}
## create a blank dataframe
columns = ["precision0", "recall0", "f1 score0"]
columns2 = ["precision1", "recall1", "f1 score1"]
svm_cv_accuracydev = []
svm_cv_accuracytest = []

index = np.arange(100, 4001, 100).tolist()
svm_cv_results_0dev = pd.DataFrame(index=index, columns=columns)
svm_cv_results_0dev = svm_cv_results_0dev.fillna(0)
svm_cv_results_0test = pd.DataFrame(index=index, columns=columns)
svm_cv_results_0test = svm_cv_results_0test.fillna(0)

svm_cv_results_1dev = pd.DataFrame(index=index, columns=columns2)
svm_cv_results_1dev = svm_cv_results_1dev.fillna(0)
svm_cv_results_1test = pd.DataFrame(index=index, columns=columns2)
svm_cv_results_1test = svm_cv_results_1test.fillna(0)

##create the loop here 
for i in np.arange(100, 4001, 100).tolist(): #start loop

  #create counts
  cv = CountVectorizer(binary=False, #not true false, actual counts
                      min_df=0.0, #min proportion 
                      max_df=1.0, #max proportion 
                      decode_error='replace', #take out bad bytes
                      max_features=i) #looping over size 
                   
  #create bag of words on data                   
  cv_training_text = cv.fit_transform(training_text)
  cv_dev_text = cv.transform(dev_text)
  cv_testing_text = cv.transform(testing_text)
  
  #fit LR model
  svm.fit(cv_training_text, training_answer)
  
  #predict
  y_dev = svm.predict(cv_dev_text)
  y_testing = svm.predict(cv_testing_text)
  
  #classification
  report = classification_report(dev_answer, y_dev, output_dict=True)
  
  svm_cv_results_0dev.loc[i] = list(report["0"].values())[0:3]
  svm_cv_results_1dev.loc[i] = list(report["1"].values())[0:3]
  svm_cv_accuracydev.append(report["accuracy"])

  #classification
  report2 = classification_report(testing_answer, y_testing, output_dict=True)
  
  svm_cv_results_0test.loc[i] = list(report2["0"].values())[0:3]
  svm_cv_results_1test.loc[i] = list(report2["1"].values())[0:3]
  svm_cv_accuracytest.append(report2["accuracy"])
```

```{r eval = F}
svm_cv_results <- cbind(py$svm_cv_results_0dev, py$svm_cv_results_1dev, 
                        "accuracy_dev" = py$svm_cv_accuracydev, 
                        py$svm_cv_results_0test, py$svm_cv_results_1test, 
                        "accuracy_testing" = py$svm_cv_accuracytest)

colnames(svm_cv_results) <- c("precision0_dev","recall0_dev","f1_score0_dev",
                              "precision1_dev","recall1_dev","f1_score1_dev",
                              "accuracy_dev", "precision0_testing",
                              "recall0_testing","f1_score0_testing",
                              "precision1_testing","recall1_testing",
                              "f1_score1_testing","accuracy_testing")
write.csv(svm_cv_results, "svm_cv_results.csv")
```

### TF-IDF

The same approach was applied here in TF-IDF. 

```{python eval = F}
## create a blank dataframe
columns = ["precision0", "recall0", "f1 score0"]
columns2 = ["precision1", "recall1", "f1 score1"]
svm_tv_accuracydev = []
svm_tv_accuracytest = []

index = np.arange(100, 4001, 100).tolist()
svm_tv_results_0dev = pd.DataFrame(index=index, columns=columns)
svm_tv_results_0dev = svm_tv_results_0dev.fillna(0)
svm_tv_results_0test = pd.DataFrame(index=index, columns=columns)
svm_tv_results_0test = svm_tv_results_0test.fillna(0)

svm_tv_results_1dev = pd.DataFrame(index=index, columns=columns2)
svm_tv_results_1dev = svm_tv_results_1dev.fillna(0)
svm_tv_results_1test = pd.DataFrame(index=index, columns=columns2)
svm_tv_results_1test = svm_tv_results_1test.fillna(0)

##create the loop here 
for i in np.arange(100, 4001, 100).tolist(): #start loop

  #create tfidf
  tv = TfidfVectorizer(use_idf=True, #use tfidf
                      min_df=0.0, #min prop
                      max_df=1.0, #max prop
                      max_features=i) #looping over size 
                   
  #create bag of words on data                   
  tv_training_text = tv.fit_transform(training_text)
  tv_dev_text = tv.transform(dev_text)
  tv_testing_text = tv.transform(testing_text)
  
  #fit LR model
  svm.fit(tv_training_text, training_answer)
  
  #predict
  y_dev = svm.predict(tv_dev_text)
  y_testing = svm.predict(tv_testing_text)
  
  #classification
  report = classification_report(dev_answer, y_dev, output_dict=True)
  
  svm_tv_results_0dev.loc[i] = list(report["0"].values())[0:3]
  svm_tv_results_1dev.loc[i] = list(report["1"].values())[0:3]
  svm_tv_accuracydev.append(report["accuracy"])

  #classification
  report2 = classification_report(testing_answer, y_testing, output_dict=True)
  
  svm_tv_results_0test.loc[i] = list(report2["0"].values())[0:3]
  svm_tv_results_1test.loc[i] = list(report2["1"].values())[0:3]
  svm_tv_accuracytest.append(report2["accuracy"])
```

```{r eval = F}
svm_tv_results <- cbind(py$svm_tv_results_0dev, py$svm_tv_results_1dev, 
                        "accuracy_dev" = py$svm_tv_accuracydev, 
                        py$svm_tv_results_0test, py$svm_tv_results_1test, 
                        "accuracy_testing" = py$svm_tv_accuracytest)

colnames(svm_tv_results) <- c("precision0_dev","recall0_dev","f1_score0_dev",
                              "precision1_dev","recall1_dev","f1_score1_dev",
                              "accuracy_dev", "precision0_testing",
                              "recall0_testing","f1_score0_testing",
                              "precision1_testing","recall1_testing",
                              "f1_score1_testing","accuracy_testing")
write.csv(svm_tv_results, "svm_tv_results.csv")
```

### Latent Semantic Analysis

Here, we will create dimensions of 100 up to 4000 to create a the same dimensionality we saw with previous models. The model is created with the entire dataframe, and then partitioned for training and testing. 

```{python}
#order the data so we can subset back in the right order 
ordered_data = training_text.append(dev_text)
ordered_data = ordered_data.append(testing_text)

#create dictionary of entire wordset, must be tokenized
dictionary = corpora.Dictionary(ordered_data.apply(nltk.word_tokenize))

#create a TDM for overall data
doc_term_matrix = [dictionary.doc2bow(doc) for doc in ordered_data.apply(nltk.word_tokenize)]
```

```{python eval = F}
## create a blank dataframe
columns = ["precision0", "recall0", "f1 score0"]
columns2 = ["precision1", "recall1", "f1 score1"]
svm_lsa_accuracydev = []
svm_lsa_accuracytest = []

index = np.arange(100, 4001, 100).tolist()
svm_lsa_results_0dev = pd.DataFrame(index=index, columns=columns)
svm_lsa_results_0dev = svm_lsa_results_0dev.fillna(0)
svm_lsa_results_0test = pd.DataFrame(index=index, columns=columns)
svm_lsa_results_0test = svm_lsa_results_0test.fillna(0)

svm_lsa_results_1dev = pd.DataFrame(index=index, columns=columns2)
svm_lsa_results_1dev = svm_lsa_results_1dev.fillna(0)
svm_lsa_results_1test = pd.DataFrame(index=index, columns=columns2)
svm_lsa_results_1test = svm_lsa_results_1test.fillna(0)

##create the loop here 
for i in np.arange(100, 4001, 100).tolist(): #start loop

  #create LSA model of all data
  lsa_model = LsiModel(doc_term_matrix, 
           num_topics=i, 
           id2word = dictionary)

  #grab V matrix
  V = gensim.matutils.corpus2dense(lsa_model[doc_term_matrix], len(lsa_model.projection.s)).T / lsa_model.projection.s
  
  #maybe cannot have negatives 
  #V = V + abs(V.min())
  
  training_V = V[0:len(training_text)]
  dev_V = V[len(training_text):len(training_text)+len(dev_text)]
  testing_V = V[len(training_text)+len(dev_text):]

  #fit LR model
  svm.fit(training_V, training_answer)
  
  #predict
  y_dev = svm.predict(dev_V)
  y_testing = svm.predict(testing_V)
  
  #classification
  report = classification_report(dev_answer, y_dev, output_dict=True)
  
  svm_lsa_results_0dev.loc[i] = list(report["0"].values())[0:3]
  svm_lsa_results_1dev.loc[i] = list(report["1"].values())[0:3]
  svm_lsa_accuracydev.append(report["accuracy"])

  #classification
  report2 = classification_report(testing_answer, y_testing, output_dict=True)
  
  svm_lsa_results_0test.loc[i] = list(report2["0"].values())[0:3]
  svm_lsa_results_1test.loc[i] = list(report2["1"].values())[0:3]
  svm_lsa_accuracytest.append(report2["accuracy"])
```

```{r eval = F}
svm_lsa_results <- cbind(py$svm_lsa_results_0dev, py$svm_lsa_results_1dev, 
                        "accuracy_dev" = py$svm_lsa_accuracydev, 
                        py$svm_lsa_results_0test, py$svm_lsa_results_1test, 
                        "accuracy_testing" = py$svm_lsa_accuracytest)

colnames(svm_lsa_results) <- c("precision0_dev","recall0_dev","f1_score0_dev",
                              "precision1_dev","recall1_dev","f1_score1_dev",
                              "accuracy_dev", "precision0_testing",
                              "recall0_testing","f1_score0_testing",
                              "precision1_testing","recall1_testing",
                              "f1_score1_testing","accuracy_testing")
write.csv(svm_lsa_results, "svm_lsa_results.csv")
```

Note: the largest shape was 3245. 

### Topics Model

Topics model has the same set up using entire dataframe to predict the category, so using the dictionary and doc_term_matrix from LSA. 

```{python eval = F}
## create a blank dataframe
columns = ["precision0", "recall0", "f1 score0"]
columns2 = ["precision1", "recall1", "f1 score1"]
svm_topics_accuracydev = []
svm_topics_accuracytest = []

index = np.arange(100, 4001, 100).tolist()
svm_topics_results_0dev = pd.DataFrame(index=index, columns=columns)
svm_topics_results_0dev = svm_topics_results_0dev.fillna(0)
svm_topics_results_0test = pd.DataFrame(index=index, columns=columns)
svm_topics_results_0test = svm_topics_results_0test.fillna(0)

svm_topics_results_1dev = pd.DataFrame(index=index, columns=columns2)
svm_topics_results_1dev = svm_topics_results_1dev.fillna(0)
svm_topics_results_1test = pd.DataFrame(index=index, columns=columns2)
svm_topics_results_1test = svm_topics_results_1test.fillna(0)

##create the loop here 
for i in np.arange(100, 4001, 100).tolist(): #start loop

  #create LDA model of all data
  lda_model = LdaModel(corpus = doc_term_matrix, #TDM
                              id2word = dictionary, #Dictionary
                              num_topics = i,
                              random_state = 100)
                              
  #get the document-topics matrix                              
  DT = lda_model[doc_term_matrix]
  DT = gensim.matutils.corpus2csc(DT)
  DT = DT.T.toarray()
  #DT = DT + abs(DT.min())
  
  training_DT = DT[0:len(training_text)]
  dev_DT = DT[len(training_text):len(training_text)+len(dev_text)]
  testing_DT = DT[len(training_text)+len(dev_text):]

  #fit LR model
  svm.fit(training_DT, training_answer)
  
  #predict
  y_dev = svm.predict(dev_DT)
  y_testing = svm.predict(testing_DT)
  
  #classification
  report = classification_report(dev_answer, y_dev, output_dict=True)
  
  svm_topics_results_0dev.loc[i] = list(report["0"].values())[0:3]
  svm_topics_results_1dev.loc[i] = list(report["1"].values())[0:3]
  svm_topics_accuracydev.append(report["accuracy"])

  #classification
  report2 = classification_report(testing_answer, y_testing, output_dict=True)
  
  svm_topics_results_0test.loc[i] = list(report2["0"].values())[0:3]
  svm_topics_results_1test.loc[i] = list(report2["1"].values())[0:3]
  svm_topics_accuracytest.append(report2["accuracy"])
```

```{r eval= F}
svm_topics_results <- cbind(py$svm_topics_results_0dev, py$svm_topics_results_1dev, 
                        "accuracy_dev" = py$svm_topics_accuracydev, 
                        py$svm_topics_results_0test, py$svm_topics_results_1test, 
                        "accuracy_testing" = py$svm_topics_accuracytest)

colnames(svm_topics_results) <- c("precision0_dev","recall0_dev","f1_score0_dev",
                              "precision1_dev","recall1_dev","f1_score1_dev",
                              "accuracy_dev", "precision0_testing",
                              "recall0_testing","f1_score0_testing",
                              "precision1_testing","recall1_testing",
                              "f1_score1_testing","accuracy_testing")
write.csv(svm_topics_results, "svm_topics_results.csv")
```

Note the largest dimension size was 3436. 

### Word2Vec

Create ids for the combination of word window and feature size: 

```{r}
#create new ids
id1 <- seq(100, 4001, 100)
id2 <- seq(3,10,1)
for (i in 1:length(id2)){ 
  
  if (i == 1){ 
    idlist <- paste(id1, id2[i], sep = "_")
  } else{ 
    idlist <- append(idlist, paste(id1, id2[i], sep = "_"))
    }

  }
```

The averaging function to convert word2vec into numeric vectors:

```{python}
#create flattening function
def document_vectorizer(corpus, model, num_features):
    vocabulary = set(model.wv.index2word)
    
    def average_word_vectors(words, model, vocabulary, num_features):
        feature_vector = np.zeros((num_features,), dtype="float64")
        nwords = 0.
        
        for word in words:
            if word in vocabulary: 
                nwords = nwords + 1.
                feature_vector = np.add(feature_vector, model.wv[word])
        if nwords:
            feature_vector = np.divide(feature_vector, nwords)

        return feature_vector

    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)
                    for tokenized_sentence in corpus]
    return np.array(features)
```

#### CBOW

```{python eval = F}
## create a blank dataframe
columns = ["precision0", "recall0", "f1 score0"]
columns2 = ["precision1", "recall1", "f1 score1"]
svm_wv_cbow_accuracydev = []
svm_wv_cbow_accuracytest = []

index = r.idlist
svm_wv_cbow_results_0dev = pd.DataFrame(index=index, columns=columns)
svm_wv_cbow_results_0dev = svm_wv_cbow_results_0dev.fillna(0)
svm_wv_cbow_results_0test = pd.DataFrame(index=index, columns=columns)
svm_wv_cbow_results_0test = svm_wv_cbow_results_0test.fillna(0)

svm_wv_cbow_results_1dev = pd.DataFrame(index=index, columns=columns2)
svm_wv_cbow_results_1dev = svm_wv_cbow_results_1dev.fillna(0)
svm_wv_cbow_results_1test = pd.DataFrame(index=index, columns=columns2)
svm_wv_cbow_results_1test = svm_wv_cbow_results_1test.fillna(0)

loop_round = 0

for i in r.idlist: #start loop

  p, q = r.idlist[loop_round].split("_")
  p = int(p)
  q = int(q)
  loop_round = loop_round + 1

  #create an overall w2v model
  w2v_model = Word2Vec(ordered_data.apply(nltk.word_tokenize), #corpus of tokenized words
              size=p, #number of features
              window=q, #size of moving window
              min_count=1, #minimum number of times word has to be seen 
              sg = 0, #cbow model
              iter=5, workers=5) #iterations and cores
  
  #flatten the features
  all_features = document_vectorizer(
                      corpus=ordered_data.apply(nltk.word_tokenize), 
                      model=w2v_model,
                      num_features=p)

  #all_features = all_features + abs(all_features.min())
  #separate out into different pieces
  training_features = all_features[0:len(training_text)]
  dev_features = all_features[len(training_text):len(training_text)+len(dev_text)]
  testing_features = all_features[len(training_text)+len(dev_text):]
  
  #fit LR model
  svm.fit(training_features, training_answer)
  
  #predict
  y_dev = svm.predict(dev_features)
  y_testing = svm.predict(testing_features)
  
  #classification
  report = classification_report(dev_answer, y_dev, output_dict=True)
  
  svm_wv_cbow_results_0dev.loc[i] = list(report["0"].values())[0:3]
  svm_wv_cbow_results_1dev.loc[i] = list(report["1"].values())[0:3]
  svm_wv_cbow_accuracydev.append(report["accuracy"])
  
  #classification
  report2 = classification_report(testing_answer, y_testing, output_dict=True)
  
  svm_wv_cbow_results_0test.loc[i] = list(report2["0"].values())[0:3]
  svm_wv_cbow_results_1test.loc[i] = list(report2["1"].values())[0:3]
  svm_wv_cbow_accuracytest.append(report2["accuracy"])
```

```{r eval = F}
svm_wv_cbow_results <- cbind(py$svm_wv_cbow_results_0dev, py$svm_wv_cbow_results_1dev, 
                        "accuracy_dev" = py$svm_wv_cbow_accuracydev, 
                        py$svm_wv_cbow_results_0test, py$svm_wv_cbow_results_1test, 
                        "accuracy_testing" = py$svm_wv_cbow_accuracytest)

colnames(svm_wv_cbow_results) <- c("precision0_dev","recall0_dev","f1_score0_dev",
                              "precision1_dev","recall1_dev","f1_score1_dev",
                              "accuracy_dev", "precision0_testing",
                              "recall0_testing","f1_score0_testing",
                              "precision1_testing","recall1_testing",
                              "f1_score1_testing","accuracy_testing")
write.csv(svm_wv_cbow_results, "svm_wv_cbow_results.csv")
```

#### Skip Gram

```{python eval = F}
## create a blank dataframe
columns = ["precision0", "recall0", "f1 score0"]
columns2 = ["precision1", "recall1", "f1 score1"]
svm_wv_skip_accuracydev = []
svm_wv_skip_accuracytest = []

index = r.idlist
svm_wv_skip_results_0dev = pd.DataFrame(index=index, columns=columns)
svm_wv_skip_results_0dev = svm_wv_skip_results_0dev.fillna(0)
svm_wv_skip_results_0test = pd.DataFrame(index=index, columns=columns)
svm_wv_skip_results_0test = svm_wv_skip_results_0test.fillna(0)

svm_wv_skip_results_1dev = pd.DataFrame(index=index, columns=columns2)
svm_wv_skip_results_1dev = svm_wv_skip_results_1dev.fillna(0)
svm_wv_skip_results_1test = pd.DataFrame(index=index, columns=columns2)
svm_wv_skip_results_1test = svm_wv_skip_results_1test.fillna(0)

loop_round = 0

for i in r.idlist: #start loop

  p, q = r.idlist[loop_round].split("_")
  p = int(p)
  q = int(q)
  loop_round = loop_round + 1

  #create an overall w2v model
  w2v_model = Word2Vec(ordered_data.apply(nltk.word_tokenize), #corpus of tokenized words
              size=p, #number of features
              window=q, #size of moving window
              min_count=1, #minimum number of times word has to be seen 
              sg = 1, #skip gram model
              iter=5, workers=5) #iterations and cores
  
  #flatten the features
  all_features = document_vectorizer(
                      corpus=ordered_data.apply(nltk.word_tokenize), 
                      model=w2v_model,
                      num_features=p)
                      
  #all_features = all_features + abs(all_features.min())
  #separate out into different pieces
  training_features = all_features[0:len(training_text)]
  dev_features = all_features[len(training_text):len(training_text)+len(dev_text)]
  testing_features = all_features[len(training_text)+len(dev_text):]
  
  #fit LR model
  svm.fit(training_features, training_answer)
  
  #predict
  y_dev = svm.predict(dev_features)
  y_testing = svm.predict(testing_features)
  
  #classification
  report = classification_report(dev_answer, y_dev, output_dict=True)
  
  svm_wv_skip_results_0dev.loc[i] = list(report["0"].values())[0:3]
  svm_wv_skip_results_1dev.loc[i] = list(report["1"].values())[0:3]
  svm_wv_skip_accuracydev.append(report["accuracy"])
  
  #classification
  report2 = classification_report(testing_answer, y_testing, output_dict=True)
  
  svm_wv_skip_results_0test.loc[i] = list(report2["0"].values())[0:3]
  svm_wv_skip_results_1test.loc[i] = list(report2["1"].values())[0:3]
  svm_wv_skip_accuracytest.append(report2["accuracy"])
```

```{r eval = F}
svm_wv_skip_results <- cbind(py$svm_wv_skip_results_0dev,
                             py$svm_wv_skip_results_1dev,
                             "accuracy_dev" = py$svm_wv_skip_accuracydev,
                             py$svm_wv_skip_results_0test,
                             py$svm_wv_skip_results_1test, 
                             "accuracy_testing" = py$svm_wv_skip_accuracytest)

colnames(svm_wv_skip_results) <- c("precision0_dev","recall0_dev","f1_score0_dev",
                              "precision1_dev","recall1_dev","f1_score1_dev",
                              "accuracy_dev", "precision0_testing",
                              "recall0_testing","f1_score0_testing",
                              "precision1_testing","recall1_testing",
                              "f1_score1_testing","accuracy_testing")
write.csv(svm_wv_skip_results, "svm_wv_skip_results_end.csv")
```

### FastText

#### CBOW

```{python eval = F}
## create a blank dataframe
columns = ["precision0", "recall0", "f1 score0"]
columns2 = ["precision1", "recall1", "f1 score1"]
svm_ft_cbow_accuracydev = []
svm_ft_cbow_accuracytest = []
index = r.idlist
svm_ft_cbow_results_0dev = pd.DataFrame(index=index, columns=columns)
svm_ft_cbow_results_0dev = svm_ft_cbow_results_0dev.fillna(0)
svm_ft_cbow_results_0test = pd.DataFrame(index=index, columns=columns)
svm_ft_cbow_results_0test = svm_ft_cbow_results_0test.fillna(0)
svm_ft_cbow_results_1dev = pd.DataFrame(index=index, columns=columns2)
svm_ft_cbow_results_1dev = svm_ft_cbow_results_1dev.fillna(0)
svm_ft_cbow_results_1test = pd.DataFrame(index=index, columns=columns2)
svm_ft_cbow_results_1test = svm_ft_cbow_results_1test.fillna(0)
loop_round = 0
for i in r.idlist: #start loop
  p, q = r.idlist[loop_round].split("_")
  p = int(p)
  q = int(q)
  loop_round = loop_round + 1
  #create an overall fasttext model
  ft_model = FastText(ordered_data.apply(nltk.word_tokenize), #corpus of tokenized words
                    size=p, #number of features
                    window=q, #size of moving window
                    sg = 0,
                    min_count=1, 
                    iter=5, workers=4)
  
  #flatten the features
  all_features = document_vectorizer(
                      corpus=ordered_data.apply(nltk.word_tokenize), 
                      model=ft_model,
                      num_features=p)
                      
  #all_features = all_features + abs(all_features.min())
  
  #separate out into different pieces
  training_features = all_features[0:len(training_text)]
  dev_features = all_features[len(training_text):len(training_text)+len(dev_text)]
  testing_features = all_features[len(training_text)+len(dev_text):]
  
  #fit LR model
  svm.fit(training_features, training_answer)
  
  #predict
  y_dev = svm.predict(dev_features)
  y_testing = svm.predict(testing_features)
  
  #classification
  report = classification_report(dev_answer, y_dev, output_dict=True)
  
  svm_ft_cbow_results_0dev.loc[i] = list(report["0"].values())[0:3]
  svm_ft_cbow_results_1dev.loc[i] = list(report["1"].values())[0:3]
  svm_ft_cbow_accuracydev.append(report["accuracy"])
  
  #classification
  report2 = classification_report(testing_answer, y_testing, output_dict=True)
  
  svm_ft_cbow_results_0test.loc[i] = list(report2["0"].values())[0:3]
  svm_ft_cbow_results_1test.loc[i] = list(report2["1"].values())[0:3]
  svm_ft_cbow_accuracytest.append(report2["accuracy"])
  
  svm_ft_cbow_results_0dev.to_csv("svm_ft_cbow_results_0dev.csv")
  svm_ft_cbow_results_1dev.to_csv("svm_ft_cbow_results_1dev.csv")
  pd.Series(svm_ft_cbow_accuracydev).to_csv("svm_ft_cbow_accuracydev.csv", header = False)
  svm_ft_cbow_results_0test.to_csv("svm_ft_cbow_results_0test.csv")
  svm_ft_cbow_results_1test.to_csv("svm_ft_cbow_results_1test.csv")
  pd.Series(svm_ft_cbow_accuracytest).to_csv("svm_ft_cbow_accuracytest.csv", header = False)
```

```{r eval = F}
svm_ft_cbow_results <- cbind(py$svm_ft_cbow_results_0dev, py$svm_ft_cbow_results_1dev, 
                        "accuracy_dev" = py$svm_ft_cbow_accuracydev, 
                        py$svm_ft_cbow_results_0test, py$svm_ft_cbow_results_1test, 
                        "accuracy_testing" = py$svm_ft_cbow_accuracytest)
colnames(svm_ft_cbow_results) <- c("precision0_dev","recall0_dev","f1_score0_dev",
                              "precision1_dev","recall1_dev","f1_score1_dev",
                              "accuracy_dev", "precision0_testing",
                              "recall0_testing","f1_score0_testing",
                              "precision1_testing","recall1_testing",
                              "f1_score1_testing","accuracy_testing")
write.csv(svm_ft_cbow_results, "svm_ft_cbow_results.csv")
```

#### Skip Gram

```{python eval = F}
## create a blank dataframe
columns = ["precision0", "recall0", "f1 score0"]
columns2 = ["precision1", "recall1", "f1 score1"]
svm_ft_skip_accuracydev = []
svm_ft_skip_accuracytest = []
index = r.idlist
svm_ft_skip_results_0dev = pd.DataFrame(index=index, columns=columns)
svm_ft_skip_results_0dev = svm_ft_skip_results_0dev.fillna(0)
svm_ft_skip_results_0test = pd.DataFrame(index=index, columns=columns)
svm_ft_skip_results_0test = svm_ft_skip_results_0test.fillna(0)
svm_ft_skip_results_1dev = pd.DataFrame(index=index, columns=columns2)
svm_ft_skip_results_1dev = svm_ft_skip_results_1dev.fillna(0)
svm_ft_skip_results_1test = pd.DataFrame(index=index, columns=columns2)
svm_ft_skip_results_1test = svm_ft_skip_results_1test.fillna(0)
loop_round = 0
for i in r.idlist: #start loop
  p, q = r.idlist[loop_round].split("_")
  p = int(p)
  q = int(q)
  loop_round = loop_round + 1
  #create an overall fasttext model
  ft_model = FastText(ordered_data.apply(nltk.word_tokenize), #corpus of tokenized words
                    size=p, #number of features
                    window=q, #size of moving window
                    sg = 0,
                    min_count=1, 
                    iter=5, workers=4)
  
  #flatten the features
  all_features = document_vectorizer(
                      corpus=ordered_data.apply(nltk.word_tokenize), 
                      model=ft_model,
                      num_features=p)
  
  #all_features = all_features + abs(all_features.min())
  #separate out into different pieces
  training_features = all_features[0:len(training_text)]
  dev_features = all_features[len(training_text):len(training_text)+len(dev_text)]
  testing_features = all_features[len(training_text)+len(dev_text):]
  
  #fit LR model
  svm.fit(training_features, training_answer)
  
  #predict
  y_dev = svm.predict(dev_features)
  y_testing = svm.predict(testing_features)
  
  #classification
  report = classification_report(dev_answer, y_dev, output_dict=True)
  
  svm_ft_skip_results_0dev.loc[i] = list(report["0"].values())[0:3]
  svm_ft_skip_results_1dev.loc[i] = list(report["1"].values())[0:3]
  svm_ft_skip_accuracydev.append(report["accuracy"])
  
  #classification
  report2 = classification_report(testing_answer, y_testing, output_dict=True)
  
  svm_ft_skip_results_0test.loc[i] = list(report2["0"].values())[0:3]
  svm_ft_skip_results_1test.loc[i] = list(report2["1"].values())[0:3]
  svm_ft_skip_accuracytest.append(report2["accuracy"])
  
  svm_ft_skip_results_0dev.to_csv("svm_ft_skip_results_0dev.csv")
  svm_ft_skip_results_1dev.to_csv("svm_ft_skip_results_1dev.csv")
  pd.Series(svm_ft_skip_accuracydev).to_csv("svm_ft_skip_accuracydev.csv", header = False)
  svm_ft_skip_results_0test.to_csv("svm_ft_skip_results_0test.csv")
  svm_ft_skip_results_1test.to_csv("svm_ft_skip_results_1test.csv")
  pd.Series(svm_ft_skip_accuracytest).to_csv("svm_ft_skip_accuracytest.csv", header = False)
```

```{r eval = F}
svm_ft_skip_results <- cbind(py$svm_ft_skip_results_0dev,
                             py$svm_ft_skip_results_1dev,
                             "accuracy_dev" = py$svm_ft_skip_accuracydev,
                             py$svm_ft_skip_results_0test,
                             py$svm_ft_skip_results_1test, 
                             "accuracy_testing" = py$svm_ft_skip_accuracytest)
colnames(svm_ft_skip_results) <- c("precision0_dev","recall0_dev","f1_score0_dev",
                              "precision1_dev","recall1_dev","f1_score1_dev",
                              "accuracy_dev", "precision0_testing",
                              "recall0_testing","f1_score0_testing",
                              "precision1_testing","recall1_testing",
                              "f1_score1_testing","accuracy_testing")
write.csv(svm_ft_skip_results, "svm_ft_skip_results_end.csv")
```

Neither FastText model would run in memory - they ran about three simulations and quit. Models were saved but will not be used. 

### Visualize

#### Bag of Words

```{r}
svm_cv <- read.csv("svm_cv_results.csv")

long_cv <- melt(svm_cv, id = "X")
long_cv <- subset(long_cv,
                  grepl("f1_score1|accuracy", variable))

ggplot(long_cv, aes(X, value, color = variable)) + 
  geom_path() + 
  xlab("Number of Features") + 
  ylab("Accuracy/F1 Score") + 
  theme_classic()
```

#### TF-IDF

```{r}
svm_tv <- read.csv("svm_tv_results.csv")

long_tv <- melt(svm_tv, id = "X")
long_tv <- subset(long_tv,
                  grepl("f1_score1|accuracy", variable))

ggplot(long_tv, aes(X, value, color = variable)) + 
  geom_path() + 
  xlab("Number of Features") + 
  ylab("Accuracy/F1 Score") + 
  theme_classic()
```

#### LSA

```{r}
svm_lsa <- read.csv("svm_lsa_results.csv")

long_lsa <- melt(svm_lsa, id = "X")
long_lsa <- subset(long_lsa,
                  grepl("f1_score1|accuracy", variable))

ggplot(long_lsa, aes(X, value, color = variable)) + 
  geom_path() + 
  xlab("Number of Features") + 
  ylab("Accuracy/F1 Score") + 
  theme_classic()
```

#### Topics

```{r}
svm_topics <- read.csv("svm_topics_results.csv")

long_topics <- melt(svm_topics, id = "X")
long_topics <- subset(long_topics,
                  grepl("f1_score1|accuracy", variable))

ggplot(long_topics, aes(X, value, color = variable)) + 
  geom_path() + 
  xlab("Number of Features") + 
  ylab("Accuracy/F1 Score") + 
  theme_classic()
```

#### Word2Vec CBOW

```{r}
svm_wv_cbow <- read.csv("svm_wv_cbow_results.csv")

long_wv_cbow <- melt(svm_wv_cbow, id = "X")
long_wv_cbow <- subset(long_wv_cbow,
                  grepl("f1_score1|accuracy", variable))
long_wv_cbow$X <- as.character(long_wv_cbow$X)

long_wv_cbow$features <- unlist(lapply(long_wv_cbow$X, function(x){strsplit(x, "_")[[1]][1]}))

long_wv_cbow$window <- unlist(lapply(long_wv_cbow$X, function(x){strsplit(x, "_")[[1]][2]}))

long_wv_cbow$features <- as.numeric(long_wv_cbow$features)
long_wv_cbow$window <- as.numeric(long_wv_cbow$window)

ggplot(long_wv_cbow, aes(features, value, color = variable)) + 
  geom_path() + 
  facet_wrap(~window) + 
  xlab("Number of Features") + 
  ylab("Accuracy/F1 Score") + 
  theme_classic()
```

#### Word2Vec SG

```{r}
svm_wv_skip <- read.csv("svm_wv_skip_results.csv")

long_wv_skip <- melt(svm_wv_skip, id = "X")
long_wv_skip <- subset(long_wv_skip,
                  grepl("f1_score1|accuracy", variable))
long_wv_skip$X <- as.character(long_wv_skip$X)

long_wv_skip$features <- unlist(lapply(long_wv_skip$X, function(x){strsplit(x, "_")[[1]][1]}))

long_wv_skip$window <- unlist(lapply(long_wv_skip$X, function(x){strsplit(x, "_")[[1]][2]}))

long_wv_skip$features <- as.numeric(long_wv_skip$features)
long_wv_skip$window <- as.numeric(long_wv_skip$window)

ggplot(long_wv_skip, aes(features, value, color = variable)) + 
  geom_path() + 
  facet_wrap(~window) + 
  xlab("Number of Features") + 
  ylab("Accuracy/F1 Score") + 
  theme_classic()
```

#### Overall

```{r}
long_wv_cbow$X <- long_wv_cbow$features
long_wv_cbow$variable <- paste(long_wv_cbow$variable, long_wv_cbow$window, "wv_cbow", sep = "_")
long_wv_cbow <- long_wv_cbow[ , -c(4,5)]

long_wv_skip$X <- long_wv_skip$features
long_wv_skip$variable <- paste(long_wv_skip$variable, long_wv_skip$window, "wv_skip", sep = "_")
long_wv_skip <- long_wv_skip[ , -c(4,5)]

long_cv$variable <- paste(long_cv$variable, "cv", sep = "_")
long_tv$variable <- paste(long_tv$variable, "tv", sep = "_")
long_lsa$variable <- paste(long_lsa$variable, "lsa", sep = "_")
long_topics$variable <- paste(long_topics$variable, "topics", sep = "_")

long_all <- rbind(long_cv, long_tv, long_lsa, long_topics, long_wv_cbow, long_wv_skip)

long_all$facet <- long_all$variable
long_all$facet <- gsub("f1_score1_dev.+", "f1_dev", long_all$facet)
long_all$facet <- gsub("f1_score1_test.+", "f1_test", long_all$facet)
long_all$facet <- gsub("accuracy_dev.+", "accuracy_dev", long_all$facet)
long_all$facet <- gsub("accuracy_test.+", "accuracy_test", long_all$facet)

long_all$variable2 <- long_all$variable
long_all$variable2 <- gsub(".+cv", "cv", long_all$variable2)
long_all$variable2 <- gsub(".+tv", "tv", long_all$variable2)
long_all$variable2 <- gsub(".+lsa", "lsa", long_all$variable2)
long_all$variable2 <- gsub(".+topics", "topics", long_all$variable2)
long_all$variable2 <- gsub(".+wv_cbow", "wv_cbow", long_all$variable2)
long_all$variable2 <- gsub(".+wv_skip", "wv_skip", long_all$variable2)


ggplot(long_all, aes(X, value, color = variable2)) + 
  geom_path() + 
  facet_wrap(~facet) + 
  xlab("Number of Features") + 
  ylab("Accuracy/F1 Score") + 
  theme_classic()

ggplot(long_all, aes(X, value, color = variable2)) + 
  geom_path() + 
  coord_cartesian(ylim = c(.75, 1)) +
  facet_wrap(~facet) + 
  xlab("Number of Features") + 
  ylab("Accuracy/F1 Score") + 
  theme_classic()
```


