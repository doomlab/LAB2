---
title: "SVC with Spacy"
author: "Erin M. Buchanan"
date: "11/6/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load the libraries 

```{python}
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.base import TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS
from sklearn.metrics import accuracy_score
from nltk.corpus import stopwords
import string
import re
import spacy
spacy.load('en')
from spacy.lang.en import English
parser = English()
```

## Set up spacy parameters

```{python}
STOPLIST = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS))

SYMBOLS = " ".join(string.punctuation).split(" ") + ["-", "...", "”", "”"]

class CleanTextTransformer(TransformerMixin):   
  def transform(self, X, **transform_params):
        return [cleanText(text) for text in X]   
  def fit(self, X, y=None, **fit_params):
        return self
  
def get_params(self, deep=True):
        return {}
    
def cleanText(text):
    text = text.strip().replace("\n", " ").replace("\r", " ")
    text = text.lower()
    return text

def tokenizeText(sample):
    tokens = parser(sample)
    lemmas = []
    for tok in tokens:
        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != "-PRON-" else tok.lower_)
    tokens = lemmas
    tokens = [tok for tok in tokens if tok not in STOPLIST]
    tokens = [tok for tok in tokens if tok not in SYMBOLS]
    return tokens
    
def printNMostInformative(vectorizer, clf, N):
    feature_names = vectorizer.get_feature_names()
    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))
    topClass1 = coefs_with_fns[:N]
    topClass2 = coefs_with_fns[:-(N + 1):-1]
    print("Class 1 best: ")
    for feat in topClass1:
        print(feat)
    print("Class 2 best: ")
    for feat in topClass2:
        print(feat)

vectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))

clf = LinearSVC()

pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])
```

## The data

```{python}
# data
training = pd.read_csv('../02.data/output_data/training_data_no_stem.csv')
lre_data = pd.read_csv('../02.data/output_data/test_data_LRE_no_stem.csv')

training_all = pd.concat([training,lre_data])

train_text = training_all['text'].tolist()
train_labels = training_all['class'].tolist()

X_train, X_test, y_train, y_test = train_test_split(train_text, train_labels, test_size=0.3)

# train
pipe.fit(X_train, y_train)
```

```{python}
prediction = pipe.predict(X_test)

print("accuracy:", accuracy_score(y_test, prediction))
print("precision:",metrics.precision_score(y_test, prediction))
print("recall:",metrics.recall_score(y_test, prediction))

print("Top 10 features used to predict: ")

printNMostInformative(vectorizer, clf, 10)
```

## Predict new data

```{python}
new_data = pd.read_csv('../02.data/output_data/test_data_new_no_stem.csv')
new_text = new_data['text'].tolist()
new_labels = new_data['class'].tolist()

prediction_new = pipe.predict(new_text)

print("accuracy:", accuracy_score(new_labels, prediction_new))
print("precision:",metrics.precision_score(new_labels, prediction_new))
print("recall:",metrics.recall_score(new_labels, prediction_new))

#add it to the dataframe
new_data['prediction'] = prediction_new

new_data.to_csv(r'spacy_svc_predicted_new.csv')
```
