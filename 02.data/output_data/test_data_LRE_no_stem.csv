"KEYWORDS","TITLE","ABSTRACT","JOURNAL","YEAR","code","text","class"
"KeywordsWordNet Computational lexicography Acquisition of lexical information Computational terminology Linguistic resources Ontologies ","building the galician wordnet methods and applications","This paper presents the different methodologies and resources used to build Galnet, the Galician version of WordNet. It reviews the different extraction processes and the lexicographical and textual sources used to develop this resource, and describes some of its applications in ontology research and terminology processing.","Language Resources and Evaluation",2018,"Yes"," wordnet computational lexicography acquisition lexical information computational terminology linguistic resources ontologies building galician wordnet methods applications paper presents methodologies resources build galnet galician version wordnet reviews extraction processes lexicographical textual sources develop resource describes applications ontology research terminology processing",1
"KeywordsNews corpus Sentiment analysis Lexicon Annotated corpus Corpus linguistics Web-crawling Word list AFINN Slovene Machine learning Document classification Monitoring sentiment dynamics ","annotated news corpora and a lexicon for sentiment analysis in slovene","In this study, we introduce Slovene web-crawled news corpora with sentiment annotation on three levels of granularity: sentence, paragraph and document levels. We describe the methodology and tools that were required for their construction. The corpora contain more than 250,000 documents with political, business, economic and financial content from five Slovene media resources on the web. More than 10,000 of them were manually annotated as negative, neutral or positive. All corpora are publicly available under a Creative Commons copyright license. We used the annotated documents to construct a Slovene sentiment lexicon, which is the first of its kind for Slovene, and to assess the sentiment classification approaches used. The constructed corpora were also utilised to monitor within-the-document sentiment dynamics, its changes over time and relations with news topics. We show that sentiment is, on average, more explicit at the beginning of documents, and it loses sharpness towards the end of documents.","Language Resources and Evaluation",2018,"No"," news corpus sentiment analysis lexicon annotated corpus corpus linguistics web crawling word list afinn slovene machine learning document classification monitoring sentiment dynamics annotated news corpora lexicon sentiment analysis slovene study introduce slovene web crawled news corpora sentiment annotation levels granularity sentence paragraph document levels describe methodology tools required construction corpora documents political business economic financial content slovene media resources web manually annotated negative neutral positive corpora publicly creative commons copyright license annotated documents construct slovene sentiment lexicon kind slovene assess sentiment classification approaches constructed corpora utilised monitor document sentiment dynamics time relations news topics show sentiment average explicit beginning documents loses sharpness end documents",0
"KeywordsMachine translation evaluation Error identification and classification Two-way ANOVA Linear regression Ensemble techniques ","factor based evaluation for english to hindi mt outputs","Design and implementation of automatic evaluation methods is an integral part of any scientific research in accelerating the development cycle of the output. This is no less true for automatic machine translation (MT) systems. However, no such global and systematic scheme exists for evaluation of performance of an MT system. The existing evaluation metrics, such as BLEU, METEOR, TER, although used extensively in literature have faced a lot of criticism from users. Moreover, performance of these metrics often varies with the pair of languages under consideration. The above observation is no less pertinent with respect to translations involving languages of the Indian subcontinent. This study aims at developing an evaluation metric for English to Hindi MT outputs. As a part of this process, a set of probable errors have been identified manually as well as automatically. Linear regression has been used for computing weight/penalty for each error, while taking human evaluations into consideration. A sentence score is computed as the weighted sum of the errors. A set of 126 models has been built using different single classifiers and ensemble of classifiers in order to find the most suitable model for allocating appropriate weight/penalty for each error. The outputs of the models have been compared with the state-of-the-art evaluation metrics. The models developed for manually identified errors correlate well with manual evaluation scores, whereas the models for the automatically identified errors have low correlation with the manual scores. This indicates the need for further improvement and development of sophisticated linguistic tools for automatic identification and extraction of errors. Although many automatic machine translation tools are being developed for many different language pairs, there is no such generalized scheme that would lead to designing meaningful metrics for their evaluation. The proposed scheme should help in developing such metrics for different language pairs in the coming days.","Language Resources and Evaluation",2018,"No"," machine translation evaluation error identification classification anova linear regression ensemble techniques factor based evaluation english hindi mt outputs design implementation automatic evaluation methods integral part scientific research accelerating development cycle output true automatic machine translation mt systems global systematic scheme exists evaluation performance mt system existing evaluation metrics bleu meteor ter extensively literature faced lot criticism users performance metrics varies pair languages consideration observation pertinent respect translations involving languages indian subcontinent study aims developing evaluation metric english hindi mt outputs part process set probable errors identified manually automatically linear regression computing weightpenalty error taking human evaluations consideration sentence score computed weighted sum errors set models built single classifiers ensemble classifiers order find suitable model allocating weightpenalty error outputs models compared state art evaluation metrics models developed manually identified errors correlate manual evaluation scores models automatically identified errors low correlation manual scores improvement development sophisticated linguistic tools automatic identification extraction errors automatic machine translation tools developed language pairs generalized scheme lead designing meaningful metrics evaluation proposed scheme developing metrics language pairs coming days",0
"KeywordsLexical resources Lexical semantics Common sense knowledge Vector representation Concept similarity NLP ","cover a linguistic resource combining common sense and lexicographic information","Lexical resources are fundamental to tackle many tasks that are central to present and prospective research in Text Mining, Information Retrieval, and connected to Natural Language Processing. In this article we introduce COVER, a novel lexical resource, along with COVERAGE, the algorithm devised to build it. In order to describe concepts, COVER proposes a compact vectorial representation that combines the lexicographic precision characterizing BabelNet and the rich common-sense knowledge featuring ConceptNet. We propose COVER as a reliable and mature resource, that has been employed in as diverse tasks as conceptual categorization, keywords extraction, and conceptual similarity. The experimental assessment is performed on the last task: we report and discuss the obtained results, pointing out future improvements. We conclude that COVER can be directly exploited to build applications, and coupled with existing resources, as well.","Language Resources and Evaluation",2018,"No"," lexical resources lexical semantics common sense knowledge vector representation concept similarity nlp cover linguistic resource combining common sense lexicographic information lexical resources fundamental tackle tasks central present prospective research text mining information retrieval connected natural language processing article introduce cover lexical resource coverage algorithm devised build order describe concepts cover proposes compact vectorial representation combines lexicographic precision characterizing babelnet rich common sense knowledge featuring conceptnet propose cover reliable mature resource employed diverse tasks conceptual categorization keywords extraction conceptual similarity experimental assessment performed task report discuss obtained results pointing future improvements conclude cover directly exploited build applications coupled existing resources ",0
"KeywordsVerbNet Multilingual NLP Levin verb classes Lexical-semantic classification ","investigating the cross lingual translatability of verbnet style classification","VerbNet—the most extensive online verb lexicon currently available for English—has proved useful in supporting a variety of NLP tasks. However, its exploitation in multilingual NLP has been limited by the fact that such classifications are available for few languages only. Since manual development of VerbNet is a major undertaking, researchers have recently translated VerbNet classes from English to other languages. However, no systematic investigation has been conducted into the applicability and accuracy of such a translation approach across different, typologically diverse languages. Our study is aimed at filling this gap. We develop a systematic method for translation of VerbNet classes from English to other languages which we first apply to Polish and subsequently to Croatian, Mandarin, Japanese, Italian, and Finnish. Our results on Polish demonstrate high translatability with all the classes (96% of English member verbs successfully translated into Polish) and strong inter-annotator agreement, revealing a promising degree of overlap in the resultant classifications. The results on other languages are equally promising. This demonstrates that VerbNet classes have strong cross-lingual potential and the proposed method could be applied to obtain gold standards for automatic verb classification in different languages. We make our annotation guidelines and the six language-specific verb classifications available with this paper.","Language Resources and Evaluation",2018,"Yes"," verbnet multilingual nlp levin verb classes lexical semantic classification investigating cross lingual translatability verbnet style classification verbnet extensive online verb lexicon english proved supporting variety nlp tasks exploitation multilingual nlp limited fact classifications languages manual development verbnet major undertaking researchers recently translated verbnet classes english languages systematic investigation conducted applicability accuracy translation approach typologically diverse languages study aimed filling gap develop systematic method translation verbnet classes english languages apply polish subsequently croatian mandarin japanese italian finnish results polish demonstrate high translatability classes english member verbs successfully translated polish strong inter annotator agreement revealing promising degree overlap resultant classifications results languages equally promising demonstrates verbnet classes strong cross lingual potential proposed method applied obtain gold standards automatic verb classification languages make annotation guidelines language specific verb classifications paper",1
"KeywordsHistorical corpus Corpus annotation Morphological analysis PoS tagging Middle Hungarian Old Hungarian Corpus query tool ","creation of an annotated corpus of old and middle hungarian court records and private correspondence","
The paper introduces a novel annotated corpus of Old and Middle Hungarian (16–18 century), the texts of which were selected in order to approximate the vernacular of the given historical periods as closely as possible. The corpus consists of testimonies of witnesses in trials and samples of private correspondence. The texts are not only analyzed morphologically, but each file contains metadata that would also facilitate sociolinguistic research. The texts were segmented into clauses, manually normalized and morphosyntactically annotated using an annotation system consisting of the PurePos PoS tagger and the Hungarian morphological analyzer HuMor originally developed for Modern Hungarian but adapted to analyze Old and Middle Hungarian morphological constructions. The automatically disambiguated morphological annotation was manually checked and corrected using an easy-to-use web-based manual disambiguation interface. The normalization process and the manual validation of the annotation required extensive teamwork and provided continuous feedback for the refinement of the computational morphology and iterative retraining of the statistical models of the tagger. The paper discusses some of the typical problems that occurred during the normalization procedure and their tentative solutions. Besides, we also describe the automatic annotation tools, the process of semi-automatic disambiguation, and the query interface, a special function of which also makes correction of the annotation possible. Displaying the original, the normalized and the parsed versions of the selected texts, the beta version of the first fully normalized and annotated historical corpus of Hungarian is freely accessible at the address http://tmk.nytud.hu/.","Language Resources and Evaluation",2018,"Yes"," historical corpus corpus annotation morphological analysis pos tagging middle hungarian hungarian corpus query tool creation annotated corpus middle hungarian court records private correspondence paper introduces annotated corpus middle hungarian century texts selected order approximate vernacular historical periods closely corpus consists testimonies witnesses trials samples private correspondence texts analyzed morphologically file metadata facilitate sociolinguistic research texts segmented clauses manually normalized morphosyntactically annotated annotation system consisting purepos pos tagger hungarian morphological analyzer humor originally developed modern hungarian adapted analyze middle hungarian morphological constructions automatically disambiguated morphological annotation manually checked corrected easy web based manual disambiguation interface normalization process manual validation annotation required extensive teamwork provided continuous feedback refinement computational morphology iterative retraining statistical models tagger paper discusses typical problems occurred normalization procedure tentative solutions describe automatic annotation tools process semi automatic disambiguation query interface special function makes correction annotation displaying original normalized parsed versions selected texts beta version fully normalized annotated historical corpus hungarian freely accessible address httptmknytudhu",1
"KeywordsUser generated content Slovene language Corpora Manually annotated datasets Text normalisation ","the janes project language resources and tools for slovene user generated content","The paper presents the results of the Janes project, which aimed to develop language resources and tools for Slovene user generated content. The paper first describes the 200 million word Janes corpus, containing tweets, forum posts, news comments, user and talk pages from Wikipedia, and blogs and blog comments, where each text is accompanied by rich metadata. The developed processing tools for Slovene user generated content are presented next, which include a tokeniser, word-normaliser, part-of-speech tagger and lemmatiser, and a named entity recogniser. A set of manually annotated datasets was also produced, both for tool training as well as for linguistic research.
 The developed resources and tools are made publicly available under Creative Commons licences in the repository of the CLARIN.SI research infrastructure and on GitHub, while the corpora are also available through the CLARIN.SI concordancers.
","Language Resources and Evaluation",2018,"Yes"," user generated content slovene language corpora manually annotated datasets text normalisation janes project language resources tools slovene user generated content paper presents results janes project aimed develop language resources tools slovene user generated content paper describes million word janes corpus tweets forum posts news comments user talk pages wikipedia blogs blog comments text accompanied rich metadata developed processing tools slovene user generated content presented include tokeniser word normaliser part speech tagger lemmatiser named entity recogniser set manually annotated datasets produced tool training linguistic research developed resources tools made publicly creative commons licences repository clarinsi research infrastructure github corpora clarinsi concordancers ",1
"KeywordsVerbal irony Social media Automatic irony detection Machine learning ","exploring the fine grained analysis and automatic detection of irony on twitter","To push the state of the art in text mining applications, research in natural language processing has increasingly been investigating automatic irony detection, but manually annotated irony corpora are scarce. We present the construction of a manually annotated irony corpus based on a fine-grained annotation scheme that allows for identification of different types of irony. We conduct a series of binary classification experiments for automatic irony recognition using a support vector machine (SVM)  that exploits a varied feature set and compare this method to a deep learning approach that is based on an LSTM network and (pre-trained) word embeddings. Evaluation on a held-out corpus shows that the SVM model outperforms the neural network approach and benefits from combining lexical, semantic and syntactic information sources. A qualitative analysis of the classification output reveals that the classifier performance may be further enhanced by integrating implicit sentiment information and context- and user-based features.","Language Resources and Evaluation",2018,"No"," verbal irony social media automatic irony detection machine learning exploring fine grained analysis automatic detection irony twitter push state art text mining applications research natural language processing increasingly investigating automatic irony detection manually annotated irony corpora scarce present construction manually annotated irony corpus based fine grained annotation scheme identification types irony conduct series binary classification experiments automatic irony recognition support vector machine svm exploits varied feature set compare method deep learning approach based lstm network pre trained word embeddings evaluation held corpus shows svm model outperforms neural network approach benefits combining lexical semantic syntactic information sources qualitative analysis classification output reveals classifier performance enhanced integrating implicit sentiment information context user based features",0
"KeywordsRST Signalling Corpus RST Discourse Treebank Coherence relations Rhetorical Structure Theory Signals Discourse markers ","rst signalling corpus a corpus of signals of coherence relations","We present the RST Signalling Corpus (Das et al. in RST signalling corpus, LDC2015T10. https://catalog.ldc.upenn.edu/LDC2015T10, 2015), a corpus annotated for signals of coherence relations. The corpus is developed over the RST Discourse Treebank (Carlson et al. in RST Discourse Treebank, LDC2002T07. https://catalog.ldc.upenn.edu/LDC2002T07, 2002) which is annotated for coherence relations. In the RST Signalling Corpus, these relations are further annotated with signalling information. The corpus includes annotation not only for discourse markers which are considered to be the most typical (or sometimes the only type of) signals in discourse, but also for a wide array of other signals such as reference, lexical, semantic, syntactic, graphical and genre features as potential indicators of coherence relations. We describe the research underlying the development of the corpus and the annotation process, and provide details of the corpus. We also present the results of an inter-annotator agreement study, illustrating the validity and reproducibility of the annotation. The corpus is available through the Linguistic Data Consortium, and can be used to investigate the psycholinguistic mechanisms behind the interpretation of relations through signalling, and also to develop discourse-specific computational systems such as discourse parsing applications.","Language Resources and Evaluation",2018,"Yes"," rst signalling corpus rst discourse treebank coherence relations rhetorical structure theory signals discourse markers rst signalling corpus corpus signals coherence relations present rst signalling corpus das al rst signalling corpus ldct httpscatalogldcupennldct corpus annotated signals coherence relations corpus developed rst discourse treebank carlson al rst discourse treebank ldct httpscatalogldcupennldct annotated coherence relations rst signalling corpus relations annotated signalling information corpus includes annotation discourse markers considered typical type signals discourse wide array signals reference lexical semantic syntactic graphical genre features potential indicators coherence relations describe research underlying development corpus annotation process provide details corpus present results inter annotator agreement study illustrating validity reproducibility annotation corpus linguistic data consortium investigate psycholinguistic mechanisms interpretation relations signalling develop discourse specific computational systems discourse parsing applications",1
"KeywordsSemantic annotations Personal health information Inter-annotator agreement Clinical narrative ","a french clinical corpus with comprehensive semantic annotations development of the medical entity and relation limsi annotated text corpus merlot","Quality annotated resources are essential for Natural Language Processing. The objective of this work is to present a corpus of clinical narratives in French annotated for linguistic, semantic and structural information, aimed at clinical information extraction. Six annotators contributed to the corpus annotation, using a comprehensive annotation scheme covering 21 entities, 11 attributes and 37 relations. All annotators trained on a small, common portion of the corpus before proceeding independently. An automatic tool was used to produce entity and attribute pre-annotations. About a tenth of the corpus was doubly annotated and annotation differences were resolved in consensus meetings. To ensure annotation consistency throughout the corpus, we devised harmonization tools to automatically identify annotation differences to be addressed to improve the overall corpus quality. The annotation project spanned over 24 months and resulted in a corpus comprising 500 documents (148,476 tokens) annotated with 44,740 entities and 26,478 relations. The average inter-annotator agreement is 0.793 F-measure for entities and 0.789 for relations. The performance of the pre-annotation tool for entities reached 0.814 F-measure when sufficient training data was available. The performance of our entity pre-annotation tool shows the value of the corpus to build and evaluate information extraction methods. In addition, we introduced harmonization methods that further improved the quality of annotations in the corpus.","Language Resources and Evaluation",2018,"Yes"," semantic annotations personal health information inter annotator agreement clinical narrative french clinical corpus comprehensive semantic annotations development medical entity relation limsi annotated text corpus merlot quality annotated resources essential natural language processing objective work present corpus clinical narratives french annotated linguistic semantic structural information aimed clinical information extraction annotators contributed corpus annotation comprehensive annotation scheme covering entities attributes relations annotators trained small common portion corpus proceeding independently automatic tool produce entity attribute pre annotations tenth corpus doubly annotated annotation differences resolved consensus meetings ensure annotation consistency corpus devised harmonization tools automatically identify annotation differences addressed improve corpus quality annotation project spanned months resulted corpus comprising documents tokens annotated entities relations average inter annotator agreement measure entities relations performance pre annotation tool entities reached measure sufficient training data performance entity pre annotation tool shows corpus build evaluate information extraction methods addition introduced harmonization methods improved quality annotations corpus",1
"KeywordsLexicon PropBank/VerbNet Semantic roles Predicate labelling Valence ","how the corpus based basque verb index lexicon was built","This article describes the method used to build the Basque Verb Index (BVI), a corpus-based lexicon. The BVI is the result of semiautomatic annotation of the EPEC corpus with verb predicate information, following the PropBank-VerbNet model. The method presented is the product of a deep study of the syntactic–semantic behaviour of verbs in EPEC-RolSem (the EPEC corpus tagged with verb predicate information). During the process of annotating EPEC-RolSem, we have identified and stored in the BVI lexicon the different role-patterns associated with all verbs appearing in the corpus. In addition, each entry in the BVI is linked to the corresponding verb entry in well-known resources such as PropBank, VerbNet, WordNet and FrameNet. We have also implemented a tool called e-ROLda to facilitate the process of looking up verb patterns in the BVI and examples in EPEC-RolSem as a basis for future studies.","Language Resources and Evaluation",2018,"Yes"," lexicon propbankverbnet semantic roles predicate labelling valence corpus based basque verb index lexicon built article describes method build basque verb index bvi corpus based lexicon bvi result semiautomatic annotation epec corpus verb predicate information propbank verbnet model method presented product deep study syntactic semantic behaviour verbs epec rolsem epec corpus tagged verb predicate information process annotating epec rolsem identified stored bvi lexicon role patterns verbs appearing corpus addition entry bvi linked verb entry resources propbank verbnet wordnet framenet implemented tool called rolda facilitate process verb patterns bvi examples epec rolsem basis future studies",1
"KeywordsText simplification Monolingual parallel corpora Annotation scheme Basque ","the corpus of basque simplified texts cbst","In this paper we present the corpus of Basque simplified texts. This corpus compiles 227 original sentences of science popularisation domain and two simplified versions of each sentence. The simplified versions have been created following different approaches: the structural, by a court translator who considers easy-to-read guidelines and the intuitive, by a teacher based on her experience. The aim of this corpus is to make a comparative analysis of simplified text. To that end, we also present the annotation scheme we have created to annotate the corpus. The annotation scheme is divided into eight macro-operations: delete, merge, split, transformation, insert, reordering, no operation and other. These macro-operations can be classified into different operations. We also relate our work and results to other languages. This corpus will be used to corroborate the decisions taken and to improve the design of the automatic text simplification system for Basque.
","Language Resources and Evaluation",2018,"Yes"," text simplification monolingual parallel corpora annotation scheme basque corpus basque simplified texts cbst paper present corpus basque simplified texts corpus compiles original sentences science popularisation domain simplified versions sentence simplified versions created approaches structural court translator considers easy read guidelines intuitive teacher based experience aim corpus make comparative analysis simplified text end present annotation scheme created annotate corpus annotation scheme divided macro operations delete merge split transformation insert reordering operation macro operations classified operations relate work results languages corpus corroborate decisions improve design automatic text simplification system basque ",1
"KeywordsComputational political sciences Computational social science Language technology Natural language processing Parliamentary proceedings ","the talk of norway a richly annotated corpus of the norwegian parliament 19982016","In this work we present the Talk of Norway (ToN) data set, a collection of Norwegian Parliament speeches from 1998 to 2016.
 Every speech is richly annotated with metadata harvested from different sources, and augmented with language type, sentence, token, lemma, part-of-speech, and morphological feature annotations. We also present a pilot study on party classification in the Norwegian Parliament, carried out in the context of a cross-faculty collaboration involving researchers from both Political Science and Computer Science. Our initial experiments demonstrate how the linguistic and institutional annotations in ToN can be used to gather insights on how different aspects of the political process affect classification.
","Language Resources and Evaluation",2018,"Yes"," computational political sciences computational social science language technology natural language processing parliamentary proceedings talk norway richly annotated corpus norwegian parliament work present talk norway ton data set collection norwegian parliament speeches speech richly annotated metadata harvested sources augmented language type sentence token lemma part speech morphological feature annotations present pilot study party classification norwegian parliament carried context cross faculty collaboration involving researchers political science computer science initial experiments demonstrate linguistic institutional annotations ton gather insights aspects political process affect classification ",1
"KeywordsDistributional semantic models Word embedding Word2vec Persian ","the impact of corpus domain on word representation a study on persian word embeddings","Word embedding, has been a great success story for natural language processing in recent years. The main purpose of this approach is providing a vector representation of words based on neural network language modeling. Using a large training corpus, the model most learns from co-occurrences of words, namely Skip-gram model, and capture semantic features of words. Moreover, adding the recently introduced character embedding model to the objective function, the model can also focus on morphological features of words. In this paper, we study the impact of training corpus on the results of word embedding and show how the genre of training data affects the type of information captured by word embedding models. We perform our experiments on the Persian language. In line of our experiments, providing two well-known evaluation datasets for Persian, namely Google semantic/syntactic analogy and Wordsim353, is also part of the contribution of this paper. The experiments include computation of word embedding from various public Persian corpora with different genres and sizes while considering comprehensive lexical and semantic comparison between them. We identify words whose usages differ between these datasets resulted totally different vector representation which ends to significant impact on different domains in which the results vary up to 9% on Google analogy and up to 6% on Wordsim353. The resulted word embedding for each of the individual corpora as well as their combinations will be publicly available for any further research based on word embedding for Persian.","Language Resources and Evaluation",2018,"No"," distributional semantic models word embedding wordvec persian impact corpus domain word representation study persian word embeddings word embedding great success story natural language processing recent years main purpose approach providing vector representation words based neural network language modeling large training corpus model learns occurrences words skip gram model capture semantic features words adding recently introduced character embedding model objective function model focus morphological features words paper study impact training corpus results word embedding show genre training data affects type information captured word embedding models perform experiments persian language line experiments providing evaluation datasets persian google semanticsyntactic analogy wordsim part contribution paper experiments include computation word embedding public persian corpora genres sizes comprehensive lexical semantic comparison identify words usages differ datasets resulted totally vector representation ends significant impact domains results vary google analogy wordsim resulted word embedding individual corpora combinations publicly research based word embedding persian",0
"KeywordsAnnotation of negation Scope of negation Polarity annotation Sentiment analysis ","sfu reviewsp neg a spanish corpus annotated with negation for sentiment analysis a typology of negation patterns","In this paper, we present SFU ReviewSP-NEG, the first Spanish corpus annotated with negation with a wide coverage freely available. We describe the methodology applied in the annotation of the corpus including the tagset, the linguistic criteria and the inter-annotator agreement tests.
 We also include a complete typology of negation patterns in Spanish. This typology has the advantage that it is easy to express in terms of a tagset for corpus annotation: the types are clearly defined, which avoids ambiguity in the annotation process, and they provide wide coverage (i.e. they resolved all the cases occurring in the corpus). We use the SFU ReviewSP as a base in order to make the annotations. 
The corpus consists of 400 reviews, 221,866 words and 9455 sentences, out of which 3022 sentences contain at least one negation structure.","Language Resources and Evaluation",2018,"Yes"," annotation negation scope negation polarity annotation sentiment analysis sfu reviewsp neg spanish corpus annotated negation sentiment analysis typology negation patterns paper present sfu reviewsp neg spanish corpus annotated negation wide coverage freely describe methodology applied annotation corpus including tagset linguistic criteria inter annotator agreement tests include complete typology negation patterns spanish typology advantage easy express terms tagset corpus annotation types defined avoids ambiguity annotation process provide wide coverage resolved cases occurring corpus sfu reviewsp base order make annotations corpus consists reviews words sentences sentences negation structure",1
"KeywordsAuthorship attribution Logistic regression Bray–Curtis distance Gallus Anonymous Monk of Lido Medieval Europe ","computational authorship attribution in medieval latin corpora the case of the monk of lido ca 110108 and gallus anonymous ca 111317","This paper applies computational methods of authorship attribution to shed light on a still open question concerning two Latin works of the twelfth century: are the anonymous authors of the Translatio s. Nicolai (ca. 1101–1108) and the Gesta principum polonorum (ca. 1113–1117) one and the same person? The Translatio was written by the so-called Monk of Lido and describes Venice’s role in the First Crusade. The Gesta were written by the so-called Gallus Anonymous and contain a panegyric of the contemporary Polish ruler, Bolesław III the Wry-Mouthed (r. 1102–1138). This study attributes authorship to these works within four corpora of Latin texts composed between the tenth and twelfth centuries, each with between 39 and 116 texts written by between 15 and 22 different authors. The goal of including four corpora is to see how robust the similarity between the target texts is to changes in text length, genre, and class balance in the corpora. In each corpus, nine different distance metrics and one machine-learning algorithm are used to classify the authors of the Translatio and Gesta. I conclude that it is highly likely that Gallus and Monk were indeed one and same anonymous author, and highlight the effectiveness of the Bray–Curtis distance and logistic regression as methods of attribution.
","Language Resources and Evaluation",2018,"No"," authorship attribution logistic regression bray curtis distance gallus anonymous monk lido medieval europe computational authorship attribution medieval latin corpora case monk lido ca gallus anonymous ca paper applies computational methods authorship attribution shed light open question latin works twelfth century anonymous authors translatio nicolai ca gesta principum polonorum ca person translatio written called monk lido describes venice role crusade gesta written called gallus anonymous panegyric contemporary polish ruler boles aw iii wry mouthed study attributes authorship works corpora latin texts composed tenth twelfth centuries texts written authors goal including corpora robust similarity target texts text length genre class balance corpora corpus distance metrics machine learning algorithm classify authors translatio gesta conclude highly gallus monk anonymous author highlight effectiveness bray curtis distance logistic regression methods attribution ",0
"KeywordsTerminology extraction Statistical machine translation Phrase-based statistical machine translation Log-likelihood Dice coefficient ","termfinder log likelihood comparison and phrase based statistical machine translation models for bilingual terminology extraction","Bilingual termbanks are important for many natural language processing applications, especially in translation workflows in industrial settings. In this paper, we apply a log-likelihood comparison method to extract monolingual terminology from the source and target sides of a parallel corpus. The initial candidate terminology list is prepared by taking all arbitrary n-gram word sequences from the corpus. Then, a well-known statistical measure (the Dice coefficient) is employed in order to remove any multi-word terms with weak associations from the candidate term list. Thereafter, the log-likelihood comparison method is applied to rank the phrasal candidate term list. Then, using a phrase-based statistical machine translation model, we create a bilingual terminology with the extracted monolingual term lists. We integrate an external knowledge source—the Wikipedia cross-language link databases—into the terminology extraction (TE) model to assist two processes: (a) the ranking of the extracted terminology list, and (b) the selection of appropriate target terms for a source term. First, we report the performance of our monolingual TE model compared to a number of the state-of-the-art TE models on English-to-Turkish and English-to-Hindi data sets. Then, we evaluate our novel bilingual TE model on an English-to-Turkish data set, and report the automatic evaluation results. We also manually evaluate our novel TE model on English-to-Spanish and English-to-Hindi data sets, and observe excellent performance for all domains.","Language Resources and Evaluation",2018,"No"," terminology extraction statistical machine translation phrase based statistical machine translation log likelihood dice coefficient termfinder log likelihood comparison phrase based statistical machine translation models bilingual terminology extraction bilingual termbanks important natural language processing applications translation workflows industrial settings paper apply log likelihood comparison method extract monolingual terminology source target sides parallel corpus initial candidate terminology list prepared taking arbitrary gram word sequences corpus statistical measure dice coefficient employed order remove multi word terms weak associations candidate term list log likelihood comparison method applied rank phrasal candidate term list phrase based statistical machine translation model create bilingual terminology extracted monolingual term lists integrate external knowledge source wikipedia cross language link databases terminology extraction te model assist processes ranking extracted terminology list selection target terms source term report performance monolingual te model compared number state art te models english turkish english hindi data sets evaluate bilingual te model english turkish data set report automatic evaluation results manually evaluate te model english spanish english hindi data sets observe excellent performance domains",0
"KeywordsSentiment analysis Twitter Machine-learning Corpora for the Spanish language ","spanish sentiment analysis in twitter at the tass workshop","This paper describes a support vector machine-based approach to different tasks related to sentiment analysis in Twitter for Spanish. We focus on parameter optimization of the models and the combination of several models by means of voting techniques. We evaluate the proposed approach in all the tasks that were defined in the five editions of the TASS workshop, between 2012 and 2016. TASS has become a framework for sentiment analysis tasks that are focused on the Spanish language. We describe our participation in this competition and the results achieved, and then we provide an analysis of and comparison with the best approaches of the teams who participated in all the tasks defined in the TASS workshops. To our knowledge, our results exceed those published to date in the sentiment analysis tasks of the TASS workshops.","Language Resources and Evaluation",2018,"No"," sentiment analysis twitter machine learning corpora spanish language spanish sentiment analysis twitter tass workshop paper describes support vector machine based approach tasks related sentiment analysis twitter spanish focus parameter optimization models combination models means voting techniques evaluate proposed approach tasks defined editions tass workshop tass framework sentiment analysis tasks focused spanish language describe participation competition results achieved provide analysis comparison approaches teams participated tasks defined tass workshops knowledge results exceed published date sentiment analysis tasks tass workshops",0
"KeywordsUnder-resourced language Rule-based Grapheme-to-phoneme conversion Automatic speech recognition Tunisian dialect ","automatic speech recognition system for tunisian dialect","Although Modern Standard Arabic is taught in schools and used in written communication and TV/radio broadcasts, all informal communication is typically carried out in dialectal Arabic.
 In this work, we focus on the design of speech tools and resources required for the development of an Automatic Speech Recognition system for the Tunisian dialect.
 The development of such a system faces the challenges of the lack of annotated resources and tools, apart from the lack of standardization at all linguistic levels (phonological, morphological, syntactic and lexical) together with the mispronunciation dictionary needed for ASR development.
 In this paper, we present a historical overview of the Tunisian dialect and its linguistic characteristics. We also describe and evaluate our rule-based phonetic tool. Next, we go deeper into the details of Tunisian dialect corpus creation. This corpus is finally approved and used to build the first ASR system for Tunisian dialect with a Word Error Rate of 22.6%.
","Language Resources and Evaluation",2018,"No"," resourced language rule based grapheme phoneme conversion automatic speech recognition tunisian dialect automatic speech recognition system tunisian dialect modern standard arabic taught schools written communication tvradio broadcasts informal communication typically carried dialectal arabic work focus design speech tools resources required development automatic speech recognition system tunisian dialect development system faces challenges lack annotated resources tools lack standardization linguistic levels phonological morphological syntactic lexical mispronunciation dictionary needed asr development paper present historical overview tunisian dialect linguistic characteristics describe evaluate rule based phonetic tool deeper details tunisian dialect corpus creation corpus finally approved build asr system tunisian dialect word error rate ",0
"KeywordsText summarization Evaluation Content evaluation Readability Task-based evaluation ","the challenging task of summary evaluation an overview","Evaluation is crucial in the research and development of automatic summarization applications, in order to determine the appropriateness of a summary based on different criteria, such as the content it contains, and the way it is presented.
 To perform an adequate evaluation is of great relevance to ensure that automatic summaries can be useful for the context and/or application they are generated for.
 To this end, researchers must be aware of the evaluation metrics, approaches, and datasets that are available, in order to decide which of them would be the most suitable to use, or to be able to propose new ones, overcoming the possible limitations that existing methods may present. In this article, a critical and historical analysis of evaluation metrics, methods, and datasets for automatic summarization systems is presented, where the strengths and weaknesses of evaluation efforts are discussed and the major challenges to solve are identified. Therefore, a clear up-to-date overview of the evolution and progress of summarization evaluation is provided, giving the reader useful insights into the past, present and latest trends in the automatic evaluation of summaries.","Language Resources and Evaluation",2018,"No"," text summarization evaluation content evaluation readability task based evaluation challenging task summary evaluation overview evaluation crucial research development automatic summarization applications order determine appropriateness summary based criteria content presented perform adequate evaluation great relevance ensure automatic summaries context application generated end researchers aware evaluation metrics approaches datasets order decide suitable propose overcoming limitations existing methods present article critical historical analysis evaluation metrics methods datasets automatic summarization systems presented strengths weaknesses evaluation efforts discussed major challenges solve identified clear date overview evolution progress summarization evaluation provided giving reader insights past present latest trends automatic evaluation summaries",0
"KeywordsGeoparsing Geotagging Geocoding NER NLP NEL NED ","whats missing in geographical parsing","Geographical data can be obtained by converting place names from free-format text into geographical coordinates. The ability to geo-locate events in textual reports represents a valuable source of information in many real-world applications such as emergency responses, real-time social media geographical event analysis, understanding location instructions in auto-response systems and more. However, geoparsing is still widely regarded as a challenge because of domain language diversity, place name ambiguity, metonymic language and limited leveraging of context as we show in our analysis. Results to date, whilst promising, are on laboratory data and unlike in wider NLP are often not cross-compared. In this study, we evaluate and analyse the performance of a number of leading geoparsers on a number of corpora and highlight the challenges in detail. We also publish an automatically geotagged Wikipedia corpus to alleviate the dearth of (open source) corpora in this domain.
","Language Resources and Evaluation",2018,"Yes"," geoparsing geotagging geocoding ner nlp nel ned whats missing geographical parsing geographical data obtained converting place names free format text geographical coordinates ability geo locate events textual reports represents valuable source information real world applications emergency responses real time social media geographical event analysis understanding location instructions auto response systems geoparsing widely regarded challenge domain language diversity place ambiguity metonymic language limited leveraging context show analysis results date whilst promising laboratory data unlike wider nlp cross compared study evaluate analyse performance number leading geoparsers number corpora highlight challenges detail publish automatically geotagged wikipedia corpus alleviate dearth open source corpora domain ",1
"Keywords(Semi)-automatic annotation Gesture analysis Video analysis Hand annotation Gesture space Motion analysis ","a semi automatic annotation tool for unobtrusive gesture analysis","In a variety of research fields, including linguistics, human–computer interaction research, psychology, sociology and behavioral studies, there is a growing interest in the role of gestural behavior related to speech and other modalities. The analysis of multimodal communication requires high-quality video data and detailed annotation of the different semiotic resources under scrutiny. In the majority of cases, the annotation of hand position, hand motion, gesture type, etc. is done manually, which is a time-consuming enterprise requiring multiple annotators and substantial resources. In this paper we present a semi-automatic alternative, in which the focus lies on minimizing the manual workload while guaranteeing highly accurate annotations. First, we discuss our approach, which consists of several processing steps such as identifying the hands in images, calculating motion of the hands, segmenting the recording in gesture and non-gesture events, etc. Second, we validate our approach against existing corpora in terms of accuracy and usefulness. The proposed approach is designed to provide annotations according to the McNeill (Hand and mind: what gestures reveal about thought, University of Chicago Press, Chicago, 1992) gesture space and the output is compatible with annotation tools such as ELAN or ANVIL.","Language Resources and Evaluation",2018,"No"," semi automatic annotation gesture analysis video analysis hand annotation gesture space motion analysis semi automatic annotation tool unobtrusive gesture analysis variety research fields including linguistics human computer interaction research psychology sociology behavioral studies growing interest role gestural behavior related speech modalities analysis multimodal communication requires high quality video data detailed annotation semiotic resources scrutiny majority cases annotation hand position hand motion gesture type manually time consuming enterprise requiring multiple annotators substantial resources paper present semi automatic alternative focus lies minimizing manual workload guaranteeing highly accurate annotations discuss approach consists processing steps identifying hands images calculating motion hands segmenting recording gesture gesture events validate approach existing corpora terms accuracy usefulness proposed approach designed provide annotations mcneill hand mind gestures reveal thought university chicago press chicago gesture space output compatible annotation tools elan anvil",0
"KeywordsWord sense induction Graph clustering Pseudowords Evaluation ","a comparison of graph based word sense induction clustering algorithms in a pseudoword evaluation framework","
This article presents a comparison of different Word Sense Induction (wsi) clustering algorithms on two novel pseudoword data sets of semantic-similarity and co-occurrence-based word graphs, with a special focus on the detection of homonymic polysemy. We follow the original definition of a pseudoword as the combination of two monosemous terms and their contexts to simulate a polysemous word. The evaluation is performed comparing the algorithm’s output on a pseudoword’s ego word graph (i.e., a graph that represents the pseudoword’s context in the corpus) with the known subdivision given by the components corresponding to the monosemous source words forming the pseudoword. The main contribution of this article is to present a self-sufficient pseudoword-based evaluation framework for wsi graph-based clustering algorithms, thereby defining a new evaluation measure (top2) and a secondary clustering process (hyperclustering). To our knowledge, we are the first to conduct and discuss a large-scale systematic pseudoword evaluation targeting the induction of coarse-grained homonymous word senses across a large number of graph clustering algorithms.","Language Resources and Evaluation",2018,"No"," word sense induction graph clustering pseudowords evaluation comparison graph based word sense induction clustering algorithms pseudoword evaluation framework article presents comparison word sense induction wsi clustering algorithms pseudoword data sets semantic similarity occurrence based word graphs special focus detection homonymic polysemy follow original definition pseudoword combination monosemous terms contexts simulate polysemous word evaluation performed comparing algorithm output pseudoword ego word graph graph represents pseudoword context corpus subdivision components monosemous source words forming pseudoword main contribution article present sufficient pseudoword based evaluation framework wsi graph based clustering algorithms defining evaluation measure top secondary clustering process hyperclustering knowledge conduct discuss large scale systematic pseudoword evaluation targeting induction coarse grained homonymous word senses large number graph clustering algorithms",0
"KeywordsWeb genre identification Information retrieval Natural language processing Random feature selection ","open set evaluation of web genre identification","Web genre detection is a task that can enhance information retrieval systems by providing rich descriptions of documents and enabling more specialized queries. Most of previous studies in this field adopt the closed-set scenario where a given palette comprises all available genre labels. However this is not a realistic setup since web genres are constantly enriched with new labels and existing web genres are evolving in time. Open-set classification, where some pages used in the evaluation phase do not belong to any of the known genres, is a more realistic setup for this task. In this case, all pages not belonging to known genres can be seen as noise. This paper focuses on systematic evaluation of open-set web genre identification when the noise is either structured or unstructured. Two open-set methods combined with alternative text representation schemes and similarity measures are tested based on two benchmark corpora. Moreover, we adopt the openness test for web genre identification that enables the observation of effectiveness for a varying number of known/unknown labels.","Language Resources and Evaluation",2018,"No"," web genre identification information retrieval natural language processing random feature selection open set evaluation web genre identification web genre detection task enhance information retrieval systems providing rich descriptions documents enabling specialized queries previous studies field adopt closed set scenario palette comprises genre labels realistic setup web genres constantly enriched labels existing web genres evolving time open set classification pages evaluation phase belong genres realistic setup task case pages belonging genres noise paper focuses systematic evaluation open set web genre identification noise structured unstructured open set methods combined alternative text representation schemes similarity measures tested based benchmark corpora adopt openness test web genre identification enables observation effectiveness varying number unknown labels",0
"KeywordsCrowdsourcing Evaluation Semantic annotation Cross-language transfer ","cross language transfer of semantic annotation via targeted crowdsourcing task design and evaluation","Modern data-driven spoken language systems (SLS) require manual semantic annotation for training spoken language understanding parsers. Multilingual porting of SLS demands significant manual effort and language resources, as this manual annotation has to be replicated. Crowdsourcing is an accessible and cost-effective alternative to traditional methods of collecting and annotating data.
 The application of crowdsourcing to simple tasks has been well investigated. However, complex tasks, like cross-language semantic annotation transfer, may generate low judgment agreement and/or poor performance. The most serious issue in cross-language porting is the absence of reference annotations in the target language; thus, crowd quality control and the evaluation of the collected annotations is difficult. In this paper we investigate targeted crowdsourcing for semantic annotation transfer that delegates to crowds a complex task such as segmenting and labeling of concepts taken from a domain ontology; and evaluation using source language annotation. To test the applicability and effectiveness of the crowdsourced annotation transfer we have considered the case of close and distant language pairs: Italian–Spanish and Italian–Greek. The corpora annotated via crowdsourcing are evaluated against source and target language expert annotations. We demonstrate that the two evaluation references (source and target) highly correlate with each other; thus, drastically reduce the need for the target language reference annotations.","Language Resources and Evaluation",2018,"No"," crowdsourcing evaluation semantic annotation cross language transfer cross language transfer semantic annotation targeted crowdsourcing task design evaluation modern data driven spoken language systems sls require manual semantic annotation training spoken language understanding parsers multilingual porting sls demands significant manual effort language resources manual annotation replicated crowdsourcing accessible cost effective alternative traditional methods collecting annotating data application crowdsourcing simple tasks investigated complex tasks cross language semantic annotation transfer generate low judgment agreement poor performance issue cross language porting absence reference annotations target language crowd quality control evaluation collected annotations difficult paper investigate targeted crowdsourcing semantic annotation transfer delegates crowds complex task segmenting labeling concepts domain ontology evaluation source language annotation test applicability effectiveness crowdsourced annotation transfer considered case close distant language pairs italian spanish italian greek corpora annotated crowdsourcing evaluated source target language expert annotations demonstrate evaluation references source target highly correlate drastically reduce target language reference annotations",0
"KeywordsElectronic health records Semantic textual similarity Natural language processing Clinical semantic textual similarity resource ","medsts a resource for clinical semantic textual similarity","The adoption of electronic health records (EHRs) has enabled a wide range of applications leveraging EHR data. However, the meaningful use of EHR data largely depends on our ability to efficiently extract and consolidate information embedded in clinical text where natural language processing (NLP) techniques are essential. Semantic textual similarity (STS) that measures the semantic similarity between text snippets plays a significant role in many NLP applications. In the general NLP domain, STS shared tasks have made available a huge collection of text snippet pairs with manual annotations in various domains. In the clinical domain, STS can enable us to detect and eliminate redundant information that may lead to a reduction in cognitive burden and an improvement in the clinical decision-making process. This paper elaborates our efforts to assemble a resource for STS in the medical domain, MedSTS. It consists of a total of 174,629 sentence pairs gathered from a clinical corpus at Mayo Clinic. A subset of MedSTS (MedSTS_ann) containing 1068 sentence pairs was annotated by two medical experts with semantic similarity scores of 0–5 (low to high similarity). We further analyzed the medical concepts in the MedSTS corpus, and tested four STS systems on the MedSTS_ann corpus. In the future, we will organize a shared task by releasing the MedSTS_ann corpus to motivate the community to tackle the real world clinical problems.","Language Resources and Evaluation",2018,"Yes"," electronic health records semantic textual similarity natural language processing clinical semantic textual similarity resource medsts resource clinical semantic textual similarity adoption electronic health records ehrs enabled wide range applications leveraging ehr data meaningful ehr data largely depends ability efficiently extract consolidate information embedded clinical text natural language processing nlp techniques essential semantic textual similarity sts measures semantic similarity text snippets plays significant role nlp applications general nlp domain sts shared tasks made huge collection text snippet pairs manual annotations domains clinical domain sts enable detect eliminate redundant information lead reduction cognitive burden improvement clinical decision making process paper elaborates efforts assemble resource sts medical domain medsts consists total sentence pairs gathered clinical corpus mayo clinic subset medsts medstsann sentence pairs annotated medical experts semantic similarity scores low high similarity analyzed medical concepts medsts corpus tested sts systems medstsann corpus future organize shared task releasing medstsann corpus motivate community tackle real world clinical problems",1
"KeywordsDepression screening Depression lexicon Lexicon evaluation Lexicon expansion Text analysis Natural language processing ","evaluating and improving lexical resources for detecting signs of depression in text","While considerable attention has been given to the analysis of texts written by depressed individuals, few studies were interested in evaluating and improving lexical resources for supporting the detection of signs of depression in text.
 In this paper, we present a search-based methodology to evaluate existing depression lexica. To meet this aim, we exploit existing resources for depression and language use and we analyze which elements of the lexicon are the most effective at revealing depression symptoms. Furthermore, we propose innovative expansion strategies able to further enhance the quality of the lexica.
","Language Resources and Evaluation",2018,"No"," depression screening depression lexicon lexicon evaluation lexicon expansion text analysis natural language processing evaluating improving lexical resources detecting signs depression text considerable attention analysis texts written depressed individuals studies interested evaluating improving lexical resources supporting detection signs depression text paper present search based methodology evaluate existing depression lexica meet aim exploit existing resources depression language analyze elements lexicon effective revealing depression symptoms propose innovative expansion strategies enhance quality lexica ",0
"KeywordsSpelling correction Real-word error Context-sensitive Language model ","real word error correction with trigrams correcting multiple errors in a sentence","Spelling correction is a fundamental task in text mining. In this study, we assess the real-word error correction model proposed by Mays, Damerau and Mercer and describe several drawbacks of the model. We propose a new variation which focuses on detecting and correcting multiple real-word errors in a sentence, by manipulating a probabilistic context-free grammar to discriminate between items in the search space. We test our approach on the Wall Street Journal corpus and show that it outperforms Hirst and Budanitsky’s WordNet-based method and Wilcox-O’Hearn, Hirst, and Budanitsky’s fixed windows size method.","Language Resources and Evaluation",2018,"No"," spelling correction real word error context sensitive language model real word error correction trigrams correcting multiple errors sentence spelling correction fundamental task text mining study assess real word error correction model proposed mays damerau mercer describe drawbacks model propose variation focuses detecting correcting multiple real word errors sentence manipulating probabilistic context free grammar discriminate items search space test approach wall street journal corpus show outperforms hirst budanitsky wordnet based method wilcox hearn hirst budanitsky fixed windows size method",0
"KeywordsTreebank Dependency grammar Indo-European Greek Latin Romance Germanic Slavic Armenian ","the proiel treebank family a standard for early attestations of indo european languages","This article describes a family of dependency treebanks of early attestations of Indo-European languages originating in the parallel treebank built by the members of the project pragmatic resources in old Indo-European languages. The treebanks all share a set of open-source software tools, including a web annotation interface, and a set of annotation schemes and guidelines developed especially for the project languages. The treebanks use an enriched dependency grammar scheme complemented by detailed morphological tags, which have proved sufficient to give detailed descriptions of these richly inflected languages, and which have been easy to adapt to new languages. We describe the tools and annotation schemes and discuss some challenges posed by the various languages that have been annotated. We also discuss problems with tokenisation, sentence division and lemmatisation, commonly encountered in ancient and mediaeval texts, and challenges associated with low levels of standardisation and ongoing morphological and syntactic change.","Language Resources and Evaluation",2018,"Yes"," treebank dependency grammar indo european greek latin romance germanic slavic armenian proiel treebank family standard early attestations indo european languages article describes family dependency treebanks early attestations indo european languages originating parallel treebank built members project pragmatic resources indo european languages treebanks share set open source software tools including web annotation interface set annotation schemes guidelines developed project languages treebanks enriched dependency grammar scheme complemented detailed morphological tags proved sufficient give detailed descriptions richly inflected languages easy adapt languages describe tools annotation schemes discuss challenges posed languages annotated discuss problems tokenisation sentence division lemmatisation commonly encountered ancient mediaeval texts challenges low levels standardisation ongoing morphological syntactic change",1
"KeywordsSentiment lexicon Greek language Word embeddings Sentiment analysis Natural language processing Opinion mining Emotion analysis Sarcasm detection ","building and evaluating resources for sentiment analysis in the greek language","
Sentiment lexicons and word embeddings constitute well-established sources of information for sentiment analysis in online social media. Although their effectiveness has been demonstrated in state-of-the-art sentiment analysis and related tasks in the English language, such publicly available resources are much less developed and evaluated for the Greek language. In this paper, we tackle the problems arising when analyzing text in such an under-resourced language. We present and make publicly available a rich set of such resources, ranging from a manually annotated lexicon, to semi-supervised word embedding vectors and annotated datasets for different tasks. Our experiments using different algorithms and parameters on our resources show promising results over standard baselines; on average, we achieve a 24.9% relative improvement in F-score on the cross-domain sentiment analysis task when training the same algorithms with our resources, compared to training them on more traditional feature sources, such as n-grams. Importantly, while our resources were built with the primary focus on the cross-domain sentiment analysis task, they also show promising results in related tasks, such as emotion analysis and sarcasm detection.","Language Resources and Evaluation",2018,"Yes"," sentiment lexicon greek language word embeddings sentiment analysis natural language processing opinion mining emotion analysis sarcasm detection building evaluating resources sentiment analysis greek language sentiment lexicons word embeddings constitute established sources information sentiment analysis online social media effectiveness demonstrated state art sentiment analysis related tasks english language publicly resources developed evaluated greek language paper tackle problems arising analyzing text resourced language present make publicly rich set resources ranging manually annotated lexicon semi supervised word embedding vectors annotated datasets tasks experiments algorithms parameters resources show promising results standard baselines average achieve relative improvement score cross domain sentiment analysis task training algorithms resources compared training traditional feature sources grams importantly resources built primary focus cross domain sentiment analysis task show promising results related tasks emotion analysis sarcasm detection",1
"KeywordsPropBank Semantic role annotation Derivational morphology Turkish Crowdsourcing Semantic role labeling ","annotation of semantic roles for the turkish proposition bank","In this work, we report large-scale semantic role annotation of arguments in the Turkish dependency treebank, and present the first comprehensive Turkish semantic role labeling (SRL) resource: Turkish Proposition Bank (PropBank). We present our annotation workflow that harnesses crowd intelligence, and discuss the procedures for ensuring annotation consistency and quality control. Our discussion focuses on syntactic variations in realization of predicate-argument structures, and the large lexicon problem caused by complex derivational morphology. We describe our approach that exploits framesets of root verbs to abstract away from syntax and increase self-consistency of the Turkish PropBank. The issues that arise in the annotation of verbs derived via valency changing morphemes, verbal nominals, and nominal verbs are explored, and evaluation results for inter-annotator agreement are provided. Furthermore, semantic layer described here is aligned with universal dependency (UD) compliant treebank and released to enable more researchers to work on the problem. Finally, we use PropBank to establish a baseline score of 79.10 F1 for Turkish SRL using the mate-tool (an open-source SRL tool based on supervised machine learning) enhanced with basic morphological features. Turkish PropBank and the extended SRL system are made publicly available.
","Language Resources and Evaluation",2018,"Yes"," propbank semantic role annotation derivational morphology turkish crowdsourcing semantic role labeling annotation semantic roles turkish proposition bank work report large scale semantic role annotation arguments turkish dependency treebank present comprehensive turkish semantic role labeling srl resource turkish proposition bank propbank present annotation workflow harnesses crowd intelligence discuss procedures ensuring annotation consistency quality control discussion focuses syntactic variations realization predicate argument structures large lexicon problem caused complex derivational morphology describe approach exploits framesets root verbs abstract syntax increase consistency turkish propbank issues arise annotation verbs derived valency changing morphemes verbal nominals nominal verbs explored evaluation results inter annotator agreement provided semantic layer aligned universal dependency ud compliant treebank released enable researchers work problem finally propbank establish baseline score f turkish srl mate tool open source srl tool based supervised machine learning enhanced basic morphological features turkish propbank extended srl system made publicly ",1
"KeywordsVietnamese treebank Quality control Consistent annotation Linguistic challenges ","ensuring annotation consistency and accuracy for vietnamese treebank","Treebanks are important resources for researchers in natural language processing. They provide training and testing materials so that different algorithms can be compared. However, it is not a trivial task to construct high-quality treebanks. We have not yet had a proper treebank for such a low-resource language as Vietnamese, which has probably lowered the performance of Vietnamese language processing. We have been building a consistent and accurate Vietnamese treebank to alleviate such situations. Our treebank is annotated with three layers: word segmentation, part-of-speech tagging, and bracketing. We developed detailed annotation guidelines for each layer by presenting Vietnamese linguistic issues as well as methods of addressing them. Here, we also describe approaches to controlling annotation quality while ensuring a reasonable annotation speed. We specifically designed an appropriate annotation process and an effective process to train annotators. In addition, we implemented several support tools to improve annotation speed and to control the consistency of the treebank. The results from experiments revealed that both inter-annotator agreement and accuracy were higher than 90%, which indicated that the treebank is reliable.
","Language Resources and Evaluation",2018,"Yes"," vietnamese treebank quality control consistent annotation linguistic challenges ensuring annotation consistency accuracy vietnamese treebank treebanks important resources researchers natural language processing provide training testing materials algorithms compared trivial task construct high quality treebanks proper treebank low resource language vietnamese lowered performance vietnamese language processing building consistent accurate vietnamese treebank alleviate situations treebank annotated layers word segmentation part speech tagging bracketing developed detailed annotation guidelines layer presenting vietnamese linguistic issues methods addressing describe approaches controlling annotation quality ensuring reasonable annotation speed specifically designed annotation process effective process train annotators addition implemented support tools improve annotation speed control consistency treebank results experiments revealed inter annotator agreement accuracy higher treebank reliable ",1
"KeywordsSlang words Sentiment lexicon Social media Sentiment classification ","slangsd building expanding and using a sentiment dictionary of slang words for short text sentiment classification","Sentiment information about social media posts is increasingly considered an important resource for customer segmentation, market understanding, and tackling other socio-economic issues.
 However, sentiment in social media is difficult to measure since user-generated content is usually short and informal. Although many traditional sentiment analysis methods have been proposed, identifying slang sentiment words remains a challenging task for practitioners. Though some slang words are available in existing sentiment lexicons, with new slang being generated with emerging memes, a dedicated lexicon will be useful for researchers and practitioners. To this end, we propose to build a slang sentiment dictionary to aid sentiment analysis.
 It is laborious and time-consuming to collect a comprehensive list of slang words and label the sentiment polarity. We present an approach to leverage web resources to construct a Slang Sentiment Dictionary (SlangSD) that is easy to expand. SlangSD is publicly available for research purposes. We empirically show the advantages of using SlangSD, the newly-built slang sentiment word dictionary for sentiment classification, and provide examples demonstrating its ease of use with a sentiment analysis system.","Language Resources and Evaluation",2018,"Yes"," slang words sentiment lexicon social media sentiment classification slangsd building expanding sentiment dictionary slang words short text sentiment classification sentiment information social media posts increasingly considered important resource customer segmentation market understanding tackling socio economic issues sentiment social media difficult measure user generated content short informal traditional sentiment analysis methods proposed identifying slang sentiment words remains challenging task practitioners slang words existing sentiment lexicons slang generated emerging memes dedicated lexicon researchers practitioners end propose build slang sentiment dictionary aid sentiment analysis laborious time consuming collect comprehensive list slang words label sentiment polarity present approach leverage web resources construct slang sentiment dictionary slangsd easy expand slangsd publicly research purposes empirically show advantages slangsd newly built slang sentiment word dictionary sentiment classification provide examples demonstrating ease sentiment analysis system",1
"KeywordsComputational paralinguistics Affective computing Political speech Machine learning Charisma Speaker ability ","a longitudinal database of irish political speech with annotations of speaker ability","This paper presents the Irish Political Speech Database, an English-language database collected from Irish political recordings. The database is collected with automated indexing and content retrieval in mind, and thus is gathered from real-world recordings (such as television interviews and election rallies) which represent the nature and quality of recordings which will be encountered in practical applications. The database is labelled for six speaker attributes: boring; charismatic; enthusiastic; inspiring; likeable; and persuasive. Each of these traits is linked to the perceived ability or appeal of the speaker, and as such are relevant to a range of content retrieval and speech analysis tasks. The six base attributes are combined to form a metric of Overall Speaker Appeal. A set of baseline experiments is presented, which demonstrate the potential of this database for affective computing studies. Classification accuracies of up to 76% are achieved, with little feature or system optimisation.","Language Resources and Evaluation",2018,"Yes"," computational paralinguistics affective computing political speech machine learning charisma speaker ability longitudinal database irish political speech annotations speaker ability paper presents irish political speech database english language database collected irish political recordings database collected automated indexing content retrieval mind gathered real world recordings television interviews election rallies represent nature quality recordings encountered practical applications database labelled speaker attributes boring charismatic enthusiastic inspiring likeable persuasive traits linked perceived ability appeal speaker relevant range content retrieval speech analysis tasks base attributes combined form metric speaker appeal set baseline experiments presented demonstrate potential database affective computing studies classification accuracies achieved feature system optimisation",1
"KeywordsKurdish BLARK Language tools Computational linguistics Natural language processing ","blark for multi dialect languages towards the kurdish blark","In this paper we introduce the Kurdish BLARK (Basic Language Resource Kit). The original BLARK has not considered multi-dialect characteristics and generally has targeted reasonably well-resourced languages. To consider these two features, we extended BLARK and applied the proposed extension to Kurdish. Kurdish language not only faces a paucity in resources, but also embraces several dialects within a complex linguistic context. This paper presents the Kurdish BLARK and shows that from Natural language processing and computational linguistics perspectives the revised BLARK provides a more applicable view of languages with similar characteristics to Kurdish.","Language Resources and Evaluation",2018,"Yes"," kurdish blark language tools computational linguistics natural language processing blark multi dialect languages kurdish blark paper introduce kurdish blark basic language resource kit original blark considered multi dialect characteristics generally targeted resourced languages features extended blark applied proposed extension kurdish kurdish language faces paucity resources embraces dialects complex linguistic context paper presents kurdish blark shows natural language processing computational linguistics perspectives revised blark applicable view languages similar characteristics kurdish",1
"KeywordsAutomatic keyphrase extraction Test collections Annotator disagreement ","creation and evaluation of large keyphrase extraction collections with multiple opinions","While several automatic keyphrase extraction (AKE) techniques have been developed and analyzed, there is little consensus on the definition of the task and a lack of overview of the effectiveness of different techniques.
 Proper evaluation of keyphrase extraction requires large test collections with multiple opinions, currently not available for research. In this paper, we (i) present a set of test collections derived from various sources with multiple annotations (which we also refer to as opinions in the remained of the paper) for each document, (ii) systematically evaluate keyphrase extraction using several supervised and unsupervised AKE techniques, (iii) and experimentally analyze the effects of disagreement on AKE evaluation. Our newly created set of test collections spans different types of topical content from general news and magazines, and is annotated with multiple annotations per article by a large annotator panel. Our annotator study shows that for a given document there seems to be a large disagreement on the preferred keyphrases, suggesting the need for multiple opinions per document. A first systematic evaluation of ranking and classification of keyphrases using both unsupervised and supervised AKE techniques on the test collections shows a superior effectiveness of supervised models, even for a low annotation effort and with basic positional and frequency features, and highlights the importance of a suitable keyphrase candidate generation approach. We also study the influence of multiple opinions, training data and document length on evaluation of keyphrase extraction.
 Our new test collection for keyphrase extraction is one of the largest of its kind and will be made available to stimulate future work to improve reliable evaluation of new keyphrase extractors.","Language Resources and Evaluation",2018,"No"," automatic keyphrase extraction test collections annotator disagreement creation evaluation large keyphrase extraction collections multiple opinions automatic keyphrase extraction ake techniques developed analyzed consensus definition task lack overview effectiveness techniques proper evaluation keyphrase extraction requires large test collections multiple opinions research paper present set test collections derived sources multiple annotations refer opinions remained paper document ii systematically evaluate keyphrase extraction supervised unsupervised ake techniques iii experimentally analyze effects disagreement ake evaluation newly created set test collections spans types topical content general news magazines annotated multiple annotations article large annotator panel annotator study shows document large disagreement preferred keyphrases suggesting multiple opinions document systematic evaluation ranking classification keyphrases unsupervised supervised ake techniques test collections shows superior effectiveness supervised models low annotation effort basic positional frequency features highlights importance suitable keyphrase candidate generation approach study influence multiple opinions training data document length evaluation keyphrase extraction test collection keyphrase extraction largest kind made stimulate future work improve reliable evaluation keyphrase extractors",0
"KeywordsText classification Natural language processing Knowledge representation Semantic enrichment Use case specification ","using semantic roles to improve text classification in the requirements domain","Engineering activities often produce considerable documentation as a by-product of the development process. Due to their complexity, technical analysts can benefit from text processing techniques able to identify concepts of interest and analyze deficiencies of the documents in an automated fashion. In practice, text sentences from the documentation are usually transformed to a vector space model, which is suitable for traditional machine learning classifiers. However, such transformations suffer from problems of synonyms and ambiguity that cause classification mistakes. For alleviating these problems, there has been a growing interest in the semantic enrichment of text. Unfortunately, using general-purpose thesaurus and encyclopedias to enrich technical documents belonging to a given domain (e.g. requirements engineering) often introduces noise and does not improve classification. In this work, we aim at boosting text classification by exploiting information about semantic roles. We have explored this approach when building a multi-label classifier for identifying special concepts, called domain actions, in textual software requirements. After evaluating various combinations of semantic roles and text classification algorithms, we found that this kind of semantically-enriched data leads to improvements of up to 18% in both precision and recall, when compared to non-enriched data. Our enrichment strategy based on semantic roles also allowed classifiers to reach acceptable accuracy levels with small training sets. Moreover, semantic roles outperformed Wikipedia- and WordNET-based enrichments, which failed to boost requirements classification with several techniques. These results drove the development of two requirements tools, which we successfully applied in the processing of textual use cases.","Language Resources and Evaluation",2018,"No"," text classification natural language processing knowledge representation semantic enrichment case specification semantic roles improve text classification requirements domain engineering activities produce considerable documentation product development process due complexity technical analysts benefit text processing techniques identify concepts interest analyze deficiencies documents automated fashion practice text sentences documentation transformed vector space model suitable traditional machine learning classifiers transformations suffer problems synonyms ambiguity classification mistakes alleviating problems growing interest semantic enrichment text general purpose thesaurus encyclopedias enrich technical documents belonging domain requirements engineering introduces noise improve classification work aim boosting text classification exploiting information semantic roles explored approach building multi label classifier identifying special concepts called domain actions textual software requirements evaluating combinations semantic roles text classification algorithms found kind semantically enriched data leads improvements precision recall compared enriched data enrichment strategy based semantic roles allowed classifiers reach acceptable accuracy levels small training sets semantic roles outperformed wikipedia wordnet based enrichments failed boost requirements classification techniques results drove development requirements tools successfully applied processing textual cases",0
"KeywordsAutomatic term recognition Terminology extraction Open source software ","atr4s toolkit with state of the art automatic terms recognition methods in scala","Automatically recognized terminology is widely used for various domain-specific texts processing tasks, such as machine translation, information retrieval or ontology construction. However, there is still no agreement on which methods are best suited for particular settings and, moreover, there is no reliable comparison of already developed methods. We believe that one of the main reasons is the lack of state-of-the-art method implementations, which are usually non-trivial to recreate—mostly, in terms of software engineering efforts. In order to address these issues, we present ATR4S, an open-source software written in Scala that comprises 13 state-of-the-art methods for automatic terminology recognition (ATR) and implements the whole pipeline from text document preprocessing, to term candidates collection, term candidate scoring, and finally, term candidate ranking. It is highly scalable, modular and configurable tool with support of automatic caching. We also compare 13 state-of-the-art methods on 7 open datasets by average precision and processing time. Experimental comparison reveals that no single method demonstrates best average precision for all datasets and that other available tools for ATR do not contain the best methods.","Language Resources and Evaluation",2018,"No"," automatic term recognition terminology extraction open source software atrs toolkit state art automatic terms recognition methods scala automatically recognized terminology widely domain specific texts processing tasks machine translation information retrieval ontology construction agreement methods suited settings reliable comparison developed methods main reasons lack state art method implementations trivial recreate terms software engineering efforts order address issues present atrs open source software written scala comprises state art methods automatic terminology recognition atr implements pipeline text document preprocessing term candidates collection term candidate scoring finally term candidate ranking highly scalable modular configurable tool support automatic caching compare state art methods open datasets average precision processing time experimental comparison reveals single method demonstrates average precision datasets tools atr methods",0
"KeywordsCombinatory categorial grammar CCG Treebank Hindi Non-projective dependencies ","hindi ccgbank a ccg treebank from the hindi dependency treebank","In this paper, we present an approach for automatically creating a combinatory categorial grammar (CCG) treebank from a dependency treebank for the subject–object–verb language Hindi. Rather than a direct conversion from dependency trees to CCG trees, we propose a two stage approach: a language independent generic algorithm first extracts a CCG lexicon from the dependency treebank. An exhaustive CCG parser then creates a treebank of CCG derivations. We also discuss special cases of this generic algorithm to handle linguistic phenomena specific to Hindi. In doing so we extract different constructions with long-range dependencies like coordinate constructions and non-projective dependencies resulting from constructions like relative clauses, noun elaboration and verbal modifiers.","Language Resources and Evaluation",2018,"No"," combinatory categorial grammar ccg treebank hindi projective dependencies hindi ccgbank ccg treebank hindi dependency treebank paper present approach automatically creating combinatory categorial grammar ccg treebank dependency treebank subject object verb language hindi direct conversion dependency trees ccg trees propose stage approach language independent generic algorithm extracts ccg lexicon dependency treebank exhaustive ccg parser creates treebank ccg derivations discuss special cases generic algorithm handle linguistic phenomena specific hindi extract constructions long range dependencies coordinate constructions projective dependencies resulting constructions relative clauses noun elaboration verbal modifiers",0
"KeywordsSummarization Discussion forums Data collection User study Inter-rater agreement Evaluation ","creating a reference data set for the summarization of discussion forum threads","In this paper we address extractive summarization of long threads in online discussion fora. We present an elaborate user evaluation study to determine human preferences in forum summarization and to create a reference data set. We showed long threads to ten different raters and asked them to create a summary by selecting the posts that they considered to be the most important for the thread. We study the agreement between human raters on the summarization task, and we show how multiple reference summaries can be combined to develop a successful model for automatic summarization. We found that although the inter-rater agreement for the summarization task was slight to fair, the automatic summarizer obtained reasonable results in terms of precision, recall, and ROUGE. Moreover, when human raters were asked to choose between the summary created by another human and the summary created by our model in a blind side-by-side comparison, they judged the model’s summary equal to or better than the human summary in over half of the cases. This shows that even for a summarization task with low inter-rater agreement, a model can be trained that generates sensible summaries. In addition, we investigated the potential for personalized summarization. However, the results for the three raters involved in this experiment were inconclusive. We release the reference summaries as a publicly available dataset.","Language Resources and Evaluation",2018,"No"," summarization discussion forums data collection user study inter rater agreement evaluation creating reference data set summarization discussion forum threads paper address extractive summarization long threads online discussion fora present elaborate user evaluation study determine human preferences forum summarization create reference data set showed long threads ten raters asked create summary selecting posts considered important thread study agreement human raters summarization task show multiple reference summaries combined develop successful model automatic summarization found inter rater agreement summarization task slight fair automatic summarizer obtained reasonable results terms precision recall rouge human raters asked choose summary created human summary created model blind side side comparison judged model summary equal human summary half cases shows summarization task low inter rater agreement model trained generates summaries addition investigated potential personalized summarization results raters involved experiment inconclusive release reference summaries publicly dataset",0
"KeywordsDefinition-to-synset mapping Pre-Qin ancient Chinese Graph-based WSD Global wordnet ","pqac wn constructing a wordnet for pre qin ancient chinese","The Princeton WordNet® (PWN) is a widely used lexical knowledge database for semantic information processing. There are now many wordnets under creation for languages worldwide. In this paper, we endeavor to construct a wordnet for Pre-Qin ancient Chinese (PQAC), called PQAC WordNet (PQAC-WN), to process the semantic information of PQAC. In previous work, most recently constructed wordnets have been established either manually by experts or automatically using resources from which translation pairs between English and the target language can be extracted. The former method, however, is time-consuming, and the latter method, owing to a lack of language resources, cannot be performed on PQAC. As a result, a method based on word definitions in a monolingual dictionary is proposed. Specifically, for each sense, kernel words are first extracted from its definition, and the senses of each kernel word are then determined by graph-based Word Sense Disambiguation. Finally, one optimal sense is chosen from the kernel word senses to guide the mapping between the word sense and PWN synset. In this research, we obtain 66 % PQAC senses that can be shared with English and another 14 % language-specific senses that were added to PQAC-WN as new synsets. Overall, the automatic mapping achieves a precision of over 85 %.","Language Resources and Evaluation",2017,"Yes"," definition synset mapping pre qin ancient chinese graph based wsd global wordnet pqac wn constructing wordnet pre qin ancient chinese princeton wordnet pwn widely lexical knowledge database semantic information processing wordnets creation languages worldwide paper endeavor construct wordnet pre qin ancient chinese pqac called pqac wordnet pqac wn process semantic information pqac previous work recently constructed wordnets established manually experts automatically resources translation pairs english target language extracted method time consuming method owing lack language resources performed pqac result method based word definitions monolingual dictionary proposed specifically sense kernel words extracted definition senses kernel word determined graph based word sense disambiguation finally optimal sense chosen kernel word senses guide mapping word sense pwn synset research obtain pqac senses shared english language specific senses added pqac wn synsets automatic mapping achieves precision ",1
"KeywordsMachine translation Crowd evaluation Pair-wise comparison English Basque ","ebaluatoia crowd evaluation for englishbasque machine translation","This work explores the feasibility of a crowd-based pair-wise comparison evaluation to get feedback on machine translation progress for under-resourced languages. Specifically, we propose a task based on simple work units to compare the outputs of five English-to-Basque systems, which we implement in a web application. In our design, we put forward two key aspects that we believe community collaboration initiatives should consider in order to attract and maintain participants, that is, providing both a community challenge and a personal challenge. We describe how these aspects can comply with a strict methodology to ensure research validity. In particular, we consider the evaluation set size and the characteristics of the test sentences, the number of evaluators per comparison pair, and a mechanism to identify dishonest participation (or participants with insufficient linguistic knowledge). We also describe our dissemination effort, which targeted both general users and interest groups. Over 500 people participated actively in the Ebaluatoia campaign and we were able to collect over 35,000 evaluations in a short period of 10 days. From the results, we complete the ranking of the systems under evaluation and establish whether the difference in quality between the systems is significant.","Language Resources and Evaluation",2017,"No"," machine translation crowd evaluation pair wise comparison english basque ebaluatoia crowd evaluation englishbasque machine translation work explores feasibility crowd based pair wise comparison evaluation feedback machine translation progress resourced languages specifically propose task based simple work units compare outputs english basque systems implement web application design put forward key aspects community collaboration initiatives order attract maintain participants providing community challenge personal challenge describe aspects comply strict methodology ensure research validity evaluation set size characteristics test sentences number evaluators comparison pair mechanism identify dishonest participation participants insufficient linguistic knowledge describe dissemination effort targeted general users interest groups people participated actively ebaluatoia campaign collect evaluations short period days results complete ranking systems evaluation establish difference quality systems significant",0
"KeywordsCorpus annotation Cross-linguistic comparison Metaphor Metaphor density Metaphor identification Register variation ","towards a metaphor annotated corpus of mandarin chinese","
Building on the success of the VU Amsterdam Metaphor Corpus, which comprises English texts annotated with metaphor following the Metaphor Identification Procedure Vrjie Universiteit (MIPVU; Steen et al. in Cogn Linguist 21(4):765–796, 2010a; Steen et al. in A method for linguistic metaphor identification: from MIP to MIPVU. John Benjamins, Amsterdam/Philadelphia, 2010b), this study has three aims: (1) to adapt and evaluate the transferability and reliability of MIPVU for Mandarin Chinese; (2) to construct a corpus of Chinese texts annotated for metaphor using the adapted procedure; and (3) to examine the distribution of metaphor-related words across Chinese texts in three different written registers: academic discourse, fiction, and news. The results of our inter-annotator reliability test show that MIPVU can be reliably applied to linguistic metaphor identification in Chinese texts. Our metaphor-annotated corpus consists of texts randomly sampled from the Lancaster Corpus of Mandarin Chinese, totaling 30,012 words (about 10,000 for each register). Data analysis reveals that approximately one out of every nine lexical units in our Chinese corpus is related to metaphor, that there is considerable variation in metaphor density across different registers and lexical categories, and that metaphor density is significantly lower in Chinese than in English texts. Our assessment of the replicability of MIPVU for Mandarin Chinese adds to the groundbreaking methodological contribution that Steen et al. (2010a, b) has made to metaphor research. The metaphor-annotated corpus of Mandarin Chinese contributes a valuable language resource for Chinese metaphor researchers, and our analysis of the distribution of metaphor-related words in the corpus offers useful new insights into the extent and use of metaphor in Chinese discourse.","Language Resources and Evaluation",2017,"Yes"," corpus annotation cross linguistic comparison metaphor metaphor density metaphor identification register variation metaphor annotated corpus mandarin chinese building success vu amsterdam metaphor corpus comprises english texts annotated metaphor metaphor identification procedure vrjie universiteit mipvu steen al cogn linguist a steen al method linguistic metaphor identification mip mipvu john benjamins amsterdamphiladelphia b study aims adapt evaluate transferability reliability mipvu mandarin chinese construct corpus chinese texts annotated metaphor adapted procedure examine distribution metaphor related words chinese texts written registers academic discourse fiction news results inter annotator reliability test show mipvu reliably applied linguistic metaphor identification chinese texts metaphor annotated corpus consists texts randomly sampled lancaster corpus mandarin chinese totaling words register data analysis reveals approximately lexical units chinese corpus related metaphor considerable variation metaphor density registers lexical categories metaphor density significantly lower chinese english texts assessment replicability mipvu mandarin chinese adds groundbreaking methodological contribution steen al a made metaphor research metaphor annotated corpus mandarin chinese contributes valuable language resource chinese metaphor researchers analysis distribution metaphor related words corpus offers insights extent metaphor chinese discourse",1
"KeywordsParallel treebank Parallel corpus Machine translation Syntax-based machine translation Constituent alignment Tree alignment Resource development ","large aligned treebanks for syntax based machine translation","We present a collection of parallel treebanks that have been automatically aligned on both the terminal and the non-terminal constituent level for use in syntax-based machine translation. 
We describe how they were constructed and applied to a syntax- and example-based machine translation system called Parse and Corpus-Based Machine Translation (PaCo-MT). For the language pair Dutch to English, we present non-terminal alignment evaluation scores for a variety of tree alignment approaches. Finally, based on the parallel treebanks created by these approaches, we evaluate the MT system itself and compare the scores with those of Moses, a current state-of-the-art statistical MT system, when trained on the same data.","Language Resources and Evaluation",2017,"Yes"," parallel treebank parallel corpus machine translation syntax based machine translation constituent alignment tree alignment resource development large aligned treebanks syntax based machine translation present collection parallel treebanks automatically aligned terminal terminal constituent level syntax based machine translation describe constructed applied syntax based machine translation system called parse corpus based machine translation paco mt language pair dutch english present terminal alignment evaluation scores variety tree alignment approaches finally based parallel treebanks created approaches evaluate mt system compare scores moses current state art statistical mt system trained data",1
"KeywordsControlled natural language Corpus linguistics Requirements Technical writing Textual genre ","towards the creation of a cnl adapted to requirements writing by combining writing recommendations and spontaneous regularities example in a space project","The Quality Department of the French National Space Agency (CNES, Centre National d’Études Spatiales) wishes to design a writing guide based on the real and regular writing of requirements. As a first step in this project, the present article proposes a linguistic analysis of requirements written in French by CNES engineers. One of our goals is to determine to what extent they conform to several rules laid down in two existing Controlled Natural Languages (CNLs), namely the Simplified Technical English developed by the AeroSpace and Defense Industries Association of Europe and the Guide for Writing Requirements proposed by the International Council on Systems Engineering. Indeed, although CNES engineers are not obliged to follow any controlled language in their writing of requirements, we believe that language regularities are likely to emerge from this task, mainly due to the writers’ experience. We are seeking to identify these regularities in order to use them as a basis for a new CNL for the writing of requirements. The issue is approached using natural language processing tools to identify sentences that do not comply with the rules or contain specific linguistic phenomena. We further review these sentences to understand why the recommendations cannot (or should not) always be applied when specifying large-scale projects.","Language Resources and Evaluation",2017,"No"," controlled natural language corpus linguistics requirements technical writing textual genre creation cnl adapted requirements writing combining writing recommendations spontaneous regularities space project quality department french national space agency cnes centre national tudes spatiales wishes design writing guide based real regular writing requirements step project present article proposes linguistic analysis requirements written french cnes engineers goals determine extent conform rules laid existing controlled natural languages cnls simplified technical english developed aerospace defense industries association europe guide writing requirements proposed international council systems engineering cnes engineers obliged follow controlled language writing requirements language regularities emerge task due writers experience seeking identify regularities order basis cnl writing requirements issue approached natural language processing tools identify sentences comply rules specific linguistic phenomena review sentences understand recommendations applied large scale projects",0
"KeywordsCorpus linguistics German historical corpus Multi-layer architecture Multiple tokenizations Normalizations Open source ","ridges herbology designing a diachronic multi layer corpus","This paper introduces a multi-layer corpus architecture with multiple tokenizations using the open source historical, diachronic corpus of German called Register in Diachronic German Science. The corpus contains herbal texts printed between the fifteenth and nineteenth centuries and is concerned with the development of a German scientific register, independent of Latin. We will discuss difficulties of transcribing, normalizing and annotating historical texts and will thereby argue for the advantages of multiple layers and multiple tokenizations. A virtually infinite number of annotations can be added to the corpus, without the need for deciding between or discarding interpretations. Thus, this flexible architecture enables multiple normalizations and types of annotation and is open to a wide range of research questions in the humanities. We provide case studies concerning the exploitation of our different normalizations as well as structural, register-specific and linguistic annotations. The corpus architecture allows for its reuse as a resource for corpus-based research approaches.","Language Resources and Evaluation",2017,"Yes"," corpus linguistics german historical corpus multi layer architecture multiple tokenizations normalizations open source ridges herbology designing diachronic multi layer corpus paper introduces multi layer corpus architecture multiple tokenizations open source historical diachronic corpus german called register diachronic german science corpus herbal texts printed fifteenth nineteenth centuries concerned development german scientific register independent latin discuss difficulties transcribing normalizing annotating historical texts argue advantages multiple layers multiple tokenizations virtually infinite number annotations added corpus deciding discarding interpretations flexible architecture enables multiple normalizations types annotation open wide range research questions humanities provide case studies exploitation normalizations structural register specific linguistic annotations corpus architecture reuse resource corpus based research approaches",1
"KeywordsEvents Annotation Meta-knowledge Subjectivity Modality Speculation ","enriching news events with meta knowledge information","Given the vast amounts of data available in digitised textual form, it is important to provide mechanisms that allow users to extract nuggets of relevant information from the ever growing volumes of potentially important documents. Text mining techniques can help, through their ability to automatically extract relevant event descriptions, which link entities with situations described in the text. However, correct and complete interpretation of these event descriptions is not possible without considering additional contextual information often present within the surrounding text. This information, which we refer to as meta-knowledge, can include (but is not restricted to) the modality, subjectivity, source, polarity and specificity of the event. We have developed a meta-knowledge annotation scheme specifically tailored for news events, which includes six aspects of event interpretation. We have applied this annotation scheme to the ACE 2005 corpus, which contains 599 documents from various written and spoken news sources. We have also identified and annotated the words and phrases evoking the different types of meta-knowledge. Evaluation of the annotated corpus shows high levels of inter-annotator agreement for five meta-knowledge attributes, and moderate level of agreement for the sixth attribute. Detailed analysis of the annotated corpus has revealed further insights into the expression mechanisms of different types of meta-knowledge, their relative frequencies and mutual correlations.","Language Resources and Evaluation",2017,"Yes"," events annotation meta knowledge subjectivity modality speculation enriching news events meta knowledge information vast amounts data digitised textual form important provide mechanisms users extract nuggets relevant information growing volumes potentially important documents text mining techniques ability automatically extract relevant event descriptions link entities situations text correct complete interpretation event descriptions additional contextual information present surrounding text information refer meta knowledge include restricted modality subjectivity source polarity specificity event developed meta knowledge annotation scheme specifically tailored news events includes aspects event interpretation applied annotation scheme ace corpus documents written spoken news sources identified annotated words phrases evoking types meta knowledge evaluation annotated corpus shows high levels inter annotator agreement meta knowledge attributes moderate level agreement sixth attribute detailed analysis annotated corpus revealed insights expression mechanisms types meta knowledge relative frequencies mutual correlations",1
"KeywordsMono-lingual text reuse Urdu news corpus Urdu text reuse detection Corpus generation ","counter corpus of urdu news text reuse","Text reuse is the act of borrowing text from existing documents to create new texts. Freely available and easily accessible large online repositories are not only making reuse of text more common in society but also harder to detect. A major hindrance in the development and evaluation of existing/new mono-lingual text reuse detection methods, especially for South Asian languages, is the unavailability of standardized benchmark corpora. Amongst other things, a gold standard corpus enables researchers to directly compare existing state-of-the-art methods. In our study, we address this gap by developing a benchmark corpus for one of the widely spoken but under resourced languages i.e. Urdu. The COrpus of Urdu News TExt Reuse (COUNTER) corpus contains 1200 documents with real examples of text reuse from the field of journalism. It has been manually annotated at document level with three levels of reuse: wholly derived, partially derived and non derived. We also apply a number of similarity estimation methods on our corpus to show how it can be used for the development, evaluation and comparison of text reuse detection systems for the Urdu language. The corpus is a vital resource for the development and evaluation of text reuse detection systems in general and specifically for Urdu language.","Language Resources and Evaluation",2017,"Yes"," mono lingual text reuse urdu news corpus urdu text reuse detection corpus generation counter corpus urdu news text reuse text reuse act borrowing text existing documents create texts freely easily accessible large online repositories making reuse text common society harder detect major hindrance development evaluation existing mono lingual text reuse detection methods south asian languages unavailability standardized benchmark corpora things gold standard corpus enables researchers directly compare existing state art methods study address gap developing benchmark corpus widely spoken resourced languages urdu corpus urdu news text reuse counter corpus documents real examples text reuse field journalism manually annotated document level levels reuse wholly derived partially derived derived apply number similarity estimation methods corpus show development evaluation comparison text reuse detection systems urdu language corpus vital resource development evaluation text reuse detection systems general specifically urdu language",1
"KeywordsCorpus linguistics Multilingual Annotation People name disambiguation ","mc4weps a multilingual corpus for web people search disambiguation","This article introduces the MC4WEPS corpus, a new resource for evaluating Web people search disambiguation tasks, and describes its design, collection and annotation process, the agreement between the different annotators, and finally introduces a baseline evaluation. This corpus is built by compiling multilingual search engines results where the queries are person names. Proper noun disambiguation is an open problem in natural language ambiguity resolution and, specifically, resolving the ambiguity of person names in Web search results is still a challenging problem. However, state-of-the-art approaches have been evaluated only with monolingual web page collections. The MC4WEPS corpus aims to provide the research community with a reference corpus for the task of disambiguating search engine results where the query is a person name shared by homonymous individuals. The features of this new corpus stand out from existing corpora for the same task, namely multilingualism and inclusion of social networking websites. These characteristics make it more representative of a real search scenario, especially for evaluating person name disambiguation in a multilingual context. The article also includes detailed information about the format and the availability of the corpus.","Language Resources and Evaluation",2017,"Yes"," corpus linguistics multilingual annotation people disambiguation mcweps multilingual corpus web people search disambiguation article introduces mcweps corpus resource evaluating web people search disambiguation tasks describes design collection annotation process agreement annotators finally introduces baseline evaluation corpus built compiling multilingual search engines results queries person names proper noun disambiguation open problem natural language ambiguity resolution specifically resolving ambiguity person names web search results challenging problem state art approaches evaluated monolingual web page collections mcweps corpus aims provide research community reference corpus task disambiguating search engine results query person shared homonymous individuals features corpus stand existing corpora task multilingualism inclusion social networking websites characteristics make representative real search scenario evaluating person disambiguation multilingual context article includes detailed information format availability corpus",1
"KeywordsMultilayer corpora Classroom annotation Coreference Information structure Treebank Parsing ","the gum corpus creating multilayer resources in the classroom","This paper presents the methodology, design principles and detailed evaluation of a new freely available multilayer corpus, collected and edited via classroom annotation using collaborative software. After briefly discussing corpus design for open, extensible corpora, five classroom annotation projects are presented, covering structural markup in TEI XML, multiple part of speech tagging, constituent and dependency parsing, information structural and coreference annotation, and Rhetorical Structure Theory analysis. Layers are inspected for annotation quality and together they coalesce to form a richly annotated corpus that can be used to study the interactions between different levels of linguistic description. The evaluation gives an indication of the expected quality of a corpus created by students with relatively little training. A multifactorial example study on lexical NP coreference likelihood is also presented, which illustrates some applications of the corpus. The results of this project show that high quality, richly annotated resources can be created effectively as part of a linguistics curriculum, opening new possibilities not just for research, but also for corpora in linguistics pedagogy.","Language Resources and Evaluation",2017,"Yes"," multilayer corpora classroom annotation coreference information structure treebank parsing gum corpus creating multilayer resources classroom paper presents methodology design principles detailed evaluation freely multilayer corpus collected edited classroom annotation collaborative software briefly discussing corpus design open extensible corpora classroom annotation projects presented covering structural markup tei xml multiple part speech tagging constituent dependency parsing information structural coreference annotation rhetorical structure theory analysis layers inspected annotation quality coalesce form richly annotated corpus study interactions levels linguistic description evaluation indication expected quality corpus created students training multifactorial study lexical np coreference likelihood presented illustrates applications corpus results project show high quality richly annotated resources created effectively part linguistics curriculum opening possibilities research corpora linguistics pedagogy",1
"KeywordsNatural language generation Referring expressions  Content selection Corpora ","stars2 a corpus of object descriptions in a visual domain","This paper presents the Stars2 corpus of definite descriptions for referring expression generation (REG). The corpus was produced in collaborative communication involving speaker-hearer pairs, and includes situations of reference that are arguably under-represented in similar work. Stars2 is intended as an incremental contribution to the research in REG and related fields, and it may be used both as training/test data for algorithms of this kind, and also to gain further insights into reference phenomena in general, with a particular focus on the issue of attribute choice in referential overspecification.","Language Resources and Evaluation",2017,"Yes"," natural language generation referring expressions content selection corpora stars corpus object descriptions visual domain paper presents stars corpus definite descriptions referring expression generation reg corpus produced collaborative communication involving speaker hearer pairs includes situations reference arguably represented similar work stars intended incremental contribution research reg related fields trainingtest data algorithms kind gain insights reference phenomena general focus issue attribute choice referential overspecification",1
"KeywordsPalestinian Arabic Palestinian corpus Arabic morphology Conventional Orthography for Dialectal Arabic Dialectal Arabic Word annotation ","curras an annotated corpus for the palestinian arabic dialect","
In this article we present Curras, the first morphologically annotated corpus of the Palestinian Arabic dialect. Palestinian Arabic is one of the many primarily spoken dialects of the Arabic language. Arabic dialects are generally under-resourced compared to Modern Standard Arabic, the primarily written and official form of Arabic. We start in the article with a background description that situates Palestinian Arabic linguistically and historically and compares it to Modern Standard Arabic and Egyptian Arabic in terms of phonological, morphological, orthographic, and lexical variations. We then describe the methodology we developed to collect Palestinian Arabic text to guarantee a variety of representative domains and genres. We also discuss the annotation process we used, which extended previous efforts for annotation guideline development, and utilized existing automatic annotation solutions for Standard Arabic and Egyptian Arabic. The annotation guidelines and annotation meta-data are described in detail. The Curras Palestinian Arabic corpus consists of more than 56 K tokens, which are annotated with rich morphological and lexical features. The inter-annotator agreement results indicate a high degree of consistency.","Language Resources and Evaluation",2017,"Yes"," palestinian arabic palestinian corpus arabic morphology conventional orthography dialectal arabic dialectal arabic word annotation curras annotated corpus palestinian arabic dialect article present curras morphologically annotated corpus palestinian arabic dialect palestinian arabic primarily spoken dialects arabic language arabic dialects generally resourced compared modern standard arabic primarily written official form arabic start article background description situates palestinian arabic linguistically historically compares modern standard arabic egyptian arabic terms phonological morphological orthographic lexical variations describe methodology developed collect palestinian arabic text guarantee variety representative domains genres discuss annotation process extended previous efforts annotation guideline development utilized existing automatic annotation solutions standard arabic egyptian arabic annotation guidelines annotation meta data detail curras palestinian arabic corpus consists tokens annotated rich morphological lexical features inter annotator agreement results high degree consistency",1
"KeywordsRegional accents recognition Acoustic approaches Complex socio-linguistic environments Algerian Modern Colloquial Arabic Speech Corpus Code-switching Language contact phenomena ","algerian modern colloquial arabic speech corpus amcasc regional accents recognition within complex socio linguistic environments","The Algerian linguistic situation is very intricate due to the ethnic, geographical and colonial occupation influences which have lead to a complex sociolinguistic environment. As a result of the contact between different languages and accents, the Algerian speech community has acquired a distinctive sociolinguistic situation. In addition to the intra- and inter- lingual variations describing day-to-day linguistic behavior of the Algerian speakers, their speech is characterized by the presence of many linguistic phenomena such as bilingualism and code switching. The study of automatic regional accent recognition in such a type of environment is a new idea in the field of automatic languages, dialect and accent recognition especially that previous studies were conducted using monolingual evaluation data. The assessment of the effectiveness of GMM-UBM and i-vectors frameworks for accent recognition approaches through the use of the Algerian Modern Colloquial Arabic Speech Corpus (AMCASC), which is a linguistic resource collected for this purpose, shows that not only the recording conditions mismatch, channels mismatch, recordings length mismatch and the amplitude clipping which have a non-desirable effect on the effectiveness of these acoustic approaches but also language contact phenomena are other perturbation sources which should be taken into consideration especially in real life applications
.","Language Resources and Evaluation",2017,"Yes"," regional accents recognition acoustic approaches complex socio linguistic environments algerian modern colloquial arabic speech corpus code switching language contact phenomena algerian modern colloquial arabic speech corpus amcasc regional accents recognition complex socio linguistic environments algerian linguistic situation intricate due ethnic geographical colonial occupation influences lead complex sociolinguistic environment result contact languages accents algerian speech community acquired distinctive sociolinguistic situation addition intra inter lingual variations describing day day linguistic behavior algerian speakers speech characterized presence linguistic phenomena bilingualism code switching study automatic regional accent recognition type environment idea field automatic languages dialect accent recognition previous studies conducted monolingual evaluation data assessment effectiveness gmm ubm vectors frameworks accent recognition approaches algerian modern colloquial arabic speech corpus amcasc linguistic resource collected purpose shows recording conditions mismatch channels mismatch recordings length mismatch amplitude clipping desirable effect effectiveness acoustic approaches language contact phenomena perturbation sources consideration real life applications ",1
"KeywordsMultimodal corpora First acquaintance conversations Gestural annotation ","the danish nomco corpus multimodal interaction in first acquaintance conversations","This article presents the Danish NOMCO Corpus, an annotated multimodal collection of video-recorded first acquaintance conversations between Danish speakers. 
The annotation includes speech transcription including word boundaries, and formal as well as functional coding of gestural behaviours, specifically head movements, facial expressions, and body posture. 
The corpus has served as the empirical basis for a number of studies of communication phenomena related to turn management, feedback exchange, information packaging and the expression of emotional attitudes. 
We describe the annotation scheme, procedure, and annotation results. We then summarise a number of studies conducted on the corpus. The corpus is available for research and teaching purposes through the authors of this article.","Language Resources and Evaluation",2017,"Yes"," multimodal corpora acquaintance conversations gestural annotation danish nomco corpus multimodal interaction acquaintance conversations article presents danish nomco corpus annotated multimodal collection video recorded acquaintance conversations danish speakers annotation includes speech transcription including word boundaries formal functional coding gestural behaviours specifically head movements facial expressions body posture corpus served empirical basis number studies communication phenomena related turn management feedback exchange information packaging expression emotional attitudes describe annotation scheme procedure annotation results summarise number studies conducted corpus corpus research teaching purposes authors article",1
"KeywordsWeb corpora Corpus evaluation Corpus similarity Varieties of English Canadian English ","building and evaluating web corpora representing national varieties of english","Corpora are essential resources for language studies, as well as for training statistical natural language processing systems. Although very large English corpora have been built, only relatively small corpora are available for many varieties of English. National top-level domains (e.g., .au, .ca) could be exploited to automatically build web corpora, but it is unclear whether such corpora would reflect the corresponding national varieties of English; i.e., would a web corpus built from the .ca domain correspond to Canadian English? In this article we build web corpora from national top-level domains corresponding to countries in which English is widely spoken. We then carry out statistical analyses of these corpora in terms of keywords, measures of corpus comparison based on the Chi-square test and spelling variants, and the frequencies of words known to be marked in particular varieties of English. We find evidence that the web corpora indeed reflect the corresponding national varieties of English. We then demonstrate, through a case study on the analysis of Canadianisms, that these corpora could be valuable lexicographical resources.","Language Resources and Evaluation",2017,"Yes"," web corpora corpus evaluation corpus similarity varieties english canadian english building evaluating web corpora representing national varieties english corpora essential resources language studies training statistical natural language processing systems large english corpora built small corpora varieties english national top level domains au ca exploited automatically build web corpora unclear corpora reflect national varieties english web corpus built ca domain correspond canadian english article build web corpora national top level domains countries english widely spoken carry statistical analyses corpora terms keywords measures corpus comparison based chi square test spelling variants frequencies words marked varieties english find evidence web corpora reflect national varieties english demonstrate case study analysis canadianisms corpora valuable lexicographical resources",1
"KeywordsCorpus construction Web corpora Boilerplate Non-destructive corpus normalization ","accurate and efficient general purpose boilerplate detection for crawled web corpora","Removal of boilerplate is one of the essential tasks in web corpus construction and web indexing. Boilerplate (redundant and automatically inserted material like menus, copyright notices, navigational elements, etc.) is usually considered to be linguistically unattractive for inclusion in a web corpus. Also, search engines should not index such material because it can lead to spurious results for search terms if these terms appear in boilerplate regions of the web page. In this paper, I present and evaluate a supervised machine-learning approach to general-purpose boilerplate detection for languages based on Latin alphabets using Multi-Layer Perceptrons (MLPs). It is both very efficient and very accurate (between 95 % and \(99\,\%\) correct classifications, depending on the input language). I show that language-specific classifiers greatly improve the accuracy of boilerplate detectors. The single features used for the classification are evaluated with regard to the merit they contribute to the classification. Furthermore, I show that the accuracy of the MLP is on a par with that of a wide range of other classifiers. My approach has been implemented in the open-source texrex web page cleaning software, and large corpora constructed using it are available from the COW initiative, including the CommonCOW corpora created from CommonCrawl datasets.
","Language Resources and Evaluation",2017,"No"," corpus construction web corpora boilerplate destructive corpus normalization accurate efficient general purpose boilerplate detection crawled web corpora removal boilerplate essential tasks web corpus construction web indexing boilerplate redundant automatically inserted material menus copyright notices navigational elements considered linguistically unattractive inclusion web corpus search engines index material lead spurious results search terms terms boilerplate regions web page paper present evaluate supervised machine learning approach general purpose boilerplate detection languages based latin alphabets multi layer perceptrons mlps efficient accurate correct classifications depending input language show language specific classifiers greatly improve accuracy boilerplate detectors single features classification evaluated regard merit contribute classification show accuracy mlp par wide range classifiers approach implemented open source texrex web page cleaning software large corpora constructed cow initiative including commoncow corpora created commoncrawl datasets ",0
"KeywordsStatistical machine translation Web crawling Crowdsourcing ","crawl and crowd to bring machine translation to under resourced languages","We present a widely applicable methodology to bring machine translation (MT) to under-resourced languages in a cost-effective and rapid manner. Our proposal relies on web crawling to automatically acquire parallel data to train statistical MT systems if any such data can be found for the language pair and domain of interest. If that is not the case, we resort to (1) crowdsourcing to translate small amounts of text (hundreds of sentences), which are then used to tune statistical MT models, and (2) web crawling of vast amounts of monolingual data (millions of sentences), which are then used to build language models for MT. We apply these to two respective use-cases for Croatian, an under-resourced language that has gained relevance since it recently attained official 
status in the European Union. The first use-case regards tourism, given the importance of this sector to Croatia’s economy, while the second has to do with tweets, due to the growing importance of social media. For tourism, we crawl parallel data from 20 web domains using two state-of-the-art crawlers and explore how to combine the crawled data with bigger amounts of general-domain data. Our domain-adapted system is evaluated on a set of three additional tourism web domains and it outperforms the baseline in terms of automatic metrics and/or vocabulary coverage. In the social media use-case, we deal with tweets from the 2014 edition of the soccer World Cup. We build domain-adapted systems by (1) translating small amounts of tweets to be used for tuning by means of crowdsourcing and (2) crawling vast amounts of monolingual tweets. These systems outperform the baseline (Microsoft Bing) by 7.94 BLEU points (5.11 TER) for Croatian-to-English and by 2.17 points (1.94 TER) for English-to-Croatian on a test set translated by means of crowdsourcing. A complementary manual analysis sheds further light on these results.","Language Resources and Evaluation",2017,"No"," statistical machine translation web crawling crowdsourcing crawl crowd bring machine translation resourced languages present widely applicable methodology bring machine translation mt resourced languages cost effective rapid manner proposal relies web crawling automatically acquire parallel data train statistical mt systems data found language pair domain interest case resort crowdsourcing translate small amounts text hundreds sentences tune statistical mt models web crawling vast amounts monolingual data millions sentences build language models mt apply respective cases croatian resourced language gained relevance recently attained official status european union case tourism importance sector croatia economy tweets due growing importance social media tourism crawl parallel data web domains state art crawlers explore combine crawled data bigger amounts general domain data domain adapted system evaluated set additional tourism web domains outperforms baseline terms automatic metrics vocabulary coverage social media case deal tweets edition soccer world cup build domain adapted systems translating small amounts tweets tuning means crowdsourcing crawling vast amounts monolingual tweets systems outperform baseline microsoft bing bleu points ter croatian english points ter english croatian test set translated means crowdsourcing complementary manual analysis sheds light results",0
"KeywordsEnlargement of morphological dictionaries Knowledge elicitation Resource development for under-resourced languages Machine translation ","assisting non expert speakers of under resourced languages in assigning stems and inflectional paradigms to new word entries of morphological dictionaries","This paper presents a new method with which to assist individuals with no background in linguistics to create monolingual dictionaries such as those used by the morphological analysers of many natural language processing applications.
 The involvement of non-expert users is especially critical for under-resourced languages which either lack or cannot afford the recruitment of a skilled workforce. Adding a word to a morphological dictionary usually requires identifying its stem along with the inflection paradigm that can be used in order to generate all the word forms of the new entry. Our method works under the assumption that the average speakers of a language can successfully answer the polar question “is x a valid form of the word w to be inserted?”, where x represents tentative alternative (inflected) forms of the new word w. The experiments show that with a small number of polar questions the correct stem and paradigm can be obtained from non-experts with high success rates. We study the impact of different heuristic and probabilistic approaches on the actual number of questions.","Language Resources and Evaluation",2017,"No"," enlargement morphological dictionaries knowledge elicitation resource development resourced languages machine translation assisting expert speakers resourced languages assigning stems inflectional paradigms word entries morphological dictionaries paper presents method assist individuals background linguistics create monolingual dictionaries morphological analysers natural language processing applications involvement expert users critical resourced languages lack afford recruitment skilled workforce adding word morphological dictionary requires identifying stem inflection paradigm order generate word forms entry method works assumption average speakers language successfully answer polar question valid form word inserted represents tentative alternative inflected forms word experiments show small number polar questions correct stem paradigm obtained experts high success rates study impact heuristic probabilistic approaches actual number questions",0
"KeywordsFrameNet Grammatical Framework Multilinguality  Natural language generation Controlled natural language ","a multilingual framenet based grammar and lexicon for controlled natural language","Berkeley FrameNet is a lexico-semantic resource for English based on the theory of frame semantics. It has been exploited in a range of natural language processing applications and has inspired the development of framenets for many languages. We present a methodological approach to the extraction and generation of a computational multilingual FrameNet-based grammar and lexicon. The approach leverages FrameNet-annotated corpora to automatically extract a set of cross-lingual semantico-syntactic valence patterns. Based on data from Berkeley FrameNet and Swedish FrameNet, the proposed approach has been implemented in Grammatical Framework (GF), a categorial grammar formalism specialized for multilingual grammars. The implementation of the grammar and lexicon is supported by the design of FrameNet, providing a frame semantic abstraction layer, an interlingual semantic application programming interface (API), over the interlingual syntactic API already provided by GF Resource Grammar Library. The evaluation of the acquired grammar and lexicon shows the feasibility of the approach. Additionally, we illustrate how the FrameNet-based grammar and lexicon are exploited in two distinct multilingual controlled natural language applications. The produced resources are available under an open source license.","Language Resources and Evaluation",2017,"No"," framenet grammatical framework multilinguality natural language generation controlled natural language multilingual framenet based grammar lexicon controlled natural language berkeley framenet lexico semantic resource english based theory frame semantics exploited range natural language processing applications inspired development framenets languages present methodological approach extraction generation computational multilingual framenet based grammar lexicon approach leverages framenet annotated corpora automatically extract set cross lingual semantico syntactic valence patterns based data berkeley framenet swedish framenet proposed approach implemented grammatical framework gf categorial grammar formalism specialized multilingual grammars implementation grammar lexicon supported design framenet providing frame semantic abstraction layer interlingual semantic application programming interface api interlingual syntactic api provided gf resource grammar library evaluation acquired grammar lexicon shows feasibility approach additionally illustrate framenet based grammar lexicon exploited distinct multilingual controlled natural language applications produced resources open source license",0
"KeywordsLarge vocabulary speech recognition Statistical language modeling Subword units Data filtering Adaptation ","modeling under resourced languages for speech recognition","One particular problem in large vocabulary continuous speech recognition for low-resourced languages is finding relevant training data for the statistical language models. Large amount of data is required, because models should estimate the probability for all possible word sequences. For Finnish, Estonian and the other fenno-ugric languages a special problem with the data is the huge amount of different word forms that are common in normal speech. The same problem exists also in other language technology applications such as machine translation, information retrieval, and in some extent also in other morphologically rich languages. In this paper we present methods and evaluations in four recent language modeling topics: selecting conversational data from the Internet, adapting models for foreign words, multi-domain and adapted neural network language modeling, and decoding with subword units. Our evaluations show that the same methods work in more than one language and that they scale down to smaller data resources.","Language Resources and Evaluation",2017,"No"," large vocabulary speech recognition statistical language modeling subword units data filtering adaptation modeling resourced languages speech recognition problem large vocabulary continuous speech recognition low resourced languages finding relevant training data statistical language models large amount data required models estimate probability word sequences finnish estonian fenno ugric languages special problem data huge amount word forms common normal speech problem exists language technology applications machine translation information retrieval extent morphologically rich languages paper present methods evaluations recent language modeling topics selecting conversational data internet adapting models foreign words multi domain adapted neural network language modeling decoding subword units evaluations show methods work language scale smaller data resources",0
"KeywordsErrors Dyslexia Visual Phonetics Resource Spanish ","a resource of errors written in spanish by people with dyslexia and its linguistic phonetic and visual analysis","In this work we introduce the analysis of DysList, a language resource for Spanish composed of a list of unique spelling errors extracted from a collection of texts written by people with dyslexia. Each of the errors was annotated with a set of characteristics as well as with visual and phonetic features. To the best of our knowledge, this is the largest resource of this kind in Spanish. We also analyzed all the features of Spanish errors and our main finding is that dyslexic errors are phonetically and visually motivated.","Language Resources and Evaluation",2017,"No"," errors dyslexia visual phonetics resource spanish resource errors written spanish people dyslexia linguistic phonetic visual analysis work introduce analysis dyslist language resource spanish composed list unique spelling errors extracted collection texts written people dyslexia errors annotated set characteristics visual phonetic features knowledge largest resource kind spanish analyzed features spanish errors main finding dyslexic errors phonetically visually motivated",0
"KeywordsNamed entity recognition Information extraction Rule-based approach Machine learning Hybrid approach Natural language processing ","studying the impact of language independent and language specific features on hybrid arabic person name recognition","In this paper, extensive experiments are conducted to study the impact of features of different categories, in isolation and gradually in an incremental manner, on Arabic Person name recognition. We present an integrated system that employs the rule-based approach with the machine learning (ML)-based approach in order to develop a consolidated hybrid system. Our feature space is comprised of language-independent and language-specific features. The explored features are naturally grouped under six categories: Person named entity tags predicted by the rule-based component, word-level features, POS features, morphological features, gazetteer features, and other contextual features. As decision tree algorithm has proved comparatively higher efficiency as a classifier in current state-of-the-art hybrid Named Entity Recognition for Arabic, it is adopted in this study as the ML technique utilized by the hybrid system. Therefore, the experiments are focused on two dimensions: the standard dataset used and the set of selected features. A number of standard datasets are used for the training and testing of the hybrid system, including ACE (2003–2004) and ANERcorp. The experimental analysis indicates that both language-independent and language-specific features play an important role in overcoming the challenges posed by Arabic language and have demonstrated critical impact on optimizing the performance of the hybrid system.","Language Resources and Evaluation",2017,"No"," named entity recognition information extraction rule based approach machine learning hybrid approach natural language processing studying impact language independent language specific features hybrid arabic person recognition paper extensive experiments conducted study impact features categories isolation gradually incremental manner arabic person recognition present integrated system employs rule based approach machine learning ml based approach order develop consolidated hybrid system feature space comprised language independent language specific features explored features naturally grouped categories person named entity tags predicted rule based component word level features pos features morphological features gazetteer features contextual features decision tree algorithm proved comparatively higher efficiency classifier current state art hybrid named entity recognition arabic adopted study ml technique utilized hybrid system experiments focused dimensions standard dataset set selected features number standard datasets training testing hybrid system including ace anercorp experimental analysis language independent language specific features play important role overcoming challenges posed arabic language demonstrated critical impact optimizing performance hybrid system",0
"KeywordsWeakly supervised learning POS tagging Cross-lingual transfer ","reassessing the value of resources for cross lingual transfer of pos tagging models","When linguistically annotated data is scarce, as is the case for many under-resourced languages, one has to resort to less complete forms of annotations obtained from crawled dictionaries and/or through cross-lingual transfer. Several recent works have shown that learning from such partially supervised data can be effective in many practical situations. In this work, we review two existing proposals for learning with ambiguous labels which extend conventional learners to the weakly supervised setting: a history-based model using a variant of the perceptron, on the one hand; an extension of the Conditional Random Fields model on the other hand. Focusing on the part-of-speech tagging task, but considering a large set of ten languages, we show (a) that good performance can be achieved even in the presence of ambiguity, provided however that both monolingual and bilingual resources are available; (b) that our two learners exploit different characteristics of the training set, and are successful in different situations; (c) that in addition to the choice of an adequate learning algorithm, many other factors are critical for achieving good performance in a cross-lingual transfer setting.","Language Resources and Evaluation",2017,"No"," weakly supervised learning pos tagging cross lingual transfer reassessing resources cross lingual transfer pos tagging models linguistically annotated data scarce case resourced languages resort complete forms annotations obtained crawled dictionaries cross lingual transfer recent works shown learning partially supervised data effective practical situations work review existing proposals learning ambiguous labels extend conventional learners weakly supervised setting history based model variant perceptron hand extension conditional random fields model hand focusing part speech tagging task large set ten languages show good performance achieved presence ambiguity provided monolingual bilingual resources learners exploit characteristics training set successful situations addition choice adequate learning algorithm factors critical achieving good performance cross lingual transfer setting",0
"KeywordsWord similarity Word embeddings Count-based models Dependency-based semantic models ","comparing explicit and predictive distributional semantic models endowed with syntactic contexts","In this article, we introduce an explicit count-based strategy to build word space models with syntactic contexts (dependencies). A filtering method is defined to reduce explicit word-context vectors. 
This traditional strategy is compared with a neural embedding (predictive) model also based on syntactic dependencies. The comparison was performed using the same parsed corpus for both models. Besides,
 the dependency-based methods are also compared with bag-of-words strategies, both count-based and predictive ones. The results show that our traditional count-based model with syntactic dependencies outperforms other strategies, including dependency-based embeddings, but just for the tasks focused on discovering similarity between words with the same function (i.e. near-synonyms).","Language Resources and Evaluation",2017,"No"," word similarity word embeddings count based models dependency based semantic models comparing explicit predictive distributional semantic models endowed syntactic contexts article introduce explicit count based strategy build word space models syntactic contexts dependencies filtering method defined reduce explicit word context vectors traditional strategy compared neural embedding predictive model based syntactic dependencies comparison performed parsed corpus models dependency based methods compared bag words strategies count based predictive results show traditional count based model syntactic dependencies outperforms strategies including dependency based embeddings tasks focused discovering similarity words function synonyms",0
"KeywordsMultilingual and multigenre corpus Crowdsourcing Speech transcription Automatic speech recognition ","creating a ground truth multilingual dataset of news and talk show transcriptions through crowdsourcing","This paper describes the development of a multilingual and multigenre manually annotated speech dataset, freely available to the research community as ground truth for the evaluation of automatic transcription systems and spoken language translation systems. The dataset includes two video genres—television broadcast news and talk-shows—and covers Flemish, English, German, and Italian, for a total of about 35 h of television speech. Besides segmentation and orthographic transcription, we added a very rich annotation on the audio signal, both at the linguistic level (e.g. filled pauses, pronunciation errors, disfluencies, speech in a foreign language) and at the acoustic level (e.g. background noise and different types of non-speech events). Furthermore, a subset of the transcriptions is translated in four directions, namely Flemish to English, German to English, German to Italian and English to Italian. The development of this dataset was organized in several phases, relying on expert transcribers as well as involving non-expert contributors through crowdsourcing. We first conducted a feasibility study to test and compare two methods for crowdsourcing speech transcription on broadcast news data. These methods are based on different transcription processes (i.e. parallel vs. iterative) and incorporate two different quality control mechanisms. With both methods, we achieved near-expert transcription quality—in terms of word error rate—for English, German and Italian data. Instead, for Flemish data we were not able to get a sufficient response from the crowd to complete the offered transcription tasks. The results obtained demonstrate that the viability of methods for crowdsourcing speech transcription significantly depends on the target language. This paper provides a detailed comparison of the results obtained with the two crowdsourcing methods tested, describes the main characteristics of the final ground truth resource created as well as the methodology adopted, and the guidelines prepared for its development.","Language Resources and Evaluation",2017,"Yes"," multilingual multigenre corpus crowdsourcing speech transcription automatic speech recognition creating ground truth multilingual dataset news talk show transcriptions crowdsourcing paper describes development multilingual multigenre manually annotated speech dataset freely research community ground truth evaluation automatic transcription systems spoken language translation systems dataset includes video genres television broadcast news talk shows covers flemish english german italian total television speech segmentation orthographic transcription added rich annotation audio signal linguistic level filled pauses pronunciation errors disfluencies speech foreign language acoustic level background noise types speech events subset transcriptions translated directions flemish english german english german italian english italian development dataset organized phases relying expert transcribers involving expert contributors crowdsourcing conducted feasibility study test compare methods crowdsourcing speech transcription broadcast news data methods based transcription processes parallel iterative incorporate quality control mechanisms methods achieved expert transcription quality terms word error rate english german italian data flemish data sufficient response crowd complete offered transcription tasks results obtained demonstrate viability methods crowdsourcing speech transcription significantly depends target language paper detailed comparison results obtained crowdsourcing methods tested describes main characteristics final ground truth resource created methodology adopted guidelines prepared development",1
"KeywordsSentiment analysis Opinion mining Sentiment lexicon Polarity detection Emotion classification Semi-automatic translation ","feel a french expanded emotion lexicon","Sentiment analysis allows the semantic evaluation of pieces of text according to the expressed sentiments and opinions. While considerable attention has been given to the polarity (positive, negative) of English words, only few studies were interested in the conveyed emotions (joy, anger, surprise, sadness, etc.) especially in other languages. In this paper, we present the elaboration and the evaluation of a new French lexicon considering both polarity and emotion. The elaboration method is based on the semi-automatic translation and expansion to synonyms of the English NRC Word Emotion Association Lexicon (NRC-EmoLex). First, online translators have been automatically queried in order to create a first version of our new French Expanded Emotion Lexicon (FEEL). Then, a human professional translator manually validated the automatically obtained entries and the associated emotions. She agreed with more than 94 % of the pre-validated entries (those found by a majority of translators) and less than 18 % of the remaining entries (those found by very few translators). This result highlights that online tools can be used to get high quality resources with low cost. Annotating a subset of terms by three different annotators shows that the associated sentiments and emotions are consistent. Finally, extensive experiments have been conducted to compare the final version of FEEL with other existing French lexicons. Various French benchmarks for polarity and emotion classifications have been used in these evaluations. Experiments have shown that FEEL obtains competitive results for polarity, and significantly better results for basic emotions.","Language Resources and Evaluation",2017,"Yes"," sentiment analysis opinion mining sentiment lexicon polarity detection emotion classification semi automatic translation feel french expanded emotion lexicon sentiment analysis semantic evaluation pieces text expressed sentiments opinions considerable attention polarity positive negative english words studies interested conveyed emotions joy anger surprise sadness languages paper present elaboration evaluation french lexicon polarity emotion elaboration method based semi automatic translation expansion synonyms english nrc word emotion association lexicon nrc emolex online translators automatically queried order create version french expanded emotion lexicon feel human professional translator manually validated automatically obtained entries emotions agreed pre validated entries found majority translators remaining entries found translators result highlights online tools high quality resources low cost annotating subset terms annotators shows sentiments emotions consistent finally extensive experiments conducted compare final version feel existing french lexicons french benchmarks polarity emotion classifications evaluations experiments shown feel obtains competitive results polarity significantly results basic emotions",1
"KeywordsSemantic annotation Software requirements Semantic role labeling ","software requirements as an application domain for natural language processing","Mapping functional requirements first to specifications and then to code is one of the most challenging tasks in software development. Since requirements are commonly written in natural language, they can be prone to ambiguity, incompleteness and inconsistency. Structured semantic representations allow requirements to be translated to formal models, which can be used to detect problems at an early stage of the development process through validation. Storing and querying such models can also facilitate software reuse. Several approaches constrain the input format of requirements to produce specifications, however they usually require considerable human effort in order to adopt domain-specific heuristics and/or controlled languages. We propose a mechanism that automates the mapping of requirements to formal representations using semantic role labeling. We describe the first publicly available dataset for this task, employ a hierarchical framework that allows requirements concepts to be annotated, and discuss how semantic role labeling can be adapted for parsing software requirements.
","Language Resources and Evaluation",2017,"No"," semantic annotation software requirements semantic role labeling software requirements application domain natural language processing mapping functional requirements specifications code challenging tasks software development requirements commonly written natural language prone ambiguity incompleteness inconsistency structured semantic representations requirements translated formal models detect problems early stage development process validation storing querying models facilitate software reuse approaches constrain input format requirements produce specifications require considerable human effort order adopt domain specific heuristics controlled languages propose mechanism automates mapping requirements formal representations semantic role labeling describe publicly dataset task employ hierarchical framework requirements concepts annotated discuss semantic role labeling adapted parsing software requirements ",0
"KeywordsAutomated gesture analysis Gesture studies Type of gestures Gestures phases Gesture recognition Gesture segmentation ","studies in automated hand gesture analysis an overview of functional types and gesture phases","
This paper presents an overview of studies on automated hand gesture analysis, which is mainly concerned with recognition and segmentation issues related to functional types and gesture phases. The issues selected for discussion have been arranged in a way that takes account of problems within the Theory of Gestures that each study seeks to address. Their principal computational factors that were involved in conducting the analysis of automated hand gesture have been examined, and an analysis of open research issues has been carried out for each application dealt with in the studies.
","Language Resources and Evaluation",2017,"No"," automated gesture analysis gesture studies type gestures gestures phases gesture recognition gesture segmentation studies automated hand gesture analysis overview functional types gesture phases paper presents overview studies automated hand gesture analysis concerned recognition segmentation issues related functional types gesture phases issues selected discussion arranged takes account problems theory gestures study seeks address principal computational factors involved conducting analysis automated hand gesture examined analysis open research issues carried application dealt studies ",0
"KeywordsControlled hybrid language Controlled visual language Controlled natural language Electronic navigational charts Maritime navigation ","a hybrid visualnatural controlled language","We define the notion of controlled hybrid language that allows information share and interaction between a controlled natural language (specified by a context-free grammar) and a controlled visual language (specified by a Symbol-Relation grammar). We present the controlled hybrid language INAUT, used to represent nautical charts of the French Naval and Hydrographic Service (SHOM) and their companion texts (Instructions nautiques).","Language Resources and Evaluation",2017,"No"," controlled hybrid language controlled visual language controlled natural language electronic navigational charts maritime navigation hybrid visualnatural controlled language define notion controlled hybrid language information share interaction controlled natural language context free grammar controlled visual language symbol relation grammar present controlled hybrid language inaut represent nautical charts french naval hydrographic service shom companion texts instructions nautiques",0
NA,"lre journal cnl introduction","A controlled natural language (CNL) is based on a natural language but includes restrictions on vocabulary, grammar, and/or semantics, in order to reduce or eliminate ambiguity and complexity.
","Language Resources and Evaluation",2017,"No"," lre journal cnl introduction controlled natural language cnl based natural language includes restrictions vocabulary grammar semantics order reduce eliminate ambiguity complexity ",0
"KeywordsMachine translation Catalogue of phrases Controlled natural language Text quality Avalanche warning ","fully automatic multi language translation with a catalogue of phrases successful employment for the swiss avalanche bulletin","The Swiss avalanche bulletin is produced twice a day in four languages. Due to the lack of time available for manual translation, a fully automated translation system is employed, based on a catalogue of predefined phrases and predetermined rules of how these phrases can be combined to produce sentences. Because this catalogue of phrases is limited to a small sublanguage, the system is able to automatically translate such sentences from German into the target languages French, Italian and English without subsequent proofreading or correction. Having been operational for two winter seasons, we assess here the quality of the produced texts based on two different surveys where participants rated texts from real avalanche bulletins from both origins, the catalogue of phrases versus manually written and translated texts. With a mean recognition rate of 55 %, users can hardly distinguish between the two types of texts, and give very similar ratings with respect to their language quality. Overall, the output from the catalogue system can be considered virtually equivalent to a text written by avalanche forecasters and then manually translated by professional translators. Furthermore, forecasters declared that all relevant situations were captured by the system with sufficient accuracy. Forecaster’s working load did not change with the introduction of the catalogue: the extra time to find matching sentences is compensated by the fact that they no longer need to double-check manually translated texts. The reduction of daily translation costs is expected to offset the initial development costs within a few years.","Language Resources and Evaluation",2017,"Yes"," machine translation catalogue phrases controlled natural language text quality avalanche warning fully automatic multi language translation catalogue phrases successful employment swiss avalanche bulletin swiss avalanche bulletin produced day languages due lack time manual translation fully automated translation system employed based catalogue predefined phrases predetermined rules phrases combined produce sentences catalogue phrases limited small sublanguage system automatically translate sentences german target languages french italian english subsequent proofreading correction operational winter seasons assess quality produced texts based surveys participants rated texts real avalanche bulletins origins catalogue phrases versus manually written translated texts recognition rate users distinguish types texts give similar ratings respect language quality output catalogue system considered virtually equivalent text written avalanche forecasters manually translated professional translators forecasters declared relevant situations captured system sufficient accuracy forecaster working load change introduction catalogue extra time find matching sentences compensated fact longer double check manually translated texts reduction daily translation costs expected offset initial development costs years",1
"KeywordsControlled natural languages Ontology authoring Semantic web, state of the art ","cnls for the semantic web a state of the art","One of the core challenges for building the semantic web is the creation of ontologies, a process known as ontology authoring. Controlled natural languages (CNLs) propose different frameworks for interfacing and creating ontologies in semantic web systems using restricted natural language. However, in order to engage non-expert users with no background in knowledge engineering, these language interfacing must be reliable, easy to understand and accepted by users. This paper includes the state-of-the-art for CNLs in terms of ontology authoring and the semantic web. In addition, it includes a detailed analysis of user evaluations with respect to each CNL and offers analytic conclusions with respect to the field.","Language Resources and Evaluation",2017,"No"," controlled natural languages ontology authoring semantic web state art cnls semantic web state art core challenges building semantic web creation ontologies process ontology authoring controlled natural languages cnls propose frameworks interfacing creating ontologies semantic web systems restricted natural language order engage expert users background knowledge engineering language interfacing reliable easy understand accepted users paper includes state art cnls terms ontology authoring semantic web addition includes detailed analysis user evaluations respect cnl offers analytic conclusions respect field",0
"KeywordsCompositionality Computational semantics Distributional semantics models ","sick through the semeval glasses lesson learned from the evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment","This paper is an extended description of SemEval-2014 Task 1, the task on the evaluation of Compositional Distributional Semantics Models on full sentences. Systems participating in the task were presented with pairs of sentences and were evaluated on their ability to predict human judgments on (1) semantic relatedness and (2) entailment.
 Training and testing data were subsets of the SICK (Sentences Involving Compositional Knowledge) data set. SICK was developed with the aim of providing a proper benchmark to evaluate compositional semantic systems, though task participation was open to systems based on any approach. Taking advantage of the SemEval experience, in this paper we analyze the SICK data set, in order to evaluate the extent to which it meets its design goal and to shed light on the linguistic phenomena that are still challenging for state-of-the-art computational semantic systems.
 Qualitative and quantitative error analyses show that many systems are quite sensitive to changes in the proportion of sentence pair types, and degrade in the presence of additional lexico-syntactic complexities which do not affect human judgements. More compositional systems seem to perform better when the task proportions are changed, but the effect needs further confirmation.
","Language Resources and Evaluation",2016,"No"," compositionality computational semantics distributional semantics models sick semeval glasses lesson learned evaluation compositional distributional semantic models full sentences semantic relatedness textual entailment paper extended description semeval task task evaluation compositional distributional semantics models full sentences systems participating task presented pairs sentences evaluated ability predict human judgments semantic relatedness entailment training testing data subsets sick sentences involving compositional knowledge data set sick developed aim providing proper benchmark evaluate compositional semantic systems task participation open systems based approach taking advantage semeval experience paper analyze sick data set order evaluate extent meets design goal shed light linguistic phenomena challenging state art computational semantic systems qualitative quantitative error analyses show systems sensitive proportion sentence pair types degrade presence additional lexico syntactic complexities affect human judgements compositional systems perform task proportions changed effect confirmation ",0
"KeywordsELRA Anthology Language resources Language processing systems evaluation Text analytics Social networks ISLRN Bibliometrics Scientometrics ","rediscovering 152years of discoveries in language resources and evaluation","This paper analyzes the content of the proceedings of the Language Resources and Evaluation Conference (LREC) over the past 17 years (1998–2014), with the goal of gaining a picture of the LREC community and the topics that are most relevant to the field. We follow the methodology used in similar studies, including the survey of the IEEE ICASSP conference proceedings from 1976 to 1990, the survey of the Association of Computational Linguistics conference proceedings over 50 years, and the survey of the proceedings of the conferences contained in the ISCA Archive over 25 years (1987–2012). We expand on results originally presented at LREC 2014, but include the proceedings of LREC 2014 itself in the study together with an analysis of various citation graphs. We show the evolution over time of the number of papers and authors, including their distribution by gender and affiliation, as well as collaborations and citation patterns among authors and papers, funding sources for reported research, and plagiarism and reuse in LREC papers; results for LREC are compared with similar results for major conferences in related fields. We also consider the evolution of research topics over time and identify the authors who introduced key terms. Finally, we propose and apply a measure of a researcher’s notability and provide the results for LREC authors. The study uses NLP methods that have been published in the corpus considered in the study. In addition to providing a revealing characterization of the LRE community, the study also demonstrates the need for establishing a system for unique identification of authors, papers and other sources to facilitate this type of analysis.","Language Resources and Evaluation",2016,"No"," elra anthology language resources language processing systems evaluation text analytics social networks islrn bibliometrics scientometrics rediscovering years discoveries language resources evaluation paper analyzes content proceedings language resources evaluation conference lrec past years goal gaining picture lrec community topics relevant field follow methodology similar studies including survey ieee icassp conference proceedings survey association computational linguistics conference proceedings years survey proceedings conferences contained isca archive years expand results originally presented lrec include proceedings lrec study analysis citation graphs show evolution time number papers authors including distribution gender affiliation collaborations citation patterns authors papers funding sources reported research plagiarism reuse lrec papers results lrec compared similar results major conferences related fields evolution research topics time identify authors introduced key terms finally propose apply measure researcher notability provide results lrec authors study nlp methods published corpus considered study addition providing revealing characterization lre community study demonstrates establishing system unique identification authors papers sources facilitate type analysis",0
"KeywordsLexical resources INESS NorGramBank Treebanking LFG Language research infrastructure Automatic syntactic analysis ","the enrichment of lexical resources through incremental parsebanking","Automatic syntactic analysis of a corpus requires detailed lexical and morphological information that cannot always be harvested from traditional dictionaries. Therefore the development of a treebank presents an opportunity to simultaneously enrich the lexicon. In building NorGramBank, we use an incremental parsebanking approach, in which a corpus is parsed and disambiguated, and after improvements to the grammar and the lexicon, reparsed. In this context we have implemented a text preprocessing interface where annotators can enter unknown words or missing lexical information either before parsing or during disambiguation. The information added to the lexicon in this way may be of great interest both to lexicographers and to other language technology efforts.","Language Resources and Evaluation",2016,"Yes"," lexical resources iness norgrambank treebanking lfg language research infrastructure automatic syntactic analysis enrichment lexical resources incremental parsebanking automatic syntactic analysis corpus requires detailed lexical morphological information harvested traditional dictionaries development treebank presents opportunity simultaneously enrich lexicon building norgrambank incremental parsebanking approach corpus parsed disambiguated improvements grammar lexicon reparsed context implemented text preprocessing interface annotators enter unknown words missing lexical information parsing disambiguation information added lexicon great interest lexicographers language technology efforts",1
"KeywordsStatistical Machine Translation Language Pair Word Alignment Source Sentence Source Word ","reordering space design in statistical machine translation","In Statistical Machine Translation (SMT), the constraints on word reorderings have a great impact on the set of potential translations that is explored during search. Notwithstanding computational issues, the reordering space of a SMT system needs to be designed with great care: if a larger search space is likely to yield better translations, it may also lead to more decoding errors, because of the added ambiguity and the interaction with the pruning strategy. In this paper, we study the reordering search space, using a state-of-the art translation system, where all reorderings are represented in a permutation lattice prior to decoding. This allows us to directly explore and compare different reordering schemes and oracle settings. We also study in detail a rule-based preordering system, varying the length and number of rules, the tagset used, as well as contrasting with purely combinatorial subsets of permutations. We carry out experiments on three language pairs in both directions: English-French, a close language pair; English-German and English-Czech, two much more challenging pairs. We show that even though it might be desirable to design better reordering spaces, model and search errors seem to be the most important issues. Therefore, improvements of the reordering space should come along with improvements of the associated models to be really effective.","Language Resources and Evaluation",2016,"No"," statistical machine translation language pair word alignment source sentence source word reordering space design statistical machine translation statistical machine translation smt constraints word reorderings great impact set potential translations explored search notwithstanding computational issues reordering space smt system designed great care larger search space yield translations lead decoding errors added ambiguity interaction pruning strategy paper study reordering search space state art translation system reorderings represented permutation lattice prior decoding directly explore compare reordering schemes oracle settings study detail rule based preordering system varying length number rules tagset contrasting purely combinatorial subsets permutations carry experiments language pairs directions english french close language pair english german english czech challenging pairs show desirable design reordering spaces model search errors important issues improvements reordering space improvements models effective",0
"KeywordsEmbodied conversational agents Prosody Dialogue acts ","towards the generation of dialogue acts in socio affective ecas a corpus based prosodic analysis","We present a corpus-based prosodic analysis with the aim of uncovering the relationship between dialogue acts, personality and prosody in view to providing guidelines for the ECA Greta’s text-to-speech system. The corpus used is the SEMAINE corpus, featuring four different personalities, further annotated for dialogue acts and prosodic features. In order to show the importance of the choice of dialogue act taxonomy, two different taxonomies were used, the first corresponding to Searle’s taxonomy of speech acts and the second, inspired by Bunt’s DIT++, including a division of directive acts into finer categories. Our results show that finer-grained distinctions are important when choosing a taxonomy. We also show with some preliminary results that the prosodic correlates of dialogue acts are not always as cited in the literature and prove more complex and variable. By studying the realisation of different directive acts, we also observe differences in the communicative strategies of the ECA depending on personality, in view to providing input to a speech system.","Language Resources and Evaluation",2016,"Yes"," embodied conversational agents prosody dialogue acts generation dialogue acts socio affective ecas corpus based prosodic analysis present corpus based prosodic analysis aim uncovering relationship dialogue acts personality prosody view providing guidelines eca greta text speech system corpus semaine corpus featuring personalities annotated dialogue acts prosodic features order show importance choice dialogue act taxonomy taxonomies searle taxonomy speech acts inspired bunt dit including division directive acts finer categories results show finer grained distinctions important choosing taxonomy show preliminary results prosodic correlates dialogue acts cited literature prove complex variable studying realisation directive acts observe differences communicative strategies eca depending personality view providing input speech system",1
"KeywordsAutomatic speech recognition Slavic languages Polish Speech corpus Text to speech ","agh corpus of polish speech","A corpus of Polish speech, which has been collected for the purpose of automatic speech recognition (ASR) and text-to-speech (TTS) systems applications, is presented. The corpus consists of several groups of recordings: read sentences, spoken commands, a phonetically balanced TTS training corpus, telephonic speech and others. In summary duration of recordings is above 25 h. Number of unique speakers amounts to 166. The majority of them being in an age group of 20–35 and one third of them being female.
 Analysis of unique word occurrence frequency in relation to larger text resources has been concluded. From them, most commonly appearing words have been found and presented. The corpus was used as training data for the ASR system. Results of cross-validation training and testing the SARMATA ASR system using our corpus have shown that phrase recognition rate is 91.9 %. The corpus was additionally evaluated in comparative test against the CORPORA corpus, which had shown major increase in phrase recognition rate in favour of our corpus.","Language Resources and Evaluation",2016,"Yes"," automatic speech recognition slavic languages polish speech corpus text speech agh corpus polish speech corpus polish speech collected purpose automatic speech recognition asr text speech tts systems applications presented corpus consists groups recordings read sentences spoken commands phonetically balanced tts training corpus telephonic speech summary duration recordings number unique speakers amounts majority age group female analysis unique word occurrence frequency relation larger text resources concluded commonly appearing words found presented corpus training data asr system results cross validation training testing sarmata asr system corpus shown phrase recognition rate corpus additionally evaluated comparative test corpora corpus shown major increase phrase recognition rate favour corpus",1
"KeywordsCorpus Evidence based medicine Annotation Crowdsourcing Text summarization ","a corpus for research in text processing for evidence based medicine","Evidence based medicine (EBM) urges the medical doctor to incorporate the latest available clinical evidence at point of care. A major stumbling block in the practice of EBM is the difficulty to keep up to date with the clinical advances. In this paper we describe a corpus designed for the development and testing of text processing tools for EBM, in particular for tasks related to the extraction and summarisation of answers and corresponding evidence related to a clinical query. The corpus is based on material from the Clinical Inquiries section of The Journal of Family Practice. It was gathered and annotated by a combination of automated information extraction, crowdsourcing tasks, and manual annotation. It has been used for the original summarisation task for which it was designed, as well as for other related tasks such as the appraisal of clinical evidence and the clustering of the results. The corpus is available at SourceForge (http://sourceforge.net/projects/ebmsumcorpus/).","Language Resources and Evaluation",2016,"Yes"," corpus evidence based medicine annotation crowdsourcing text summarization corpus research text processing evidence based medicine evidence based medicine ebm urges medical doctor incorporate latest clinical evidence point care major stumbling block practice ebm difficulty date clinical advances paper describe corpus designed development testing text processing tools ebm tasks related extraction summarisation answers evidence related clinical query corpus based material clinical inquiries section journal family practice gathered annotated combination automated information extraction crowdsourcing tasks manual annotation original summarisation task designed related tasks appraisal clinical evidence clustering results corpus sourceforge httpsourceforgenetprojectsebmsumcorpus",1
"KeywordsHistorical Arabic corpus Corpus tools Natural language processing Arabic word usage over time Semantic change ","exploring and exploiting a historical corpus for arabic","

This paper presents a historical Arabic corpus named HAC. At this early embryonic stage of the project, we report about the design, the architecture and some of the experiments which we have conducted on HAC. The corpus, and accordingly the search results, will be represented using a primary XML exchange format. This will serve as an intermediate exchange tool within the project and will allow the user to process the results offline using some external tools. HAC is made up of Classical Arabic texts that cover 1600 years of language use; the Quranic text, Modern Standard Arabic texts, as well as a variety of monolingual Arabic dictionaries. The development of this historical corpus assists linguists and Arabic language learners to effectively explore, understand, and discover interesting knowledge hidden in millions of instances of language use. We used techniques from the field of natural language processing to process the data and a graph-based representation for the corpus. We provided researchers with an export facility to render further linguistic analysis possible.","Language Resources and Evaluation",2016,"Yes"," historical arabic corpus corpus tools natural language processing arabic word usage time semantic change exploring exploiting historical corpus arabic paper presents historical arabic corpus named hac early embryonic stage project report design architecture experiments conducted hac corpus search results represented primary xml exchange format serve intermediate exchange tool project user process results offline external tools hac made classical arabic texts cover years language quranic text modern standard arabic texts variety monolingual arabic dictionaries development historical corpus assists linguists arabic language learners effectively explore understand discover interesting knowledge hidden millions instances language techniques field natural language processing process data graph based representation corpus provided researchers export facility render linguistic analysis ",1
"KeywordsCorpus annotation Annotation guidelines Clinical text Chunking Named entities ","annotating patient clinical records with syntactic chunks and named entities the harvey corpus","The free text notes typed by physicians during patient consultations contain valuable information for the study of disease and treatment. These notes are difficult to process by existing natural language analysis tools since they are highly telegraphic (omitting many words), and contain many spelling mistakes, inconsistencies in punctuation, and non-standard word order. To support information extraction and classification tasks over such text, we describe a de-identified corpus of free text notes, a shallow syntactic and named entity annotation scheme for this kind of text, and an approach to training domain specialists with no linguistic background to annotate the text. Finally, we present a statistical chunking system for such clinical text with a stable learning rate and good accuracy, indicating that the manual annotation is consistent and that the annotation scheme is tractable for machine learning.","Language Resources and Evaluation",2016,"Yes"," corpus annotation annotation guidelines clinical text chunking named entities annotating patient clinical records syntactic chunks named entities harvey corpus free text notes typed physicians patient consultations valuable information study disease treatment notes difficult process existing natural language analysis tools highly telegraphic omitting words spelling mistakes inconsistencies punctuation standard word order support information extraction classification tasks text describe de identified corpus free text notes shallow syntactic named entity annotation scheme kind text approach training domain specialists linguistic background annotate text finally present statistical chunking system clinical text stable learning rate good accuracy indicating manual annotation consistent annotation scheme tractable machine learning",1
"KeywordsImplicit argument Deverbal nominalizations Argument structure Thematic roles Semantic corpus annotation Linguistic resource ","iarg ancora spanish corpus annotated with implicit arguments","This article presents the Spanish Iarg-AnCora corpus (400 k-words, 13,883 sentences) annotated with the implicit arguments of deverbal nominalizations (18,397 occurrences). We describe the methodology used to create it, focusing on the annotation scheme and criteria adopted. The corpus was manually annotated and an interannotator agreement test was conducted (81 % observed agreement) in order to ensure the reliability of the final resource. The annotation of implicit arguments results in an important gain in argument and thematic role coverage (128 % on average). It is the first corpus annotated with implicit arguments for the Spanish language with a wide coverage that is freely available. This corpus can subsequently be used by machine learning-based semantic role labeling systems, and for the linguistic analysis of implicit arguments grounded on real data. Semantic analyzers are essential components of current language technology applications, which need to obtain a deeper understanding of the text in order to make inferences at the highest level to obtain qualitative improvements in the results.","Language Resources and Evaluation",2016,"Yes"," implicit argument deverbal nominalizations argument structure thematic roles semantic corpus annotation linguistic resource iarg ancora spanish corpus annotated implicit arguments article presents spanish iarg ancora corpus words sentences annotated implicit arguments deverbal nominalizations occurrences describe methodology create focusing annotation scheme criteria adopted corpus manually annotated interannotator agreement test conducted observed agreement order ensure reliability final resource annotation implicit arguments results important gain argument thematic role coverage average corpus annotated implicit arguments spanish language wide coverage freely corpus subsequently machine learning based semantic role labeling systems linguistic analysis implicit arguments grounded real data semantic analyzers essential components current language technology applications obtain deeper understanding text order make inferences highest level obtain qualitative improvements results",1
"KeywordsActive listening Multimodal feedback Backchannels Head gestures Attention Multimodal corpus ","the alico corpus analysing the active listener","The Active Listening Corpus (ALICO) is a multimodal data set of spontaneous dyadic conversations in German with diverse speech and gestural annotations of both dialogue partners. The annotations consist of short feedback expression transcriptions with corresponding communicative function interpretations as well as segmentations of interpausal units, words, rhythmic prominence intervals and vowel-to-vowel intervals. Additionally, ALICO contains head gesture annotations of both interlocutors. The corpus contributes to research on spontaneous human–human interaction, on functional relations between modalities, and timing variability in dialogue. It also provides data that differentiates between distracted and attentive listeners. We describe the main characteristics of the corpus and briefly present the most important results obtained from analyses in recent years.","Language Resources and Evaluation",2016,"Yes"," active listening multimodal feedback backchannels head gestures attention multimodal corpus alico corpus analysing active listener active listening corpus alico multimodal data set spontaneous dyadic conversations german diverse speech gestural annotations dialogue partners annotations consist short feedback expression transcriptions communicative function interpretations segmentations interpausal units words rhythmic prominence intervals vowel vowel intervals additionally alico head gesture annotations interlocutors corpus contributes research spontaneous human human interaction functional relations modalities timing variability dialogue data differentiates distracted attentive listeners describe main characteristics corpus briefly present important results obtained analyses recent years",1
"KeywordsGenres on the web Reliability testing Annotation guidelines Crowdsourcing ","crowdsourcing for web genre annotation","Recently, genre collection and automatic genre identification for the web has attracted much attention. However, currently there is no genre-annotated corpus of web pages where inter-annotator reliability has been established, i.e. the corpora are either not tested for inter-annotator reliability or exhibit low inter-coder agreement. Annotation has also mostly been carried out by a small number of experts, leading to concerns with regard to scalability of these annotation efforts and transferability of the schemes to annotators outside these small expert groups. In this paper, we tackle these problems by using crowd-sourcing for genre annotation, leading to the Leeds Web Genre Corpus—the first web corpus which is, demonstrably reliably annotated for genre and which can be easily and cost-effectively expanded using naive annotators. We also show that the corpus is source and topic diverse.","Language Resources and Evaluation",2016,"Yes"," genres web reliability testing annotation guidelines crowdsourcing crowdsourcing web genre annotation recently genre collection automatic genre identification web attracted attention genre annotated corpus web pages inter annotator reliability established corpora tested inter annotator reliability exhibit low inter coder agreement annotation carried small number experts leading concerns regard scalability annotation efforts transferability schemes annotators small expert groups paper tackle problems crowd sourcing genre annotation leading leeds web genre corpus web corpus demonstrably reliably annotated genre easily cost effectively expanded naive annotators show corpus source topic diverse",1
"KeywordsPartial annotation Domain adaptation Dictionary Word segmentation POS tagging Non-maleficence of language resources ","a comparative study of dictionaries and corpora as methods for language resource addition","In this paper, we investigate the relative effect of two strategies for language resource addition for Japanese morphological analysis, a joint task of word segmentation and part-of-speech tagging. The first strategy is adding entries to the dictionary and the second is adding annotated sentences to the training corpus. The experimental results showed that addition of annotated sentences to the training corpus is better than the addition of entries to the dictionary. In particular, adding annotated sentences is especially efficient when we add new words with contexts of several real occurrences as partially annotated sentences, i.e. sentences in which only some words are annotated with word boundary information. According to this knowledge, we performed real annotation experiments on invention disclosure texts and observed word segmentation accuracy. Finally we investigated various language resource addition cases and introduced the notion of non-maleficence, asymmetricity, and additivity of language resources for a task. In the WS case, we found that language resource addition is non-maleficent (adding new resources causes no harm in other domains) and sometimes additive (adding new resources helps other domains). We conclude that it is reasonable for us, NLP tool providers, to distribute only one general-domain model trained from all the language resources we have.","Language Resources and Evaluation",2016,"No"," partial annotation domain adaptation dictionary word segmentation pos tagging maleficence language resources comparative study dictionaries corpora methods language resource addition paper investigate relative effect strategies language resource addition japanese morphological analysis joint task word segmentation part speech tagging strategy adding entries dictionary adding annotated sentences training corpus experimental results showed addition annotated sentences training corpus addition entries dictionary adding annotated sentences efficient add words contexts real occurrences partially annotated sentences sentences words annotated word boundary information knowledge performed real annotation experiments invention disclosure texts observed word segmentation accuracy finally investigated language resource addition cases introduced notion maleficence asymmetricity additivity language resources task ws case found language resource addition maleficent adding resources harm domains additive adding resources helps domains conclude reasonable nlp tool providers distribute general domain model trained language resources ",0
"KeywordsAppraisal Parameters Corpus linguistics Evaluative language Customer review texts ","how products are evaluated evaluation in customer review texts","This study, drawing on insights from the Appraisal framework, the parameter-based approach to evaluation and corpus linguistics, investigates the evaluative language used in customer review texts. The primary goal of this investigation is to develop a framework of evaluation that can be used to account adequately for evaluative expressions in customer review texts, and the ultimate goal is to support the argument that the modelling and theorising of evaluation is context-specific. Based on the investigation into a corpus compiled of review texts retrieved from www.amazon.co.uk, this study proposes a data-driven, parameter-based and appraisal-informed framework of evaluation which comprises four parameters—Quality, Satisfactoriness, Recommendability and Worthiness. Since these parameters are not thought-up, but are generalised from real data, it is arguable that the proposed framework of evaluation is certainly valid and thus can be used to describe and analyse evaluative language used in this particular context. This in turn indicates that the description and theorising of evaluation is indeed highly dependent on the discourse type that is under examination.","Language Resources and Evaluation",2016,"No"," appraisal parameters corpus linguistics evaluative language customer review texts products evaluated evaluation customer review texts study drawing insights appraisal framework parameter based approach evaluation corpus linguistics investigates evaluative language customer review texts primary goal investigation develop framework evaluation account adequately evaluative expressions customer review texts ultimate goal support argument modelling theorising evaluation context specific based investigation corpus compiled review texts retrieved wwwamazonuk study proposes data driven parameter based appraisal informed framework evaluation comprises parameters quality satisfactoriness recommendability worthiness parameters thought generalised real data arguable proposed framework evaluation valid describe analyse evaluative language context turn description theorising evaluation highly dependent discourse type examination",0
"KeywordsVerbal lexicon WordNet VerbNet FrameNet  PropBank SemLink ","predicate matrix automatically extending the semantic interoperability between predicate resources","This paper presents a novel approach to improve the interoperability between four semantic resources that incorporate predicate information. Our proposal defines a set of automatic methods for mapping the semantic knowledge included in WordNet, VerbNet, PropBank and FrameNet. We use advanced graph-based word sense disambiguation algorithms and corpus alignment methods to automatically establish the appropriate mappings among their lexical entries and roles. We study different settings for each method using SemLink as a gold-standard for evaluation. The results show that the new approach provides productive and reliable mappings. In fact, the mappings obtained automatically outnumber the set of original mappings in SemLink. Finally, we also present a new version of the Predicate Matrix, a lexical-semantic resource resulting from the integration of the mappings obtained by our automatic methods and SemLink.","Language Resources and Evaluation",2016,"No"," verbal lexicon wordnet verbnet framenet propbank semlink predicate matrix automatically extending semantic interoperability predicate resources paper presents approach improve interoperability semantic resources incorporate predicate information proposal defines set automatic methods mapping semantic knowledge included wordnet verbnet propbank framenet advanced graph based word sense disambiguation algorithms corpus alignment methods automatically establish mappings lexical entries roles study settings method semlink gold standard evaluation results show approach productive reliable mappings fact mappings obtained automatically outnumber set original mappings semlink finally present version predicate matrix lexical semantic resource resulting integration mappings obtained automatic methods semlink",0
"KeywordsLanguage identification Tweets Short texts Multilingualism Similar languages ","tweetlid a benchmark for tweet language identification","Language identification, as the task of determining the language a given text is written in, has progressed substantially in recent decades. However, three main issues remain still unresolved: (1) distinction of similar languages, (2) detection of multilingualism in a single document, and (3) identifying the language of short texts. In this paper, we describe our work on the development of a benchmark to encourage further research in these three directions, set forth an evaluation framework suitable for the task, and make a dataset of annotated tweets publicly available for research purposes. We also describe the shared task we organized to validate and assess the evaluation framework and dataset with systems submitted by seven different participants, and analyze the performance of these systems. The evaluation of the results submitted by the participants of the shared task helped us shed some light on the shortcomings of state-of-the-art language identification systems, and gives insight into the extent to which the brevity, multilingualism, and language similarity found in texts exacerbate the performance of language identifiers. Our dataset with nearly 35,000 tweets and the evaluation framework provide researchers and practitioners with suitable resources to further study the aforementioned issues on language identification within a common setting that enables to compare results with one another.","Language Resources and Evaluation",2016,"Yes"," language identification tweets short texts multilingualism similar languages tweetlid benchmark tweet language identification language identification task determining language text written progressed substantially recent decades main issues remain unresolved distinction similar languages detection multilingualism single document identifying language short texts paper describe work development benchmark encourage research directions set evaluation framework suitable task make dataset annotated tweets publicly research purposes describe shared task organized validate assess evaluation framework dataset systems submitted participants analyze performance systems evaluation results submitted participants shared task helped shed light shortcomings state art language identification systems insight extent brevity multilingualism language similarity found texts exacerbate performance language identifiers dataset tweets evaluation framework provide researchers practitioners suitable resources study aforementioned issues language identification common setting enables compare results ",1
"KeywordsStudent response analysis Short answer scoring Recognizing textual entailment Semantic textual similarity ","the joint student response analysis and recognizing textual entailment challenge making sense of student responses in educational applications","We present the results of the joint student response analysis (SRA) and 8th recognizing textual entailment challenge. The goal of this challenge was to bring together researchers from the educational natural language processing and computational semantics communities. The goal of the SRA task is to assess student responses to questions in the science domain, focusing on correctness and completeness of the response content. Nine teams took part in the challenge, submitting a total of 18 runs using methods and features adapted from previous research on automated short answer grading, recognizing textual entailment and semantic textual similarity. We provide an extended analysis of the results focusing on the impact of evaluation metrics, application scenarios and the methods and features used by the participants. We conclude that additional research is required to be able to leverage syntactic dependency features and external semantic resources for this task, possibly due to limited coverage of scientific domains in existing resources. However, each of three approaches to using features and models adjusted to application scenarios achieved better system performance, meriting further investigation by the research community.","Language Resources and Evaluation",2016,"No"," student response analysis short answer scoring recognizing textual entailment semantic textual similarity joint student response analysis recognizing textual entailment challenge making sense student responses educational applications present results joint student response analysis sra th recognizing textual entailment challenge goal challenge bring researchers educational natural language processing computational semantics communities goal sra task assess student responses questions science domain focusing correctness completeness response content teams part challenge submitting total runs methods features adapted previous research automated short answer grading recognizing textual entailment semantic textual similarity provide extended analysis results focusing impact evaluation metrics application scenarios methods features participants conclude additional research required leverage syntactic dependency features external semantic resources task possibly due limited coverage scientific domains existing resources approaches features models adjusted application scenarios achieved system performance meriting investigation research community",0
"KeywordsLatent semantic analysis WordNet Term alignment  Semantic similarity ","robust semantic text similarity using lsa machine learning and linguistic resources","Semantic textual similarity is a measure of the degree of semantic equivalence between two pieces of text. We describe the SemSim system and its performance in the *SEM 2013 and SemEval-2014 tasks on semantic textual similarity. At the core of our system lies a robust distributional word similarity component that combines latent semantic analysis and machine learning augmented with data from several linguistic resources. We used a simple term alignment algorithm to handle longer pieces of text. Additional wrappers and resources were used to handle task specific challenges that include processing Spanish text, comparing text sequences of different lengths, handling informal words and phrases, and matching words with sense definitions. In the *SEM 2013 task on Semantic Textual Similarity, our best performing system ranked first among the 89 submitted runs. In the SemEval-2014 task on Multilingual Semantic Textual Similarity, we ranked a close second in both the English and Spanish subtasks. In the SemEval-2014 task on Cross-Level Semantic Similarity, we ranked first in Sentence–Phrase, Phrase–Word, and Word–Sense subtasks and second in the Paragraph–Sentence subtask.","Language Resources and Evaluation",2016,"No"," latent semantic analysis wordnet term alignment semantic similarity robust semantic text similarity lsa machine learning linguistic resources semantic textual similarity measure degree semantic equivalence pieces text describe semsim system performance sem semeval tasks semantic textual similarity core system lies robust distributional word similarity component combines latent semantic analysis machine learning augmented data linguistic resources simple term alignment algorithm handle longer pieces text additional wrappers resources handle task specific challenges include processing spanish text comparing text sequences lengths handling informal words phrases matching words sense definitions sem task semantic textual similarity performing system ranked submitted runs semeval task multilingual semantic textual similarity ranked close english spanish subtasks semeval task cross level semantic similarity ranked sentence phrase phrase word word sense subtasks paragraph sentence subtask",0
"KeywordsResource-poor languages Interlinear glossed text ODIN ","enriching a massively multilingual database of interlinear glossed text","The majority of the world’s languages have little to no NLP resources or tools. This is due to a lack of training data (“resources”) over which tools, such as taggers or parsers, can be trained. In recent years, there have been increasing efforts to apply NLP methods to a much broader swath of the world’s languages. In many cases this involves bootstrapping the learning process with enriched or partially enriched resources. We propose that Interlinear Glossed Text (IGT), a very common form of annotated data used in the field of linguistics, has great potential for bootstrapping NLP tools for resource-poor languages. Although IGT is generally very richly annotated, and can be enriched even further (e.g., through structural projection), much of the content is not easily consumable by machines since it remains “trapped” in linguistic scholarly documents and in human readable form. In this paper, we describe the expansion of the ODIN resource—a database containing many thousands of instances of IGT for over a thousand languages. We enrich the original IGT data by adding word alignment and syntactic structure. To make the data in ODIN more readily consumable by tool developers and NLP researchers, we adopt and extend a new XML format for IGT, called Xigt. We also develop two packages for manipulating IGT data: one, INTENT, enriches raw IGT automatically, and the other, XigtEdit, is a graphical IGT editor.","Language Resources and Evaluation",2016,"Yes"," resource poor languages interlinear glossed text odin enriching massively multilingual database interlinear glossed text majority world languages nlp resources tools due lack training data resources tools taggers parsers trained recent years increasing efforts apply nlp methods broader swath world languages cases involves bootstrapping learning process enriched partially enriched resources propose interlinear glossed text igt common form annotated data field linguistics great potential bootstrapping nlp tools resource poor languages igt generally richly annotated enriched structural projection content easily consumable machines remains trapped linguistic scholarly documents human readable form paper describe expansion odin resource database thousands instances igt thousand languages enrich original igt data adding word alignment syntactic structure make data odin readily consumable tool developers nlp researchers adopt extend xml format igt called xigt develop packages manipulating igt data intent enriches raw igt automatically xigtedit graphical igt editor",1
"KeywordsSimilarity Evaluation Semantic textual similarity ","cross level semantic similarity an evaluation framework for universal measures of similarity","Semantic similarity has typically been measured across items of approximately similar sizes. As a result, similarity measures have largely ignored the fact that different types of linguistic item can potentially have similar or even identical meanings, and therefore are designed to compare only one type of linguistic item. Furthermore, nearly all current similarity benchmarks within NLP contain pairs of approximately the same size, such as word or sentence pairs, preventing the evaluation of methods that are capable of comparing different sized items. To address this, we introduce a new semantic evaluation called cross-level semantic similarity (CLSS), which measures the degree to which the meaning of a larger linguistic item, such as a paragraph, is captured by a smaller item, such as a sentence. Our pilot CLSS task was presented as part of SemEval-2014, which attracted 19 teams who submitted 38 systems. CLSS data contains a rich mixture of pairs, spanning from paragraphs to word senses to fully evaluate similarity measures that are capable of comparing items of any type. Furthermore, data sources were drawn from diverse corpora beyond just newswire, including domain-specific texts and social media. We describe the annotation process and its challenges, including a comparison with crowdsourcing, and identify the factors that make the dataset a rigorous assessment of a method’s quality. Furthermore, we examine in detail the systems participating in the SemEval task to identify the common factors associated with high performance and which aspects proved difficult to all systems. Our findings demonstrate that CLSS poses a significant challenge for similarity methods and provides clear directions for future work on universal similarity methods that can compare any pair of items.","Language Resources and Evaluation",2016,"No"," similarity evaluation semantic textual similarity cross level semantic similarity evaluation framework universal measures similarity semantic similarity typically measured items approximately similar sizes result similarity measures largely fact types linguistic item potentially similar identical meanings designed compare type linguistic item current similarity benchmarks nlp pairs approximately size word sentence pairs preventing evaluation methods capable comparing sized items address introduce semantic evaluation called cross level semantic similarity clss measures degree meaning larger linguistic item paragraph captured smaller item sentence pilot clss task presented part semeval attracted teams submitted systems clss data rich mixture pairs spanning paragraphs word senses fully evaluate similarity measures capable comparing items type data sources drawn diverse corpora newswire including domain specific texts social media describe annotation process challenges including comparison crowdsourcing identify factors make dataset rigorous assessment method quality examine detail systems participating semeval task identify common factors high performance aspects proved difficult systems findings demonstrate clss poses significant challenge similarity methods clear directions future work universal similarity methods compare pair items",0
"KeywordsIntonation Automatic intonation recognition Sp_ToBI Cat_ToBI ","a tool for automatic transcription of intonation etitobi a tobi transcriber for spanish and catalan","This article presents Eti_ToBI, a tool that automatically labels intonational events in Spanish and Catalan utterances according to the Sp_ToBI and Cat_ToBI current conventions. The system consists in a Praat script that assigns ToBI labels to pitch movements basing the assignments on lexical data introduced by the researcher and the acoustical data that it extracts from sound files. The first part of the article explains the methodological approach that has made possible the automatisation and describes the algorithms used by the script to perform the analysis. The second part presents the reliability results for both Catalan and Spanish corpora showing a level of agreement equal to the one shown by human transcribers among them in the literature.","Language Resources and Evaluation",2016,"No"," intonation automatic intonation recognition sptobi cattobi tool automatic transcription intonation etitobi tobi transcriber spanish catalan article presents etitobi tool automatically labels intonational events spanish catalan utterances sptobi cattobi current conventions system consists praat script assigns tobi labels pitch movements basing assignments lexical data introduced researcher acoustical data extracts sound files part article explains methodological approach made automatisation describes algorithms script perform analysis part presents reliability results catalan spanish corpora showing level agreement equal shown human transcribers literature",0
"KeywordsSemantic role Predicate-argument structure Chinese Proposition Bank Semantic role labeling ","generalizing the semantic roles in the chinese proposition bank","The Chinese Proposition Bank (CPB) is a 
corpus annotated with semantic roles for the arguments of verbal and nominalized predicates. The semantic roles for the core arguments are defined in a predicate-specific manner. That is, a set of semantic roles, numerically identified, are defined for each sense of a predicate lemma and recorded in a valency lexicon called frame files. The predicate-specific manner in which the semantic roles are defined reduces the cognitive burden on the annotators since they only need to internalize a few roles at a time and this has contributed to the consistency in annotation. It was also a sensible approach given the contentious issue of how many semantic roles are needed if one were to adopt of set of global semantic roles that apply to all predicates. A downside of this approach, however, is that the predicate-specific roles may not be consistent across predicates, and this inconsistency has a negative impact on training automatic systems. Given the progress that has been made in defining semantic roles in the last decade or so, time is ripe for adopting a set of general semantic roles. In this article, we describe our effort to “re-annotate” the CPB with a set of “global” semantic roles that are predicate-independent and investigate their impact on automatic semantic role labeling systems. When defining these global semantic roles, we strive to make them compatible with a recently published ISO standards on the annotation of semantic roles (ISO 24617-4:2014 SemAF-SR) while taking the linguistic characteristics of the Chinese language into account. We show that in spite of the much larger number of global semantic roles, the accuracy of an off-the-shelf semantic role labeling system retrained on the data re-annotated with global semantic roles is comparable to that trained on the data set with the original predicate-specific semantic roles. We also argue that the re-annotated data set, together with the original data, provides the user with more flexibility when using the corpus.","Language Resources and Evaluation",2016,"Yes"," semantic role predicate argument structure chinese proposition bank semantic role labeling generalizing semantic roles chinese proposition bank chinese proposition bank cpb corpus annotated semantic roles arguments verbal nominalized predicates semantic roles core arguments defined predicate specific manner set semantic roles numerically identified defined sense predicate lemma recorded valency lexicon called frame files predicate specific manner semantic roles defined reduces cognitive burden annotators internalize roles time contributed consistency annotation approach contentious issue semantic roles needed adopt set global semantic roles apply predicates downside approach predicate specific roles consistent predicates inconsistency negative impact training automatic systems progress made defining semantic roles decade time ripe adopting set general semantic roles article describe effort annotate cpb set global semantic roles predicate independent investigate impact automatic semantic role labeling systems defining global semantic roles strive make compatible recently published iso standards annotation semantic roles iso semaf sr taking linguistic characteristics chinese language account show spite larger number global semantic roles accuracy shelf semantic role labeling system retrained data annotated global semantic roles comparable trained data set original predicate specific semantic roles argue annotated data set original data user flexibility corpus",1
"KeywordsMultimodal database Theatrical improvisations Motion capture system Continuous emotion ","the usc creativeit database of multimodal dyadic interactions from speech and full body motion capture to continuous emotional annotations","Improvised acting is a viable technique to study expressive human communication and to shed light into actors’ creativity. The USC CreativeIT database provides a novel, freely-available multimodal resource for the study of theatrical improvisation and rich expressive human behavior (speech and body language) in dyadic interactions. The theoretical design of the database is based on the well-established improvisation technique of Active Analysis in order to provide naturally induced affective and expressive, goal-driven interactions. This database contains dyadic theatrical improvisations performed by 16 actors, providing detailed full body motion capture data and audio data of each participant in an interaction. The carefully engineered data collection, the improvisation design to elicit natural emotions and expressive speech and body language, as well as the well-developed annotation processes provide a gateway to study and model various aspects of theatrical performance, expressive behaviors and human communication and interaction.","Language Resources and Evaluation",2016,"Yes"," multimodal database theatrical improvisations motion capture system continuous emotion usc creativeit database multimodal dyadic interactions speech full body motion capture continuous emotional annotations improvised acting viable technique study expressive human communication shed light actors creativity usc creativeit database freely multimodal resource study theatrical improvisation rich expressive human behavior speech body language dyadic interactions theoretical design database based established improvisation technique active analysis order provide naturally induced affective expressive goal driven interactions database dyadic theatrical improvisations performed actors providing detailed full body motion capture data audio data participant interaction carefully engineered data collection improvisation design elicit natural emotions expressive speech body language developed annotation processes provide gateway study model aspects theatrical performance expressive behaviors human communication interaction",1
"KeywordsSentiment analysis Twitter SemEval ","developing a successful semeval task in sentiment analysis of twitter and other social media texts","We present the development and evaluation of a semantic analysis task that lies at the intersection of two very trendy lines of research in contemporary computational linguistics: (1) sentiment analysis, and (2) natural language processing of social media text. The task was part of SemEval, the International Workshop on Semantic Evaluation, a semantic evaluation forum previously known as SensEval. The task ran in 2013 and 2014, attracting the highest number of participating teams at SemEval in both years, and there is an ongoing edition in 2015. The task included the creation of a large contextual and message-level polarity corpus consisting of tweets, SMS messages, LiveJournal messages, and a special test set of sarcastic tweets. The evaluation attracted 44 teams in 2013 and 46 in 2014, who used a variety of approaches. The best teams were able to outperform several baselines by sizable margins with improvement across the 2 years the task has been run. We hope that the long-lasting role of this task and the accompanying datasets will be to serve as a test bed for comparing different approaches, thus facilitating research.","Language Resources and Evaluation",2016,"No"," sentiment analysis twitter semeval developing successful semeval task sentiment analysis twitter social media texts present development evaluation semantic analysis task lies intersection trendy lines research contemporary computational linguistics sentiment analysis natural language processing social media text task part semeval international workshop semantic evaluation semantic evaluation forum previously senseval task ran attracting highest number participating teams semeval years ongoing edition task included creation large contextual message level polarity corpus consisting tweets sms messages livejournal messages special test set sarcastic tweets evaluation attracted teams variety approaches teams outperform baselines sizable margins improvement years task run hope long lasting role task accompanying datasets serve test bed comparing approaches facilitating research",0
"KeywordsLanguage technology Multilingual technologies Machine translation Language resources META-NET META-SHARE ","the strategic impact of meta net on the regional national and international level","This article provides an overview of the dissemination work carried out in META-NET from 2010 until 2015; we describe its impact on the regional, national and international level, mainly with regard to politics and the funding situation for LT topics. The article documents the initiative’s work throughout Europe in order to boost progress and innovation in our field.","Language Resources and Evaluation",2016,"No"," language technology multilingual technologies machine translation language resources meta net meta share strategic impact meta net regional national international level article overview dissemination work carried meta net describe impact regional national international level regard politics funding situation lt topics article documents initiative work europe order boost progress innovation field",0
"KeywordsKnowledge representation Image selection Image annotation EcoLexicon ","image selection and annotation for an environmental knowledge base","Images play an important role in the representation and acquisition of specialized knowledge. Not surprisingly, terminological knowledge bases (TKBs) often include images as a way to enhance the information in concept entries. However, the selection of these images should not be random, but rather based on specific guidelines that take into account the type and nature of the concept being described. This paper presents a proposal on how to combine the features of images with the conceptual propositions in EcoLexicon, a multilingual TKB on the environment. This proposal is based on the following: (1) the combinatory possibilities of concept types; (2) image types, such as photographs, drawings and flow charts; (3) morphological features or visual knowledge patterns (VKPs), such as labels, colours, arrows, and their effect on the functional nature of each image type. Currently, images are stored in association with concept entries according to the semantic content of their definitions, but they are not described or annotated according to the parameters that guided their selection, which would undoubtedly contribute to the systematization and automatization of the process. First, the images included in EcoLexicon were analyzed in terms of their adequateness, the semantic relations expressed, the concept types and their VKPs. Then, with these data, guidelines for image selection and annotation were created. The final aim is twofold: (1) to systematize the selection of images and (2) to start annotating old and new images so that the system can automatically allocate them in different concept entries based on shared conceptual propositions.","Language Resources and Evaluation",2016,"No"," knowledge representation image selection image annotation ecolexicon image selection annotation environmental knowledge base images play important role representation acquisition specialized knowledge surprisingly terminological knowledge bases tkbs include images enhance information concept entries selection images random based specific guidelines account type nature concept paper presents proposal combine features images conceptual propositions ecolexicon multilingual tkb environment proposal based combinatory possibilities concept types image types photographs drawings flow charts morphological features visual knowledge patterns vkps labels colours arrows effect functional nature image type images stored association concept entries semantic content definitions annotated parameters guided selection undoubtedly contribute systematization automatization process images included ecolexicon analyzed terms adequateness semantic relations expressed concept types vkps data guidelines image selection annotation created final aim twofold systematize selection images start annotating images system automatically allocate concept entries based shared conceptual propositions",0
"KeywordsMorphological tagging Data-driven lemmatization Averaged perceptron Finnish Open-source ","finnpos an open source morphological tagging and lemmatization toolkit for finnish","This paper describes FinnPos, an open-source morphological tagging and lemmatization toolkit for Finnish. The morphological tagging model is based on the averaged structured perceptron classifier. Given training data, new taggers are estimated in a computationally efficient manner using a combination of beam search and model cascade. The lemmatization is performed employing a combination of a rule-based morphological analyzer, OMorFi, and a data-driven lemmatization model. The toolkit is readily applicable for tagging and lemmatization of running text with models learned from the recently published Finnish Turku Dependency Treebank and FinnTreeBank. Empirical evaluation on these corpora shows that FinnPos performs favorably compared to reference systems in terms of tagging and lemmatization accuracy. In addition, we demonstrate that our system is highly competitive with regard to computational efficiency of learning new models and assigning analyses to novel sentences.","Language Resources and Evaluation",2016,"No"," morphological tagging data driven lemmatization averaged perceptron finnish open source finnpos open source morphological tagging lemmatization toolkit finnish paper describes finnpos open source morphological tagging lemmatization toolkit finnish morphological tagging model based averaged structured perceptron classifier training data taggers estimated computationally efficient manner combination beam search model cascade lemmatization performed employing combination rule based morphological analyzer omorfi data driven lemmatization model toolkit readily applicable tagging lemmatization running text models learned recently published finnish turku dependency treebank finntreebank empirical evaluation corpora shows finnpos performs favorably compared reference systems terms tagging lemmatization accuracy addition demonstrate system highly competitive regard computational efficiency learning models assigning analyses sentences",0
NA,"editors introduction to the special issue papers from lrec 2014","This special issue of Language Resources and Evaluation includes a selection of extended papers from the Ninth Language Resources and Evaluation Conference, held in Reykjavik, Iceland, in May 2014. The conference drew a record number of participants, reflecting the steadily increasing interest and activity in the field of language resource creation, enhancement, management, and evaluation since the first LREC was held in 1998. The selection of articles for this special issue was made on the basis of recommendations from LREC reviewers, who were asked to indicate suitability for publication of an extended version in LRE for each paper they reviewed.","Language Resources and Evaluation",2016,"No"," editors introduction special issue papers lrec special issue language resources evaluation includes selection extended papers ninth language resources evaluation conference held reykjavik iceland conference drew record number participants reflecting steadily increasing interest activity field language resource creation enhancement management evaluation lrec held selection articles special issue made basis recommendations lrec reviewers asked suitability publication extended version lre paper reviewed",0
"KeywordsReferential translation machine RTM Semantic similarity Machine translation Performance prediction Machine translation performance prediction ","referential translation machines for predicting semantic similarity","Referential translation machines (RTMs) are a computational model effective at judging monolingual and bilingual similarity while identifying translation acts between any two data sets with respect to interpretants, data close to the task instances. RTMs pioneer a language-independent approach to all similarity tasks and remove the need to access any task- or domain-specific information or resource. We use RTMs for predicting the semantic similarity of text and present state-of-the-art results showing that RTMs can achieve better results on the test set than on the training set. Interpretants are used to derive features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and the presence of the acts of translation, which may ubiquitously be observed in communication. RTMs can achieve top performance at SemEval in various semantic similarity prediction tasks as well as similarity prediction tasks in bilingual settings. We obtain rankings of various prediction tasks using the performance of RTM and relative evaluation metrics, which can help identify which tasks and subtasks require more work by design.","Language Resources and Evaluation",2016,"No"," referential translation machine rtm semantic similarity machine translation performance prediction machine translation performance prediction referential translation machines predicting semantic similarity referential translation machines rtms computational model effective judging monolingual bilingual similarity identifying translation acts data sets respect interpretants data close task instances rtms pioneer language independent approach similarity tasks remove access task domain specific information resource rtms predicting semantic similarity text present state art results showing rtms achieve results test set training set interpretants derive features measuring closeness test sentences training data difficulty translating presence acts translation ubiquitously observed communication rtms achieve top performance semeval semantic similarity prediction tasks similarity prediction tasks bilingual settings obtain rankings prediction tasks performance rtm relative evaluation metrics identify tasks subtasks require work design",0
"KeywordsStemming algorithm Hausa language Root word Over-stemming Under-stemming Stemmer accuracy ","stemming hausa text using affix stripping rules and reference look up","Stemming is a process of reducing a derivational or inflectional word to its root or stem by stripping all its affixes. It is been used in applications such as information retrieval, machine translation, and text summarization, as their pre-processing step to increase efficiency. Currently, there are a few stemming algorithms which have been developed for languages such as English, Arabic, Turkish, Malay and Amharic. Unfortunately, no algorithm has been used to stem text in Hausa, a Chadic language spoken in West Africa. To address this need, we propose stemming Hausa text using affix-stripping rules and reference lookup. We stemmed Hausa text, using 78 affix stripping rules applied in 4 steps and a reference look-up consisting of 1500 Hausa root words. The over-stemming index, under-stemming index, stemmer weight, word stemmed factor, correctly stemmed words factor and average words conflation factor were calculated to determine the effect of reference look-up on the strength and accuracy of the stemmer. It was observed that reference look-up aided in reducing both over-stemming and under-stemming errors, increased accuracy and has a tendency to reduce the strength of an affix stripping stemmer. The rationality behind the approach used is discussed and directions for future research are identified.","Language Resources and Evaluation",2016,"No"," stemming algorithm hausa language root word stemming stemming stemmer accuracy stemming hausa text affix stripping rules reference stemming process reducing derivational inflectional word root stem stripping affixes applications information retrieval machine translation text summarization pre processing step increase efficiency stemming algorithms developed languages english arabic turkish malay amharic algorithm stem text hausa chadic language spoken west africa address propose stemming hausa text affix stripping rules reference lookup stemmed hausa text affix stripping rules applied steps reference consisting hausa root words stemming index stemming index stemmer weight word stemmed factor correctly stemmed words factor average words conflation factor calculated determine effect reference strength accuracy stemmer observed reference aided reducing stemming stemming errors increased accuracy tendency reduce strength affix stripping stemmer rationality approach discussed directions future research identified",0
"KeywordsSentiment analysis Polarity lexicon Polarity extraction Turkish WordNet ","sentiturknet a turkish polarity lexicon for sentiment analysis","Sentiment analysis aims to extract the sentiment polarity of given segment of text. Polarity resources that indicate the sentiment polarity of words are commonly used in different approaches. While English is the richest language in regard to having such resources, the majority of other languages, including Turkish, lack polarity resources. In this work we present the first comprehensive Turkish polarity resource, SentiTurkNet, where three polarity scores are assigned to each synset in the Turkish WordNet, indicating its positivity, negativity, and objectivity (neutrality) levels. Our method is general and applicable to other languages. Evaluation results for Turkish show that the polarity scores obtained through this method are more accurate compared to those obtained through direct translation (mapping) from SentiWordNet.","Language Resources and Evaluation",2016,"Yes"," sentiment analysis polarity lexicon polarity extraction turkish wordnet sentiturknet turkish polarity lexicon sentiment analysis sentiment analysis aims extract sentiment polarity segment text polarity resources sentiment polarity words commonly approaches english richest language regard resources majority languages including turkish lack polarity resources work present comprehensive turkish polarity resource sentiturknet polarity scores assigned synset turkish wordnet indicating positivity negativity objectivity neutrality levels method general applicable languages evaluation results turkish show polarity scores obtained method accurate compared obtained direct translation mapping sentiwordnet",1
"KeywordsNatural language processing Name-based Text Categorization Semantic similarity ","text categorization from category name in an industry motivated scenario","In this work we suggest a novel Text Categorization (TC) scenario, motivated by an ad-hoc industrial need to assign documents to a set of predefined categories, while labeled training data for the categories is not available. The scenario is applicable in many industrial settings and is interesting from the academic perspective. We present a new dataset geared for the main characteristics of the scenario, and utilize it to investigate the name-based TC approach, which uses the category names as its only input and does not require training data. We evaluate and analyze the performance of state-of-the-art methods for this dataset to identify the shortcomings of these methods for our scenario, and suggest ways for overcoming these shortcomings. We utilize statistical correlation measured over a target corpus for improving the state-of-the-art, and offer a different classification scheme based on the characteristics of the setting. We evaluate our improvements and adaptations and show superior performance of our suggested method.","Language Resources and Evaluation",2015,"No"," natural language processing based text categorization semantic similarity text categorization category industry motivated scenario work suggest text categorization tc scenario motivated ad hoc industrial assign documents set predefined categories labeled training data categories scenario applicable industrial settings interesting academic perspective present dataset geared main characteristics scenario utilize investigate based tc approach category names input require training data evaluate analyze performance state art methods dataset identify shortcomings methods scenario suggest ways overcoming shortcomings utilize statistical correlation measured target corpus improving state art offer classification scheme based characteristics setting evaluate improvements adaptations show superior performance suggested method",0
"Keywordssyntactic annotation XML format corpus corpora Treebank Tiger XML ","tiger2 serialising the iso synaf syntactic object model","
This paper introduces <tiger2/>, an XML format developed to serialise the object model defined by the ISO Syntactic Annotation Framework SynAF. Based on widespread best practices we adapt a popular XML format for syntactic annotation, TigerXML, with additional features to support a variety of syntactic phenomena including constituent and dependency structures, binding, and different node types such as compounds or empty elements. We also define interfaces to other formats and standards including the Morpho-syntactic Annotation Framework MAF and the ISOCat Data Category Registry. Finally a case study of the German Treebank TueBa-D/Z is presented, showcasing the handling of constituent structures, topological fields and coreference annotation in tandem.","Language Resources and Evaluation",2015,"No"," syntactic annotation xml format corpus corpora treebank tiger xml tiger serialising iso synaf syntactic object model paper introduces tiger xml format developed serialise object model defined iso syntactic annotation framework synaf based widespread practices adapt popular xml format syntactic annotation tigerxml additional features support variety syntactic phenomena including constituent dependency structures binding node types compounds empty elements define interfaces formats standards including morpho syntactic annotation framework maf isocat data category registry finally case study german treebank tueba presented showcasing handling constituent structures topological fields coreference annotation tandem",0
"KeywordsInterlinear glossed text (IGT) Annotation Storage format ","xigt extensible interlinear glossed text for natural language processing","This paper presents Xigt, an extensible storage format for interlinear glossed text (IGT). We review design desiderata for such a format based on our own use cases as well as general best practices, and then explore existing representations of IGT through the lens of those desiderata. We give an overview of the data model and XML serialization of Xigt, and then describe its application to the use case of representing a large, noisy, heterogeneous set of IGT.","Language Resources and Evaluation",2015,"No"," interlinear glossed text igt annotation storage format xigt extensible interlinear glossed text natural language processing paper presents xigt extensible storage format interlinear glossed text igt review design desiderata format based cases general practices explore existing representations igt lens desiderata give overview data model xml serialization xigt describe application case representing large noisy heterogeneous set igt",0
"KeywordsHistorical language resources Slovene language Text Encoding Initiative Non-standard language normalisation ","the imp historical slovene language resources","The paper describes the combined results of several projects which constitute a basic language resource infrastructure for printed historical Slovene. The IMP language resources consist of a digital library, an annotated corpus and a lexicon, which are interlinked and uniformly encoded following the Text Encoding Initiative Guidelines. The library holds about 650 units (mostly complete books) consisting of facsimiles with 45,000 pages as well as hand-corrected and structured transcriptions. The hand-annotated corpus has 300,000 tokens, where each word is tagged with its modernised word form, lemma, part-of-speech and, in cases of archaic words, its nearest contemporary equivalents. This information was extracted into the lexicon, which also covers an extended target-annotated corpus, resulting in 20,000 lemmas (of these 4,000 archaic) with 50,000 modern word forms and 70,000 attested forms. We have also developed a program to modernise, tag and lemmatise historical Slovene, and annotated the digital library with it, producing an automatically annotated corpus of 15 million words. To serve the humanities, the digital library and lexicon are available for reading and browsing on the web and the corpora via a concordancer. For language technology research and development the resources are available in source TEI XML under the Creative Commons Attribution licence. The paper presents the IMP resources, available from http://nl.ijs.si/imp/, the process of their compilation, encoding and dissemination, and concludes with directions for future research.","Language Resources and Evaluation",2015,"Yes"," historical language resources slovene language text encoding initiative standard language normalisation imp historical slovene language resources paper describes combined results projects constitute basic language resource infrastructure printed historical slovene imp language resources consist digital library annotated corpus lexicon interlinked uniformly encoded text encoding initiative guidelines library holds units complete books consisting facsimiles pages hand corrected structured transcriptions hand annotated corpus tokens word tagged modernised word form lemma part speech cases archaic words nearest contemporary equivalents information extracted lexicon covers extended target annotated corpus resulting lemmas archaic modern word forms attested forms developed program modernise tag lemmatise historical slovene annotated digital library producing automatically annotated corpus million words serve humanities digital library lexicon reading browsing web corpora concordancer language technology research development resources source tei xml creative commons attribution licence paper presents imp resources httpnlijssiimp process compilation encoding dissemination concludes directions future research",1
"KeywordsLexical normalization Twitter Social media Corpus Evaluation ","tweetnorm a benchmark for lexical normalization of spanish tweets","
The language used in social media is often characterized by the abundance of informal and non-standard writing. The normalization of this non-standard language can be crucial to facilitate the subsequent textual processing and to consequently help boost the performance of natural language processing tools applied to social media text. In this paper we present a benchmark for lexical normalization of social media posts, specifically for tweets in Spanish language. We describe the tweet normalization challenge we organized recently, analyze the performance achieved by the different systems submitted to the challenge, and delve into the characteristics of systems to identify the features that were useful. The organization of this challenge has led to the production of a benchmark for lexical normalization of social media, including an evaluation framework, as well as an annotated corpus of Spanish tweets—TweetNorm_es—, which we make publicly available. The creation of this benchmark and the evaluation has brought to light the types of words that submitted systems did best with, and posits the main shortcomings to be addressed in future work.","Language Resources and Evaluation",2015,"Yes"," lexical normalization twitter social media corpus evaluation tweetnorm benchmark lexical normalization spanish tweets language social media characterized abundance informal standard writing normalization standard language crucial facilitate subsequent textual processing boost performance natural language processing tools applied social media text paper present benchmark lexical normalization social media posts specifically tweets spanish language describe tweet normalization challenge organized recently analyze performance achieved systems submitted challenge delve characteristics systems identify features organization challenge led production benchmark lexical normalization social media including evaluation framework annotated corpus spanish tweets tweetnormes make publicly creation benchmark evaluation brought light types words submitted systems posits main shortcomings addressed future work",1
"KeywordsConstructions Subcategorization frames Argument structure Topicalization Detopicalization Pronominal constructions ","constructions at argument structure level in the sensem corpora","
In this paper we present the annotation scheme of constructions at the argument-structure level in the Spanish and Catalan Corpora SenSem. Constructions are accounted for as form-meaning pairs following the theoretical underpinning of Construction Grammar. Regarding meaning, we propose a hierarchy of constructions taking into account, at the highest level, the prominence of the logical subject in the sentence. Thus, we differentiate between topicalized and detopicalized sentences, which is an innovative proposal to solve some terminological issues related to pronominal constructions in Spanish. We further develop this classification taking into account the semantic relation of the logical subject with the verb and its coindexation, if any, with other participants. As regards form, the basic features we consider are syntagmatic categories and syntactic functions. Furthermore, we annotate the form the verb requires, that is, if it requires a pronoun in order to convey a particular meaning. Other relevant contributions are the annotation of some linguistic phenomena not taken into account in other similar resources, such as reciprocal, dative or impersonal constructions. Finally, we present the frequencies of all these constructions in Spanish.","Language Resources and Evaluation",2015,"No"," constructions subcategorization frames argument structure topicalization detopicalization pronominal constructions constructions argument structure level sensem corpora paper present annotation scheme constructions argument structure level spanish catalan corpora sensem constructions accounted form meaning pairs theoretical underpinning construction grammar meaning propose hierarchy constructions taking account highest level prominence logical subject sentence differentiate topicalized detopicalized sentences innovative proposal solve terminological issues related pronominal constructions spanish develop classification taking account semantic relation logical subject verb coindexation participants form basic features syntagmatic categories syntactic functions annotate form verb requires requires pronoun order convey meaning relevant contributions annotation linguistic phenomena account similar resources reciprocal dative impersonal constructions finally present frequencies constructions spanish",0
"KeywordsMultimodal interaction Video corpus Head-mounted eye-tracking Multifocal approach ","insight interaction a multimodal and multifocal dialogue corpus","Research on the multimodal aspects of interactional language use requires high-quality multimodal resources. In contrast to the vast amount of available written language corpora and collections of transcribed spoken language, truly multimodal corpora including visual as well as auditory data are scarce. In this paper, we first discuss a few notable exceptions that do provide high-quality and multiple-angle video recordings of face-to-face conversations. We then present a new multimodal corpus design that adds two dimensions to the existing resources. First, the recording set-up was designed in such a way as to have a full view of the dialogue partners’ gestural behaviour, including hand gestures, facial expressions and body posture. Second, by recording the participant perspective and behaviour during conversation, using head-mounted scene cameras and eye-trackers, we obtained a 3D landscape of the conversation, with detailed production information (scene camera and sound) and indices of cognitive processing (eye movements for gaze analysis) for both participants. In its current form, the resulting InSight Interaction Corpus consists of 15 recorded face-to-face interactions of 20 min each, of which five have been transcribed and annotated for a range of linguistic and gestural features, using the ELAN multimodal annotation tool.","Language Resources and Evaluation",2015,"Yes"," multimodal interaction video corpus head mounted eye tracking multifocal approach insight interaction multimodal multifocal dialogue corpus research multimodal aspects interactional language requires high quality multimodal resources contrast vast amount written language corpora collections transcribed spoken language multimodal corpora including visual auditory data scarce paper discuss notable exceptions provide high quality multiple angle video recordings face face conversations present multimodal corpus design adds dimensions existing resources recording set designed full view dialogue partners gestural behaviour including hand gestures facial expressions body posture recording participant perspective behaviour conversation head mounted scene cameras eye trackers obtained d landscape conversation detailed production information scene camera sound indices cognitive processing eye movements gaze analysis participants current form resulting insight interaction corpus consists recorded face face interactions min transcribed annotated range linguistic gestural features elan multimodal annotation tool",1
"KeywordsPrecision grammar Grammar engineering Grammar diagnostics Deep parsing Parse mining ","gdelta a missing link in the grammar engineering toolchain","The development of precision grammars is an inherently resource-intensive process; their complexity means that changes made to one area of a grammar often introduce unexpected flow-on effects elsewhere in the grammar which may only be discovered after some time has been invested in updating numerous test suite items. In this paper, we present the browser-based gDelta tool, which aims to provide grammar engineers with more immediate feedback on the impact of changes made to a grammar by comparing parser output from two different grammar versions. We describe an attribute weighting algorithm for highlighting components of the grammar that have been strongly impacted by a modification to the grammar, as well as a technique for clustering test suite items whose parsability has changed, in order to locate related groups of effects. These two techniques are used to present the grammar engineer with different views on the grammar to inform them of different aspects of change in a data-driven manner.","Language Resources and Evaluation",2015,"No"," precision grammar grammar engineering grammar diagnostics deep parsing parse mining gdelta missing link grammar engineering toolchain development precision grammars inherently resource intensive process complexity means made area grammar introduce unexpected flow effects grammar discovered time invested updating numerous test suite items paper present browser based gdelta tool aims provide grammar engineers feedback impact made grammar comparing parser output grammar versions describe attribute weighting algorithm highlighting components grammar strongly impacted modification grammar technique clustering test suite items parsability changed order locate related groups effects techniques present grammar engineer views grammar inform aspects change data driven manner",0
"KeywordsArabic corpora Corpus evaluation Corpus design Corpus compilation Arabic language resources Natural language processing (NLP) ","a 700m arabic corpus kacst arabic corpus design and construction","Compared with English, Arabic is a poorly-resourced language within the field of corpus linguistics. A lack of sufficient data and research has negatively affected Arabic corpus-based researchers and natural language processing practitioners. Although a number of Arabic corpora have been developed in recent years, the overall situation has improved little. The aim of this paper is twofold. First, it reviews 14 Arabic corpora categorized by their designated purpose, target language, mode of text, size, text date, location, text type/medium, text domain, representativeness, and balance. The review also describes the availability of the reviewed corpora, the presence of tokenization, lemmatization and tagging, and whether there are any tools available to search and explore them. Second, it introduces the King Abdulaziz City for Science and Technology (KACST) Arabic corpus, which was designed and created to overcome the limitations of existing Arabic corpora. The KACST Arabic corpus is a large and diverse Arabic corpus with clearly defined design criteria. It is carefully sampled, and its contents are classified based on time, region, medium, domain, and topic, and it can be searched and explored using these classifications. The KACST Arabic corpus comprises more than 700 million words from the pre-Islamic era to the present day (a period covering more than 1,500 years), collected from 10 diverse mediums. Each text has been further classified more specifically into domains and topics. The KACST Arabic corpus is freely available to explore on the Internet (http://www.kacstac.org.sa) using a variety of tools.","Language Resources and Evaluation",2015,"Yes"," arabic corpora corpus evaluation corpus design corpus compilation arabic language resources natural language processing nlp m arabic corpus kacst arabic corpus design construction compared english arabic poorly resourced language field corpus linguistics lack sufficient data research negatively affected arabic corpus based researchers natural language processing practitioners number arabic corpora developed recent years situation improved aim paper twofold reviews arabic corpora categorized designated purpose target language mode text size text date location text typemedium text domain representativeness balance review describes availability reviewed corpora presence tokenization lemmatization tagging tools search explore introduces king abdulaziz city science technology kacst arabic corpus designed created overcome limitations existing arabic corpora kacst arabic corpus large diverse arabic corpus defined design criteria carefully sampled contents classified based time region medium domain topic searched explored classifications kacst arabic corpus comprises million words pre islamic era present day period covering years collected diverse mediums text classified specifically domains topics kacst arabic corpus freely explore internet httpwwwkacstacorgsa variety tools",1
"KeywordsLearner corpus Textual revision Feedback English as a second language Multi-layer corpus annotation Corpus search and visualization ","cityu corpus of essay drafts of english language learners a corpus of textual revision in second language writing","
Learner corpora consist of texts produced by non-native speakers. In addition to these texts, some learner corpora also contain error annotations, which can reveal common errors made by language learners, and provide training material for automatic error correction. We present a novel type of error-annotated learner corpus containing sequences of revised essay drafts written by non-native speakers of English. Sentences in these drafts are annotated with comments by language tutors, and are aligned to sentences in subsequent drafts. We describe the compilation process of our corpus, present its encoding in TEI XML, and report agreement levels on the error annotations. Further, we demonstrate the potential of the corpus to facilitate research on textual revision in L2 writing, by conducting a case study on verb tenses using ANNIS, a corpus search and visualization platform.","Language Resources and Evaluation",2015,"Yes"," learner corpus textual revision feedback english language multi layer corpus annotation corpus search visualization cityu corpus essay drafts english language learners corpus textual revision language writing learner corpora consist texts produced native speakers addition texts learner corpora error annotations reveal common errors made language learners provide training material automatic error correction present type error annotated learner corpus sequences revised essay drafts written native speakers english sentences drafts annotated comments language tutors aligned sentences subsequent drafts describe compilation process corpus present encoding tei xml report agreement levels error annotations demonstrate potential corpus facilitate research textual revision l writing conducting case study verb tenses annis corpus search visualization platform",1
"KeywordsParaphrasing Paraphrase typology Corpus annotation Inter-annotator agreement ","corpus annotation with paraphrase types new annotation scheme and inter annotator agreement measures","Paraphrase corpora annotated with the types of paraphrases they contain constitute an essential resource for the understanding of the phenomenon of paraphrasing and the improvement of paraphrase-related systems in natural language processing. In this article, a new annotation scheme for paraphrase-type annotation is set out, together with newly created measures for the computation of inter-annotator agreement. Three corpora different in nature and in two languages have been annotated using this infrastructure. The annotation results and the inter-annotator agreement scores for these corpora are proof of the adequacy and robustness of our proposal.","Language Resources and Evaluation",2015,"No"," paraphrasing paraphrase typology corpus annotation inter annotator agreement corpus annotation paraphrase types annotation scheme inter annotator agreement measures paraphrase corpora annotated types paraphrases constitute essential resource understanding phenomenon paraphrasing improvement paraphrase related systems natural language processing article annotation scheme paraphrase type annotation set newly created measures computation inter annotator agreement corpora nature languages annotated infrastructure annotation results inter annotator agreement scores corpora proof adequacy robustness proposal",0
"KeywordsFairy tale corpus Annotation scheme Inter-annotator agreement Direct quotations Prosody Intonation stylization Text-to-speech Expressivity ","the gv lex corpus of tales in french","
A corpus of French tales is presented. Its two parts, a text corpus and a speech corpus, were designed for studying the relationships between the textual structures of tales and speech prosody, with the targeted application of an expressive text-to-speech synthesis system embedded in a humanoid robot. The 89-tale text corpus, and the 12-tale speech corpus were annotated using a common tale description framework. Lexical level annotations include extended definitions of enumerations, time, place and person named entities, as well as part of speech tags. Supra-lexical level annotations include the segmentation of tales into a sequence of episodes, the localization and attribution of direct quotations, together with tale protagonists co-references. Annotation distributions and inter-annotator agreement were analyzed. The largest coverage and strongest agreement were observed for person named entities, characters’ direct quotations, and their associated coreference chains. Speech corpus annotations were extended to allow the analysis of the relations between tale linguistic information and prosodic properties observed in associated speech. Word and phoneme boundaries were inferred through semi-automatic procedures, resulting in linguistic annotations aligned with the speech signal. Intonation stylization models were used to ease the visual and statistical analysis of tale’s prosody. Additional meta-information is provided with the speech corpus, allowing describing tale characters according to their gender, age, size, valence and kind. The corpora described in this article are publicly available through the European Language Resources Association catalog.","Language Resources and Evaluation",2015,"Yes"," fairy tale corpus annotation scheme inter annotator agreement direct quotations prosody intonation stylization text speech expressivity gv lex corpus tales french corpus french tales presented parts text corpus speech corpus designed studying relationships textual structures tales speech prosody targeted application expressive text speech synthesis system embedded humanoid robot tale text corpus tale speech corpus annotated common tale description framework lexical level annotations include extended definitions enumerations time place person named entities part speech tags supra lexical level annotations include segmentation tales sequence episodes localization attribution direct quotations tale protagonists references annotation distributions inter annotator agreement analyzed largest coverage strongest agreement observed person named entities characters direct quotations coreference chains speech corpus annotations extended analysis relations tale linguistic information prosodic properties observed speech word phoneme boundaries inferred semi automatic procedures resulting linguistic annotations aligned speech signal intonation stylization models ease visual statistical analysis tale prosody additional meta information provided speech corpus allowing describing tale characters gender age size valence kind corpora article publicly european language resources association catalog",1
"KeywordsParallel corpus Multilingual corpus Comparative corpus linguistics ","a massively parallel corpus the bible in 100 languages","We describe the creation of a massively parallel corpus based on 100 translations of the Bible. We discuss some of the difficulties in acquiring and processing the raw material as well as the potential of the Bible as a corpus for natural language processing. Finally we present a statistical analysis of the corpora collected and a detailed comparison between the English translation and other English corpora.","Language Resources and Evaluation",2015,"Yes"," parallel corpus multilingual corpus comparative corpus linguistics massively parallel corpus bible languages describe creation massively parallel corpus based translations bible discuss difficulties acquiring processing raw material potential bible corpus natural language processing finally present statistical analysis corpora collected detailed comparison english translation english corpora",1
"KeywordsCode-switching speech Spontaneous spoken corpus development Mandarin–English Speech recognition Language recognition ","mandarinenglish code switching speech corpus in south east asia seame","
This paper introduces the South East Asia Mandarin–English corpus, a 63-h spontaneous Mandarin–English code-switching transcribed speech corpus suitable for LVCSR and language change detection/identification research. The corpus is recorded under unscripted interview and conversational settings from 157 Singaporean and Malaysian speakers who spoke a mixture of Mandarin and English within a single sentence. About 82 % of the transcribed utterances are intra-sentential code-switching speech and the corpus will be release by LDC in 2015. This paper presents an analysis of the code-switching statistics of the corpus, such as the duration of monolingual segments and the frequency of language turns in code-switch utterances. We also summarize the development effort, details such as the processing time for transcription, validation and language boundary labelling. Lastly, we present textual analyses of code-switch segments examining the word length of monolingual segments in code-switch utterances and the most common single word and two-word phrase of such segments.
","Language Resources and Evaluation",2015,"Yes"," code switching speech spontaneous spoken corpus development mandarin english speech recognition language recognition mandarinenglish code switching speech corpus south east asia seame paper introduces south east asia mandarin english corpus spontaneous mandarin english code switching transcribed speech corpus suitable lvcsr language change detectionidentification research corpus recorded unscripted interview conversational settings singaporean malaysian speakers spoke mixture mandarin english single sentence transcribed utterances intra sentential code switching speech corpus release ldc paper presents analysis code switching statistics corpus duration monolingual segments frequency language turns code switch utterances summarize development effort details processing time transcription validation language boundary labelling lastly present textual analyses code switch segments examining word length monolingual segments code switch utterances common single word word phrase segments ",1
"KeywordsDiscourse TreeBank Discourse relations Chinese Explicit and implicit discourse connectives ","the chinese discourse treebank a chinese corpus annotated with discourse relations","
The paper presents the Chinese Discourse TreeBank, a corpus annotated with Penn Discourse TreeBank style discourse relations that take the form of a predicate taking two arguments. We first characterize the syntactic and statistical distributions of Chinese discourse connectives as well as the role of Chinese punctuation marks in discourse annotation, and then describe how we design our annotation strategy procedure based on this characterization. The Chinese-specific features of our annotation strategy include annotating explicit and implicit discourse relations in one single pass, defining the argument labels on semantic, rather than syntactic, grounds, as well as annotating the semantic type of implicit discourse relations directly. We also introduce a flat, 11-valued semantic type classification scheme for discourse relations. We finally demonstrate the feasibility of our approach with evaluation results.","Language Resources and Evaluation",2015,"Yes"," discourse treebank discourse relations chinese explicit implicit discourse connectives chinese discourse treebank chinese corpus annotated discourse relations paper presents chinese discourse treebank corpus annotated penn discourse treebank style discourse relations form predicate taking arguments characterize syntactic statistical distributions chinese discourse connectives role chinese punctuation marks discourse annotation describe design annotation strategy procedure based characterization chinese specific features annotation strategy include annotating explicit implicit discourse relations single pass defining argument labels semantic syntactic grounds annotating semantic type implicit discourse relations directly introduce flat valued semantic type classification scheme discourse relations finally demonstrate feasibility approach evaluation results",1
"KeywordsMultiparty conversation in L2 Proficiency Eye gaze Multimodal corpus Annotation ","multimodal corpus of multiparty conversations in l1 and l2 languages and findings obtained from it","To investigate the differences in communicative activities by the same interlocutors in Japanese (their L1) and in English (their L2), an 8-h multimodal corpus of multiparty conversations was collected. Three subjects participated in each conversational group, and they had conversations on free-flowing and goal-oriented topics in Japanese and in English. Their utterances, eye gazes, and gestures were recorded with microphones, eye trackers, and video cameras. The utterances and eye gazes were manually annotated. Their utterances were transcribed, and the transcriptions of each participant were aligned with those of the others along the time axis. Quantitative analyses were made to compare the communicative activities caused by the differences in conversational languages, the conversation types, and the levels of language expertise in L2. The results reveal different utterance characteristics and gaze patterns that reflect the differences in difficulty felt by the participants in each conversational condition. Both total and average durations of utterances were shorter in their L2 than in their L1 conversations. Differences in eye gazes were mainly found in those toward the information senders: Speakers were gazed at more in their second-language than in their native-language conversations. Our findings on the characteristics of conversations in the second language suggest possible directions for future research in psychology, cognitive science, and human–computer interaction technologies.","Language Resources and Evaluation",2015,"Yes"," multiparty conversation l proficiency eye gaze multimodal corpus annotation multimodal corpus multiparty conversations l l languages findings obtained investigate differences communicative activities interlocutors japanese l english l multimodal corpus multiparty conversations collected subjects participated conversational group conversations free flowing goal oriented topics japanese english utterances eye gazes gestures recorded microphones eye trackers video cameras utterances eye gazes manually annotated utterances transcribed transcriptions participant aligned time axis quantitative analyses made compare communicative activities caused differences conversational languages conversation types levels language expertise l results reveal utterance characteristics gaze patterns reflect differences difficulty felt participants conversational condition total average durations utterances shorter l l conversations differences eye gazes found information senders speakers gazed language native language conversations findings characteristics conversations language suggest directions future research psychology cognitive science human computer interaction technologies",1
"KeywordsAnnotation evaluation Discourse analysis Rhetorical Structure Theory Translation strategies ","a qualitative comparison method for rhetorical structures identifying different discourse structures in multilingual corpora","Explaining why the same passage may have different rhetorical structures when conveyed in different languages remains an open question. Starting from a trilingual translation corpus, this paper aims to provide a new qualitative method for the comparison of rhetorical structures in different languages and to specify why translated texts may differ in their rhetorical structures. To achieve these aims we have carried out a contrastive analysis, comparing a corpus of parallel English, Spanish and Basque texts, using Rhetorical Structure Theory. We propose a method to describe the main linguistic differences among the rhetorical structures of the three languages in the two annotation stages (segmentation and rhetorical analysis). We show a new type of comparison that has important advantages with regard to the quantitative method usually employed: it provides an accurate measurement of inter-annotator agreement, and it pinpoints sources of disagreement among annotators. With the use of this new method, we show how translation strategies affect discourse structure.","Language Resources and Evaluation",2015,"No"," annotation evaluation discourse analysis rhetorical structure theory translation strategies qualitative comparison method rhetorical structures identifying discourse structures multilingual corpora explaining passage rhetorical structures conveyed languages remains open question starting trilingual translation corpus paper aims provide qualitative method comparison rhetorical structures languages translated texts differ rhetorical structures achieve aims carried contrastive analysis comparing corpus parallel english spanish basque texts rhetorical structure theory propose method describe main linguistic differences rhetorical structures languages annotation stages segmentation rhetorical analysis show type comparison important advantages regard quantitative method employed accurate measurement inter annotator agreement pinpoints sources disagreement annotators method show translation strategies affect discourse structure",0
"KeywordsResource-scarce Data selection Corpus design  Speech recognition ","efficient data selection for asr","Automatic speech recognition (ASR) technology has matured over the past few decades and has made significant impacts in a variety of fields, from assistive technologies to commercial products. However, ASR system development is a resource intensive activity and requires language resources in the form of text annotated audio recordings and pronunciation dictionaries. Unfortunately, many languages found in the developing world fall into the resource-scarce category and due to this resource scarcity the deployment of ASR systems in the developing world is severely inhibited. One approach to assist with resource-scarce ASR system development, is to select “useful” training samples which could reduce the resources needed to collect new corpora. In this work, we propose a new data selection framework which can be used to design a speech recognition corpus. We show for limited data sets, independent of language and bandwidth, the most effective strategy for data selection is frequency-matched selection and that the widely-used maximum entropy methods generally produced the least promising results. In our model, the frequency-matched selection method corresponds to a logarithmic relationship between accuracy and corpus size; we also investigated other model relationships, and found that a hyperbolic relationship (as suggested from simple asymptotic arguments in learning theory) may lead to somewhat better performance under certain conditions.","Language Resources and Evaluation",2015,"No"," resource scarce data selection corpus design speech recognition efficient data selection asr automatic speech recognition asr technology matured past decades made significant impacts variety fields assistive technologies commercial products asr system development resource intensive activity requires language resources form text annotated audio recordings pronunciation dictionaries languages found developing world fall resource scarce category due resource scarcity deployment asr systems developing world severely inhibited approach assist resource scarce asr system development select training samples reduce resources needed collect corpora work propose data selection framework design speech recognition corpus show limited data sets independent language bandwidth effective strategy data selection frequency matched selection widely maximum entropy methods generally produced promising results model frequency matched selection method corresponds logarithmic relationship accuracy corpus size investigated model relationships found hyperbolic relationship suggested simple asymptotic arguments learning theory lead performance conditions",0
"KeywordsResources Summarisation Arabic Under-resourced languages ","creating language resources for under resourced languages methodologies and experiments with arabic","Language resources are important for those working on computational methods to analyse and study languages. These resources are needed to help advancing the research in fields such as natural language processing, machine learning, information retrieval and text analysis in general. We describe the creation of useful resources for languages that currently lack them, taking resources for Arabic summarisation as a case study. We illustrate three different paradigms for creating language resources, namely: (1) using crowdsourcing to produce a small resource rapidly and relatively cheaply; (2) translating an existing gold-standard dataset, which is relatively easy but potentially of lower quality; and (3) using manual effort with appropriately skilled human participants to create a resource that is more expensive but of high quality. The last of these was used as a test collection for TAC-2011. An evaluation of the resources is also presented.","Language Resources and Evaluation",2015,"No"," resources summarisation arabic resourced languages creating language resources resourced languages methodologies experiments arabic language resources important working computational methods analyse study languages resources needed advancing research fields natural language processing machine learning information retrieval text analysis general describe creation resources languages lack taking resources arabic summarisation case study illustrate paradigms creating language resources crowdsourcing produce small resource rapidly cheaply translating existing gold standard dataset easy potentially lower quality manual effort appropriately skilled human participants create resource expensive high quality test collection tac evaluation resources presented",0
"KeywordsCorpus annotation Polarity Sentiment analysis  Natural language processing ","the good the bad and the implicit a comprehensive approach to annotating explicit and implicit sentiment","We present a fine-grained scheme for the annotation of polar sentiment in text, that accounts for explicit sentiment (so-called private states), as well as implicit expressions of sentiment (polar facts). Polar expressions are annotated below sentence level and classified according to their subjectivity status. Additionally, they are linked to one or more targets with a specific polar orientation and intensity. Other components of the annotation scheme include source attribution and the identification and classification of expressions that modify polarity. In previous research, little attention has been given to implicit sentiment, which represents a substantial amount of the polar expressions encountered in our data. An English and Dutch corpus of financial newswire text, consisting of over 45,000 words each, was annotated using our scheme. A subset of this corpus was used to conduct an inter-annotator agreement study, which demonstrated that the proposed scheme can be used to reliably annotate explicit and implicit sentiment in real-world textual data, making the created corpora a useful resource for sentiment analysis.","Language Resources and Evaluation",2015,"No"," corpus annotation polarity sentiment analysis natural language processing good bad implicit comprehensive approach annotating explicit implicit sentiment present fine grained scheme annotation polar sentiment text accounts explicit sentiment called private states implicit expressions sentiment polar facts polar expressions annotated sentence level classified subjectivity status additionally linked targets specific polar orientation intensity components annotation scheme include source attribution identification classification expressions modify polarity previous research attention implicit sentiment represents substantial amount polar expressions encountered data english dutch corpus financial newswire text consisting words annotated scheme subset corpus conduct inter annotator agreement study demonstrated proposed scheme reliably annotate explicit implicit sentiment real world textual data making created corpora resource sentiment analysis",0
"KeywordsParsing Dependency grammar Child language Syntactic annotation ","parsing hebrew childes transcripts","We present a syntactic parser of (transcripts of) spoken Hebrew: a dependency parser of the Hebrew CHILDES database. CHILDES is a corpus of child–adult linguistic interactions. Its Hebrew section has recently been morphologically analyzed and disambiguated, paving the way for syntactic annotation. This paper describes a novel annotation scheme of dependency relations reflecting constructions of child and child-directed Hebrew utterances. A subset of the corpus was annotated with dependency relations according to this scheme, and was used to train two parsers (MaltParser and MEGRASP) with which the rest of the data were parsed. The adequacy of the annotation scheme to the CHILDES data is established through numerous evaluation scenarios. The paper also discusses different annotation approaches to several linguistic phenomena, as well as the contribution of morphological features to the accuracy of parsing.","Language Resources and Evaluation",2015,"Yes"," parsing dependency grammar child language syntactic annotation parsing hebrew childes transcripts present syntactic parser transcripts spoken hebrew dependency parser hebrew childes database childes corpus child adult linguistic interactions hebrew section recently morphologically analyzed disambiguated paving syntactic annotation paper describes annotation scheme dependency relations reflecting constructions child child directed hebrew utterances subset corpus annotated dependency relations scheme train parsers maltparser megrasp rest data parsed adequacy annotation scheme childes data established numerous evaluation scenarios paper discusses annotation approaches linguistic phenomena contribution morphological features accuracy parsing",1
"KeywordsTreebank Error detection Entropy ","vietnamese treebank construction and entropy based error detection","Treebanks, especially the Penn treebank for natural language processing (NLP) in English, play an essential role in both research into and the application of NLP. However, many languages still lack treebanks and building a treebank can be very complicated and difficult. This work has a twofold objective. Firstly, to share our results in constructing a large Vietnamese treebank (VTB) with three levels of annotation including word segmentation, part-of-speech tagging, and syntactic analysis. Major steps in the treebank construction process are described with particular regard to specific Vietnamese properties such as lack of word delimiter and isolation. Those properties make sentences highly syntactically ambiguous, and therefore it is difficult to ensure a high level of agreement among annotators. Various studies of Vietnamese syntax were employed not only to define annotations but also to systematically deal with ambiguities. Annotators were supported by automatic labelling tools, which are based on statistical machine learning methods, for sentence pre-processing and a tree editor for supporting manual annotation. As a result, an annotation agreement of around 90 % was achieved. Our second objective is to present our method for automatically finding errors and inconsistencies in treebank corpora and its application to the construction of the VTB. This method employs the Shannon entropy measure in a manner that the more reduced entropy the more corrected errors in a treebank. The method ranks error candidates by using a scoring function based on conditional entropy. Our experiments showed that this method detected high-error-density subsets of original error candidate sets, and that the corpus entropy was significantly reduced after error correction. The size of these subsets was only about one third of the whole set, while these subsets contained 80–90 % of the total errors. This method can also be applied to languages similar to Vietnamese.","Language Resources and Evaluation",2015,"Yes"," treebank error detection entropy vietnamese treebank construction entropy based error detection treebanks penn treebank natural language processing nlp english play essential role research application nlp languages lack treebanks building treebank complicated difficult work twofold objective firstly share results constructing large vietnamese treebank vtb levels annotation including word segmentation part speech tagging syntactic analysis major steps treebank construction process regard specific vietnamese properties lack word delimiter isolation properties make sentences highly syntactically ambiguous difficult ensure high level agreement annotators studies vietnamese syntax employed define annotations systematically deal ambiguities annotators supported automatic labelling tools based statistical machine learning methods sentence pre processing tree editor supporting manual annotation result annotation agreement achieved objective present method automatically finding errors inconsistencies treebank corpora application construction vtb method employs shannon entropy measure manner reduced entropy corrected errors treebank method ranks error candidates scoring function based conditional entropy experiments showed method detected high error density subsets original error candidate sets corpus entropy significantly reduced error correction size subsets set subsets contained total errors method applied languages similar vietnamese",1
"KeywordsWordnet development Multilingual lexicon extraction  Word-sense disambiguation Distributional similarity ","constructing a poor mans wordnet in a resource rich world","In this paper we present a language-independent, fully modular and automatic approach to bootstrap a wordnet for a new language by recycling different types of already existing language resources, such as machine-readable dictionaries, parallel corpora, and Wikipedia. The approach, which we apply here to Slovene, takes into account monosemous and polysemous words, general and specialised vocabulary as well as simple and multi-word lexemes. The extracted words are then assigned one or several synset ids, based on a classifier that relies on several features including distributional similarity. Finally, we identify and remove highly dubious (literal, synset) pairs, based on simple distributional information extracted from a large corpus in an unsupervised way. Automatic, manual and task-based evaluations show that the resulting resource, the latest version of the Slovene wordnet, is already a valuable source of lexico-semantic information.","Language Resources and Evaluation",2015,"Yes"," wordnet development multilingual lexicon extraction word sense disambiguation distributional similarity constructing poor mans wordnet resource rich world paper present language independent fully modular automatic approach bootstrap wordnet language recycling types existing language resources machine readable dictionaries parallel corpora wikipedia approach apply slovene takes account monosemous polysemous words general specialised vocabulary simple multi word lexemes extracted words assigned synset ids based classifier relies features including distributional similarity finally identify remove highly dubious literal synset pairs based simple distributional information extracted large corpus unsupervised automatic manual task based evaluations show resulting resource latest version slovene wordnet valuable source lexico semantic information",1
"KeywordsLoanwords Transliteration Detection N-gram EM algorithm Korean ","an unsupervised method for identifying loanwords in korean","This paper presents an unsupervised method for developing a character-based n-gram classifier that identifies loanwords or transliterated foreign words in Korean text. The classifier is trained on an unlabeled corpus using the Expectation Maximization algorithm, building on seed words extracted from the corpus. Words with high token frequency serve as native seed words. Words with seeming traces of vowel insertion to repair consonant clusters serve as foreign seed words. What counts as a trace of insertion is determined using phoneme co-occurrence statistics in conjunction with ideas and findings in phonology. Experiments show that the method can produce an unsupervised classifier that performs at a level comparable to that of a supervised classifier. In a cross-validation experiment using a corpus of about 9.2 million words and a lexicon of about 71,000 words, mean F-scores of the best unsupervised classifier and the corresponding supervised classifier were 94.77 and 96.67 %, respectively. Experiments also suggest that the method can be readily applied to other languages with similar phonotactics such as Japanese.","Language Resources and Evaluation",2015,"No"," loanwords transliteration detection gram em algorithm korean unsupervised method identifying loanwords korean paper presents unsupervised method developing character based gram classifier identifies loanwords transliterated foreign words korean text classifier trained unlabeled corpus expectation maximization algorithm building seed words extracted corpus words high token frequency serve native seed words words traces vowel insertion repair consonant clusters serve foreign seed words counts trace insertion determined phoneme occurrence statistics conjunction ideas findings phonology experiments show method produce unsupervised classifier performs level comparable supervised classifier cross validation experiment corpus million words lexicon words scores unsupervised classifier supervised classifier experiments suggest method readily applied languages similar phonotactics japanese",0
"KeywordsText classification Food domain Social media  Linguistically informed feature engineering Polarity classification ","detecting conditional healthiness of food items from natural language text","In this article, we explore the feasibility of extracting suitable and unsuitable food items for particular health conditions from natural language text. We refer to this task as conditional healthiness classification. For that purpose, we annotate a corpus extracted from forum entries of a food-related website. We identify different relation types that hold between food items and health conditions going beyond a binary distinction of suitability and unsuitability and devise various supervised classifiers using different types of features. We examine the impact of different task-specific resources, such as a healthiness lexicon that lists the healthiness status of a food item and a sentiment lexicon. Moreover, we also consider task-specific linguistic features that disambiguate a context in which mentions of a food item and a health condition co-occur and compare them with standard features using bag of words, part-of-speech information and syntactic parses. We also investigate in how far individual food items and health conditions correlate with specific relation types and try to harness this information for classification.","Language Resources and Evaluation",2015,"No"," text classification food domain social media linguistically informed feature engineering polarity classification detecting conditional healthiness food items natural language text article explore feasibility extracting suitable unsuitable food items health conditions natural language text refer task conditional healthiness classification purpose annotate corpus extracted forum entries food related website identify relation types hold food items health conditions binary distinction suitability unsuitability devise supervised classifiers types features examine impact task specific resources healthiness lexicon lists healthiness status food item sentiment lexicon task specific linguistic features disambiguate context mentions food item health condition occur compare standard features bag words part speech information syntactic parses investigate individual food items health conditions correlate specific relation types harness information classification",0
NA,"barbara mcgillivray methods in latin computational linguistics brills studies in historical linguistics","The book under review opens the series of Brill’s Studies in Historical Linguistics with a new methodological approach to the observation of diachronic phenomena which moves the focus from qualitative to quantitative aspects. For this reason the author aims to illustrate how to apply computational methods to historical language data, in particular to a corpus of lemmatized and morpho-syntactically annotated Latin texts. The volume is addressed to a heterogeneous audience composed of computational linguists, Latin linguists and those within the growing community of Latin computational linguists (for an overview on methods and tools at disposition of Latin computational linguists, see Passarotti 2010; Babeu 2011; Spinazzè 2015). Such communities are likely to have different backgrounds and cultural gaps which need to be filled in at least in a cursory fashion in order to understand the overall structure of the work presented here. This difficult task is achieved through a prudent...","Language Resources and Evaluation",2015,"No"," barbara mcgillivray methods latin computational linguistics brills studies historical linguistics book review opens series brill studies historical linguistics methodological approach observation diachronic phenomena moves focus qualitative quantitative aspects reason author aims illustrate apply computational methods historical language data corpus lemmatized morpho syntactically annotated latin texts volume addressed heterogeneous audience composed computational linguists latin linguists growing community latin computational linguists overview methods tools disposition latin computational linguists passarotti babeu spinazz communities backgrounds cultural gaps filled cursory fashion order understand structure work presented difficult task achieved prudent",0
"KeywordsSpanish Treebank Dependency annotation Technical corpus ","dependency structure annotation in the iula spanish lsp treebank","This paper presents the IULA Spanish LSP Treebank, an open-source treebank of over 40,000 sentences, developed in the framework of the European project METANET4U. The IULA Spanish LSP Treebank is the first technical corpus of Spanish annotated at surface syntactic level, following the dependency grammar theory. We present the method we used to create the resource and the linguistic annotations that the treebank provides, using examples and comparing with similar resources. We also provide the statistics of the treebank and the evaluation results.","Language Resources and Evaluation",2015,"Yes"," spanish treebank dependency annotation technical corpus dependency structure annotation iula spanish lsp treebank paper presents iula spanish lsp treebank open source treebank sentences developed framework european project metanetu iula spanish lsp treebank technical corpus spanish annotated surface syntactic level dependency grammar theory present method create resource linguistic annotations treebank examples comparing similar resources provide statistics treebank evaluation results",1
"KeywordsStatistical machine translation Domain adaptation  Web crawling Optimisation ","domain adaptation of statistical machine translation with domain focused web crawling","In this paper, we tackle the problem of domain adaptation of statistical machine translation (SMT) by exploiting domain-specific data acquired by domain-focused crawling of text from the World Wide Web. We design and empirically evaluate a procedure for automatic acquisition of monolingual and parallel text and their exploitation for system training, tuning, and testing in a phrase-based SMT framework. We present a strategy for using such resources depending on their availability and quantity supported by results of a large-scale evaluation carried out for the domains of environment and labour legislation, two language pairs (English–French and English–Greek) and in both directions: into and from English. In general, machine translation systems trained and tuned on a general domain perform poorly on specific domains and we show that such systems can be adapted successfully by retuning model parameters using small amounts of parallel in-domain data, and may be further improved by using additional monolingual and parallel training data for adaptation of language and translation models. The average observed improvement in BLEU achieved is substantial at 15.30 points absolute.","Language Resources and Evaluation",2015,"No"," statistical machine translation domain adaptation web crawling optimisation domain adaptation statistical machine translation domain focused web crawling paper tackle problem domain adaptation statistical machine translation smt exploiting domain specific data acquired domain focused crawling text world wide web design empirically evaluate procedure automatic acquisition monolingual parallel text exploitation system training tuning testing phrase based smt framework present strategy resources depending availability quantity supported results large scale evaluation carried domains environment labour legislation language pairs english french english greek directions english general machine translation systems trained tuned general domain perform poorly specific domains show systems adapted successfully retuning model parameters small amounts parallel domain data improved additional monolingual parallel training data adaptation language translation models average observed improvement bleu achieved substantial points absolute",0
"KeywordsFinite state computational morphology Tswana Disjunctive orthography Tokenisation Verb morphology ","tswana finite state tokenisation","Tswana, a Bantu language in the Sotho group, is characterised by an agglutinative morphology and a disjunctive orthography, which mainly affects the verb category. In particular, verbal prefixes are usually written disjunctively, while suffixes follow a conjunctive writing style. Therefore, Tswana tokenisation cannot be based solely on whitespace, as is the case in many alphabetic, segmented languages, including the conjunctively written Nguni group of South African Bantu languages. This paper shows how a combination of two finite state tokeniser transducers and a finite state morphological analyser are combined to solve the Tswana (verb) tokenisation problem. The approach has the important advantage of bringing the processing of Tswana, beyond the morphological analysis level, in line with what is appropriate for the Nguni languages. This means that the challenge of the disjunctive orthography is met at the tokenisation/morphological analysis level and does not in principle propagate to subsequent levels of analysis such as POS tagging and shallow parsing, etc. The tokenisation approach is novel and, when implemented and evaluated, yields an F1-score of 95 % with respect to a hand tokenised gold standard.","Language Resources and Evaluation",2015,"No"," finite state computational morphology tswana disjunctive orthography tokenisation verb morphology tswana finite state tokenisation tswana bantu language sotho group characterised agglutinative morphology disjunctive orthography affects verb category verbal prefixes written disjunctively suffixes follow conjunctive writing style tswana tokenisation based solely whitespace case alphabetic segmented languages including conjunctively written nguni group south african bantu languages paper shows combination finite state tokeniser transducers finite state morphological analyser combined solve tswana verb tokenisation problem approach important advantage bringing processing tswana morphological analysis level line nguni languages means challenge disjunctive orthography met tokenisationmorphological analysis level principle propagate subsequent levels analysis pos tagging shallow parsing tokenisation approach implemented evaluated yields f score respect hand tokenised gold standard",0
"KeywordsPropBank Finnish SRL ","the finnish proposition bank","We present the Finnish PropBank, a resource for semantic role labeling (SRL) of Finnish based on the Turku Dependency Treebank whose syntax is annotated in the well-known Stanford Dependency (SD) scheme. The contribution of this paper consists of the lexicon of the verbs and their arguments present in the treebank, as well as the predicate-argument annotation of all verb occurrences in the treebank text. We demonstrate that the annotation is of high quality, that the SD scheme is highly compatible with PropBank annotation, and further that the additional dependencies present in the Turku Dependency Treebank are clearly beneficial for PropBank annotation. Further, we also use the PropBank to provide a strong baseline for automated Finnish SRL using a machine learning SRL system developed for the SemEval’14 shared task on broad-coverage semantic dependency parsing. The PropBank as well as the SRL system are available under a free license at http://bionlp.utu.fi/.","Language Resources and Evaluation",2015,"Yes"," propbank finnish srl finnish proposition bank present finnish propbank resource semantic role labeling srl finnish based turku dependency treebank syntax annotated stanford dependency sd scheme contribution paper consists lexicon verbs arguments present treebank predicate argument annotation verb occurrences treebank text demonstrate annotation high quality sd scheme highly compatible propbank annotation additional dependencies present turku dependency treebank beneficial propbank annotation propbank provide strong baseline automated finnish srl machine learning srl system developed semeval shared task broad coverage semantic dependency parsing propbank srl system free license httpbionlputufi",1
"KeywordsTone Algorithm Sesotho Bantu languages Text-to-Speech systems ","improving a tone labeling algorithm for sesotho","
We report on a study that aimed to improve an existing tone label prediction algorithm for Sesotho, an official language of South Africa. Tone is an important prosodic feature of Sesotho, since speakers use tone to distinguish meaning. In order to implement tone in a Text-to-Speech system for Sesotho, a tone modeling algorithm must receive as input the tone labels of the syllables of each word. Then it can predict the appropriate intonation of the word. Since Sesotho does not mark tone labels in orthography, the labels have to be predicted according to the tonal rules of the language. The existing tone label prediction algorithm has two drawbacks, namely it implements three tonal rules only and is restricted to the clitic phrase domain. In our study, we developed an algorithm that implements four additional tonal rules and addresses all parts of speech. The results show that the latter algorithm significantly improves the existing one by increasing the number of matched tone labels.
","Language Resources and Evaluation",2015,"No"," tone algorithm sesotho bantu languages text speech systems improving tone labeling algorithm sesotho report study aimed improve existing tone label prediction algorithm sesotho official language south africa tone important prosodic feature sesotho speakers tone distinguish meaning order implement tone text speech system sesotho tone modeling algorithm receive input tone labels syllables word predict intonation word sesotho mark tone labels orthography labels predicted tonal rules language existing tone label prediction algorithm drawbacks implements tonal rules restricted clitic phrase domain study developed algorithm implements additional tonal rules addresses parts speech results show algorithm significantly improves existing increasing number matched tone labels ",0
"KeywordsPhonetic transcription dictionary Pronunciation dictionary Grapheme to phoneme (G2P) conversion Letter-to-sound mapping Letter-to-phoneme (L2P) transcription Romanian language ","romanian phonetic transcription dictionary for speeding up language technology development","This paper intends to present a machine readable Romanian language pronunciation dictionary called NaviRo. The dictionary contains 138,500 unique words from the DexOnline dictionary together with their phonetic transcriptions in speech assessment method phonetic alphabet. The development of the pronunciation dictionary and the performed validation tests are also described in the paper. NaviRo pronunciation dictionary is freely available on the project website (http://users.utcluj.ro/~jdomokos/naviro) in plain text, Hidden Markov Model Toolkit and Festival speech synthesis system dictionary format. There are also available for download the used grapheme and phoneme sets and the audio samples for the used phonemes. The use of these resources is completely unrestricted for any research purposes in order to speed up Romanian language speech technology research.","Language Resources and Evaluation",2015,"Yes"," phonetic transcription dictionary pronunciation dictionary grapheme phoneme gp conversion letter sound mapping letter phoneme lp transcription romanian language romanian phonetic transcription dictionary speeding language technology development paper intends present machine readable romanian language pronunciation dictionary called naviro dictionary unique words dexonline dictionary phonetic transcriptions speech assessment method phonetic alphabet development pronunciation dictionary performed validation tests paper naviro pronunciation dictionary freely project website httpusersutclujrojdomokosnaviro plain text hidden markov model toolkit festival speech synthesis system dictionary format download grapheme phoneme sets audio samples phonemes resources completely unrestricted research purposes order speed romanian language speech technology research",1
"KeywordsMultilingual lexicon Under-resourced languages Malay Iban ","lexicontx rapid construction of a multilingual lexicon with under resourced languages","Most efforts at automatically creating multilingual lexicons require input lexical resources with rich content (e.g. semantic networks, domain codes, semantic categories) or large corpora. Such material is often unavailable and difficult to construct for under-resourced languages. In some cases, particularly for some ethnic languages, even unannotated corpora are still in the process of collection. We show how multilingual lexicons with under-resourced languages can be constructed using simple bilingual translation lists, which are more readily available. The prototype multilingual lexicon developed comprise six member languages: English, Malay, Chinese, French, Thai and Iban, the last of which is an under-resourced language in Borneo. Quick evaluations showed that 91.2  % of 500 random multilingual entries in the generated lexicon require minimal or no human correction.","Language Resources and Evaluation",2014,"Yes"," multilingual lexicon resourced languages malay iban lexicontx rapid construction multilingual lexicon resourced languages efforts automatically creating multilingual lexicons require input lexical resources rich content semantic networks domain codes semantic categories large corpora material unavailable difficult construct resourced languages cases ethnic languages unannotated corpora process collection show multilingual lexicons resourced languages constructed simple bilingual translation lists readily prototype multilingual lexicon developed comprise member languages english malay chinese french thai iban resourced language borneo quick evaluations showed random multilingual entries generated lexicon require minimal human correction",1
"KeywordsWordnet Bilingual lexicon Quality assessment Knowledge representation Word-sense disambiguation ","is it possible to create a very large wordnet in 100 days an evaluation","Wordnets are large-scale lexical databases of related words and concepts, useful for language-aware software applications. They have recently been built for many languages by using various approaches. The Finnish wordnet, FinnWordNet (FiWN), was created by translating the more than 200,000 word senses in the English Princeton WordNet (PWN) 3.0 in 100 days. To ensure quality, they were translated by professional translators. The direct translation approach was based on the assumption that most synsets in PWN represent language-independent real-world concepts. Thus also the semantic relations between synsets were assumed mostly language-independent, so the structure of PWN could be reused as well. This approach allowed the creation of an extensive Finnish wordnet directly aligned with PWN and also provided us with a translation relation and thus a bilingual wordnet usable as a dictionary. In this paper, we address several concerns raised with regard to our approach, many of them for the first time. We evaluate the craftsmanship of the translators by checking the spelling and translation quality, the viability of the approach by assessing the synonym quality both on the lexeme and concept level, as well as the usefulness of the resulting lexical resource both for humans and in a language-technological task. We discovered no new problems compared with those already known in PWN. As a whole, the paper contributes to the scientific discourse on what it takes to create a very large wordnet. As a side-effect of the evaluation, we extended FiWN to contain 208,645 word senses in 120,449 synsets, effectively making version 2.0 of FiWN currently the largest wordnet in the world by these statistics.","Language Resources and Evaluation",2014,"Yes"," wordnet bilingual lexicon quality assessment knowledge representation word sense disambiguation create large wordnet days evaluation wordnets large scale lexical databases related words concepts language aware software applications recently built languages approaches finnish wordnet finnwordnet fiwn created translating word senses english princeton wordnet pwn days ensure quality translated professional translators direct translation approach based assumption synsets pwn represent language independent real world concepts semantic relations synsets assumed language independent structure pwn reused approach allowed creation extensive finnish wordnet directly aligned pwn provided translation relation bilingual wordnet usable dictionary paper address concerns raised regard approach time evaluate craftsmanship translators checking spelling translation quality viability approach assessing synonym quality lexeme concept level usefulness resulting lexical resource humans language technological task discovered problems compared pwn paper contributes scientific discourse takes create large wordnet side effect evaluation extended fiwn word senses synsets effectively making version fiwn largest wordnet world statistics",1
"KeywordsNLP Grammatical error detection systems Evaluation Annotation Crowdsourcing ","bucking the trend improved evaluation and annotation practices for esl error detection systems","The last decade has seen an explosion in the number of people learning English as a second language (ESL). In China alone, it is estimated to be over 300 million (Yang in Engl Today 22, 2006). Even in predominantly English-speaking countries, the proportion of non-native speakers can be very substantial. For example, the US National Center for Educational Statistics reported that nearly 10 % of the students in the US public school population speak a language other than English and have limited English proficiency (National Center for Educational Statistics (NCES) in Public school student counts, staff, and graduate counts by state: school year 2000–2001, 2002). As a result, the last few years have seen a rapid increase in the development of NLP tools to detect and correct grammatical errors so that appropriate feedback can be given to ESL writers, a large and growing segment of the world’s population. As a byproduct of this surge in interest, there have been many NLP research papers on the topic, a Synthesis Series book (Leacock et al. in Automated grammatical error detection for language learners. Synthesis lectures on human language technologies. Morgan Claypool, Waterloo 2010), a recurring workshop (Tetreault et al. in Proceedings of the NAACL workshop on innovative use of NLP for building educational applications (BEA), 2012), and a shared task competition (Dale et al. in Proceedings of the seventh workshop on building educational applications using NLP (BEA), pp 54–62, 2012; Dale and Kilgarriff in Proceedings of the European workshop on natural language generation (ENLG), pp 242–249, 2011). Despite this growing body of work, several issues affecting the annotation for and evaluation of ESL error detection systems have received little attention. In this paper, we describe these issues in detail and present our research on alleviating their effects.","Language Resources and Evaluation",2014,"No"," nlp grammatical error detection systems evaluation annotation crowdsourcing bucking trend improved evaluation annotation practices esl error detection systems decade explosion number people learning english language esl china estimated million yang engl today predominantly english speaking countries proportion native speakers substantial national center educational statistics reported students public school population speak language english limited english proficiency national center educational statistics nces public school student counts staff graduate counts state school year result years rapid increase development nlp tools detect correct grammatical errors feedback esl writers large growing segment world population byproduct surge interest nlp research papers topic synthesis series book leacock al automated grammatical error detection language learners synthesis lectures human language technologies morgan claypool waterloo recurring workshop tetreault al proceedings naacl workshop innovative nlp building educational applications bea shared task competition dale al proceedings seventh workshop building educational applications nlp bea pp dale kilgarriff proceedings european workshop natural language generation enlg pp growing body work issues affecting annotation evaluation esl error detection systems received attention paper describe issues detail present research alleviating effects",0
"KeywordsLearner translation corpus Multiple translation corpus LTC-UPF Error-annotation English–Catalan translation Translator training ","the upf learner translation corpus as a resource for translator training","The learner translation corpus developed at the School of Translation and Interpreting of Pompeu Fabra University in Barcelona is a web-searchable resource created for pedagogical and research purposes. It comprises a multiple translation corpus (English–Catalan) featuring automatic linguistic annotation and manual error annotation, complemented with an interface for monolingual or bilingual querying of the data. The corpus can be used to identify common errors in the students’ work and to analyse their patterns of language use. It provides easy access to error samples and to multiple versions of the same source text sequence to be used as learning materials in various courses in the translator-training university curriculum.","Language Resources and Evaluation",2014,"Yes"," learner translation corpus multiple translation corpus ltc upf error annotation english catalan translation translator training upf learner translation corpus resource translator training learner translation corpus developed school translation interpreting pompeu fabra university barcelona web searchable resource created pedagogical research purposes comprises multiple translation corpus english catalan featuring automatic linguistic annotation manual error annotation complemented interface monolingual bilingual querying data corpus identify common errors students work analyse patterns language easy access error samples multiple versions source text sequence learning materials courses translator training university curriculum",1
"KeywordsLearner corpus Error annotation Czech ","building a learner corpus","The need for data about the acquisition of Czech by non-native learners prompted the compilation of the first learner corpus of Czech. After introducing its basic design and parameters, including a multi-tier manual annotation scheme and error taxonomy, we focus on the more technical aspects: the transcription of hand-written source texts, process of annotation, and options for exploiting the result, together with tools used for these tasks and decisions behind the choices. To support or even substitute manual annotation we assign some error tags automatically and use automatic annotation tools (tagger, spell checker).","Language Resources and Evaluation",2014,"Yes"," learner corpus error annotation czech building learner corpus data acquisition czech native learners prompted compilation learner corpus czech introducing basic design parameters including multi tier manual annotation scheme error taxonomy focus technical aspects transcription hand written source texts process annotation options exploiting result tools tasks decisions choices support substitute manual annotation assign error tags automatically automatic annotation tools tagger spell checker",1
"KeywordsCorpora Language learning Vocabulary Frequency Frequency lists ","corpus based vocabulary lists for language learners for nine languages","We present the KELLY project and its work on developing monolingual and bilingual word lists for language learning, using corpus methods, for nine languages and thirty-six language pairs. We describe the method and discuss the many challenges encountered. We have loaded the data into an online database to make it accessible for anyone to explore and we present our own first explorations of it. The focus of the paper is thus twofold, covering pedagogical and methodological aspects of the lists’ construction, and linguistic aspects of the by-product of the project, the KELLY database.","Language Resources and Evaluation",2014,"Yes"," corpora language learning vocabulary frequency frequency lists corpus based vocabulary lists language learners languages present kelly project work developing monolingual bilingual word lists language learning corpus methods languages thirty language pairs describe method discuss challenges encountered loaded data online database make accessible explore present explorations focus paper twofold covering pedagogical methodological aspects lists construction linguistic aspects product project kelly database",1
"KeywordsBCCWJ Japanese Balanced corpus Design Annotation Dual POS analysis Evaluation Shonagon Chunagon ","balanced corpus of contemporary written japanese","
The balanced corpus of contemporary written Japanese (BCCWJ) is Japan’s first 100 million words balanced corpus. It consists of three subcorpora (publication subcorpus, library subcorpus, and special-purpose subcorpus) and covers a wide range of text registers including books in general, magazines, newspapers, governmental white papers, best-selling books, an internet bulletin-board, a blog, school textbooks, minutes of the national diet, publicity newsletters of local governments, laws, and poetry verses. A random sampling technique is utilized whenever possible in order to maximize the representativeness of the corpus. The corpus is annotated in terms of dual POS analysis, document structure, and bibliographical information. The BCCWJ is currently accessible in three different ways including Chunagon a web-based interface to the dual POS analysis data. Lastly, results of some pilot evaluation of the corpus with respect to the textual diversity are reported. The analyses include POS distribution, word-class distribution, entropy of orthography, sentence length, and variation of the adjective predicate. High textual diversity is observed in all these analyses.","Language Resources and Evaluation",2014,"Yes"," bccwj japanese balanced corpus design annotation dual pos analysis evaluation shonagon chunagon balanced corpus contemporary written japanese balanced corpus contemporary written japanese bccwj japan million words balanced corpus consists subcorpora publication subcorpus library subcorpus special purpose subcorpus covers wide range text registers including books general magazines newspapers governmental white papers selling books internet bulletin board blog school textbooks minutes national diet publicity newsletters local governments laws poetry verses random sampling technique utilized order maximize representativeness corpus corpus annotated terms dual pos analysis document structure bibliographical information bccwj accessible ways including chunagon web based interface dual pos analysis data lastly results pilot evaluation corpus respect textual diversity reported analyses include pos distribution word class distribution entropy orthography sentence length variation adjective predicate high textual diversity observed analyses",1
"KeywordsLearner corpus Error annotation Second language acquisition Czech ","evaluating and automating the annotation of a learner corpus","The paper describes a corpus of texts produced by non-native speakers of Czech. We discuss its annotation scheme, consisting of three interlinked tiers, designed to handle a wide range of error types present in the input. Each tier corrects different types of errors; links between the tiers allow capturing errors in word order and complex discontinuous expressions. Errors are not only corrected, but also classified. The annotation scheme is tested on a data set including approx. 175,000 words with fair inter-annotator agreement results. We also explore the possibility of applying automated linguistic annotation tools (taggers, spell checkers and grammar checkers) to the learner text to support or even substitute manual annotation.","Language Resources and Evaluation",2014,"Yes"," learner corpus error annotation language acquisition czech evaluating automating annotation learner corpus paper describes corpus texts produced native speakers czech discuss annotation scheme consisting interlinked tiers designed handle wide range error types present input tier corrects types errors links tiers capturing errors word order complex discontinuous expressions errors corrected classified annotation scheme tested data set including approx words fair inter annotator agreement results explore possibility applying automated linguistic annotation tools taggers spell checkers grammar checkers learner text support substitute manual annotation",1
"KeywordsParallel corpus Interactionist theory CALL Corpus CALL Data driven learning Language teaching/learning ","from input to output the potential of parallel corpora for call","The aim of this paper is to illustrate the potential of a parallel corpus in the context of (computer-assisted) language learning. In order to do so, we propose to answer two main questions (1) what corpus (data) to use and (2) how to use the corpus (data). We provide an answer to the what-question by describing the importance and particularities of compiling and processing a corpus for pedagogical purposes. In order to answer the how-question, we first investigate the central concepts of the interactionist theory of second language acquisition: comprehensible input, input enhancement, comprehensible output and output enhancement. By means of two case studies, we illustrate how the abovementioned concepts can be realized in concrete corpus-based language learning activities. We propose a design for a receptive and productive language task and describe how a parallel corpus can be at the basis of powerful language learning activities. The Dutch Parallel Corpus, a ten-million word sentence aligned and annotated parallel corpus, is used to develop these language tasks.","Language Resources and Evaluation",2014,"No"," parallel corpus interactionist theory call corpus call data driven learning language teachinglearning input output potential parallel corpora call aim paper illustrate potential parallel corpus context computer assisted language learning order propose answer main questions corpus data corpus data provide answer question describing importance particularities compiling processing corpus pedagogical purposes order answer question investigate central concepts interactionist theory language acquisition comprehensible input input enhancement comprehensible output output enhancement means case studies illustrate abovementioned concepts realized concrete corpus based language learning activities propose design receptive productive language task describe parallel corpus basis powerful language learning activities dutch parallel corpus ten million word sentence aligned annotated parallel corpus develop language tasks",0
"KeywordsParallel corpora Linguistic resources Highly multilingual European Union Translation memory JRC-Acquis DGT-Acquis DGT-TM DCEP ECDC-TM EAC-TM JRC EuroVoc Indexer JEX EuroVoc Eur-Lex ","an overview of the european unions highly multilingual parallel corpora","Starting in 2006, the European Commission’s Joint Research Centre and other European Union organisations have made available a number of large-scale highly-multilingual parallel language resources. In this article, we give a comparative overview of these resources and we explain the specific nature of each of them. This article provides answers to a number of question, including: What are these linguistic resources? What is the difference between them? Why were they originally created and why was the data released publicly? What can they be used for and what are the limitations of their usability? What are the text types, subject domains and languages covered? How to avoid overlapping document sets? How do they compare regarding the formatting and the translation alignment? What are their usage conditions? What other types of multilingual linguistic resources does the EU have? This article thus aims to clarify what the similarities and differences between the various resources are and what they can be used for. It will also serve as a reference publication for those resources, for which a more detailed description has been lacking so far (EAC-TM, ECDC-TM and DGT-Acquis).","Language Resources and Evaluation",2014,"Yes"," parallel corpora linguistic resources highly multilingual european union translation memory jrc acquis dgt acquis dgt tm dcep ecdc tm eac tm jrc eurovoc indexer jex eurovoc eur lex overview european unions highly multilingual parallel corpora starting european commission joint research centre european union organisations made number large scale highly multilingual parallel language resources article give comparative overview resources explain specific nature article answers number question including linguistic resources difference originally created data released publicly limitations usability text types subject domains languages covered avoid overlapping document sets compare formatting translation alignment usage conditions types multilingual linguistic resources eu article aims clarify similarities differences resources serve reference publication resources detailed description lacking eac tm ecdc tm dgt acquis",1
"KeywordsCollocation Collocation error Miscollocation CALL Collocation error detection Collocation error correction ","towards advanced collocation error correction in spanish learner corpora","Collocations in the sense of idiosyncratic binary lexical co-occurrences are one of the biggest challenges for any language learner. Even advanced learners make collocation mistakes in that they literally translate collocation elements from their native tongue, create new words as collocation elements, choose a wrong subcategorization for one of the elements, etc. Therefore, automatic collocation error detection and correction is increasingly in demand. However, while state-of-the-art models predict, with a reasonable accuracy, whether a given co-occurrence is a valid collocation or not, only few of them manage to suggest appropriate corrections with an acceptable hit rate. Most often, a ranked list of correction options is offered from which the learner has then to choose. This is clearly unsatisfactory. Our proposal focuses on this critical part of the problem in the context of the acquisition of Spanish as second language. For collocation error detection, we use a frequency-based technique. To improve on collocation error correction, we discuss three different metrics with respect to their capability to select the most appropriate correction of miscollocations found in our learner corpus.","Language Resources and Evaluation",2014,"No"," collocation collocation error miscollocation call collocation error detection collocation error correction advanced collocation error correction spanish learner corpora collocations sense idiosyncratic binary lexical occurrences biggest challenges language learner advanced learners make collocation mistakes literally translate collocation elements native tongue create words collocation elements choose wrong subcategorization elements automatic collocation error detection correction increasingly demand state art models predict reasonable accuracy occurrence valid collocation manage suggest corrections acceptable hit rate ranked list correction options offered learner choose unsatisfactory proposal focuses critical part problem context acquisition spanish language collocation error detection frequency based technique improve collocation error correction discuss metrics respect capability select correction miscollocations found learner corpus",0
"KeywordsMonolingual treebank Semantic similarity Corpus Alignment Semantic relations Parallel text Comparable text Tree alignment Paraphrase ","construction of an aligned monolingual treebank for studying semantic similarity","Modern paraphrase research would benefit from large corpora with detailed annotations. However, currently these corpora are still thin on the ground. In this paper, we describe the development of such a corpus for Dutch, which takes the form of a parallel monolingual treebank consisting of over 2 million tokens and covering various text genres, including both parallel and comparable text. This publicly available corpus is richly annotated with alignments between syntactic nodes, which are also classified using five different semantic similarity relations. A quarter of the corpus is manually annotated, and this informs the development of an automatic tree aligner used to annotate the remainder of the corpus. We argue that this corpus is the first of this size and kind, and offers great potential for paraphrasing research.","Language Resources and Evaluation",2014,"Yes"," monolingual treebank semantic similarity corpus alignment semantic relations parallel text comparable text tree alignment paraphrase construction aligned monolingual treebank studying semantic similarity modern paraphrase research benefit large corpora detailed annotations corpora thin ground paper describe development corpus dutch takes form parallel monolingual treebank consisting million tokens covering text genres including parallel comparable text publicly corpus richly annotated alignments syntactic nodes classified semantic similarity relations quarter corpus manually annotated informs development automatic tree aligner annotate remainder corpus argue corpus size kind offers great potential paraphrasing research",1
"KeywordsVerb subcategorization Hebrew Lexicography ","a hebrew verbcomplement dictionary","We present a verb–complement dictionary of Modern Hebrew, automatically extracted from text corpora. Carefully examining a large set of examples, we defined ten types of verb complements that cover the vast majority of the occurrences of verb complements in the corpora. We explored several collocation measures as indicators of the strength of the association between the verb and its complement. We then used these measures to automatically extract verb complements from corpora. The result is a wide-coverage, accurate dictionary that lists not only the likely complements for each verb, but also the likelihood of each complement. We evaluated the quality of the extracted dictionary both intrinsically and extrinsically. Intrinsically, we showed high precision and recall on randomly (but systematically) selected verbs. Extrinsically, we showed that using the extracted information is beneficial for two applications, prepositional phrase attachment disambiguation and Arabic-to-Hebrew machine translation.","Language Resources and Evaluation",2014,"Yes"," verb subcategorization hebrew lexicography hebrew verbcomplement dictionary present verb complement dictionary modern hebrew automatically extracted text corpora carefully examining large set examples defined ten types verb complements cover vast majority occurrences verb complements corpora explored collocation measures indicators strength association verb complement measures automatically extract verb complements corpora result wide coverage accurate dictionary lists complements verb likelihood complement evaluated quality extracted dictionary intrinsically extrinsically intrinsically showed high precision recall randomly systematically selected verbs extrinsically showed extracted information beneficial applications prepositional phrase attachment disambiguation arabic hebrew machine translation",1
"KeywordsNamed entity recognition Annotation Classifier ensembles Subtype classification ","fine grained dutch named entity recognition","This paper describes the creation of a fine-grained named entity annotation scheme and corpus for Dutch, and experiments on automatic main type and subtype named entity recognition. We give an overview of existing named entity annotation schemes, and motivate our own, which describes six main types (persons, organizations, locations, products, events and miscellaneous named entities) and finer-grained information on subtypes and metonymic usage. This was applied to a one-million-word subset of the Dutch SoNaR reference corpus. The classifier for main type named entities achieves a micro-averaged F-score of 84.91 %, and is publicly available, along with the corpus and annotations.","Language Resources and Evaluation",2014,"Yes"," named entity recognition annotation classifier ensembles subtype classification fine grained dutch named entity recognition paper describes creation fine grained named entity annotation scheme corpus dutch experiments automatic main type subtype named entity recognition give overview existing named entity annotation schemes motivate describes main types persons organizations locations products events miscellaneous named entities finer grained information subtypes metonymic usage applied million word subset dutch sonar reference corpus classifier main type named entities achieves micro averaged score publicly corpus annotations",1
"KeywordsAnnotation Corpus annotation Machine assistance Syriac studies Bayesian data analysis User study Language resource evaluation ","evaluating machine assisted annotation in under resourced settings","Machine assistance is vital to managing the cost of corpus annotation projects. Identifying effective forms of machine assistance through principled evaluation is particularly important and challenging in under-resourced domains and highly heterogeneous corpora, as the quality of machine assistance varies. We perform a fine-grained evaluation of two machine-assistance techniques in the context of an under-resourced corpus annotation project. This evaluation requires a carefully controlled user study crafted to test a number of specific hypotheses. We show that human annotators performing morphological analysis of text in a Semitic language perform their task significantly more accurately and quickly when even mediocre pre-annotations are provided. When pre-annotations are at least 70 % accurate, annotator speed and accuracy show statistically significant relative improvements of 25–35 and 5–7 %, respectively. However, controlled user studies are too costly to be suitable for under-resourced corpus annotation projects. Thus, we also present an alternative analysis methodology that models the data as a combination of latent variables in a Bayesian framework. We show that modeling the effects of interesting confounding factors can generate useful insights. In particular, correction propagation appears to be most effective for our task when implemented with minimal user involvement. More importantly, by explicitly accounting for confounding variables, this approach has the potential to yield fine-grained evaluations using data collected in a natural environment outside of costly controlled user studies.","Language Resources and Evaluation",2014,"No"," annotation corpus annotation machine assistance syriac studies bayesian data analysis user study language resource evaluation evaluating machine assisted annotation resourced settings machine assistance vital managing cost corpus annotation projects identifying effective forms machine assistance principled evaluation important challenging resourced domains highly heterogeneous corpora quality machine assistance varies perform fine grained evaluation machine assistance techniques context resourced corpus annotation project evaluation requires carefully controlled user study crafted test number specific hypotheses show human annotators performing morphological analysis text semitic language perform task significantly accurately quickly mediocre pre annotations provided pre annotations accurate annotator speed accuracy show statistically significant relative improvements controlled user studies costly suitable resourced corpus annotation projects present alternative analysis methodology models data combination latent variables bayesian framework show modeling effects interesting confounding factors generate insights correction propagation appears effective task implemented minimal user involvement importantly explicitly accounting confounding variables approach potential yield fine grained evaluations data collected natural environment costly controlled user studies",0
"KeywordsText simplification Aligned monolingual corpora Simplification operations ","text simplification resources for spanish","In this paper we present the development of a text simplification system for Spanish. Text simplification is the adaptation of a text for the special needs of certain groups of readers, such as language learners, people with cognitive difficulties, and elderly people, among others. There is a clear need for simplified texts, but manual production and adaptation of existing text is labour-intensive and costly. Automatic simplification is a field which attracts growing attention in Natural Language Processing, but, to the best of our knowledge, there are no existing simplification tools for Spanish. We present a corpus study which aims to identify the operations a text simplification system needs to carry out in order to produce an output similar to what human editors produce when they simplify news texts. We also present a first prototype for automatic simplification, which shows that the most important simplification operations can be successfully treated.","Language Resources and Evaluation",2014,"No"," text simplification aligned monolingual corpora simplification operations text simplification resources spanish paper present development text simplification system spanish text simplification adaptation text special groups readers language learners people cognitive difficulties elderly people clear simplified texts manual production adaptation existing text labour intensive costly automatic simplification field attracts growing attention natural language processing knowledge existing simplification tools spanish present corpus study aims identify operations text simplification system carry order produce output similar human editors produce simplify news texts present prototype automatic simplification shows important simplification operations successfully treated",0
"KeywordsCross-lingual word sense disambiguation Word sense induction Sense clustering Parallel corpora WordNet ","data driven synset induction and disambiguation for wordnet development","Automatic methods for wordnet development in languages other than English generally exploit information found in Princeton WordNet (PWN) and translations extracted from parallel corpora. A common approach consists in preserving the structure of PWN and transferring its content in new languages using alignments, possibly combined with information extracted from multilingual semantic resources. Even if the role of PWN remains central in this process, these automatic methods offer an alternative to the manual elaboration of new wordnets. However, their limited coverage has a strong impact on that of the resulting resources. Following this line of research, we apply a cross-lingual word sense disambiguation method to wordnet development. Our approach exploits the output of a data-driven sense induction method that generates sense clusters in new languages, similar to wordnet synsets, by identifying word senses and relations in parallel corpora. We apply our cross-lingual word sense disambiguation method to the task of enriching a French wordnet resource, the WOLF, and show how it can be efficiently used for increasing its coverage. Although our experiments involve the English–French language pair, the proposed methodology is general enough to be applied to the development of wordnet resources in other languages for which parallel corpora are available. Finally, we show how the disambiguation output can serve to reduce the granularity of new wordnets and the degree of polysemy present in PWN.","Language Resources and Evaluation",2014,"Yes"," cross lingual word sense disambiguation word sense induction sense clustering parallel corpora wordnet data driven synset induction disambiguation wordnet development automatic methods wordnet development languages english generally exploit information found princeton wordnet pwn translations extracted parallel corpora common approach consists preserving structure pwn transferring content languages alignments possibly combined information extracted multilingual semantic resources role pwn remains central process automatic methods offer alternative manual elaboration wordnets limited coverage strong impact resulting resources line research apply cross lingual word sense disambiguation method wordnet development approach exploits output data driven sense induction method generates sense clusters languages similar wordnet synsets identifying word senses relations parallel corpora apply cross lingual word sense disambiguation method task enriching french wordnet resource wolf show efficiently increasing coverage experiments involve english french language pair proposed methodology general applied development wordnet resources languages parallel corpora finally show disambiguation output serve reduce granularity wordnets degree polysemy present pwn",1
"KeywordsForeign Language Natural Language Processing Language Learner Language Resource Parallel Corpus ","introduction to the special issue on resources and tools for language learners","This special issue of Language Resources and Evaluation is devoted to Resources and Tools for Language Learners.","Language Resources and Evaluation",2014,"No"," foreign language natural language processing language learner language resource parallel corpus introduction special issue resources tools language learners special issue language resources evaluation devoted resources tools language learners",0
"KeywordsDialogue act Language model Sentence structure Speech act Speech recognition Syntax ","automatic dialogue act recognition with syntactic features","This work studies the usefulness of syntactic information in the context of automatic dialogue act recognition in Czech. Several pieces of evidence are presented in this work that support our claim that syntax might bring valuable information for dialogue act recognition. In particular, a parallel is drawn with the related domain of automatic punctuation generation and a set of syntactic features derived from a deep parse tree is further proposed and successfully used in a Czech dialogue act recognition system based on conditional random fields. We finally discuss the possible reasons why so few works have exploited this type of information before and propose future research directions to further progress in this area.","Language Resources and Evaluation",2014,"No"," dialogue act language model sentence structure speech act speech recognition syntax automatic dialogue act recognition syntactic features work studies usefulness syntactic information context automatic dialogue act recognition czech pieces evidence presented work support claim syntax bring valuable information dialogue act recognition parallel drawn related domain automatic punctuation generation set syntactic features derived deep parse tree proposed successfully czech dialogue act recognition system based conditional random fields finally discuss reasons works exploited type information propose future research directions progress area",0
"KeywordsText data mining Language modeling Topic identification Duplicity detection ","general framework for mining processing and storing large amounts of electronic texts for language modeling purposes","The paper describes a general framework for mining large amounts of text data from a defined set of Web pages. The acquired data are meant to constitute a corpus for training robust and reliable language models and thus the framework needs to also incorporate algorithms for appropriate text processing and duplicity detection in order to secure quality and consistency of the data. As we expect the resulting corpus to be very large, we have also implemented topic detection algorithms that allow us to automatically select subcorpora for domain-specific language models. The description of the framework architecture and the implemented algorithms is complemented with a detailed evaluation section. It analyses the basic properties of the gathered Czech corpus containing more than one billion text tokens collected using the described framework, shows the results of the topic detection methods and finally also describes the design and outcomes of the automatic speech recognition experiments with domain-specific language models estimated from the collected data.","Language Resources and Evaluation",2014,"No"," text data mining language modeling topic identification duplicity detection general framework mining processing storing large amounts electronic texts language modeling purposes paper describes general framework mining large amounts text data defined set web pages acquired data meant constitute corpus training robust reliable language models framework incorporate algorithms text processing duplicity detection order secure quality consistency data expect resulting corpus large implemented topic detection algorithms automatically select subcorpora domain specific language models description framework architecture implemented algorithms complemented detailed evaluation section analyses basic properties gathered czech corpus billion text tokens collected framework shows results topic detection methods finally describes design outcomes automatic speech recognition experiments domain specific language models estimated collected data",0
"KeywordsDigital Humanities Web-based linguistic tools Language data management IGT Shared linguistic methodologies Single system usage evaluation ","typecraft collaborative databasing and resource sharing for linguists","Interlinear Glossed Text (IGT) is a well established data format within philology and the structural and generative fields of linguistics. The best known format for an IGT is the one found in linguistic publications, where one line of text is followed by one line of glosses and one line of free translation. Although used in different functions, IGTs are ubiquitous in linguistic research and publications. Yet they also have been criticised for being fabricated and unreliable in some of their uses. However that might be, IGTs represent linguistic knowledge, and in particular for less-resourced languages, they are not rarely the only structured data available. Under the auspices of the Digital Humanities, linguists increasingly focus on the advantages of Semantic Web technologies. Presenting the modules and procedures of the web-based linguistic application TypeCraft (TC), we outline how the creation of IGTs can become an integral part of a shared linguistic methodology. Linguistic services have the potential of allowing efficient data management, and their strength lies in facilitating new forms of collaboration beyond social networking. They pave the way towards what one might call shared methodologies. In this paper we would like to discuss the linguistic value of web-based technology. By presenting the functionalities of TC and giving a detailed summary of online linguistic data creation and retrieval, we will present external and internal criteria for a single system evaluation of TC centred on usage objectives.","Language Resources and Evaluation",2014,"No"," digital humanities web based linguistic tools language data management igt shared linguistic methodologies single system usage evaluation typecraft collaborative databasing resource sharing linguists interlinear glossed text igt established data format philology structural generative fields linguistics format igt found linguistic publications line text line glosses line free translation functions igts ubiquitous linguistic research publications criticised fabricated unreliable igts represent linguistic knowledge resourced languages rarely structured data auspices digital humanities linguists increasingly focus advantages semantic web technologies presenting modules procedures web based linguistic application typecraft tc outline creation igts integral part shared linguistic methodology linguistic services potential allowing efficient data management strength lies facilitating forms collaboration social networking pave call shared methodologies paper discuss linguistic web based technology presenting functionalities tc giving detailed summary online linguistic data creation retrieval present external internal criteria single system evaluation tc centred usage objectives",0
"KeywordsMultilingualism Translation divergence Syntactic projection ","capturing divergence in dependency trees to improve syntactic projection","Obtaining syntactic parses is an important step in many NLP pipelines. However, most of the world’s languages do not have a large amount of syntactically annotated data available for building parsers. Syntactic projection techniques attempt to address this issue by using parallel corpora consisting of resource-poor and resource-rich language pairs, taking advantage of a parser for the resource-rich language and word alignment between the languages to project the parses onto the data for the resource-poor language. These projection methods can suffer, however, when syntactic structures for some sentence pairs in the two languages look quite different. In this paper, we investigate the use of small, parallel, annotated corpora to automatically detect divergent structural patterns between two languages. We then use these detected patterns to improve projection algorithms and dependency parsers, allowing for better performing NLP tools for resource-poor languages, particularly those that may not have large amounts of annotated data necessary for traditional, fully-supervised methods. While this detection process is not exhaustive, we demonstrate that common patterns of divergence can be identified automatically without prior knowledge of a given language pair, and the patterns can be used to improve performance of syntactic projection and parsing.","Language Resources and Evaluation",2014,"No"," multilingualism translation divergence syntactic projection capturing divergence dependency trees improve syntactic projection obtaining syntactic parses important step nlp pipelines world languages large amount syntactically annotated data building parsers syntactic projection techniques attempt address issue parallel corpora consisting resource poor resource rich language pairs taking advantage parser resource rich language word alignment languages project parses data resource poor language projection methods suffer syntactic structures sentence pairs languages paper investigate small parallel annotated corpora automatically detect divergent structural patterns languages detected patterns improve projection algorithms dependency parsers allowing performing nlp tools resource poor languages large amounts annotated data traditional fully supervised methods detection process exhaustive demonstrate common patterns divergence identified automatically prior knowledge language pair patterns improve performance syntactic projection parsing",0
"KeywordsTurkish Natural language processing Language resources ","turkish and its challenges for language processing","We present a short survey and exposition of some of the important aspects of Turkish that have proven challenging for natural language processing. Most of the challenges stem from the complex morphology of Turkish and how morphology interacts with syntax. We also provide a short overview of the major tools and resources developed for Turkish natural language processing over the last two decades.","Language Resources and Evaluation",2014,"No"," turkish natural language processing language resources turkish challenges language processing present short survey exposition important aspects turkish proven challenging natural language processing challenges stem complex morphology turkish morphology interacts syntax provide short overview major tools resources developed turkish natural language processing decades",0
"KeywordsDependency structure Syntactico–semantic relation  Paninian karak Modern Bangla grammar and language ","a dependency annotation scheme for bangla treebank","Dependency grammar is considered appropriate for many Indian languages. In this paper, we present a study of the dependency relations in Bangla language. We have categorized these relations in three different levels, namely intrachunk relations, interchunk relations and interclause relations. Each of these levels is further categorized and an annotation scheme has been developed. Both syntactic and semantic features have been taken into consideration for describing the relations. In our scheme, there are 63 such syntactico–semantic relations. We have verified the scheme by tagging a corpus of 4167 Bangla sentences to create a treebank (KGPBenTreebank).","Language Resources and Evaluation",2014,"Yes"," dependency structure syntactico semantic relation paninian karak modern bangla grammar language dependency annotation scheme bangla treebank dependency grammar considered indian languages paper present study dependency relations bangla language categorized relations levels intrachunk relations interchunk relations interclause relations levels categorized annotation scheme developed syntactic semantic features consideration describing relations scheme syntactico semantic relations verified scheme tagging corpus bangla sentences create treebank kgpbentreebank",1
"KeywordsLinguistic annotation Standards Language resources Interoperability ","the linguistic annotation framework a standard for annotation interchange and merging","This paper overviews the International Standards Organization–Linguistic Annotation Framework (ISO–LAF) developed in ISO TC37 SC4. We describe the XML serialization of ISO–LAF, the Graph Annotation Format (GrAF) and discuss the rationale behind the various decisions that were made in determining the standard. We describe the structure of the GrAF headers in detail and provide multiple examples of GrAF representation for text and multi-media. Finally, we discuss the next steps for standardization of interchange formats for linguistic annotations.","Language Resources and Evaluation",2014,"No"," linguistic annotation standards language resources interoperability linguistic annotation framework standard annotation interchange merging paper overviews international standards organization linguistic annotation framework iso laf developed iso tc sc describe xml serialization iso laf graph annotation format graf discuss rationale decisions made determining standard describe structure graf headers detail provide multiple examples graf representation text multi media finally discuss steps standardization interchange formats linguistic annotations",0
"KeywordsTreebank Finnish Parsing Morphology ","building the essential resources for finnish the turku dependency treebank","In this paper, we present the final version of a publicly available treebank of Finnish, the Turku Dependency Treebank. The treebank contains 204,399 tokens (15,126 sentences) from 10 different text sources and has been manually annotated in a Finnish-specific version of the well-known Stanford Dependency scheme. The morphological analyses of the treebank have been assigned using a novel machine learning method to disambiguate readings given by an existing tool. As the second main contribution, we present the first open source Finnish dependency parser, trained on the newly introduced treebank. The parser achieves a labeled attachment score of 81 %. The treebank data as well as the parsing pipeline are available under an open license at http://bionlp.utu.fi/.","Language Resources and Evaluation",2014,"Yes"," treebank finnish parsing morphology building essential resources finnish turku dependency treebank paper present final version publicly treebank finnish turku dependency treebank treebank tokens sentences text sources manually annotated finnish specific version stanford dependency scheme morphological analyses treebank assigned machine learning method disambiguate readings existing tool main contribution present open source finnish dependency parser trained newly introduced treebank parser achieves labeled attachment score treebank data parsing pipeline open license httpbionlputufi",1
"KeywordsMachine translation Human evaluation Error analysis ","involving language professionals in the evaluation of machine translation","
Significant breakthroughs in machine translation (MT) only seem possible if human translators are taken into the loop. While automatic evaluation and scoring mechanisms such as BLEU have enabled the fast development of systems, it is not clear how systems can meet real-world (quality) requirements in industrial translation scenarios today. The taraXŰ project has paved the way for wide usage of multiple MT outputs through various feedback loops in system development. The project has integrated human translators into the development process thus collecting feedback for possible improvements. This paper describes results from detailed human evaluation. Performance of different types of translation systems has been compared and analysed via ranking, error analysis and post-editing.","Language Resources and Evaluation",2014,"No"," machine translation human evaluation error analysis involving language professionals evaluation machine translation significant breakthroughs machine translation mt human translators loop automatic evaluation scoring mechanisms bleu enabled fast development systems clear systems meet real world quality requirements industrial translation scenarios today tarax project paved wide usage multiple mt outputs feedback loops system development project integrated human translators development process collecting feedback improvements paper describes results detailed human evaluation performance types translation systems compared analysed ranking error analysis post editing",0
NA,"i mani and j pustejovsky interpreting motion grounded representations for spatial language","The book under review is the fifth volume of the series Exploration in Language and Space of OUP. It is organized into six chapters: the odd-numbered chapters and part of Chapter 6 are written by Mani, whereas the even-numbered chapters by Pustejovsky.","Language Resources and Evaluation",2014,"No"," mani pustejovsky interpreting motion grounded representations spatial language book review volume series exploration language space oup organized chapters odd numbered chapters part chapter written mani numbered chapters pustejovsky",0
"KeywordsInformation extraction Lexical ontology Wordnet Clustering Semantic relations ","eco and ontopt a flexible approach for creating a portuguese wordnet automatically","A wordnet is an important tool for developing natural language processing applications for a language. However, most wordnets are handcrafted by experts, which limits their growth. In this article, we propose an automatic approach to create wordnets by exploiting textual resources, dubbed ECO. After extracting semantic relation instances, identified by discriminating textual patterns, ECO discovers synonymy clusters, used as synsets, and attaches the remaining relations to suitable synsets. Besides introducing each step of ECO, we report on how it was implemented to create Onto.PT, a public lexical ontology for Portuguese. Onto.PT is the result of the automatic exploitation of Portuguese dictionaries and thesauri, and it aims to minimise the main limitations of existing Portuguese lexical knowledge bases.","Language Resources and Evaluation",2014,"Yes"," information extraction lexical ontology wordnet clustering semantic relations eco ontopt flexible approach creating portuguese wordnet automatically wordnet important tool developing natural language processing applications language wordnets handcrafted experts limits growth article propose automatic approach create wordnets exploiting textual resources dubbed eco extracting semantic relation instances identified discriminating textual patterns eco discovers synonymy clusters synsets attaches remaining relations suitable synsets introducing step eco report implemented create pt public lexical ontology portuguese pt result automatic exploitation portuguese dictionaries thesauri aims minimise main limitations existing portuguese lexical knowledge bases",1
"KeywordsDependency treebank Annotation scheme Harmonization ","hamledt harmonized multi language dependency treebank","We present HamleDT—a HArmonized Multi-LanguagE Dependency Treebank. HamleDT is a compilation 
of existing dependency treebanks (or dependency conversions of other treebanks), transformed so that they all conform to the same annotation style. In the present article, we provide a thorough investigation and discussion of a number of phenomena that are comparable across languages, though their annotation in treebanks often differs. We claim that transformation procedures can be designed to automatically identify most such phenomena and convert them to a unified annotation style. This unification is beneficial both to comparative corpus linguistics and to machine learning of syntactic parsing.","Language Resources and Evaluation",2014,"Yes"," dependency treebank annotation scheme harmonization hamledt harmonized multi language dependency treebank present hamledt harmonized multi language dependency treebank hamledt compilation existing dependency treebanks dependency conversions treebanks transformed conform annotation style present article provide investigation discussion number phenomena comparable languages annotation treebanks differs claim transformation procedures designed automatically identify phenomena convert unified annotation style unification beneficial comparative corpus linguistics machine learning syntactic parsing",1
"KeywordsHLT programme STEVIN HLT policy design Dutch ","joint research coordination and programming for hlt for dutch in the low countries","Since 1999, the Dutch Language Union (NTU) fosters the exchange of plans and policy initiatives amongst government officials of Flanders and the Netherlands on human language technology for Dutch (HLTD). One of the outcomes is the STEVIN R&D programme for HLTD, coordinated by the NTU and funded by the Flemish and Dutch governments. STEVIN is an example of successful joint research programming. Its set-up, highlights and scientific results are presented as well as an outlook to future initiatives.","Language Resources and Evaluation",2013,"No"," hlt programme stevin hlt policy design dutch joint research coordination programming hlt dutch low countries dutch language union ntu fosters exchange plans policy initiatives government officials flanders netherlands human language technology dutch hltd outcomes stevin programme hltd coordinated ntu funded flemish dutch governments stevin successful joint research programming set highlights scientific results presented outlook future initiatives",0
"KeywordsGradable adjectives Scales Intensity relation WordNet ","large huge or gigantic identifying and encoding intensity relations among adjectives in wordnet","We propose a new semantic relation for gradable adjectives in WordNet, which enriches the present, vague, similar relation with information on the degree or intensity with which different adjectives express a shared attribute. Using lexical-semantic patterns, we mine the Web for evidence of the relative strength of adjectives like “large”, “huge” and “gigantic” with respect to their attribute (“size”). The pairwise orderings we derive allow us to construct scales on which the adjectives are located. To represent the intensity relation among gradable adjectives in WordNet, we combine ordered scales with the current WordNet dumbbells based on the relation between a pair of central adjectives and a group of undifferentiated semantically similar adjectives. A new intensity relation links the adjectives in the dumbbells and their concurrent representation on scales. Besides capturing the semantics of gradable adjectives in a way that is both intuitively clear as well as consistent with corpus data, the introduction of an intensity relation would potentially result in several specific benefits for NLP.","Language Resources and Evaluation",2013,"No"," gradable adjectives scales intensity relation wordnet large huge gigantic identifying encoding intensity relations adjectives wordnet propose semantic relation gradable adjectives wordnet enriches present vague similar relation information degree intensity adjectives express shared attribute lexical semantic patterns mine web evidence relative strength adjectives large huge gigantic respect attribute size pairwise orderings derive construct scales adjectives located represent intensity relation gradable adjectives wordnet combine ordered scales current wordnet dumbbells based relation pair central adjectives group undifferentiated semantically similar adjectives intensity relation links adjectives dumbbells concurrent representation scales capturing semantics gradable adjectives intuitively clear consistent corpus data introduction intensity relation potentially result specific benefits nlp",0
"KeywordsText categorization Ontologies Thesauri Unlabeled short texts ","classifying unlabeled short texts using a fuzzy declarative approach","Web 2.0 provides user-friendly tools that allow persons to create and publish content online. User generated content often takes the form of short texts (e.g., blog posts, news feeds, snippets, etc). This has motivated an increasing interest on the analysis of short texts and, specifically, on their categorisation. Text categorisation is the task of classifying documents into a certain number of predefined categories. Traditional text classification techniques are mainly based on word frequency statistical analysis and have been proved inadequate for the classification of short texts where word occurrence is too small. On the other hand, the classic approach to text categorization is based on a learning process that requires a large number of labeled training texts to achieve an accurate performance. However labeled documents might not be available, when unlabeled documents can be easily collected. This paper presents an approach to text categorisation which does not need a pre-classified set of training documents. The proposed method only requires the category names as user input. Each one of these categories is defined by means of an ontology of terms modelled by a set of what we call proximity equations. Hence, our method is not category occurrence frequency based, but highly depends on the definition of that category and how the text fits that definition. Therefore, the proposed approach is an appropriate method for short text classification where the frequency of occurrence of a category is very small or even zero. Another feature of our method is that the classification process is based on the ability of an extension of the standard Prolog language, named  Bousi~Prolog , for flexible matching and knowledge representation. This declarative approach provides a text classifier which is quick and easy to build, and a classification process which is easy for the user to understand. The results of experiments showed that the proposed method achieved a reasonably useful performance.","Language Resources and Evaluation",2013,"No"," text categorization ontologies thesauri unlabeled short texts classifying unlabeled short texts fuzzy declarative approach web user friendly tools persons create publish content online user generated content takes form short texts blog posts news feeds snippets motivated increasing interest analysis short texts specifically categorisation text categorisation task classifying documents number predefined categories traditional text classification techniques based word frequency statistical analysis proved inadequate classification short texts word occurrence small hand classic approach text categorization based learning process requires large number labeled training texts achieve accurate performance labeled documents unlabeled documents easily collected paper presents approach text categorisation pre classified set training documents proposed method requires category names user input categories defined means ontology terms modelled set call proximity equations method category occurrence frequency based highly depends definition category text fits definition proposed approach method short text classification frequency occurrence category small feature method classification process based ability extension standard prolog language named bousiprolog flexible matching knowledge representation declarative approach text classifier quick easy build classification process easy user understand results experiments showed proposed method achieved performance",0
"KeywordsSearch engines Web-as-corpus Basque NLP Morphological query expansion Language-filtering words ","morphological query expansion and language filtering words for improving basque web retrieval","The experience of a user of major search engines or other web information retrieval services looking for information in the Basque language is far from satisfactory: they only return pages with exact matches but no inflections (necessary for an agglutinative language like Basque), many results in other languages (no search engine gives the option to restrict its results to Basque), etc. This paper proposes using morphological query expansion and language-filtering words in combination with the APIs of search engines as a very cost-effective solution to build appropriate web search services for Basque. The implementation details of the methodology (choosing the most appropriate language-filtering words, the number of them, the most frequent inflections for the morphological query expansion, etc.) have been specified by corpora-based studies. The improvements produced have been measured in terms of precision and recall both over corpora and real web searches. Morphological query expansion can improve recall up to 47 % and language-filtering words can raise precision from 15 % to around 90 %, although with a loss in recall of about 30–35 %. The proposed methodology has already been successfully used in the Basque search service Elebila (http://www.elebila.eu) and the web-as-corpus tool CorpEus (http://www.corpeus.org), and the approach could be applied to other morphologically rich or under-resourced languages as well.","Language Resources and Evaluation",2013,"No"," search engines web corpus basque nlp morphological query expansion language filtering words morphological query expansion language filtering words improving basque web retrieval experience user major search engines web information retrieval services information basque language satisfactory return pages exact matches inflections agglutinative language basque results languages search engine option restrict results basque paper proposes morphological query expansion language filtering words combination apis search engines cost effective solution build web search services basque implementation details methodology choosing language filtering words number frequent inflections morphological query expansion corpora based studies improvements produced measured terms precision recall corpora real web searches morphological query expansion improve recall language filtering words raise precision loss recall proposed methodology successfully basque search service elebila httpwwwelebilaeu web corpus tool corpeus httpwwwcorpeusorg approach applied morphologically rich resourced languages ",0
"KeywordsWord Sense Induction Word Sense Disambiguation Lexical Semantics ","evaluating word sense induction and disambiguation methods","Word Sense Induction (WSI) is the task of identifying the different uses (senses) of a target word in a given text in an unsupervised manner, i.e. without relying on any external resources such as dictionaries or sense-tagged data. This paper presents a thorough description of the SemEval-2010 WSI task and a new evaluation setting for sense induction methods. Our contributions are two-fold: firstly, we provide a detailed analysis of the Semeval-2010 WSI task evaluation results and identify the shortcomings of current evaluation measures. Secondly, we present a new evaluation setting by assessing participating systems’ performance according to the skewness of target words’ distribution of senses showing that there are methods able to perform well above the Most Frequent Sense (MFS) baseline in highly skewed distributions.","Language Resources and Evaluation",2013,"No"," word sense induction word sense disambiguation lexical semantics evaluating word sense induction disambiguation methods word sense induction wsi task identifying senses target word text unsupervised manner relying external resources dictionaries sense tagged data paper presents description semeval wsi task evaluation setting sense induction methods contributions fold firstly provide detailed analysis semeval wsi task evaluation results identify shortcomings current evaluation measures present evaluation setting assessing participating systems performance skewness target words distribution senses showing methods perform frequent sense mfs baseline highly skewed distributions",0
"KeywordsMetadata Infrastructure CLARIN ","creating testing clarin metadata components","The CLARIN Metadata Infrastructure (CMDI) that is being developed in Common Language Resources and Technology Infrastructure (CLARIN) is a computer-supported framework that combines a flexible component approach with the explicit declaration of semantics. The goal of the Dutch CLARIN project “Creating & Testing CLARIN Metadata Components” was to create metadata components and profiles for a wide variety of existing resources housed at two data centres according to the CMDI specifications. In doing so the principles of the framework were tested. The results of the project are of benefit to other CLARIN-projects that are expected to adhere to the CMDI framework and its accompanying tools.","Language Resources and Evaluation",2013,"No"," metadata infrastructure clarin creating testing clarin metadata components clarin metadata infrastructure cmdi developed common language resources technology infrastructure clarin computer supported framework combines flexible component approach explicit declaration semantics goal dutch clarin project creating testing clarin metadata components create metadata components profiles wide variety existing resources housed data centres cmdi specifications principles framework tested results project benefit clarin projects expected adhere cmdi framework accompanying tools",0
"KeywordsCollaborative annotation Arabic Treebank Quran Corpus ","supervised collaboration for syntactic annotation of quranic arabic","The Quranic Arabic Corpus (http://corpus.quran.com) is a collaboratively constructed linguistic resource initiated at the University of Leeds, with multiple layers of annotation including part-of-speech tagging, morphological segmentation (Dukes and Habash 2010) and syntactic analysis using dependency grammar (Dukes and Buckwalter 2010). The motivation behind this work is to produce a resource that enables further analysis of the Quran, the 1,400 year-old central religious text of Islam. This project contrasts with other Arabic treebanks by providing a deep linguistic model based on the historical traditional grammar known as i′rāb (إعراب). By adapting this well-known canon of Quranic grammar into a familiar tagset, it is possible to encourage online annotation by Arabic linguists and Quranic experts. This article presents a new approach to linguistic annotation of an Arabic corpus: online supervised collaboration using a multi-stage approach. The different stages include automatic rule-based tagging, initial manual verification, and online supervised collaborative proofreading. A popular website attracting thousands of visitors per day, the Quranic Arabic Corpus has approximately 100 unpaid volunteer annotators each suggesting corrections to existing linguistic tagging. To ensure a high-quality resource, a small number of expert annotators are promoted to a supervisory role, allowing them to review or veto suggestions made by other collaborators. The Quran also benefits from a large body of existing historical grammatical analysis, which may be leveraged during this review. In this paper we evaluate and report on the effectiveness of the chosen annotation methodology. We also discuss the unique challenges of annotating Quranic Arabic online and describe the custom linguistic software used to aid collaborative annotation.","Language Resources and Evaluation",2013,"Yes"," collaborative annotation arabic treebank quran corpus supervised collaboration syntactic annotation quranic arabic quranic arabic corpus httpcorpusquran collaboratively constructed linguistic resource initiated university leeds multiple layers annotation including part speech tagging morphological segmentation dukes habash syntactic analysis dependency grammar dukes buckwalter motivation work produce resource enables analysis quran year central religious text islam project contrasts arabic treebanks providing deep linguistic model based historical traditional grammar adapting canon quranic grammar familiar tagset encourage online annotation arabic linguists quranic experts article presents approach linguistic annotation arabic corpus online supervised collaboration multi stage approach stages include automatic rule based tagging initial manual verification online supervised collaborative proofreading popular website attracting thousands visitors day quranic arabic corpus approximately unpaid volunteer annotators suggesting corrections existing linguistic tagging ensure high quality resource small number expert annotators promoted supervisory role allowing review veto suggestions made collaborators quran benefits large body existing historical grammatical analysis leveraged review paper evaluate report effectiveness chosen annotation methodology discuss unique challenges annotating quranic arabic online describe custom linguistic software aid collaborative annotation",1
"KeywordsEvocation Free association WordNet relations ","evocation analyzing and propagating a semantic link based on free word association","Studies of lexical–semantic relations aim to understand the mechanism of semantic memory and the organization of the mental lexicon. However, standard paradigmatic relations such as “hypernym” and “hyponym” cannot capture connections among concepts from different parts of speech. WordNet, which organizes synsets (i.e., synonym sets) using these lexical–semantic relations, is rather sparse in its connectivity. According to WordNet statistics, the average number of outgoing/incoming arcs for the hypernym/hyponym relation per synset is 1.33. Evocation, defined as how much a concept (expressed by one or more words) brings to mind another, is proposed as a new directed and weighted measure for the semantic relatedness among concepts. Commonly applied semantic relations and relatedness measures do not seem to be fully compatible with data that reflect evocations among concepts. They are compatible but evocation captures MORE. This work aims to provide a reliable and extendable dataset of concepts evoked by, and evoking, other concepts to enrich WordNet, the existing semantic network. We propose the use of disambiguated free word association data (first responses to verbal stimuli) to infer and collect evocation ratings. WordNet aims to represent the organization of mental lexicon, and free word association which has been used by psycholinguists to explore semantic organization can contribute to the understanding. This work was carried out in two phases. In the first phase, it was confirmed that existing free word association norms can be converted into evocation data computationally. In the second phase, a two-stage association-annotation procedure of collecting evocation data from human judgment was compared to the state-of-the-art method, showing that introducing free association can greatly improve the quality of the evocation data generated. Evocation can be incorporated into WordNet as directed links with scales, and benefits various natural language processing applications.","Language Resources and Evaluation",2013,"No"," evocation free association wordnet relations evocation analyzing propagating semantic link based free word association studies lexical semantic relations aim understand mechanism semantic memory organization mental lexicon standard paradigmatic relations hypernym hyponym capture connections concepts parts speech wordnet organizes synsets synonym sets lexical semantic relations sparse connectivity wordnet statistics average number outgoingincoming arcs hypernymhyponym relation synset evocation defined concept expressed words brings mind proposed directed weighted measure semantic relatedness concepts commonly applied semantic relations relatedness measures fully compatible data reflect evocations concepts compatible evocation captures work aims provide reliable extendable dataset concepts evoked evoking concepts enrich wordnet existing semantic network propose disambiguated free word association data responses verbal stimuli infer collect evocation ratings wordnet aims represent organization mental lexicon free word association psycholinguists explore semantic organization contribute understanding work carried phases phase confirmed existing free word association norms converted evocation data computationally phase stage association annotation procedure collecting evocation data human judgment compared state art method showing introducing free association greatly improve quality evocation data generated evocation incorporated wordnet directed links scales benefits natural language processing applications",0
"KeywordsTemporal information Temporal tagger Named entity recognition Named entity normalization TIMEX2 TIMEX3 ","multilingual and cross domain temporal tagging","Extraction and normalization of temporal expressions from documents are important steps towards deep text understanding and a prerequisite for many NLP tasks such as information extraction, question answering, and document summarization. There are different ways to express (the same) temporal information in documents. However, after identifying temporal expressions, they can be normalized according to some standard format. This allows the usage of temporal information in a term- and language-independent way. In this paper, we describe the challenges of temporal tagging in different domains, give an overview of existing annotated corpora, and survey existing approaches for temporal tagging. Finally, we present our publicly available temporal tagger HeidelTime, which is easily extensible to further languages due to its strict separation of source code and language resources like patterns and rules. We present a broad evaluation on multiple languages and domains on existing corpora as well as on a newly created corpus for a language/domain combination for which no annotated corpus has been available so far.","Language Resources and Evaluation",2013,"Yes"," temporal information temporal tagger named entity recognition named entity normalization timex timex multilingual cross domain temporal tagging extraction normalization temporal expressions documents important steps deep text understanding prerequisite nlp tasks information extraction question answering document summarization ways express temporal information documents identifying temporal expressions normalized standard format usage temporal information term language independent paper describe challenges temporal tagging domains give overview existing annotated corpora survey existing approaches temporal tagging finally present publicly temporal tagger heideltime easily extensible languages due strict separation source code language resources patterns rules present broad evaluation multiple languages domains existing corpora newly created corpus languagedomain combination annotated corpus ",1
"KeywordsSpoken language Discourse Recordings Transcription conventions Web concordancer ","compilation transcription and usage of a reference speech corpus the case of the slovene corpus gos","In recent years, building reference speech corpora was an important part of the activities which provided the necessary linguistic infrastructure in many European countries, for languages with many speakers (e.g., French, German, Spanish, Italian) as well as for those with smaller numbers of speakers (e.g., Swedish, Dutch, Czech, Slovak). This paper describes the process of the creation of a reference speech corpus and its distribution to potential users, as it was done in the case of the Slovene corpus GOS. The corpus structure and fieldwork experiences with recording, labelling system, and two levels of transcription (pronunciation-based and standardized) are described, as well as the main characteristics of the corpus interface (web concordancer) and the availability of the original corpus files.","Language Resources and Evaluation",2013,"Yes"," spoken language discourse recordings transcription conventions web concordancer compilation transcription usage reference speech corpus case slovene corpus gos recent years building reference speech corpora important part activities provided linguistic infrastructure european countries languages speakers french german spanish italian smaller numbers speakers swedish dutch czech slovak paper describes process creation reference speech corpus distribution potential users case slovene corpus gos corpus structure fieldwork experiences recording labelling system levels transcription pronunciation based standardized main characteristics corpus interface web concordancer availability original corpus files",1
"KeywordsSMS corpus Corpus creation English Chinese Crowdsourcing Mechanical turk Zhubajie ","creating a live public short message service corpus the nus sms corpus","Short Message Service (SMS) messages are short messages sent from one person to another from their mobile phones. They represent a means of personal communication that is an important communicative artifact in our current digital era. As most existing studies have used private access to SMS corpora, comparative studies using the same raw SMS data have not been possible up to now. We describe our efforts to collect a public SMS corpus to address this problem. We use a battery of methodologies to collect the corpus, paying particular attention to privacy issues to address contributors’ concerns. Our live project collects new SMS message submissions, checks their quality, and adds valid messages. We release the resultant corpus as XML and as SQL dumps, along with monthly corpus statistics. We opportunistically collect as much metadata about the messages and their senders as possible, so as to enable different types of analyses. To date, we have collected more than 71,000 messages, focusing on English and Mandarin Chinese.","Language Resources and Evaluation",2013,"Yes"," sms corpus corpus creation english chinese crowdsourcing mechanical turk zhubajie creating live public short message service corpus nus sms corpus short message service sms messages short messages person mobile phones represent means personal communication important communicative artifact current digital era existing studies private access sms corpora comparative studies raw sms data describe efforts collect public sms corpus address problem battery methodologies collect corpus paying attention privacy issues address contributors concerns live project collects sms message submissions checks quality adds valid messages release resultant corpus xml sql dumps monthly corpus statistics opportunistically collect metadata messages senders enable types analyses date collected messages focusing english mandarin chinese",1
"KeywordsDiachronic corpus Historical Spanish Linguistic annotation TEI ","an open diachronic corpus of historical spanish","The impact-es diachronic corpus of historical Spanish compiles over one hundred books—containing approximately 8 million words—in addition to a complementary lexicon which links more than 10,000 lemmas with attestations of the different variants found in the documents. This textual corpus and the accompanying lexicon have been released under an open license (Creative Commons by-nc-sa) in order to permit their intensive exploitation in linguistic research. Approximately 7 % of the words in the corpus (a selection aimed at enhancing the coverage of the most frequent word forms) have been annotated with their lemma, part of speech, and modern equivalent. This paper describes the annotation criteria followed and the standards, based on the Text Encoding Initiative recommendations, used to represent the texts in digital form.","Language Resources and Evaluation",2013,"Yes"," diachronic corpus historical spanish linguistic annotation tei open diachronic corpus historical spanish impact es diachronic corpus historical spanish compiles hundred books approximately million words addition complementary lexicon links lemmas attestations variants found documents textual corpus accompanying lexicon released open license creative commons nc sa order permit intensive exploitation linguistic research approximately words corpus selection aimed enhancing coverage frequent word forms annotated lemma part speech modern equivalent paper describes annotation criteria standards based text encoding initiative recommendations represent texts digital form",1
"KeywordsProsodic corpus Radio news corpus Dialogue corpus Spanish corpus Catalan corpus ","glissando a corpus for multidisciplinary prosodic studies in spanish and catalan","Literature review on prosody reveals the lack of corpora for prosodic studies in Catalan and Spanish. In this paper, we present a corpus intended to fill this gap. The corpus comprises two distinct data-sets, a news subcorpus and a dialogue subcorpus, the latter containing either conversational or task-oriented speech. More than 25 h were recorded by twenty eight speakers per language. Among these speakers, eight were professional (four radio news broadcasters and four advertising actors). The entire material presented here has been transcribed, aligned with the acoustic signal and prosodically annotated. Two major objectives have guided the design of this project: (i) to offer a wide coverage of representative real-life communicative situations which allow for the characterization of prosody in these two languages; and (ii) to conduct research studies which enable us to contrast the speakers different speaking styles and discursive practices. All material contained in the corpus is provided under a Creative Commons Attribution 3.0 Unported License.","Language Resources and Evaluation",2013,"Yes"," prosodic corpus radio news corpus dialogue corpus spanish corpus catalan corpus glissando corpus multidisciplinary prosodic studies spanish catalan literature review prosody reveals lack corpora prosodic studies catalan spanish paper present corpus intended fill gap corpus comprises distinct data sets news subcorpus dialogue subcorpus conversational task oriented speech recorded twenty speakers language speakers professional radio news broadcasters advertising actors entire material presented transcribed aligned acoustic signal prosodically annotated major objectives guided design project offer wide coverage representative real life communicative situations characterization prosody languages ii conduct research studies enable contrast speakers speaking styles discursive practices material contained corpus provided creative commons attribution unported license",1
"KeywordsCHILDES Hebrew Transcription of spoken language Morphological analysis Morphological disambiguation ","the hebrew childes corpus transcription and morphological analysis","We present a corpus of transcribed spoken Hebrew that reflects spoken interactions between children and adults. The corpus is an integral part of the CHILDES database, which distributes similar corpora for over 25 languages. We introduce a dedicated transcription scheme for the spoken Hebrew data that is sensitive to both the phonology and the standard orthography of the language. We also introduce a morphological analyzer that was specifically developed for this corpus. The analyzer adequately covers the entire corpus, producing detailed correct analyses for all tokens. Evaluation on a new corpus reveals high coverage as well. Finally, we describe a morphological disambiguation module that selects the correct analysis of each token in context. The result is a high-quality morphologically-annotated CHILDES corpus of Hebrew, along with a set of tools that can be applied to new corpora.","Language Resources and Evaluation",2013,"Yes"," childes hebrew transcription spoken language morphological analysis morphological disambiguation hebrew childes corpus transcription morphological analysis present corpus transcribed spoken hebrew reflects spoken interactions children adults corpus integral part childes database distributes similar corpora languages introduce dedicated transcription scheme spoken hebrew data sensitive phonology standard orthography language introduce morphological analyzer specifically developed corpus analyzer adequately covers entire corpus producing detailed correct analyses tokens evaluation corpus reveals high coverage finally describe morphological disambiguation module selects correct analysis token context result high quality morphologically annotated childes corpus hebrew set tools applied corpora",1
"KeywordsConceptual metaphor theory Corpus annotation Human experimentation ","conceptual metaphor theory meets the data a corpus based human annotation study","Metaphor makes our thoughts more vivid and fills our communication with richer imagery. Furthermore, according to the conceptual metaphor theory (CMT) of Lakoff and Johnson (Metaphors we live by. University of Chicago Press, Chicago, 1980), metaphor also plays an important structural role in the organization and processing of conceptual knowledge. According to this account, the phenomenon of metaphor is not restricted to similarity-based extensions of meanings of individual words, but instead involves activating fixed mappings that reconceptualize one whole area of experience in terms of another. CMT produced a significant resonance in the fields of philosophy, linguistics, cognitive science and artificial intelligence and still underlies a large proportion of modern research on metaphor. However, there has to date been no comprehensive corpus-based study of conceptual metaphor, which would provide an empirical basis for evaluating the CMT using real-world linguistic data. The annotation scheme and the empirical study we present in this paper is a step towards filling this gap. We test our annotation procedure in an experimental setting involving multiple annotators and estimate their agreement on the task. The goal of the study is to investigate (1) how intuitive the conceptual metaphor explanation of linguistic metaphors is for human annotators and whether it is possible to consistently annotate interconceptual mappings; (2) what are the main difficulties that the annotators experience during the annotation process; (3) whether one conceptual metaphor is sufficient to explain a linguistic metaphor or whether a chain of conceptual metaphors is needed. The resulting corpus annotated for conceptual mappings provides a new, valuable dataset for linguistic, computational and cognitive experiments on metaphor.","Language Resources and Evaluation",2013,"No"," conceptual metaphor theory corpus annotation human experimentation conceptual metaphor theory meets data corpus based human annotation study metaphor makes thoughts vivid fills communication richer imagery conceptual metaphor theory cmt lakoff johnson metaphors live university chicago press chicago metaphor plays important structural role organization processing conceptual knowledge account phenomenon metaphor restricted similarity based extensions meanings individual words involves activating fixed mappings reconceptualize area experience terms cmt produced significant resonance fields philosophy linguistics cognitive science artificial intelligence underlies large proportion modern research metaphor date comprehensive corpus based study conceptual metaphor provide empirical basis evaluating cmt real world linguistic data annotation scheme empirical study present paper step filling gap test annotation procedure experimental setting involving multiple annotators estimate agreement task goal study investigate intuitive conceptual metaphor explanation linguistic metaphors human annotators consistently annotate interconceptual mappings main difficulties annotators experience annotation process conceptual metaphor sufficient explain linguistic metaphor chain conceptual metaphors needed resulting corpus annotated conceptual mappings valuable dataset linguistic computational cognitive experiments metaphor",0
"KeywordsFrame-based terminology Knowledge visualization Multimodality Image-text interface Terminological resources ","a corpus based approach to the multimodal analysis of specialized knowledge","Modern communication environments have changed the cognitive patterns of individuals, who are now used to the interaction of information encoded in different semiotic modalities, especially visual and linguistic. Despite this, the main premise of Corpus Linguistics is still ruling: our perception of and experience with the world is conveyed in texts, which nowadays need to be studied from a multimodal perspective. Therefore, multimodal corpora are becoming extremely useful to extract specialized knowledge and explore the insights of specialized language and its relation to non-language-specific representations of knowledge. It is our assertion that the analysis of the image-text interface can help us understand the way visual and linguistic information converge in subject-field texts. In this article, we use Frame-based terminology to sketch a novel proposal to study images in a corpus rich in pictorial representations for their inclusion in a terminological resource on the environment. Our corpus-based approach provides the methodological underpinnings to create meaning within terminographic entries, thus facilitating specialized knowledge transfer and acquisition through images.","Language Resources and Evaluation",2013,"No"," frame based terminology knowledge visualization multimodality image text interface terminological resources corpus based approach multimodal analysis specialized knowledge modern communication environments changed cognitive patterns individuals interaction information encoded semiotic modalities visual linguistic main premise corpus linguistics ruling perception experience world conveyed texts nowadays studied multimodal perspective multimodal corpora extremely extract specialized knowledge explore insights specialized language relation language specific representations knowledge assertion analysis image text interface understand visual linguistic information converge subject field texts article frame based terminology sketch proposal study images corpus rich pictorial representations inclusion terminological resource environment corpus based approach methodological underpinnings create meaning terminographic entries facilitating specialized knowledge transfer acquisition images",0
"KeywordsTwitter n-gram corpus Social media Demographics Metadata Gender Time ","twitter n gram corpus with demographic metadata","Social media is a natural laboratory for linguistic and sociological purposes. In micro-blogging platforms such as Twitter, people share hundreds of millions of short messages about their lives and experiences on a daily basis. These messages, coupled with metadata about their authors, provide an opportunity to understand a wide variety of phenomena ranging from political polarization to geographic and demographic lexical variation. Lack of publicly available micro-blogging datasets has been a hindrance to replicable research. In this paper, I introduce Rovereto Twitter n-gram corpus, a publicly available n-gram dataset of Twitter messages, which contains gender-of-the-author and time-of-posting tags associated with the n-grams. I compare this dataset to a more traditional web-based corpus and present a case study which shows the potential of combining an n-gram corpus with demographic metadata.","Language Resources and Evaluation",2013,"Yes"," twitter gram corpus social media demographics metadata gender time twitter gram corpus demographic metadata social media natural laboratory linguistic sociological purposes micro blogging platforms twitter people share hundreds millions short messages lives experiences daily basis messages coupled metadata authors provide opportunity understand wide variety phenomena ranging political polarization geographic demographic lexical variation lack publicly micro blogging datasets hindrance replicable research paper introduce rovereto twitter gram corpus publicly gram dataset twitter messages gender author time posting tags grams compare dataset traditional web based corpus present case study shows potential combining gram corpus demographic metadata",1
"KeywordsACL Anthology Network Bibliometrics Scientometrics Citation analysis Citation summaries ","the acl anthology network corpus","We introduce the ACL Anthology Network (AAN), a comprehensive manually curated networked database of citations, collaborations, and summaries in the field of Computational Linguistics. We also present a number of statistics about the network including the most cited authors, the most central collaborators, as well as network statistics about the paper citation, author citation, and author collaboration networks.","Language Resources and Evaluation",2013,"Yes"," acl anthology network bibliometrics scientometrics citation analysis citation summaries acl anthology network corpus introduce acl anthology network aan comprehensive manually curated networked database citations collaborations summaries field computational linguistics present number statistics network including cited authors central collaborators network statistics paper citation author citation author collaboration networks",1
"KeywordsSentiment analysis Electronic lexica Corpus analysis Financial information extraction ","is there a language of sentiment an analysis of lexical resources for sentiment analysis","In recent years, sentiment analysis (SA) has emerged as a rapidly expanding field of application and research in the area of information retrieval. In order to facilitate the task of selecting lexical resources for automated SA systems, this paper sets out a detailed analysis of four widely used sentiment lexica. The analysis provides an overview of the coverage of each lexicon individually, the overlap and consistency of the four resources and a corpus analysis of the distribution of the resources’ lexical contents in general and specialised language. This work aims to explore the characteristics of affective language as represented by these lexica and the implications of the findings for developers of SA systems.","Language Resources and Evaluation",2013,"No"," sentiment analysis electronic lexica corpus analysis financial information extraction language sentiment analysis lexical resources sentiment analysis recent years sentiment analysis sa emerged rapidly expanding field application research area information retrieval order facilitate task selecting lexical resources automated sa systems paper sets detailed analysis widely sentiment lexica analysis overview coverage lexicon individually overlap consistency resources corpus analysis distribution resources lexical contents general specialised language work aims explore characteristics affective language represented lexica implications findings developers sa systems",0
"KeywordsMachine learning Arabic text categorization Arabic text classification ","comparative evaluation of text classification techniques using a large diverse arabic dataset","A vast amount of valuable human knowledge is recorded in documents. The rapid growth in the number of machine-readable documents for public or private access necessitates the use of automatic text classification. While a lot of effort has been put into Western languages—mostly English—minimal experimentation has been done with Arabic. This paper presents, first, an up-to-date review of the work done in the field of Arabic text classification and, second, a large and diverse dataset that can be used for benchmarking Arabic text classification algorithms. The different techniques derived from the literature review are illustrated by their application to the proposed dataset. The results of various feature selections, weighting methods, and classification algorithms show, on average, the superiority of support vector machine, followed by the decision tree algorithm (C4.5) and Naïve Bayes. The best classification accuracy was 97 % for the Islamic Topics dataset, and the least accurate was 61 % for the Arabic Poems dataset.","Language Resources and Evaluation",2013,"No"," machine learning arabic text categorization arabic text classification comparative evaluation text classification techniques large diverse arabic dataset vast amount valuable human knowledge recorded documents rapid growth number machine readable documents public private access necessitates automatic text classification lot effort put western languages english minimal experimentation arabic paper presents date review work field arabic text classification large diverse dataset benchmarking arabic text classification algorithms techniques derived literature review illustrated application proposed dataset results feature selections weighting methods classification algorithms show average superiority support vector machine decision tree algorithm c na ve bayes classification accuracy islamic topics dataset accurate arabic poems dataset",0
"KeywordsTask-performance evaluation Referring expressions Demonstrative pronouns Situated dialogue Japanese ","a task performance evaluation of referring expressions in situated collaborative task dialogues","Appropriate evaluation of referring expressions is critical for the design of systems that can effectively collaborate with humans. A widely used method is to simply evaluate the degree to which an algorithm can reproduce the same expressions as those in previously collected corpora. Several researchers, however, have noted the need of a task-performance evaluation measuring the effectiveness of a referring expression in the achievement of a given task goal. This is particularly important in collaborative situated dialogues. Using referring expressions used by six pairs of Japanese speakers collaboratively solving Tangram puzzles, we conducted a task-performance evaluation of referring expressions with 36 human evaluators. Particularly we focused on the evaluation of demonstrative pronouns generated by a machine learning-based algorithm. Comparing the results of this task-performance evaluation with the results of a previously conducted corpus-matching evaluation (Spanger et al. in Lang Resour Eval, 2010b), we confirmed the limitation of a corpus-matching evaluation and discuss the need for a task-performance evaluation.","Language Resources and Evaluation",2013,"No"," task performance evaluation referring expressions demonstrative pronouns situated dialogue japanese task performance evaluation referring expressions situated collaborative task dialogues evaluation referring expressions critical design systems effectively collaborate humans widely method simply evaluate degree algorithm reproduce expressions previously collected corpora researchers noted task performance evaluation measuring effectiveness referring expression achievement task goal important collaborative situated dialogues referring expressions pairs japanese speakers collaboratively solving tangram puzzles conducted task performance evaluation referring expressions human evaluators focused evaluation demonstrative pronouns generated machine learning based algorithm comparing results task performance evaluation results previously conducted corpus matching evaluation spanger al lang resour eval b confirmed limitation corpus matching evaluation discuss task performance evaluation",0
"KeywordsCall-centre data Automatic speech recognition system Opinion detection Business concept detection Disfluency ","spontaneous speech and opinion detection mining call centre transcripts","Opinion mining on conversational telephone speech tackles two challenges: the robustness of speech transcriptions and the relevance of opinion models. The two challenges are critical in an industrial context such as marketing. The paper addresses jointly these two issues by analyzing the influence of speech transcription errors on the detection of opinions and business concepts. We present both modules: the speech transcription system, which consists in a successful adaptation of a conversational speech transcription system to call-centre data and the information extraction module, which is based on a semantic modeling of business concepts, opinions and sentiments with complex linguistic rules. Three models of opinions are implemented based on the discourse theory, the appraisal theory and the marketers’ expertise, respectively. The influence of speech recognition errors on the information extraction module is evaluated by comparing its outputs on manual versus automatic transcripts. The F-scores obtained are 0.79 for business concepts detection, 0.74 for opinion detection and 0.67 for the extraction of relations between opinions and their target. This result and the in-depth analysis of the errors show the feasibility of opinion detection based on complex rules on call-centre transcripts.","Language Resources and Evaluation",2013,"No"," call centre data automatic speech recognition system opinion detection business concept detection disfluency spontaneous speech opinion detection mining call centre transcripts opinion mining conversational telephone speech tackles challenges robustness speech transcriptions relevance opinion models challenges critical industrial context marketing paper addresses jointly issues analyzing influence speech transcription errors detection opinions business concepts present modules speech transcription system consists successful adaptation conversational speech transcription system call centre data information extraction module based semantic modeling business concepts opinions sentiments complex linguistic rules models opinions implemented based discourse theory appraisal theory marketers expertise influence speech recognition errors information extraction module evaluated comparing outputs manual versus automatic transcripts scores obtained business concepts detection opinion detection extraction relations opinions target result depth analysis errors show feasibility opinion detection based complex rules call centre transcripts",0
"KeywordsText annotation Web-based annotation tool GATE Cloud-based text annotation service ","gate teamware a web based collaborative text annotation framework","This paper presents GATE Teamware—an open-source, web-based, collaborative text annotation framework. It enables users to carry out complex corpus annotation projects, involving distributed annotator teams. Different user roles are provided (annotator, manager, administrator) with customisable user interface functionalities, in order to support the complex workflows and user interactions that occur in corpus annotation projects. Documents may be pre-processed automatically, so that human annotators can begin with text that has already been pre-annotated and thus making them more efficient. The user interface is simple to learn, aimed at non-experts, and runs in an ordinary web browser, without need of additional software installation. GATE Teamware has been evaluated through the creation of several gold standard corpora and internal projects, as well as through external evaluation in commercial and EU text annotation projects. It is available as on-demand service on GateCloud.net, as well as open-source for self-installation.","Language Resources and Evaluation",2013,"No"," text annotation web based annotation tool gate cloud based text annotation service gate teamware web based collaborative text annotation framework paper presents gate teamware open source web based collaborative text annotation framework enables users carry complex corpus annotation projects involving distributed annotator teams user roles provided annotator manager administrator customisable user interface functionalities order support complex workflows user interactions occur corpus annotation projects documents pre processed automatically human annotators begin text pre annotated making efficient user interface simple learn aimed experts runs ordinary web browser additional software installation gate teamware evaluated creation gold standard corpora internal projects external evaluation commercial eu text annotation projects demand service gatecloudnet open source installation",0
"KeywordsPOS tagging Lemmatization Spelling variation Orthographic variation Historical text Middle Dutch ","dealing with orthographic variation in a tagger lemmatizer for fourteenth century dutch charters","In this paper we describe a tagger-lemmatizer for fourteenth century Dutch charters (as found in the corpus van Reenen/Mulder), with a special focus on the treatment of the extensive orthographic variation in this material. We show that despite the difficulties caused by the variation, we are still able to reach about 95 % accuracy in a tenfold cross-validation experiment for both tagging and lemmatization. We can deal effectively with the variation in tokenization (as applied by the authors) by pre-normalization (retokenization). For variation in spelling, however, we choose to expand our lexicon with predicted spelling variants. For those forms which can also not be found in this expanded lexicon, we first derive the word class and subsequently search for the most similar lexicon word. Interestingly, our techniques for recognizing spelling variants turn out to be vital for lemmatization accuracy, but much less important for tagging accuracy.","Language Resources and Evaluation",2013,"No"," pos tagging lemmatization spelling variation orthographic variation historical text middle dutch dealing orthographic variation tagger lemmatizer fourteenth century dutch charters paper describe tagger lemmatizer fourteenth century dutch charters found corpus van reenenmulder special focus treatment extensive orthographic variation material show difficulties caused variation reach accuracy tenfold cross validation experiment tagging lemmatization deal effectively variation tokenization applied authors pre normalization retokenization variation spelling choose expand lexicon predicted spelling variants forms found expanded lexicon derive word class subsequently search similar lexicon word interestingly techniques recognizing spelling variants turn vital lemmatization accuracy important tagging accuracy",0
"KeywordsSemEval 2010 Cross lingual Lexical substitution ","the cross lingual lexical substitution task","In this paper we provide an account of the cross-lingual lexical substitution task run as part of SemEval-2010. In this task both annotators (native Spanish speakers, proficient in English) and participating systems had to find Spanish translations for target words in the context of an English sentence. Because only translations of a single lexical unit were required, this task does not necessitate a full blown translation system. This we hope encouraged those working specifically on lexical semantics to participate without a requirement for them to use machine translation software, though they were free to use whatever resources they chose. In this paper we pay particular attention to the resources used by the various participating systems and present analyses to demonstrate the relative strengths of the systems as well as the requirements they have in terms of resources. In addition to the analyses of individual systems we also present the results of a combined system based on voting from the individual systems. We demonstrate that the system produces better results at finding the most frequent translation from the annotators compared to the highest ranked translation provided by individual systems. This supports our other analyses that the systems are heterogeneous, with different strengths and weaknesses.","Language Resources and Evaluation",2013,"No"," semeval cross lingual lexical substitution cross lingual lexical substitution task paper provide account cross lingual lexical substitution task run part semeval task annotators native spanish speakers proficient english participating systems find spanish translations target words context english sentence translations single lexical unit required task necessitate full blown translation system hope encouraged working specifically lexical semantics participate requirement machine translation software free resources chose paper pay attention resources participating systems present analyses demonstrate relative strengths systems requirements terms resources addition analyses individual systems present results combined system based voting individual systems demonstrate system produces results finding frequent translation annotators compared highest ranked translation provided individual systems supports analyses systems heterogeneous strengths weaknesses",0
"KeywordsAmazon Turk Lexical substitution Word sense disambiguation Language resource creation Crowdsourcing ","creating a system for lexical substitutions from scratch using crowdsourcing","This article describes the creation and application of the Turk Bootstrap Word Sense Inventory for 397 frequent nouns, which is a publicly available resource for lexical substitution. This resource was acquired using Amazon Mechanical Turk. In a bootstrapping process with massive collaborative input, substitutions for target words in context are elicited and clustered by sense; then, more contexts are collected. Contexts that cannot be assigned to a current target word’s sense inventory re-enter the bootstrapping loop and get a supply of substitutions. This process yields a sense inventory with its granularity determined by substitutions as opposed to psychologically motivated concepts. It comes with a large number of sense-annotated target word contexts. Evaluation on data quality shows that the process is robust against noise from the crowd, produces a less fine-grained inventory than WordNet and provides a rich body of high precision substitution data at low cost. Using the data to train a system for lexical substitutions, we show that amount and quality of the data is sufficient for producing high quality substitutions automatically. In this system, co-occurrence cluster features are employed as a means to cheaply model topicality.","Language Resources and Evaluation",2013,"Yes"," amazon turk lexical substitution word sense disambiguation language resource creation crowdsourcing creating system lexical substitutions scratch crowdsourcing article describes creation application turk bootstrap word sense inventory frequent nouns publicly resource lexical substitution resource acquired amazon mechanical turk bootstrapping process massive collaborative input substitutions target words context elicited clustered sense contexts collected contexts assigned current target word sense inventory enter bootstrapping loop supply substitutions process yields sense inventory granularity determined substitutions opposed psychologically motivated concepts large number sense annotated target word contexts evaluation data quality shows process robust noise crowd produces fine grained inventory wordnet rich body high precision substitution data low cost data train system lexical substitutions show amount quality data sufficient producing high quality substitutions automatically system occurrence cluster features employed means cheaply model topicality",1
"KeywordsSemEval Null instantiation Semantic roles Frame semantics ","beyond sentence level semantic role labeling linking argument structures in discourse","Semantic role labeling is traditionally viewed as a sentence-level task concerned with identifying semantic arguments that are overtly realized in a fairly local context (i.e., a clause or sentence). However, this local view potentially misses important information that can only be recovered if local argument structures are linked across sentence boundaries. One important link concerns semantic arguments that remain locally unrealized (null instantiations) but can be inferred from the context. In this paper, we report on the SemEval 2010 Task-10 on “Linking Events and Their Participants in Discourse”, that addressed this problem. We discuss the corpus that was created for this task, which contains annotations on multiple levels: predicate argument structure (FrameNet and PropBank), null instantiations, and coreference. We also provide an analysis of the task and its difficulties.","Language Resources and Evaluation",2013,"Yes"," semeval null instantiation semantic roles frame semantics sentence level semantic role labeling linking argument structures discourse semantic role labeling traditionally viewed sentence level task concerned identifying semantic arguments overtly realized fairly local context clause sentence local view potentially misses important information recovered local argument structures linked sentence boundaries important link concerns semantic arguments remain locally unrealized null instantiations inferred context paper report semeval task linking events participants discourse addressed problem discuss corpus created task annotations multiple levels predicate argument structure framenet propbank null instantiations coreference provide analysis task difficulties",1
"KeywordsSynonymy networks Semantic relatedness Collaboratively constructed resources Wiktionary Semi-automatic enrichment Random walks Small worlds ","semi automatic enrichment of crowdsourced synonymy networks the wisigoth system applied to wiktionary","Semantic lexical resources are a mainstay of various Natural Language Processing applications. However, comprehensive and reliable resources are rare and not often freely available. Handcrafted resources are too costly for being a general solution while automatically-built resources need to be validated by experts or at least thoroughly evaluated. We propose in this paper a picture of the current situation with regard to lexical resources, their building and their evaluation. We give an in-depth description of Wiktionary, a freely available and collaboratively built multilingual dictionary. Wiktionary is presented here as a promising raw resource for NLP. We propose a semi-automatic approach based on random walks for enriching Wiktionary synonymy network that uses both endogenous and exogenous data. We take advantage of the wiki infrastructure to propose a validation “by crowds”. Finally, we present an implementation called WISIGOTH, which supports our approach.","Language Resources and Evaluation",2013,"No"," synonymy networks semantic relatedness collaboratively constructed resources wiktionary semi automatic enrichment random walks small worlds semi automatic enrichment crowdsourced synonymy networks wisigoth system applied wiktionary semantic lexical resources mainstay natural language processing applications comprehensive reliable resources rare freely handcrafted resources costly general solution automatically built resources validated experts evaluated propose paper picture current situation regard lexical resources building evaluation give depth description wiktionary freely collaboratively built multilingual dictionary wiktionary presented promising raw resource nlp propose semi automatic approach based random walks enriching wiktionary synonymy network endogenous exogenous data advantage wiki infrastructure propose validation crowds finally present implementation called wisigoth supports approach",0
"KeywordsText classification Language identification Microblogs Content analysis ","microblog language identification overcoming the limitations of short unedited and idiomatic text","Multilingual posts can potentially affect the outcomes of content analysis on microblog platforms. To this end, language identification can provide a monolingual set of content for analysis. We find the unedited and idiomatic language of microblogs to be challenging for state-of-the-art language identification methods. To account for this, we identify five microblog characteristics that can help in language identification: the language profile of the blogger (blogger), the content of an attached hyperlink (link), the language profile of other users mentioned (mention) in the post, the language profile of a tag (tag), and the language of the original post (conversation), if the post we examine is a reply. Further, we present methods that combine these priors in a post-dependent and post-independent way. We present test results on 1,000 posts from five languages (Dutch, English, French, German, and Spanish), which show that our priors improve accuracy by 5 % over a domain specific baseline, and show that post-dependent combination of the priors achieves the best performance. When suitable training data does not exist, our methods still outperform a domain unspecific baseline. We conclude with an examination of the language distribution of a million tweets, along with temporal analysis, the usage of twitter features across languages, and a correlation study between classifications made and geo-location and language metadata fields.","Language Resources and Evaluation",2013,"No"," text classification language identification microblogs content analysis microblog language identification overcoming limitations short unedited idiomatic text multilingual posts potentially affect outcomes content analysis microblog platforms end language identification provide monolingual set content analysis find unedited idiomatic language microblogs challenging state art language identification methods account identify microblog characteristics language identification language profile blogger blogger content attached hyperlink link language profile users mentioned mention post language profile tag tag language original post conversation post examine reply present methods combine priors post dependent post independent present test results posts languages dutch english french german spanish show priors improve accuracy domain specific baseline show post dependent combination priors achieves performance suitable training data exist methods outperform domain unspecific baseline conclude examination language distribution million tweets temporal analysis usage twitter features languages correlation study classifications made geo location language metadata fields",0
"KeywordsIrony detection Figurative language processing Negation Web text analysis ","a multidimensional approach for detecting irony in twitter","Irony is a pervasive aspect of many online texts, one made all the more difficult by the absence of face-to-face contact and vocal intonation. As our media increasingly become more social, the problem of irony detection will become even more pressing. We describe here a set of textual features for recognizing irony at a linguistic level, especially in short texts created via social media such as Twitter postings or “tweets”. Our experiments concern four freely available data sets that were retrieved from Twitter using content words (e.g. “Toyota”) and user-generated tags (e.g. “#irony”). We construct a new model of irony detection that is assessed along two dimensions: representativeness and relevance. Initial results are largely positive, and provide valuable insights into the figurative issues facing tasks such as sentiment analysis, assessment of online reputations, or decision making.","Language Resources and Evaluation",2013,"No"," irony detection figurative language processing negation web text analysis multidimensional approach detecting irony twitter irony pervasive aspect online texts made difficult absence face face contact vocal intonation media increasingly social problem irony detection pressing describe set textual features recognizing irony linguistic level short texts created social media twitter postings tweets experiments concern freely data sets retrieved twitter content words toyota user generated tags irony construct model irony detection assessed dimensions representativeness relevance initial results largely positive provide valuable insights figurative issues facing tasks sentiment analysis assessment online reputations decision making",0
"KeywordsWikipedia Infobox Attributes Temporal data ","whad wikipedia historical attributes data","This paper describes the generation of temporally anchored infobox attribute data from the Wikipedia history of revisions. By mining (attribute, value) pairs from the revision history of the English Wikipedia we are able to collect a comprehensive knowledge base that contains data on how attributes change over time. When dealing with the Wikipedia edit history, vandalic and erroneous edits are a concern for data quality. We present a study of vandalism identification in Wikipedia edits that uses only features from the infoboxes, and show that we can obtain, on this dataset, an accuracy comparable to a state-of-the-art vandalism identification method that is based on the whole article. Finally, we discuss different characteristics of the extracted dataset, which we make available for further study.","Language Resources and Evaluation",2013,"No"," wikipedia infobox attributes temporal data whad wikipedia historical attributes data paper describes generation temporally anchored infobox attribute data wikipedia history revisions mining attribute pairs revision history english wikipedia collect comprehensive knowledge base data attributes change time dealing wikipedia edit history vandalic erroneous edits concern data quality present study vandalism identification wikipedia edits features infoboxes show obtain dataset accuracy comparable state art vandalism identification method based article finally discuss characteristics extracted dataset make study",0
"KeywordsCoreference resolution and evaluation NLP system analysis Machine learning based NLP tools SemEval-2010 (Task 1) Discourse entities ","coreference resolution an empirical study based on semeval 2010 shared task 1","This paper presents an empirical evaluation of coreference resolution that covers several interrelated dimensions. The main goal is to complete the comparative analysis from the SemEval-2010 task on Coreference Resolution in Multiple Languages. To do so, the study restricts the number of languages and systems involved, but extends and deepens the analysis of the system outputs, including a more qualitative discussion. The paper compares three automatic coreference resolution systems for three languages (English, Catalan and Spanish) in four evaluation settings, and using four evaluation measures. Given that our main goal is not to provide a comparison between resolution algorithms, these are merely used as tools to shed light on the different conditions under which coreference resolution is evaluated. Although the dimensions are strongly interdependent, making it very difficult to extract general principles, the study reveals a series of interesting issues in relation to coreference resolution: the portability of systems across languages, the influence of the type and quality of input annotations, and the behavior of the scoring measures.","Language Resources and Evaluation",2013,"No"," coreference resolution evaluation nlp system analysis machine learning based nlp tools semeval task discourse entities coreference resolution empirical study based semeval shared task paper presents empirical evaluation coreference resolution covers interrelated dimensions main goal complete comparative analysis semeval task coreference resolution multiple languages study restricts number languages systems involved extends deepens analysis system outputs including qualitative discussion paper compares automatic coreference resolution systems languages english catalan spanish evaluation settings evaluation measures main goal provide comparison resolution algorithms tools shed light conditions coreference resolution evaluated dimensions strongly interdependent making difficult extract general principles study reveals series interesting issues relation coreference resolution portability systems languages influence type quality input annotations behavior scoring measures",0
"KeywordsNormalization of short texts Statistical machine translation Automatic extraction of rules Perplexity ","automatic normalization of short texts by combining statistical and rule based techniques","Short texts are typically composed of small number of words, most of which are abbreviations, typos and other kinds of noise. This makes the noise to signal ratio relatively high for this specific category of text. A high proportion of noise in the data is undesirable for analysis procedures as well as machine learning applications. Text normalization techniques are used to reduce the noise and improve the quality of text for processing and analysis purposes. In this work, we propose a combination of statistical and rule-based techniques to normalize short texts. More specifically, we focus our attention on SMS messages. We base our normalization approach on a statistical machine translation system which translates from noisy data to clean data. This system is trained on a small manually annotated set. Then, we study several automatic methods to extract more general rules from the normalizations generated with the statistical machine translation system. We illustrate the proposed methodology by conducting some experiments with a SMS Haitian-Créole data collection. In order to evaluate the performance of our methodology we use several Haitian-Créole dictionaries, the well-known perplexity criteria and the achieved reduction of vocabulary.","Language Resources and Evaluation",2013,"No"," normalization short texts statistical machine translation automatic extraction rules perplexity automatic normalization short texts combining statistical rule based techniques short texts typically composed small number words abbreviations typos kinds noise makes noise signal ratio high specific category text high proportion noise data undesirable analysis procedures machine learning applications text normalization techniques reduce noise improve quality text processing analysis purposes work propose combination statistical rule based techniques normalize short texts specifically focus attention sms messages base normalization approach statistical machine translation system translates noisy data clean data system trained small manually annotated set study automatic methods extract general rules normalizations generated statistical machine translation system illustrate proposed methodology conducting experiments sms haitian cr ole data collection order evaluate performance methodology haitian cr ole dictionaries perplexity criteria achieved reduction vocabulary",0
"KeywordsSemantic Relation Language Resource Lexical Unit Deduction Rule Lexical Meaning ","introduction to the special issue on wordnets and relations","Since its inception a quarter century ago, Princeton WordNet [PWN] (Miller 1995; Fellbaum 1998) has had a profound influence on research and applications in lexical semantics, computational linguistics and natural language processing. The numerous uses of this lexical resource have motivated the building of wordnets1 in several dozen languages, including even a “dead” language, Latin. This special issue looks at certain aspects of wordnet construction and organisation.","Language Resources and Evaluation",2013,"No"," semantic relation language resource lexical unit deduction rule lexical meaning introduction special issue wordnets relations inception quarter century ago princeton wordnet pwn miller fellbaum profound influence research applications lexical semantics computational linguistics natural language processing numerous lexical resource motivated building wordnets dozen languages including dead language latin special issue aspects wordnet construction organisation",0
"KeywordsSpeech database Noise database Spoken dialogue interaction Open-air noise environment ","the moveon database motorcycle environment speech and noise database for command and control applications","The MoveOn speech and noise database was purposely designed and implemented in support of research on spoken dialogue interaction in a motorcycle environment. The distinctiveness of the MoveOn database results from the requirements of the application domain—an information support and operational command and control system for the two-wheel police force—and also from the specifics of the adverse open-air acoustic environment. In this article, we first outline the target application, motivating the database design and purpose, and then report on the implementation details. The main challenges related to the choice of equipment, the organization of recording sessions, and some difficulties that were experienced during this effort, are discussed. We offer a detailed account of the database statistics, the suggested data splits in subsets, and discuss results from automatic speech recognition experiments which illustrate the degree of complexity of the operational environment.","Language Resources and Evaluation",2013,"Yes"," speech database noise database spoken dialogue interaction open air noise environment moveon database motorcycle environment speech noise database command control applications moveon speech noise database purposely designed implemented support research spoken dialogue interaction motorcycle environment distinctiveness moveon database results requirements application domain information support operational command control system wheel police force specifics adverse open air acoustic environment article outline target application motivating database design purpose report implementation details main challenges related choice equipment organization recording sessions difficulties experienced effort discussed offer detailed account database statistics suggested data splits subsets discuss results automatic speech recognition experiments illustrate degree complexity operational environment",1
"KeywordsKeyphrase extraction Scientific document processing SemEval-2010 Shared task ","automatic keyphrase extraction from scientific articles","This paper describes the organization and results of the automatic keyphrase extraction task held at the Workshop on Semantic Evaluation 2010 (SemEval-2010). The keyphrase extraction task was specifically geared towards scientific articles. Systems were automatically evaluated by matching their extracted keyphrases against those assigned by the authors as well as the readers to the same documents. We outline the task, present the overall ranking of the submitted systems, and discuss the improvements to the state-of-the-art in keyphrase extraction.","Language Resources and Evaluation",2013,"No"," keyphrase extraction scientific document processing semeval shared task automatic keyphrase extraction scientific articles paper describes organization results automatic keyphrase extraction task held workshop semantic evaluation semeval keyphrase extraction task specifically geared scientific articles systems automatically evaluated matching extracted keyphrases assigned authors readers documents outline task present ranking submitted systems discuss improvements state art keyphrase extraction",0
"KeywordsSpanish Grammar Deep processing HPSG LKB DELPH-IN ","the spanish delph in grammar","In this article we present a Spanish grammar implemented in the Linguistic Knowledge Builder system and grounded in the theoretical framework of Head-driven Phrase Structure Grammar. The grammar is being developed in an international multilingual context, the DELPH-IN Initiative, contributing to an open-source repository of software and linguistic resources for various Natural Language Processing applications. We will show how we have refined and extended a core grammar, derived from the LinGO Grammar Matrix, to achieve a broad-coverage grammar. The Spanish DELPH-IN grammar is the most comprehensive grammar for Spanish deep processing, and it is being deployed in the construction of a treebank for Spanish of 60,000 sentences based in a technical corpus in the framework of the European project METANET4U (Enhancing the European Linguistic Infrastructure, GA 270893GA; http://www.meta-net.eu/projects/METANET4U/.) and a smaller treebank of about 15,000 sentences based in a corpus from the press.","Language Resources and Evaluation",2013,"No"," spanish grammar deep processing hpsg lkb delph spanish delph grammar article present spanish grammar implemented linguistic knowledge builder system grounded theoretical framework head driven phrase structure grammar grammar developed international multilingual context delph initiative contributing open source repository software linguistic resources natural language processing applications show refined extended core grammar derived lingo grammar matrix achieve broad coverage grammar spanish delph grammar comprehensive grammar spanish deep processing deployed construction treebank spanish sentences based technical corpus framework european project metanetu enhancing european linguistic infrastructure ga ga httpwwwmeta neteuprojectsmetanetu smaller treebank sentences based corpus press",0
"KeywordsPolarity classification Sentiment analysis Bootstrapping methods Feature engineering Text classification ","bootstrapping polarity classifiers with rule based classification","In this article, we examine the effectiveness of bootstrapping supervised machine-learning polarity classifiers with the help of a domain-independent rule-based classifier that relies on a lexical resource, i.e., a polarity lexicon and a set of linguistic rules. The benefit of this method is that though no labeled training data are required, it allows a classifier to capture in-domain knowledge by training a supervised classifier with in-domain features, such as bag of words, on instances labeled by a rule-based classifier. Thus, this approach can be considered as a simple and effective method for domain adaptation. Among the list of components of this approach, we investigate how important the quality of the rule-based classifier is and what features are useful for the supervised classifier. In particular, the former addresses the issue in how far linguistic modeling is relevant for this task. We not only examine how this method performs under more difficult settings in which classes are not balanced and mixed reviews are included in the data set but also compare how this linguistically-driven method relates to state-of-the-art statistical domain adaptation.","Language Resources and Evaluation",2013,"No"," polarity classification sentiment analysis bootstrapping methods feature engineering text classification bootstrapping polarity classifiers rule based classification article examine effectiveness bootstrapping supervised machine learning polarity classifiers domain independent rule based classifier relies lexical resource polarity lexicon set linguistic rules benefit method labeled training data required classifier capture domain knowledge training supervised classifier domain features bag words instances labeled rule based classifier approach considered simple effective method domain adaptation list components approach investigate important quality rule based classifier features supervised classifier addresses issue linguistic modeling relevant task examine method performs difficult settings classes balanced mixed reviews included data set compare linguistically driven method relates state art statistical domain adaptation",0
"KeywordsSentiment ambiguous adjectives Sentiment analysis Word sense disambiguation SemEval ","semeval 2010 task 18 disambiguating sentiment ambiguous adjectives","Sentiment ambiguous adjectives, which have been neglected by most previous researches, pose a challenging task in sentiment analysis. We present an evaluation task at SemEval-2010, designed to provide a framework for comparing different approaches on this problem. The task focuses on 14 Chinese sentiment ambiguous adjectives, and provides manually labeled test data. There are 8 teams submitting 16 systems in this task. In this paper, we define the task, describe the data creation, list the participating systems, and discuss different approaches.","Language Resources and Evaluation",2013,"No"," sentiment ambiguous adjectives sentiment analysis word sense disambiguation semeval semeval task disambiguating sentiment ambiguous adjectives sentiment ambiguous adjectives neglected previous researches pose challenging task sentiment analysis present evaluation task semeval designed provide framework comparing approaches problem task focuses chinese sentiment ambiguous adjectives manually labeled test data teams submitting systems task paper define task describe data creation list participating systems discuss approaches",0
"KeywordsTarget Word Natural Language Processing Word Sense Disambiguation Collective Intelligence Language Resource ","collective intelligence and language resources introduction to the special issue on collaboratively constructed language resources","In recent years, collective intelligence has become a field of active research due to the rise of Web 2.0 and the availability of Web-based technologies that support distributed collaboration. Malone et al. (2009) define collective intelligence broadly as “groups of individuals acting collectively in ways that seem intelligent.” The applications of this phenomenon are wide-reaching: recent publications (Malone 2004; Howe 2008; Surowiecki 2004; Benkler 2006; Tapscott and Williams 2006), and a compendium of nearly 250 examples of Web-based collective intelligence collected by the MIT Center for Collective Intelligence 1 clearly demonstrate the diversity of ways in which collective intelligence can be applied. The field is now about to consolidate itself and launch its own conference which will be held for the first time in 2012. 2","Language Resources and Evaluation",2013,"No"," target word natural language processing word sense disambiguation collective intelligence language resource collective intelligence language resources introduction special issue collaboratively constructed language resources recent years collective intelligence field active research due rise web availability web based technologies support distributed collaboration malone al define collective intelligence broadly groups individuals acting collectively ways intelligent applications phenomenon wide reaching recent publications malone howe surowiecki benkler tapscott williams compendium examples web based collective intelligence collected mit center collective intelligence demonstrate diversity ways collective intelligence applied field consolidate launch conference held time ",0
"KeywordsInformation retrieval Text summarization Crowdsourcing services Crowdflower Mechanical Turk ","analyzing the capabilities of crowdsourcing services for text summarization","This paper presents a detailed analysis of the use of crowdsourcing services for the Text Summarization task in the context of the tourist domain. In particular, our aim is to retrieve relevant information about a place or an object pictured in an image in order to provide a short summary which will be of great help for a tourist. For tackling this task, we proposed a broad set of experiments using crowdsourcing services that could be useful as a reference for others who want to rely also on crowdsourcing. From the analysis carried out through our experimental setup and the results obtained, we can conclude that although crowdsourcing services were not good to simply gather gold-standard summaries (i.e., from the results obtained for experiments 1, 2 and 4), the encouraging results obtained in the third and sixth experiments motivate us to strongly believe that they can be successfully employed for finding some patterns of behaviour humans have when generating summaries, and for validating and checking other tasks. Furthermore, this analysis serves as a guideline for the types of experiments that might or might not work when using crowdsourcing in the context of text summarization.","Language Resources and Evaluation",2013,"No"," information retrieval text summarization crowdsourcing services crowdflower mechanical turk analyzing capabilities crowdsourcing services text summarization paper presents detailed analysis crowdsourcing services text summarization task context tourist domain aim retrieve relevant information place object pictured image order provide short summary great tourist tackling task proposed broad set experiments crowdsourcing services reference rely crowdsourcing analysis carried experimental setup results obtained conclude crowdsourcing services good simply gather gold standard summaries results obtained experiments encouraging results obtained sixth experiments motivate strongly successfully employed finding patterns behaviour humans generating summaries validating checking tasks analysis serves guideline types experiments work crowdsourcing context text summarization",0
"KeywordsLexical resources Swedish WordNet Interoperability LMF SALDO ","saldo a touch of yin to wordnets yang","The English-language Princeton WordNet (PWN) and some wordnets for other languages have been extensively used as lexical–semantic knowledge sources in language technology applications, due to their free availability and their size. The ubiquitousness of PWN-type wordnets tends to overshadow the fact that they represent one out of many possible choices for structuring a lexical–semantic resource, and it could be enlightening to look at a differently structured resource both from the point of view of theoretical–methodological considerations and from the point of view of practical text processing requirements. The resource described here—SALDO—is such a lexical–semantic resource, intended primarily for use in language technology applications, and offering an alternative organization to PWN-style wordnets. We present our work on SALDO, compare it with PWN, and discuss some implications of the differences. We also describe an integrated infrastructure for computational lexical resources where SALDO forms the central component.","Language Resources and Evaluation",2013,"Yes"," lexical resources swedish wordnet interoperability lmf saldo saldo touch yin wordnets yang english language princeton wordnet pwn wordnets languages extensively lexical semantic knowledge sources language technology applications due free availability size ubiquitousness pwn type wordnets overshadow fact represent choices structuring lexical semantic resource enlightening differently structured resource point view theoretical methodological considerations point view practical text processing requirements resource saldo lexical semantic resource intended primarily language technology applications offering alternative organization pwn style wordnets present work saldo compare pwn discuss implications differences describe integrated infrastructure computational lexical resources saldo forms central component",1
"KeywordsPart–whole relations Meronymy Holonymy German wordnet GermaNet Compounds Compound-internal relations ","using partwhole relations for automatic deduction of compound internal relations in germanet","This paper provides a deduction-based approach for automatically classifying compound-internal relations in GermaNet, the German version of the Princeton WordNet for English. More specifically, meronymic relations between simplex  and compound nouns provide the necessary input to the deduction patterns that involve different types of compound-internal relations. The scope of these deductions extends to all four meronymic relations modeled in version 6.0 of GermaNet: component, member, substance, and portion. This deduction-based approach provides an effective method for automatically enriching the set of semantic relations included in GermaNet.","Language Resources and Evaluation",2013,"Yes"," part relations meronymy holonymy german wordnet germanet compounds compound internal relations partwhole relations automatic deduction compound internal relations germanet paper deduction based approach automatically classifying compound internal relations germanet german version princeton wordnet english specifically meronymic relations simplex compound nouns provide input deduction patterns involve types compound internal relations scope deductions extends meronymic relations modeled version germanet component member substance portion deduction based approach effective method automatically enriching set semantic relations included germanet",1
"KeywordsAligned wordnets BalkaNet EuroWordNet Interlingual mapping Lexical ontology Ontology projection Princeton WordNet Romanian ","the romanian wordnet in a nutshell","The project on the Romanian wordnet has been under continuous development for more than 10 years now. It has been in constant use in many projects and applications which determined, to a large extent, the content and coverage of various lexical domains. The article presents the most recent developments of the Romanian wordnet and offers quantitative data for its current version.","Language Resources and Evaluation",2013,"Yes"," aligned wordnets balkanet eurowordnet interlingual mapping lexical ontology ontology projection princeton wordnet romanian romanian wordnet nutshell project romanian wordnet continuous development years constant projects applications determined large extent content coverage lexical domains article presents recent developments romanian wordnet offers quantitative data current version",1
"KeywordsAutomatic treebank conversion Feature-based approach Part of speech Constituency syntactic structure ","a feature based approach to better automatic treebank conversion","In the field of constituency parsing, there exist multiple human-labeled treebanks which are built on non-overlapping text samples and follow different annotation standards. Due to the extreme cost of annotating parse trees by human, it is desirable to automatically convert one treebank (called source treebank) to the standard of another treebank (called target treebank) which we are interested in. Conversion results can be manually corrected to obtain higher-quality annotations or can be directly used as additional training data for building syntactic parsers. To perform automatic treebank conversion, we divide constituency parses into two separate levels: the part-of-speech (POS) and syntactic structure (bracketing structures and constituent labels), and conduct conversion on these two levels respectively with a feature-based approach. The basic idea of the approach is to encode original annotations in a source treebank as guide features during the conversion process. Experiments on two Chinese treebanks show that our approach can convert POS tags and syntactic structures with the accuracy of 96.6 and 84.8 %, respectively, which are the best reported results on this task.","Language Resources and Evaluation",2013,"No"," automatic treebank conversion feature based approach part speech constituency syntactic structure feature based approach automatic treebank conversion field constituency parsing exist multiple human labeled treebanks built overlapping text samples follow annotation standards due extreme cost annotating parse trees human desirable automatically convert treebank called source treebank standard treebank called target treebank interested conversion results manually corrected obtain higher quality annotations directly additional training data building syntactic parsers perform automatic treebank conversion divide constituency parses separate levels part speech pos syntactic structure bracketing structures constituent labels conduct conversion levels feature based approach basic idea approach encode original annotations source treebank guide features conversion process experiments chinese treebanks show approach convert pos tags syntactic structures accuracy reported results task",0
"KeywordsArabic WordNet Hyponymy extraction Maximal frequent sequence WordNet-based application ","on the evaluation and improvement of arabic wordnet coverage and usability","Built on the basis of the methods developed for Princeton WordNet and EuroWordNet, Arabic WordNet (AWN) has been an interesting project which combines WordNet structure compliance with Arabic particularities. In this paper, some AWN shortcomings related to coverage and usability are addressed. The use of AWN in question/answering (Q/A) helped us to deeply evaluate the resource from an experience-based perspective. Accordingly, an enrichment of AWN was built by semi-automatically extending its content. Indeed, existing approaches and/or resources developed for other languages were adapted and used for AWN. The experiments conducted in Arabic Q/A have shown an improvement of both AWN coverage as well as usability. Concerning coverage, a great amount of named entities extracted from YAGO were connected with corresponding AWN synsets. Also, a significant number of new verbs and nouns (including Broken Plural forms) were added. In terms of usability, thanks to the use of AWN, the performance for the AWN-based Q/A application registered an overall improvement with respect to the following three measures: accuracy (+9.27 % improvement), mean reciprocal rank (+3.6 improvement) and number of answered questions (+12.79 % improvement).","Language Resources and Evaluation",2013,"Yes"," arabic wordnet hyponymy extraction maximal frequent sequence wordnet based application evaluation improvement arabic wordnet coverage usability built basis methods developed princeton wordnet eurowordnet arabic wordnet awn interesting project combines wordnet structure compliance arabic particularities paper awn shortcomings related coverage usability addressed awn questionanswering helped deeply evaluate resource experience based perspective enrichment awn built semi automatically extending content existing approaches resources developed languages adapted awn experiments conducted arabic shown improvement awn coverage usability coverage great amount named entities extracted yago connected awn synsets significant number verbs nouns including broken plural forms added terms usability awn performance awn based application registered improvement respect measures accuracy improvement reciprocal rank improvement number answered questions improvement",1
"KeywordsHyponym and hypernym learning Text mining Ontology induction Wordnet evaluation ","tailoring the automated construction of large scale taxonomies using the web","It has long been a dream to have available a single, centralized, semantic thesaurus or terminology taxonomy to support research in a variety of fields. Much human and computational effort has gone into constructing such resources, including the original WordNet and subsequent wordnets in various languages. To produce such resources one has to overcome well-known problems in achieving both wide coverage and internal consistency within a single wordnet and across many wordnets. In particular, one has to ensure that alternative valid taxonomizations covering the same basic terms are recognized and treated appropriately. In this paper we describe a pipeline of new, powerful, minimally supervised, automated algorithms that can be used to construct terminology taxonomies and wordnets, in various languages, by harvesting large amounts of online domain-specific or general text. We illustrate the effectiveness of the algorithms both to build localized, domain-specific wordnets and to highlight and investigate certain deeper ontological problems such as parallel generalization hierarchies. We show shortcomings and gaps in the manually-constructed English WordNet in various domains.","Language Resources and Evaluation",2013,"No"," hyponym hypernym learning text mining ontology induction wordnet evaluation tailoring automated construction large scale taxonomies web long dream single centralized semantic thesaurus terminology taxonomy support research variety fields human computational effort constructing resources including original wordnet subsequent wordnets languages produce resources overcome problems achieving wide coverage internal consistency single wordnet wordnets ensure alternative valid taxonomizations covering basic terms recognized treated appropriately paper describe pipeline powerful minimally supervised automated algorithms construct terminology taxonomies wordnets languages harvesting large amounts online domain specific general text illustrate effectiveness algorithms build localized domain specific wordnets highlight investigate deeper ontological problems parallel generalization hierarchies show shortcomings gaps manually constructed english wordnet domains",0
"KeywordsParsing Textual entailments ","parser evaluation using textual entailments","Parser Evaluation using Textual Entailments (PETE) is a shared task in the SemEval-2010 Evaluation Exercises on Semantic Evaluation. The task involves recognizing textual entailments based on syntactic information alone. PETE introduces a new parser evaluation scheme that is formalism independent, less prone to annotation error, and focused on semantically relevant distinctions. This paper describes the PETE task, gives an error analysis of the top-performing Cambridge system, and introduces a standard entailment module that can be used with any parser that outputs Stanford typed dependencies.","Language Resources and Evaluation",2013,"No"," parsing textual entailments parser evaluation textual entailments parser evaluation textual entailments pete shared task semeval evaluation exercises semantic evaluation task involves recognizing textual entailments based syntactic information pete introduces parser evaluation scheme formalism independent prone annotation error focused semantically relevant distinctions paper describes pete task error analysis top performing cambridge system introduces standard entailment module parser outputs stanford typed dependencies",0
"KeywordsWordnet WordNet Synset Lexical unit plWordNet Wordnet relations Constitutive relations Register Aspect ","the chicken and egg problem in wordnet design synonymy synsets and constitutive relations","Wordnets are built of synsets, not of words. A synset consists of words. Synonymy is a relation between words. Words go into a synset because they are synonyms. Later, a wordnet treats words as synonymous because they belong in the same synset\(\ldots\) Such circularity, a well-known problem, poses a practical difficulty in wordnet construction, notably when it comes to maintaining consistency. We propose to make a wordnet a net of words or, to be more precise, lexical units. We discuss our assumptions and present their implementation in a steadily growing Polish wordnet. A small set of constitutive relations allows us to construct synsets automatically out of groups of lexical units with the same connectivity. Our analysis includes a thorough comparative overview of systems of relations in several influential wordnets. The additional synset-forming mechanisms include stylistic registers and verb aspect.","Language Resources and Evaluation",2013,"No"," wordnet wordnet synset lexical unit plwordnet wordnet relations constitutive relations register aspect chicken egg problem wordnet design synonymy synsets constitutive relations wordnets built synsets words synset consists words synonymy relation words words synset synonyms wordnet treats words synonymous belong synsetldots circularity problem poses practical difficulty wordnet construction notably maintaining consistency propose make wordnet net words precise lexical units discuss assumptions present implementation steadily growing polish wordnet small set constitutive relations construct synsets automatically groups lexical units connectivity analysis includes comparative overview systems relations influential wordnets additional synset forming mechanisms include stylistic registers verb aspect",0
"KeywordsConcept concreteness Dictionary definitions Semantic lexicons Polysemy Computational lexicography ","the potentials and limitations of modelling concept concreteness in computational semantic lexicons with dictionary definitions","This paper explores the feasibility of modelling concept concreteness perceived by humans and representing it in computational semantic lexicons, addressing an issue at the crossroads of computational linguistics, lexicography, and psycholinguistics. The inherent distinction between concrete words and abstract words in psychology has relied mostly on subjective human ratings. This practice is hardly scalable and does not consider the effect of polysemy. In view of this, we attempt to obtain a measure of concreteness from dictionary definitions comparable to human judgement, capitalising on conventional lexicographic assumptions and the regularities exhibited in the surface structures of sense definitions. The structural pattern of a definition is analysed and scored on a 7-point scale of concreteness ratings. The definition scores turned out to be quite effective for a dichotomous distinction between concrete and abstract concepts and more consistent with human ratings for the former. Beyond the two-way distinction, however, the results were more variable. The study has thus revealed the potentials and limitations of our approach, suggesting that different defining styles probably reflect the describability of concepts, and describability alone may not be sufficient for differentiating the degree of concreteness. The range of definition patterns has to be reconsidered, in combination with other inseparable factors constituting our perception of concreteness, for better modelling on a finer scale of concreteness distinction to enrich semantic lexicons for natural language processing.","Language Resources and Evaluation",2013,"No"," concept concreteness dictionary definitions semantic lexicons polysemy computational lexicography potentials limitations modelling concept concreteness computational semantic lexicons dictionary definitions paper explores feasibility modelling concept concreteness perceived humans representing computational semantic lexicons addressing issue crossroads computational linguistics lexicography psycholinguistics inherent distinction concrete words abstract words psychology relied subjective human ratings practice scalable effect polysemy view attempt obtain measure concreteness dictionary definitions comparable human judgement capitalising conventional lexicographic assumptions regularities exhibited surface structures sense definitions structural pattern definition analysed scored point scale concreteness ratings definition scores turned effective dichotomous distinction concrete abstract concepts consistent human ratings distinction results variable study revealed potentials limitations approach suggesting defining styles reflect describability concepts describability sufficient differentiating degree concreteness range definition patterns reconsidered combination inseparable factors constituting perception concreteness modelling finer scale concreteness distinction enrich semantic lexicons natural language processing",0
"KeywordsLexical resources WordNet Machine learning ","constructing and utilizing wordnets using statistical methods","Lexical databases following the wordnet paradigm capture information about words, word senses, and their relationships. A large number of existing tools and datasets are based on the original WordNet, so extending the landscape of resources aligned with WordNet leads to great potential for interoperability and to substantial synergies. Wordnets are being compiled for a considerable number of languages, however most have yet to reach a comparable level of coverage. We propose a method for automatically producing such resources for new languages based on WordNet, and analyse the implications of this approach both from a linguistic perspective as well as by considering natural language processing tasks. Our approach takes advantage of the original WordNet in conjunction with translation dictionaries. A small set of training associations is used to learn a statistical model for predicting associations between terms and senses. The associations are represented using a variety of scores that take into account structural properties as well as semantic relatedness and corpus frequency information. Although the resulting wordnets are imperfect in terms of their quality and coverage of language-specific phenomena, we show that they constitute a cheap and suitable alternative for many applications, both for monolingual tasks as well as for cross-lingual interoperability. Apart from analysing the resources directly, we conducted tests on semantic relatedness assessment and cross-lingual text classification with very promising results.","Language Resources and Evaluation",2012,"No"," lexical resources wordnet machine learning constructing utilizing wordnets statistical methods lexical databases wordnet paradigm capture information words word senses relationships large number existing tools datasets based original wordnet extending landscape resources aligned wordnet leads great potential interoperability substantial synergies wordnets compiled considerable number languages reach comparable level coverage propose method automatically producing resources languages based wordnet analyse implications approach linguistic perspective natural language processing tasks approach takes advantage original wordnet conjunction translation dictionaries small set training associations learn statistical model predicting associations terms senses associations represented variety scores account structural properties semantic relatedness corpus frequency information resulting wordnets imperfect terms quality coverage language specific phenomena show constitute cheap suitable alternative applications monolingual tasks cross lingual interoperability analysing resources directly conducted tests semantic relatedness assessment cross lingual text classification promising results",0
"KeywordsMultilingual wordnets Formal ontology Information system ","challenges for a multilingual wordnet","Wordnets have been created in many languages, revealing both their lexical commonalities and diversity. The next challenge is to make multilingual wordnets fully interoperable. The EuroWordNet experience revealed the shortcomings of an interlingua based on a natural language. Instead, we propose a model based on the division of the lexicon and a language-independent, formal ontology that serves as the hub interlinking the language-specific lexicons. The ontology avoids the idiosyncracies of the lexicon and furthermore allows formal reasoning about the concepts it contains. We address the division of labor between ontology and lexicon. Finally, we illustrate our model in the context of a domain-specific multilingual information system based on a central ontology and interconnected wordnets in seven languages.","Language Resources and Evaluation",2012,"No"," multilingual wordnets formal ontology information system challenges multilingual wordnet wordnets created languages revealing lexical commonalities diversity challenge make multilingual wordnets fully interoperable eurowordnet experience revealed shortcomings interlingua based natural language propose model based division lexicon language independent formal ontology serves hub interlinking language specific lexicons ontology avoids idiosyncracies lexicon formal reasoning concepts address division labor ontology lexicon finally illustrate model context domain specific multilingual information system based central ontology interconnected wordnets languages",0
"KeywordsInteroperability Standards Language resources ","global interoperability for language resources introduction to the special section","This special section of Language Resources and Evaluation contains a selection of presentations from ICGL that focus on interoperability for lexical and semantic databases and ontologies. These resources in effect constitute the “hub” of semantic interoperability by providing means to link language resources such as corpora to common categories and concepts. As such, interoperability within and among these databases is the necessary next step to enable semantic compatibility for language data.","Language Resources and Evaluation",2012,"No"," interoperability standards language resources global interoperability language resources introduction special section special section language resources evaluation selection presentations icgl focus interoperability lexical semantic databases ontologies resources effect constitute hub semantic interoperability providing means link language resources corpora common categories concepts interoperability databases step enable semantic compatibility language data",0
"KeywordsLanguage Resources Named Entities Web 2.0 Standards ","web 20 language resources and standards to automatically build a multilingual named entity lexicon","This paper proposes to advance in the current state-of-the-art of automatic Language Resource (LR) building by taking into consideration three elements: (1) the knowledge available in existing LRs, (2) the vast amount of information available from the collaborative paradigm that has emerged from the Web 2.0 and (3) the use of standards to improve interoperability. We present a case study in which a set of LRs for different languages (WordNet for English and Spanish and Parole-Simple-Clips for Italian) are extended with Named Entities (NE) by exploiting Wikipedia and the aforementioned LRs. The practical result is a multilingual NE lexicon connected to these LRs and to two ontologies: SUMO and SIMPLE. Furthermore, the paper addresses an important problem which affects the Computational Linguistics area in the present, interoperability, by making use of the ISO LMF standard to encode this lexicon. The different steps of the procedure (mapping, disambiguation, extraction, NE identification and postprocessing) are comprehensively explained and evaluated. The resulting resource contains 974,567, 137,583 and 125,806 NEs for English, Spanish and Italian respectively. Finally, in order to check the usefulness of the constructed resource, we apply it into a state-of-the-art Question Answering system and evaluate its impact; the NE lexicon improves the system’s accuracy by 28.1%. Compared to previous approaches to build NE repositories, the current proposal represents a step forward in terms of automation, language independence, amount of NEs acquired and richness of the information represented.","Language Resources and Evaluation",2012,"Yes"," language resources named entities web standards web language resources standards automatically build multilingual named entity lexicon paper proposes advance current state art automatic language resource lr building taking consideration elements knowledge existing lrs vast amount information collaborative paradigm emerged web standards improve interoperability present case study set lrs languages wordnet english spanish parole simple clips italian extended named entities ne exploiting wikipedia aforementioned lrs practical result multilingual ne lexicon connected lrs ontologies sumo simple paper addresses important problem affects computational linguistics area present interoperability making iso lmf standard encode lexicon steps procedure mapping disambiguation extraction ne identification postprocessing comprehensively explained evaluated resulting resource nes english spanish italian finally order check usefulness constructed resource apply state art question answering system evaluate impact ne lexicon improves system accuracy compared previous approaches build ne repositories current proposal represents step forward terms automation language independence amount nes acquired richness information represented",1
"KeywordsLexica Terminology Semantic Web Linked data Ontologies ","interchanging lexical resources on the semantic web","Lexica and terminology databases play a vital role in many NLP applications, but currently most such resources are published in application-specific formats, or with custom access interfaces, leading to the problem that much of this data is in “data silos” and hence difficult to access. The Semantic Web and in particular the Linked Data initiative provide effective solutions to this problem, as well as possibilities for data reuse by inter-lexicon linking, and incorporation of data categories by dereferencable URIs. The Semantic Web focuses on the use of ontologies to describe semantics on the Web, but currently there is no standard for providing complex lexical information for such ontologies and for describing the relationship between the lexicon and the ontology. We present our model, lemon, which aims to address these gaps while building on existing work, in particular the Lexical Markup Framework, the ISOcat Data Category Registry, SKOS (Simple Knowledge Organization System) and the LexInfo and LIR ontology-lexicon models.","Language Resources and Evaluation",2012,"No"," lexica terminology semantic web linked data ontologies interchanging lexical resources semantic web lexica terminology databases play vital role nlp applications resources published application specific formats custom access interfaces leading problem data data silos difficult access semantic web linked data initiative provide effective solutions problem possibilities data reuse inter lexicon linking incorporation data categories dereferencable uris semantic web focuses ontologies describe semantics web standard providing complex lexical information ontologies describing relationship lexicon ontology present model lemon aims address gaps building existing work lexical markup framework isocat data category registry skos simple knowledge organization system lexinfo lir ontology lexicon models",0
"KeywordsOnline resources Text corpora Sublexical variables Psycholinguistics Greek Syllabification Bigrams ","iplr an online resource for greek word level and sublexical information","We present a new online psycholinguistic resource for Greek based on analyses of written corpora combined with text processing technologies developed at the Institute for Language & Speech Processing (ILSP), Greece. The “ILSP PsychoLinguistic Resource” (IPLR) is a freely accessible service via a dedicated web page, at http://speech.ilsp.gr/iplr. IPLR provides analyses of user-submitted letter strings (words and nonwords) as well as frequency tables for important units and conditions such as syllables, bigrams, and neighbors, calculated over two word lists based on printed text corpora and their phonetic transcription. Online tools allow retrieval of words matching user-specified orthographic or phonetic patterns. All results and processing code (in the Python programming language) are freely available for noncommercial educational or research use.","Language Resources and Evaluation",2012,"Yes"," online resources text corpora sublexical variables psycholinguistics greek syllabification bigrams iplr online resource greek word level sublexical information present online psycholinguistic resource greek based analyses written corpora combined text processing technologies developed institute language speech processing ilsp greece ilsp psycholinguistic resource iplr freely accessible service dedicated web page httpspeechilspgriplr iplr analyses user submitted letter strings words nonwords frequency tables important units conditions syllables bigrams neighbors calculated word lists based printed text corpora phonetic transcription online tools retrieval words matching user orthographic phonetic patterns results processing code python programming language freely noncommercial educational research ",1
"KeywordsNominalization Argument structure Semantic corpus annotation Heuristic rules ","annotating the argument structure of deverbal nominalizations in spanish","Over recent years, there has been a growing interest in the computational treatment of nominalized Noun Phrases due to the rich semantic information they contain. These Noun Phrases can be understood as verbal paraphrases and, just like them, they can also denote argument and thematic-role relations. This paper presents the methodology followed to annotate the argument structure of deverbal nominalizations in the Spanish AnCora-Es corpus. We focus on the automated annotation process that is mostly based on the semantic information specified in a verbal lexicon but also on the syntactic and semantic information annotated in the corpus. The heuristic rules that make use of this information rely on linguistic assumptions that are also evaluated as we evaluate the reliability of the automated process. The automated annotation was manually checked in order to ensure the accuracy of the final resource. We demonstrate its feasibility (77% F-measure) and show that it facilitates corpus annotation, which is always a time-consuming and costly process. The result is the enrichment of the AnCora-Es corpus with the argument structure and thematic roles of deverbal nominalizations. It is the first Spanish corpus with this kind of information that is freely available.","Language Resources and Evaluation",2012,"Yes"," nominalization argument structure semantic corpus annotation heuristic rules annotating argument structure deverbal nominalizations spanish recent years growing interest computational treatment nominalized noun phrases due rich semantic information noun phrases understood verbal paraphrases denote argument thematic role relations paper presents methodology annotate argument structure deverbal nominalizations spanish ancora es corpus focus automated annotation process based semantic information verbal lexicon syntactic semantic information annotated corpus heuristic rules make information rely linguistic assumptions evaluated evaluate reliability automated process automated annotation manually checked order ensure accuracy final resource demonstrate feasibility measure show facilitates corpus annotation time consuming costly process result enrichment ancora es corpus argument structure thematic roles deverbal nominalizations spanish corpus kind information freely ",1
"KeywordsText mining Information extraction Multilinguality Saving effort Rule-based Machine learning Cross-lingual projection Methods Algorithms Sentiment analysis Summarisation Quotation recognition String similarity calculation Media monitoring ","a survey of methods to ease the development of highly multilingual text mining applications","Multilingual text processing is useful because the information content found in different languages is complementary, both regarding facts and opinions. While Information Extraction and other text mining software can, in principle, be developed for many languages, most text analysis tools have only been applied to small sets of languages because the development effort per language is large. Self-training tools obviously alleviate the problem, but even the effort of providing training data and of manually tuning the results is usually considerable. In this paper, we gather insights by various multilingual system developers on how to minimise the effort of developing natural language processing applications for many languages. We also explain the main guidelines underlying our own effort to develop complex text mining software for tens of languages. While these guidelines—most of all: extreme simplicity—can be very restrictive and limiting, we believe to have shown the feasibility of the approach through the development of the Europe Media Monitor (EMM) family of applications (http://emm.newsbrief.eu/overview.html). EMM is a set of complex media monitoring tools that process and analyse up to 100,000 online news articles per day in between twenty and fifty languages. We will also touch upon the kind of language resources that would make it easier for all to develop highly multilingual text mining applications. We will argue that—to achieve this—the most needed resources would be freely available, simple, parallel and uniform multilingual dictionaries, corpora and software tools.","Language Resources and Evaluation",2012,"Yes"," text mining information extraction multilinguality saving effort rule based machine learning cross lingual projection methods algorithms sentiment analysis summarisation quotation recognition string similarity calculation media monitoring survey methods ease development highly multilingual text mining applications multilingual text processing information content found languages complementary facts opinions information extraction text mining software principle developed languages text analysis tools applied small sets languages development effort language large training tools alleviate problem effort providing training data manually tuning results considerable paper gather insights multilingual system developers minimise effort developing natural language processing applications languages explain main guidelines underlying effort develop complex text mining software tens languages guidelines extreme simplicity restrictive limiting shown feasibility approach development europe media monitor emm family applications httpemmnewsbriefeuoverviewhtml emm set complex media monitoring tools process analyse online news articles day twenty fifty languages touch kind language resources make easier develop highly multilingual text mining applications argue achieve needed resources freely simple parallel uniform multilingual dictionaries corpora software tools",1
"KeywordsFrameNet Frame semantics Lexical semantics interoperability WordNet Lexicon Corpus Semantic role Thematic role Lexical resource Crowdsourcing ","framenet current collaborations and future goals","This paper will focus on recent and near-term future developments at FrameNet (FN) and the interoperability issues they raise. We begin by discussing the current state of the Berkeley FN database including major changes in the data format for the latest data release. We then briefly review two recent local projects, ""Rapid Vanguarding”, which has created a new interface for the frame and lexical unit definition process based on the Word Sketch Engine of Kilgarriff et al. (2004), and “Beyond the Core”, which has developed tools for annotating constructions, and created a sample “construction” of especially “interesting” constructions which are neither simply lexical nor easy for the standard parsers to parse. We also cover two current collaborations, FN’s part in the development of the manually annotated subcorpus of the American National Corpus, and a pilot study on aligning WordNet and FrameNet, to exploit the complementary strengths of these quite different resources. We discuss FN-related research on Spanish, Japanese, German (SALSA), Chinese and other languages, and the language-independence of frames, along with interesting FN-related work by others, and a sketch of a large group of image-schematic frames which are now being added to FN. We close with some ideas about how FrameNet can be opened up, to allow broader participation in the development process without losing precision and coherence, including a small-scale study on acquiring data for FN using Amazon’s Mechanical Turk crowd-sourcing system.","Language Resources and Evaluation",2012,"No"," framenet frame semantics lexical semantics interoperability wordnet lexicon corpus semantic role thematic role lexical resource crowdsourcing framenet current collaborations future goals paper focus recent term future developments framenet fn interoperability issues raise begin discussing current state berkeley fn database including major data format latest data release briefly review recent local projects rapid vanguarding created interface frame lexical unit definition process based word sketch engine kilgarriff al core developed tools annotating constructions created sample construction interesting constructions simply lexical easy standard parsers parse cover current collaborations fn part development manually annotated subcorpus american national corpus pilot study aligning wordnet framenet exploit complementary strengths resources discuss fn related research spanish japanese german salsa chinese languages language independence frames interesting fn related work sketch large group image schematic frames added fn close ideas framenet opened broader participation development process losing precision coherence including small scale study acquiring data fn amazon mechanical turk crowd sourcing system",0
NA,"e francesconi s montemagni w peters d tiscornia semantic processing of legal texts where the language of law meets the law of language lecture notes in computer science lecture notes in artificial intelligence vol 6036","The volume Semantic Processing of Legal Texts contains a total of thirteen papers that share the common theme of processing legal documents. One undisputable merit of the book is that of being the first collection to focus specifically on computational linguistic aspects of this task. Otherwise the papers in the collection represent a variety of topics as distinct as ontology engineering, multi-label classification, and translation quality assurance. They deal with theoretical foundations as well as commercial applications, and the authors’ affiliations range from universities to industry. The book is based on selected papers presented at the first workshop on Semantic Processing of Legal Texts (held at LREC 2008 in Marrakech) but comprises further, invited contributions.","Language Resources and Evaluation",2012,"No"," francesconi montemagni peters tiscornia semantic processing legal texts language law meets law language lecture notes computer science lecture notes artificial intelligence vol volume semantic processing legal texts total thirteen papers share common theme processing legal documents undisputable merit book collection focus specifically computational linguistic aspects task papers collection represent variety topics distinct ontology engineering multi label classification translation quality assurance deal theoretical foundations commercial applications authors affiliations range universities industry book based selected papers presented workshop semantic processing legal texts held lrec marrakech comprises invited contributions",0
"KeywordsAppraisal Corpus annotation Inter-annotator agreement Opinion Subjectivity Systemic Functional Linguistics ","annotating expressions of appraisal in english","In the context of Systemic Functional Linguistics, Appraisal is a theory describing the types of language utilised in communicating emotion and opinion. Robust automatic analyses of Appraisal could contribute in a number of ways to computational sentiment analysis by: distinguishing various types of evaluation, for example affect, ethics or aesthetics; discriminating between an author’s opinions and the opinions of authors referenced by the author and determining the strength of evaluations. This paper reviews the typology described by Appraisal, presents a methodology for annotating Appraisal, and the use of this to annotate a corpus of book reviews. It discusses an inter-annotator agreement study, and considers instances of systematic disagreement that indicate areas in which Appraisal may be refined or clarified. Although the annotation task is difficult, there are many instances where the annotators agree; these are used to create a gold-standard corpus for future experimentation with Appraisal.","Language Resources and Evaluation",2012,"Yes"," appraisal corpus annotation inter annotator agreement opinion subjectivity systemic functional linguistics annotating expressions appraisal english context systemic functional linguistics appraisal theory describing types language utilised communicating emotion opinion robust automatic analyses appraisal contribute number ways computational sentiment analysis distinguishing types evaluation affect ethics aesthetics discriminating author opinions opinions authors referenced author determining strength evaluations paper reviews typology appraisal presents methodology annotating appraisal annotate corpus book reviews discusses inter annotator agreement study considers instances systematic disagreement areas appraisal refined clarified annotation task difficult instances annotators agree create gold standard corpus future experimentation appraisal",1
"KeywordsWord sense annotation Multilabel learning Inter-annotator reliability ","multiplicity and word sense evaluating and learning from multiply labeled word sense annotations","Supervised machine learning methods to model word sense often rely on human labelers to provide a single, ground truth label for each word in its context. We examine issues in establishing ground truth word sense labels using a fine-grained sense inventory from WordNet. Our data consist of a sentence corpus of 1,000 sentences: 100 for each of ten moderately polysemous words. Each word was given multiple sense labels—or a multilabel—from trained and untrained annotators. The multilabels give a nuanced representation of the degree of agreement on instances. A suite of assessment metrics is used to analyze the sets of multilabels, such as comparisons of sense distributions across annotators. Our assessment indicates that the general annotation procedure is reliable, but that words differ regarding how reliably annotators can assign WordNet sense labels, independent of the number of senses. We also investigate the performance of an unsupervised machine learning method to infer ground truth labels from various combinations of labels from the trained and untrained annotators. We find tentative support for the hypothesis that performance depends on the quality of the set of multilabels, independent of the number of labelers or their training.","Language Resources and Evaluation",2012,"Yes"," word sense annotation multilabel learning inter annotator reliability multiplicity word sense evaluating learning multiply labeled word sense annotations supervised machine learning methods model word sense rely human labelers provide single ground truth label word context examine issues establishing ground truth word sense labels fine grained sense inventory wordnet data consist sentence corpus sentences ten moderately polysemous words word multiple sense labels multilabel trained untrained annotators multilabels give nuanced representation degree agreement instances suite assessment metrics analyze sets multilabels comparisons sense distributions annotators assessment general annotation procedure reliable words differ reliably annotators assign wordnet sense labels independent number senses investigate performance unsupervised machine learning method infer ground truth labels combinations labels trained untrained annotators find tentative support hypothesis performance depends quality set multilabels independent number labelers training",1
"KeywordsLinguistic annotation Multi-layer annotation Conflicting tokenizations Tokenization alignment Corpus linguistics ","by all these lovely tokens merging conflicting tokenizations","Given the contemporary trend to modular NLP architectures and multiple annotation frameworks, the existence of concurrent tokenizations of the same text represents a pervasive problem in everyday’s NLP practice and poses a non-trivial theoretical problem to the integration of linguistic annotations and their interpretability in general. This paper describes a solution for integrating different tokenizations using a standoff XML format, and discusses the consequences from a corpus-linguistic perspective.","Language Resources and Evaluation",2012,"No"," linguistic annotation multi layer annotation conflicting tokenizations tokenization alignment corpus linguistics lovely tokens merging conflicting tokenizations contemporary trend modular nlp architectures multiple annotation frameworks existence concurrent tokenizations text represents pervasive problem everyday nlp practice poses trivial theoretical problem integration linguistic annotations interpretability general paper describes solution integrating tokenizations standoff xml format discusses consequences corpus linguistic perspective",0
"KeywordsSpeech corpus Alcohol detection Intoxication Speaker features and forensic phonetics ","alcohol language corpus the first public corpus of alcoholized german speech","The Alcohol Language Corpus (ALC) is the first publicly available speech corpus comprising intoxicated and sober speech of 162 female and male German speakers. Recordings are done in the automotive environment to allow for the development of automatic alcohol detection and to ensure a consistent acoustic environment for the alcoholized and the sober recording. The recorded speech covers a variety of contents and speech styles. Breath and blood alcohol concentration measurements are provided for all speakers. A transcription according to SpeechDat/Verbmobil standards and disfluency tagging as well as an automatic phonetic segmentation are part of the corpus. An Emu version of ALC allows easy access to basic speech parameters as well as the us of R for statistical analysis of selected parts of ALC. ALC is available without restriction for scientific or commercial use at the Bavarian Archive for Speech Signals.","Language Resources and Evaluation",2012,"Yes"," speech corpus alcohol detection intoxication speaker features forensic phonetics alcohol language corpus public corpus alcoholized german speech alcohol language corpus alc publicly speech corpus comprising intoxicated sober speech female male german speakers recordings automotive environment development automatic alcohol detection ensure consistent acoustic environment alcoholized sober recording recorded speech covers variety contents speech styles breath blood alcohol concentration measurements provided speakers transcription speechdatverbmobil standards disfluency tagging automatic phonetic segmentation part corpus emu version alc easy access basic speech parameters statistical analysis selected parts alc alc restriction scientific commercial bavarian archive speech signals",1
"KeywordsMulti-modal corpus Referring expressions Collaborative task Japanese ","rex j japanese referring expression corpus of situated dialogs","Identifying objects in conversation is a fundamental human capability necessary to achieve efficient collaboration on any real world task. Hence the deepening of our understanding of human referential behaviour is indispensable for the creation of systems that collaborate with humans in a meaningful way. We present the construction of REX-J, a multi-modal Japanese corpus of referring expressions in situated dialogs, based on the collaborative task of solving the Tangram puzzle. This corpus contains 24 dialogs with over 4 h of recordings and over 1,400 referring expressions. We outline the characteristics of the collected data and point out the important differences from previous corpora. The corpus records extra-linguistic information during the interaction (e.g. the position of pieces, the actions on the pieces) in synchronization with the participants’ utterances. This in turn allows us to discuss the importance of creating a unified model of linguistic and extra-linguistic information from a new perspective. Demonstrating the potential uses of this corpus, we present the analysis of a specific type of referring expression (“action-mentioning expression”) as well as the results of research into the generation of demonstrative pronouns. Furthermore, we discuss some perspectives on potential uses of this corpus as well as our planned future work, underlining how it is a valuable addition to the existing databases in the community for the study and modeling of referring expressions in situated dialog.","Language Resources and Evaluation",2012,"Yes"," multi modal corpus referring expressions collaborative task japanese rex japanese referring expression corpus situated dialogs identifying objects conversation fundamental human capability achieve efficient collaboration real world task deepening understanding human referential behaviour indispensable creation systems collaborate humans meaningful present construction rex multi modal japanese corpus referring expressions situated dialogs based collaborative task solving tangram puzzle corpus dialogs recordings referring expressions outline characteristics collected data point important differences previous corpora corpus records extra linguistic information interaction position pieces actions pieces synchronization participants utterances turn discuss importance creating unified model linguistic extra linguistic information perspective demonstrating potential corpus present analysis specific type referring expression action mentioning expression results research generation demonstrative pronouns discuss perspectives potential corpus planned future work underlining valuable addition existing databases community study modeling referring expressions situated dialog",1
"KeywordsText corpora Corpus annotation Emotional ontology Emotional categories Emotional dimensions ","emotales creating a corpus of folk tales with emotional annotations","Emotions are inherent to any human activity, including human–computer interactions, and that is the reason why recognizing emotions expressed in natural language is becoming a key feature for the design of more natural user interfaces. In order to obtain useful corpora for this purpose, the manual classification of texts according to their emotional content has been the technique most commonly used by the research community. The use of corpora is widespread in Natural Language Processing, and the existing corpora annotated with emotions support the development, training and evaluation of systems using this type of data. In this paper we present the development of an annotated corpus oriented to the narrative domain, called EmoTales, which uses two different approaches to represent emotional states: emotional categories and emotional dimensions. The corpus consists of a collection of 1,389 English sentences from 18 different folk tales, annotated by 36 different people. Our model of the corpus development process includes a post-processing stage performed after the annotation of the corpus, in which a reference value for each sentence was chosen by taking into account the tags assigned by annotators and some general knowledge about emotions, which is codified in an ontology. The whole process is presented in detail, and revels significant results regarding the corpus such as inter-annotator agreement, while discussing topics such as how human annotators deal with emotional content when performing their work, and presenting some ideas for the application of this corpus that may inspire the research community to develop new ways to annotate corpora using a large set of emotional tags.","Language Resources and Evaluation",2012,"Yes"," text corpora corpus annotation emotional ontology emotional categories emotional dimensions emotales creating corpus folk tales emotional annotations emotions inherent human activity including human computer interactions reason recognizing emotions expressed natural language key feature design natural user interfaces order obtain corpora purpose manual classification texts emotional content technique commonly research community corpora widespread natural language processing existing corpora annotated emotions support development training evaluation systems type data paper present development annotated corpus oriented narrative domain called emotales approaches represent emotional states emotional categories emotional dimensions corpus consists collection english sentences folk tales annotated people model corpus development process includes post processing stage performed annotation corpus reference sentence chosen taking account tags assigned annotators general knowledge emotions codified ontology process presented detail revels significant results corpus inter annotator agreement discussing topics human annotators deal emotional content performing work presenting ideas application corpus inspire research community develop ways annotate corpora large set emotional tags",1
"KeywordsPart-of-speech tagging Maximum entropy models Morphosyntactic lexicon French Language resource development ","coupling an annotated corpus and a lexicon for state of the art pos tagging","This paper investigates how to best couple hand-annotated data with information extracted from an external lexical resource to improve part-of-speech tagging performance. Focusing mostly on French tagging, we introduce a maximum entropy Markov model-based tagging system that is enriched with information extracted from a morphological resource. This system gives a 97.75 % accuracy on the French Treebank, an error reduction of 25 % (38 % on unknown words) over the same tagger without lexical information. We perform a series of experiments that help understanding how this lexical information helps improving tagging accuracy. We also conduct experiments on datasets and lexicons of varying sizes in order to assess the best trade-off between annotating data versus developing a lexicon. We find that the use of a lexicon improves the quality of the tagger at any stage of development of either resource, and that for fixed performance levels the availability of the full lexicon consistently reduces the need for supervised data by at least one half.","Language Resources and Evaluation",2012,"No"," part speech tagging maximum entropy models morphosyntactic lexicon french language resource development coupling annotated corpus lexicon state art pos tagging paper investigates couple hand annotated data information extracted external lexical resource improve part speech tagging performance focusing french tagging introduce maximum entropy markov model based tagging system enriched information extracted morphological resource system accuracy french treebank error reduction unknown words tagger lexical information perform series experiments understanding lexical information helps improving tagging accuracy conduct experiments datasets lexicons varying sizes order assess trade annotating data versus developing lexicon find lexicon improves quality tagger stage development resource fixed performance levels availability full lexicon consistently reduces supervised data half",0
"KeywordsCooperation Emotions Multimodal Dialogue Annotation ","the rovereto emotion and cooperation corpus a new resource to investigate cooperation and emotions","The Rovereto Emotion and Cooperation Corpus (RECC) is a new resource collected to investigate the relationship between cooperation and emotions in an interactive setting. Previous attempts at collecting corpora to study emotions have shown that this data are often quite difficult to classify and analyse, and coding schemes to analyse emotions are often found not to be reliable. We collected a corpus of task-oriented (MapTask-style) dialogues in Italian, in which the segments of emotional interest are identified using psycho-physiological indexes (Heart Rate and Galvanic Skin Conductance) which are highly reliable. We then annotated these segments in accordance with novel multimodal annotation schemes for cooperation (in terms of effort) and facial expressions (an indicator of emotional state). High agreement was obtained among coders on all the features. The RECC corpus is to our knowledge the first resource with psycho-physiological data aligned with verbal and nonverbal behaviour data.","Language Resources and Evaluation",2012,"Yes"," cooperation emotions multimodal dialogue annotation rovereto emotion cooperation corpus resource investigate cooperation emotions rovereto emotion cooperation corpus recc resource collected investigate relationship cooperation emotions interactive setting previous attempts collecting corpora study emotions shown data difficult classify analyse coding schemes analyse emotions found reliable collected corpus task oriented maptask style dialogues italian segments emotional interest identified psycho physiological indexes heart rate galvanic skin conductance highly reliable annotated segments accordance multimodal annotation schemes cooperation terms effort facial expressions indicator emotional state high agreement obtained coders features recc corpus knowledge resource psycho physiological data aligned verbal nonverbal behaviour data",1
"KeywordsModern Standard Arabic Speech corpus Text corpus Phonetically rich Phonetically balanced Automatic continuous speech recognition ","phonetically rich and balanced text and speech corpora for arabic language","This paper describes the preparation, recording, analyzing, and evaluation of a new speech corpus for Modern Standard Arabic (MSA). The speech corpus contains a total of 415 sentences recorded by 40 (20 male and 20 female) Arabic native speakers from 11 different Arab countries representing three major regions (Levant, Gulf, and Africa). Three hundred and sixty seven sentences are considered as phonetically rich and balanced, which are used for training Arabic Automatic Speech Recognition (ASR) systems. The rich characteristic is in the sense that it must contain all phonemes of Arabic language, whereas the balanced characteristic is in the sense that it must preserve the phonetic distribution of Arabic language. The remaining 48 sentences are created for testing purposes, which are mostly foreign to the training sentences and there are hardly any similarities in words. In order to evaluate the speech corpus, Arabic ASR systems were developed using the Carnegie Mellon University (CMU) Sphinx 3 tools at both training and testing/decoding levels. The speech engine uses 3-emitting state Hidden Markov Models (HMM) for tri-phone based acoustic models. Based on experimental analysis of about 8 h of training speech data, the acoustic model is best using continuous observation’s probability model of 16 Gaussian mixture distributions and the state distributions were tied to 500 senones. The language model contains uni-grams, bi-grams, and tri-grams. For same speakers with different sentences, Arabic ASR systems obtained average Word Error Rate (WER) of 9.70%. For different speakers with same sentences, Arabic ASR systems obtained average WER of 4.58%, whereas for different speakers with different sentences, Arabic ASR systems obtained average WER of 12.39%.","Language Resources and Evaluation",2012,"Yes"," modern standard arabic speech corpus text corpus phonetically rich phonetically balanced automatic continuous speech recognition phonetically rich balanced text speech corpora arabic language paper describes preparation recording analyzing evaluation speech corpus modern standard arabic msa speech corpus total sentences recorded male female arabic native speakers arab countries representing major regions levant gulf africa hundred sixty sentences considered phonetically rich balanced training arabic automatic speech recognition asr systems rich characteristic sense phonemes arabic language balanced characteristic sense preserve phonetic distribution arabic language remaining sentences created testing purposes foreign training sentences similarities words order evaluate speech corpus arabic asr systems developed carnegie mellon university cmu sphinx tools training testingdecoding levels speech engine emitting state hidden markov models hmm tri phone based acoustic models based experimental analysis training speech data acoustic model continuous observation probability model gaussian mixture distributions state distributions tied senones language model uni grams bi grams tri grams speakers sentences arabic asr systems obtained average word error rate wer speakers sentences arabic asr systems obtained average wer speakers sentences arabic asr systems obtained average wer ",1
"KeywordsMorphosyntactic annotation Multilinguality Language encoding standards ","multext east morphosyntactic resources for central and eastern european languages","The paper presents the MULTEXT-East language resources, a multilingual dataset for language engineering research, focused on the morphosyntactic level of linguistic description. The MULTEXT-East dataset includes the morphosyntactic specifications, morphosyntactic lexica, and a parallel corpus, the novel “1984” by George Orwell, which is sentence aligned and contains hand-validated morphosyntactic descriptions and lemmas. The resources are uniformly encoded in XML, using the Text Encoding Initiative Guidelines, TEI P5, and cover 16 languages, mainly from Central and Eastern Europe: Bulgarian, Croatian, Czech, English, Estonian, Hungarian, Macedonian, Persian, Polish, Resian, Romanian, Russian, Serbian, Slovak, Slovene, and Ukrainian. This dataset, unique in terms of languages covered and the wealth of encoding, is extensively documented, and freely available for research purposes. The paper overviews the MULTEXT-East resources by type and language and gives some conclusions and directions for further work.","Language Resources and Evaluation",2012,"Yes"," morphosyntactic annotation multilinguality language encoding standards multext east morphosyntactic resources central eastern european languages paper presents multext east language resources multilingual dataset language engineering research focused morphosyntactic level linguistic description multext east dataset includes morphosyntactic specifications morphosyntactic lexica parallel corpus george orwell sentence aligned hand validated morphosyntactic descriptions lemmas resources uniformly encoded xml text encoding initiative guidelines tei p cover languages central eastern europe bulgarian croatian czech english estonian hungarian macedonian persian polish resian romanian russian serbian slovak slovene ukrainian dataset unique terms languages covered wealth encoding extensively documented freely research purposes paper overviews multext east resources type language conclusions directions work",1
"KeywordsOperational interoperability Conceptual interoperability Parsing Tagging Adverbial clause Speech Writing The International Corpus of English ","creating an interoperable language resource for interoperable linguistic studies","There are two different levels of interoperability for language resources: operational interoperability and conceptual interoperability. The former refers to the standardization of the formal aspects of language resources so that different resources can work together. The latter refers to the standardization of the notional representation of the semantic content of the analysis. This article addresses both issues but focuses on the latter through a description of the annotation and analysis of the International Corpus of English, which is a corpus for the study of English as a global language. The project is parameterised by component, regional sub-corpora and a set of pre-defined textual categories. The one-million-word British component has been constructed, grammatically tagged, and syntactically parsed. This article is first of all a description of steps taken to ensure conformity within the project. These include corpus design, part-of-speech tagging, and syntactic parsing. The article will then present a study that examines the use of adverbial clauses across speech and writing, illustrating the imminent necessity for interoperable analysis of linguistic data.","Language Resources and Evaluation",2012,"Yes"," operational interoperability conceptual interoperability parsing tagging adverbial clause speech writing international corpus english creating interoperable language resource interoperable linguistic studies levels interoperability language resources operational interoperability conceptual interoperability refers standardization formal aspects language resources resources work refers standardization notional representation semantic content analysis article addresses issues focuses description annotation analysis international corpus english corpus study english global language project parameterised component regional corpora set pre defined textual categories million word british component constructed grammatically tagged syntactically parsed article description steps ensure conformity project include corpus design part speech tagging syntactic parsing article present study examines adverbial clauses speech writing illustrating imminent necessity interoperable analysis linguistic data",1
"KeywordsAutomatic annotation Intrinsic and extrinsic evaluation Syntactic alternation Dative alternation Logistic regression ","evaluating automatic annotation automatically detecting and enriching instances of the dative alternation","In this article, we automatically create two large and richly annotated data sets for studying the English dative alternation. With an intrinsic and an extrinsic evaluation, we address the question of whether such data sets that are obtained and enriched automatically are suitable for linguistic research, even if they contain errors. The extrinsic evaluation consists of building logistic regression models with these data sets. We conclude that the automatic approach for detecting instances of the dative alternation still needs human intervention, but that it is indeed possible to annotate the instances with features that are syntactic, semantic and discourse-related in nature. Only the automatic classification of the concreteness of nouns is problematic.","Language Resources and Evaluation",2012,"Yes"," automatic annotation intrinsic extrinsic evaluation syntactic alternation dative alternation logistic regression evaluating automatic annotation automatically detecting enriching instances dative alternation article automatically create large richly annotated data sets studying english dative alternation intrinsic extrinsic evaluation address question data sets obtained enriched automatically suitable linguistic research errors extrinsic evaluation consists building logistic regression models data sets conclude automatic approach detecting instances dative alternation human intervention annotate instances features syntactic semantic discourse related nature automatic classification concreteness nouns problematic",1
"KeywordsSuperlatives Annotation Wikipedia Resources Adjectives ","textwiki a superlative resource","Although superlatives are commonly used in natural language, so far there has been no large-scale computational investigation of the types of comparisons they express. This article describes a comprehensive annotation scheme for superlatives, which classifies superlatives according to their surface forms and motivates an initial focus on so-called “ISA superlatives”. This type of superlative comparison is especially suitable for a computational approach because both their targets and comparison sets are explicitly realised in the text, and the proposed annotation scheme offers guidelines for annotating the spans of such comparative elements. The annotations are tested and evaluated on 500 tokens of superlatives with good inter-annotator agreement. In addition to providing a platform for investigating superlatives on a larger scale, this research also introduces a new text-based Wikipedia corpus in which all superlative instances have been annotated according to the proposed annotation scheme, and which has been used to develop a tool that can reliably distinguish between different superlative types, and identify the comparative components of ISA superlatives.","Language Resources and Evaluation",2012,"Yes"," superlatives annotation wikipedia resources adjectives textwiki superlative resource superlatives commonly natural language large scale computational investigation types comparisons express article describes comprehensive annotation scheme superlatives classifies superlatives surface forms motivates initial focus called isa superlatives type superlative comparison suitable computational approach targets comparison sets explicitly realised text proposed annotation scheme offers guidelines annotating spans comparative elements annotations tested evaluated tokens superlatives good inter annotator agreement addition providing platform investigating superlatives larger scale research introduces text based wikipedia corpus superlative instances annotated proposed annotation scheme develop tool reliably distinguish superlative types identify comparative components isa superlatives",1
"KeywordsAbstract anaphora Abstract entities Coreference annotation Semantic annotation ","annotating abstract anaphora","In this paper, we present first results from annotating abstract (discourse-deictic) anaphora in German. Our annotation guidelines provide linguistic tests for identifying the antecedent, and for determining the semantic types of both the antecedent and the anaphor. The corpus consists of selected speaker turns from the Europarl corpus. To date, 100 texts have been annotated according to these guidelines. The annotations show that anaphoric personal and demonstrative pronouns differ with respect to the distance to their antecedents. A semantic analysis reveals that, contrary to suggestions put forward in the literature, referents of anaphors do not tend to be more abstract than the referents of their antecedents.","Language Resources and Evaluation",2012,"Yes"," abstract anaphora abstract entities coreference annotation semantic annotation annotating abstract anaphora paper present results annotating abstract discourse deictic anaphora german annotation guidelines provide linguistic tests identifying antecedent determining semantic types antecedent anaphor corpus consists selected speaker turns europarl corpus date texts annotated guidelines annotations show anaphoric personal demonstrative pronouns differ respect distance antecedents semantic analysis reveals contrary suggestions put forward literature referents anaphors tend abstract referents antecedents",1
"KeywordsArabic language Text mining Named Entity Recognition Event detection Morphological analysis Root extraction ","a real time named entity recognition system for arabic text mining","Arabic is the most widely spoken language in the Arab World. Most people of the Islamic World understand the Classic Arabic language because it is the language of the Qur’an. Despite the fact that in the last decade the number of Arabic Internet users (Middle East and North and East of Africa) has increased considerably, systems to analyze Arabic digital resources automatically are not as easily available as they are for English. Therefore, in this work, an attempt is made to build a real time Named Entity Recognition system that can be used in web applications to detect the appearance of specific named entities and events in news written in Arabic. Arabic is a highly inflectional language, thus we will try to minimize the impact of Arabic affixes on the quality of the pattern recognition model applied to identify named entities. These patterns are built up by processing and integrating different gazetteers, from DBPedia (http://dbpedia.org/About, 2009) to GATE (A general architecture for text engineering, 2009) and ANERGazet (http://users.dsic.upv.es/grupos/nle/?file=kop4.php).","Language Resources and Evaluation",2012,"Yes"," arabic language text mining named entity recognition event detection morphological analysis root extraction real time named entity recognition system arabic text mining arabic widely spoken language arab world people islamic world understand classic arabic language language qur fact decade number arabic internet users middle east north east africa increased considerably systems analyze arabic digital resources automatically easily english work attempt made build real time named entity recognition system web applications detect appearance specific named entities events news written arabic arabic highly inflectional language minimize impact arabic affixes quality pattern recognition model applied identify named entities patterns built processing integrating gazetteers dbpedia httpdbpediaorg gate general architecture text engineering anergazet httpusersdsicupvesgruposnlefilekopphp",1
NA,"alexander mehler serge sharoff and marina santini eds genres on the web computational models and emprical studies","This comprehensive book makes many original contributions to the field of genres on the web. The identification and characterization of genres is of obvious interest to “pure” linguistics, but as this book makes clear, there are some important practical applications. Chief amongst these will be the advent of genre-aware search engines, where users will be able to specify not only their topics of interest, but the desired genre of the returned web pages, as in the WEGA search engine described in this book by Stein et al. Crowston et al. give the example of someone wishing to buy a digital camera. A traditional search engine would return pages on the topic of the specified brand of digital cameras, most of which will just be the web sites of sellers. But what the buyer really wants is information about this type of camera in certain genres only, such as product reviews and opinion-bearing blogs, which provide the opinions of people who have already bought that camera. The...","Language Resources and Evaluation",2012,"No"," alexander mehler serge sharoff marina santini eds genres web computational models emprical studies comprehensive book makes original contributions field genres web identification characterization genres obvious interest pure linguistics book makes clear important practical applications chief advent genre aware search engines users topics interest desired genre returned web pages wega search engine book stein al crowston al give wishing buy digital camera traditional search engine return pages topic brand digital cameras web sites sellers buyer information type camera genres product reviews opinion bearing blogs provide opinions people bought camera ",0
"KeywordsLinguistic annotation Standards Language resources Annotation processing software ","bridging the gaps interoperability for language engineering architectures using graf","This paper explores interoperability for data represented using the Graph Annotation Framework (GrAF) (Ide and Suderman, 2007) and the data formats utilized by two general-purpose annotation systems: the General Architecture for Text Engineering (GATE) (Cunningham et al., 2002) and the Unstructured Information Management Architecture (UIMA) (Ferrucci and Lally in Nat Lang Eng 10(3–4):327–348, 2004). GrAF is intended to serve as a “pivot” to enable interoperability among different formats, and both GATE and UIMA are at least implicitly designed with an eye toward interoperability with other formats and tools. We describe the steps required to perform a round-trip rendering from GrAF to GATE and GrAF to UIMA CAS and back again, and outline the commonalities as well as the differences and gaps that came to light in the process.","Language Resources and Evaluation",2012,"No"," linguistic annotation standards language resources annotation processing software bridging gaps interoperability language engineering architectures graf paper explores interoperability data represented graph annotation framework graf ide suderman data formats utilized general purpose annotation systems general architecture text engineering gate cunningham al unstructured information management architecture uima ferrucci lally nat lang eng graf intended serve pivot enable interoperability formats gate uima implicitly designed eye interoperability formats tools describe steps required perform round trip rendering graf gate graf uima cas back outline commonalities differences gaps light process",0
"KeywordsQuestion answering Evaluation CLEF ","question answering at the cross language evaluation forum 20032010","The paper offers an overview of the key issues raised during the 8 years’ activity of the Multilingual Question Answering Track at the Cross Language Evaluation Forum (CLEF). The general aim of the track has been to test both monolingual and cross-language Question Answering (QA) systems that process queries and documents in several European languages, also drawing attention to a number of challenging issues for research in multilingual QA. The paper gives a brief description of how the task has evolved over the years and of the way in which the data sets have been created, presenting also a short summary of the different types of questions developed. The document collections adopted in the competitions are outlined as well, and data about participation is provided. Moreover, the main measures used to evaluate system performances are explained and an overall analysis of the results achieved is presented.","Language Resources and Evaluation",2012,"No"," question answering evaluation clef question answering cross language evaluation forum paper offers overview key issues raised years activity multilingual question answering track cross language evaluation forum clef general aim track test monolingual cross language question answering qa systems process queries documents european languages drawing attention number challenging issues research multilingual qa paper description task evolved years data sets created presenting short summary types questions developed document collections adopted competitions outlined data participation provided main measures evaluate system performances explained analysis results achieved presented",0
"KeywordsLinguistic annotation Semantic role labelling Frame semantics Semi-automatic annotation ","is it worth the effort assessing the benefits of partial automatic pre labeling for frame semantic annotation","Corpora with high-quality linguistic annotations are an essential component in many NLP applications and a valuable resource for linguistic research. For obtaining these annotations, a large amount of manual effort is needed, making the creation of these resources time-consuming and costly. One attempt to speed up the annotation process is to use supervised machine-learning systems to automatically assign (possibly erroneous) labels to the data and ask human annotators to correct them where necessary. However, it is not clear to what extent these automatic pre-annotations are successful in reducing human annotation effort, and what impact they have on the quality of the resulting resource. In this article, we present the results of an experiment in which we assess the usefulness of partial semi-automatic annotation for frame labeling. We investigate the impact of automatic pre-annotation of differing quality on annotation time, consistency and accuracy. While we found no conclusive evidence that it can speed up human annotation, we found that automatic pre-annotation does increase its overall quality.","Language Resources and Evaluation",2012,"No"," linguistic annotation semantic role labelling frame semantics semi automatic annotation worth effort assessing benefits partial automatic pre labeling frame semantic annotation corpora high quality linguistic annotations essential component nlp applications valuable resource linguistic research obtaining annotations large amount manual effort needed making creation resources time consuming costly attempt speed annotation process supervised machine learning systems automatically assign possibly erroneous labels data human annotators correct clear extent automatic pre annotations successful reducing human annotation effort impact quality resulting resource article present results experiment assess usefulness partial semi automatic annotation frame labeling investigate impact automatic pre annotation differing quality annotation time consistency accuracy found conclusive evidence speed human annotation found automatic pre annotation increase quality",0
"KeywordsLinguistic annotation Annotation tools Inter-operability ","inter operability and reusability the science of annotation","Annotating linguistic data has become a major field of interest, both for supplying the necessary data for machine learning approaches to NLP applications, and as a research issue in its own right. This comprises issues of technical formats, tools, and methodologies of annotation. We provide a brief overview of these notions and then introduce the papers assembled in this special issue.","Language Resources and Evaluation",2012,"No"," linguistic annotation annotation tools inter operability inter operability reusability science annotation annotating linguistic data major field interest supplying data machine learning approaches nlp applications research issue comprises issues technical formats tools methodologies annotation provide overview notions introduce papers assembled special issue",0
"KeywordsSentence and clause structure Dependency and coordination Annotation ","annotation of sentence structure","The focus of this article is on the creation of a collection of sentences manually annotated with respect to their sentence structure. We show that the concept of linear segments—linguistically motivated units, which may be easily detected automatically—serves as a good basis for the identification of clauses in Czech. The segment annotation captures such relationships as subordination, coordination, apposition and parenthesis; based on segmentation charts, individual clauses forming a complex sentence are identified. The annotation of a sentence structure enriches a dependency-based framework with explicit syntactic information on relations among complex units like clauses. We have gathered a collection of 3,444 sentences from the Prague Dependency Treebank, which were annotated with respect to their sentence structure (these sentences comprise 10,746 segments forming 6,341 clauses). The main purpose of the project is to gain a development data—promising results for Czech NLP tools (as a dependency parser or a machine translation system for related languages) that adopt an idea of clause segmentation have been already reported. The collection of sentences with annotated sentence structure provides the possibility of further improvement of such tools.","Language Resources and Evaluation",2012,"Yes"," sentence clause structure dependency coordination annotation annotation sentence structure focus article creation collection sentences manually annotated respect sentence structure show concept linear segments linguistically motivated units easily detected automatically serves good basis identification clauses czech segment annotation captures relationships subordination coordination apposition parenthesis based segmentation charts individual clauses forming complex sentence identified annotation sentence structure enriches dependency based framework explicit syntactic information relations complex units clauses gathered collection sentences prague dependency treebank annotated respect sentence structure sentences comprise segments forming clauses main purpose project gain development data promising results czech nlp tools dependency parser machine translation system related languages adopt idea clause segmentation reported collection sentences annotated sentence structure possibility improvement tools",1
NA,"chu ren huang nicoletta calzolari aldo gangemi alessandro lenci alessandro oltramari and laurent prvot eds ontology and the lexicon a natural language processing perspective studies in natural language processing","The relationship between ontologies and natural language lexicons is a hotly debated one. An ontology is a formalized system of concepts (potentially of a specific domain) and the relations these concepts entertain. A lexicon, on the other hand, is the language component that contains the conventionalized knowledge of natural language speakers about lexical items (mostly words, but also morphemes and idioms). Ontologies ‘operate’ on the conceptual level, lexicons on the linguistic level. Ontologies systematize and relate concepts, lexicons systematize and relate words and other lexical items. However, as semantic relations between lexical items reflect meaning relatedness and meaning is essentially conceptual, both notions appear to be very close to one another (and are often wrongly used interchangeably). The interplay of and mapping between ontologies and lexical resources is therefore a vital and challenging field of research, one which has gained additional momentum and importance...","Language Resources and Evaluation",2012,"No"," chu ren huang nicoletta calzolari aldo gangemi alessandro lenci alessandro oltramari laurent prvot eds ontology lexicon natural language processing perspective studies natural language processing relationship ontologies natural language lexicons hotly debated ontology formalized system concepts potentially specific domain relations concepts entertain lexicon hand language component conventionalized knowledge natural language speakers lexical items words morphemes idioms ontologies operate conceptual level lexicons linguistic level ontologies systematize relate concepts lexicons systematize relate words lexical items semantic relations lexical items reflect meaning relatedness meaning essentially conceptual notions close wrongly interchangeably interplay mapping ontologies lexical resources vital challenging field research gained additional momentum importance",0
"KeywordsAfrican language resources Pragmatics Corpus search infrastructure ","information structure in african languages corpora and tools","In this paper, we describe tools and resources for the study of African languages developed at the Collaborative Research Centre 632 “Information Structure”. These include deeply annotated data collections of 25 sub-Saharan languages that are described together with their annotation scheme, as well as the corpus tool ANNIS, which provides unified access to a broad variety of annotations created with a range of different tools. With the application of ANNIS to several African data collections, we illustrate its suitability for the purpose of language documentation, distributed access, and the creation of data archives.","Language Resources and Evaluation",2011,"Yes"," african language resources pragmatics corpus search infrastructure information structure african languages corpora tools paper describe tools resources study african languages developed collaborative research centre information structure include deeply annotated data collections saharan languages annotation scheme corpus tool annis unified access broad variety annotations created range tools application annis african data collections illustrate suitability purpose language documentation distributed access creation data archives",1
"KeywordsContemporary Persian Corpus EAGLES-based tagset Ezafe construction Homographs ","lessons from building a persian written corpus peykare","This paper addresses some of the issues learned during the course of building a written language resource, called ‘Peykare’, for the contemporary Persian. After defining five linguistic varieties and 24 different registers based on these linguistic varieties, we collected texts for Peykare to do a linguistic analysis, including cross-register differences. For tokenization of Persian, we propose a descriptive generalization to normalize orthographic variations existing in texts. To annotate Peykare, we use EAGLES guidelines which result to have a hierarchy in the part-of-speech tags. To this aim, we apply a semi-automatic approach for the annotation methodology. In the paper, we also give a special attention to the Ezafe construction and homographs which are important in Persian text analyses.","Language Resources and Evaluation",2011,"Yes"," contemporary persian corpus eagles based tagset ezafe construction homographs lessons building persian written corpus peykare paper addresses issues learned building written language resource called peykare contemporary persian defining linguistic varieties registers based linguistic varieties collected texts peykare linguistic analysis including cross register differences tokenization persian propose descriptive generalization normalize orthographic variations existing texts annotate peykare eagles guidelines result hierarchy part speech tags aim apply semi automatic approach annotation methodology paper give special attention ezafe construction homographs important persian text analyses",1
"KeywordsPlagiarism Plagiarism detection Corpus creation Language resources ","developing a corpus of plagiarised short answers","Plagiarism is widely acknowledged to be a significant and increasing problem for higher education institutions (McCabe 2005; Judge 2008). A wide range of solutions, including several commercial systems, have been proposed to assist the educator in the task of identifying plagiarised work, or even to detect them automatically. Direct comparison of these systems is made difficult by the problems in obtaining genuine examples of plagiarised student work. We describe our initial experiences with constructing a corpus consisting of answers to short questions in which plagiarism has been simulated. This corpus is designed to represent types of plagiarism that are not included in existing corpora and will be a useful addition to the set of resources available for the evaluation of plagiarism detection systems.","Language Resources and Evaluation",2011,"Yes"," plagiarism plagiarism detection corpus creation language resources developing corpus plagiarised short answers plagiarism widely acknowledged significant increasing problem higher education institutions mccabe judge wide range solutions including commercial systems proposed assist educator task identifying plagiarised work detect automatically direct comparison systems made difficult problems obtaining genuine examples plagiarised student work describe initial experiences constructing corpus consisting answers short questions plagiarism simulated corpus designed represent types plagiarism included existing corpora addition set resources evaluation plagiarism detection systems",1
"KeywordsComputational grammar GPSG Persian language ","a computational grammar for persian based on gpsg","In this paper, we present our attempts to design and implement a large-coverage computational grammar for the Persian language based on the Generalized Phrase Structured Grammar (GPSG) model. This grammatical model was developed for continuous speech recognition (CSR) applications, but is suitable for other applications that need the syntactic analysis of Persian. In this work, we investigate various syntactic structures relevant to the modern Persian language, and then describe these structures according to a phrase structure model. Noun (N), Verb (V), Adjective (ADJ), Adverb (ADV), and Preposition (P) are considered basic syntactic categories, and X-bar theory is used to define Noun phrases, Verb phrases, Adjective phrases, Adverbial phrases, and Prepositional phrases. However, we have to extend Noun phrase levels in X-bar theory to four levels due to certain complexities in the structure of Noun phrases in the Persian language. A set of 120 grammatical rules for describing different phrase structures of Persian is extracted, and a few instances of the rules are presented in this paper. These rules cover the major syntactic structures of the modern Persian language. For evaluation, the obtained grammatical model is utilized in a bottom-up chart parser for parsing 100 Persian sentences. Our grammatical model can take 89 sentences into account. Incorporating this grammar in a Persian CSR system leads to a 31% reduction in word error rate.","Language Resources and Evaluation",2011,"No"," computational grammar gpsg persian language computational grammar persian based gpsg paper present attempts design implement large coverage computational grammar persian language based generalized phrase structured grammar gpsg model grammatical model developed continuous speech recognition csr applications suitable applications syntactic analysis persian work investigate syntactic structures relevant modern persian language describe structures phrase structure model noun verb adjective adj adverb adv preposition considered basic syntactic categories bar theory define noun phrases verb phrases adjective phrases adverbial phrases prepositional phrases extend noun phrase levels bar theory levels due complexities structure noun phrases persian language set grammatical rules describing phrase structures persian extracted instances rules presented paper rules cover major syntactic structures modern persian language evaluation obtained grammatical model utilized bottom chart parser parsing persian sentences grammatical model sentences account incorporating grammar persian csr system leads reduction word error rate",0
"KeywordsParallel corpus Swahili English Machine translation Projection of annotation African language technology ","exploring the sawa corpus collection and deployment of a parallel corpus englishswahili","Research in machine translation and corpus annotation has greatly benefited from the increasing availability of word-aligned parallel corpora. This paper presents ongoing research on the development and application of the sawa corpus, a two-million-word parallel corpus English—Swahili. We describe the data collection phase and zero in on the difficulties of finding appropriate and easily accessible data for this language pair. In the data annotation phase, the corpus was semi-automatically sentence and word-aligned and morphosyntactic information was added to both the English and Swahili portion of the corpus. The annotated parallel corpus allows us to investigate two possible uses. We describe experiments with the projection of part-of-speech tagging annotation from English onto Swahili, as well as the development of a basic statistical machine translation system for this language pair, using the parallel corpus and a consolidated database of existing English—Swahili translation dictionaries. We particularly focus on the difficulties of translating English into the morphologically more complex Bantu language of Swahili.","Language Resources and Evaluation",2011,"Yes"," parallel corpus swahili english machine translation projection annotation african language technology exploring sawa corpus collection deployment parallel corpus englishswahili research machine translation corpus annotation greatly benefited increasing availability word aligned parallel corpora paper presents ongoing research development application sawa corpus million word parallel corpus english swahili describe data collection phase difficulties finding easily accessible data language pair data annotation phase corpus semi automatically sentence word aligned morphosyntactic information added english swahili portion corpus annotated parallel corpus investigate describe experiments projection part speech tagging annotation english swahili development basic statistical machine translation system language pair parallel corpus consolidated database existing english swahili translation dictionaries focus difficulties translating english morphologically complex bantu language swahili",1
"KeywordsEllipsis Annotation Evaluation VP ellipsis ","an annotated corpus for the analysis of vp ellipsis","Verb Phrase Ellipsis (VPE) has been studied in great depth in theoretical linguistics, but empirical studies of VPE are rare. We extend the few previous corpus studies with an annotated corpus of VPE in all 25 sections of the Wall Street Journal corpus (WSJ) distributed with the Penn Treebank. We annotated the raw files using a stand-off annotation scheme that codes the auxiliary verb triggering the elided verb phrase, the start and end of the antecedent, the syntactic type of antecedent (VP, TV, NP, PP or AP), and the type of syntactic pattern between the source and target clauses of the VPE and its antecedent. We found 487 instances of VPE (including predicative ellipsis, antecedent-contained deletion, comparative constructions, and pseudo-gapping) plus 67 cases of related phenomena such as do so anaphora. Inter-annotator agreement was high, with a 0.97 average F-score for three annotators for one section of the WSJ. Our annotation is theory neutral, and has better coverage than earlier efforts that relied on automatic methods, e.g. simply searching the parsed version of the Penn Treebank for empty VP’s achieves a high precision (0.95) but low recall (0.58) when compared with our manual annotation. The distribution of VPE source–target patterns deviates highly from the standard examples found in the theoretical linguistics literature on VPE, once more underlining the value of corpus studies. The resulting corpus will be useful for studying VPE phenomena as well as for evaluating natural language processing systems equipped with ellipsis resolution algorithms, and we propose evaluation measures for VPE detection and VPE antecedent selection. The stand-off annotation is freely available for research purposes.","Language Resources and Evaluation",2011,"Yes"," ellipsis annotation evaluation vp ellipsis annotated corpus analysis vp ellipsis verb phrase ellipsis vpe studied great depth theoretical linguistics empirical studies vpe rare extend previous corpus studies annotated corpus vpe sections wall street journal corpus wsj distributed penn treebank annotated raw files stand annotation scheme codes auxiliary verb triggering elided verb phrase start end antecedent syntactic type antecedent vp tv np pp ap type syntactic pattern source target clauses vpe antecedent found instances vpe including predicative ellipsis antecedent contained deletion comparative constructions pseudo gapping cases related phenomena anaphora inter annotator agreement high average score annotators section wsj annotation theory neutral coverage earlier efforts relied automatic methods simply searching parsed version penn treebank empty vp achieves high precision low recall compared manual annotation distribution vpe source target patterns deviates highly standard examples found theoretical linguistics literature vpe underlining corpus studies resulting corpus studying vpe phenomena evaluating natural language processing systems equipped ellipsis resolution algorithms propose evaluation measures vpe detection vpe antecedent selection stand annotation freely research purposes",1
"KeywordsBlogs Sentiment analysis Corpus annotation Evaluation Polarity French language ","annotating opinionevaluation of blogs the blogoscopy corpus","The blog phenomenon is universal. Blogs are characterized by their evaluative use, in that they enable Internet users to express their opinion on a given subject. From this point of view, they are an ideal resource for the constitution of an annotated sentiment analysis corpus, crossing the subject and the opinion expressed on this subject. This paper presents the Blogoscopy corpus for the French language which was built up with personal thematic blogs. The annotation was governed by three principles: theoretical, as opinion is grounded in a linguistic theory of evaluation, practical, as every opinion is linked to an object, and methodological as annotation rules and successive phases are defined to ensure quality and thoroughness.","Language Resources and Evaluation",2011,"Yes"," blogs sentiment analysis corpus annotation evaluation polarity french language annotating opinionevaluation blogs blogoscopy corpus blog phenomenon universal blogs characterized evaluative enable internet users express opinion subject point view ideal resource constitution annotated sentiment analysis corpus crossing subject opinion expressed subject paper presents blogoscopy corpus french language built personal thematic blogs annotation governed principles theoretical opinion grounded linguistic theory evaluation practical opinion linked object methodological annotation rules successive phases defined ensure quality thoroughness",1
"KeywordsCorpus construction Specialised corpus Web-derived corpus Virtual corpus Website ranking Boilerplate removal Term recognition ","constructing specialised corpora through analysing domain representativeness of websites","The role of the Web for text corpus construction is becoming increasingly significant. However, the contribution of the Web is largely confined to building a general virtual corpus or low quality specialised corpora. In this paper, we introduce a new technique called SPARTAN for constructing specialised corpora from the Web by systematically analysing website contents. Our evaluations show that the corpora constructed using our technique are independent of the search engines employed. In particular, SPARTAN-derived corpora outperform all corpora based on existing techniques for the task of term recognition.","Language Resources and Evaluation",2011,"Yes"," corpus construction specialised corpus web derived corpus virtual corpus website ranking boilerplate removal term recognition constructing specialised corpora analysing domain representativeness websites role web text corpus construction increasingly significant contribution web largely confined building general virtual corpus low quality specialised corpora paper introduce technique called spartan constructing specialised corpora web systematically analysing website contents evaluations show corpora constructed technique independent search engines employed spartan derived corpora outperform corpora based existing techniques task term recognition",1
"KeywordsSpeech corpus Structural metadata extraction Speech disfluency Filler Slavic language ","design creation and analysis of czech corpora for structural metadata extraction from speech","Structural metadata extraction (MDE) research aims to develop techniques for automatic conversion of raw speech recognition output to forms that are more useful to humans and downstream automatic processes. The MDE annotation includes inserting boundaries of sentence-like units to the flow of speech, labeling non-content words like filled pauses and discourse markers for optional removal, and identifying sections of disfluent speech. This paper describes design, creation, and analysis of data resources for structural MDE from spoken Czech. The annotation is based on the LDC’s MDE annotation standard for English, with changes applied to accommodate specific phenomena of Czech. In addition to the necessary language-dependent modifications, we further proposed and applied several language-independent modifications slightly refining the original annotation scheme. We created two Czech MDE speech corpora—one in the domain of broadcast news and the other in the domain of broadcast conversations. Both corpora have already been published at LDC. The analysis section of this paper presents a variety of statistics about fillers, edit disfluencies, and sentence-like units. The two Czech corpora are not only compared with each other, but also with statistics relating to the available English MDE corpora. We also report the statistics indicating that edit disfluencies have a different part of speech (POS) distribution in comparison with the overall POS distribution. The findings from the corpus analysis should help guide strategies for developing automatic MDE systems.","Language Resources and Evaluation",2011,"Yes"," speech corpus structural metadata extraction speech disfluency filler slavic language design creation analysis czech corpora structural metadata extraction speech structural metadata extraction mde research aims develop techniques automatic conversion raw speech recognition output forms humans downstream automatic processes mde annotation includes inserting boundaries sentence units flow speech labeling content words filled pauses discourse markers optional removal identifying sections disfluent speech paper describes design creation analysis data resources structural mde spoken czech annotation based ldc mde annotation standard english applied accommodate specific phenomena czech addition language dependent modifications proposed applied language independent modifications slightly refining original annotation scheme created czech mde speech corpora domain broadcast news domain broadcast conversations corpora published ldc analysis section paper presents variety statistics fillers edit disfluencies sentence units czech corpora compared statistics relating english mde corpora report statistics indicating edit disfluencies part speech pos distribution comparison pos distribution findings corpus analysis guide strategies developing automatic mde systems",1
"KeywordsSpeech recognition Lwazi corpus Resource-scarce languages South African languages ","collecting and evaluating speech recognition corpora for 11 south african languages","We describe the Lwazi corpus for automatic speech recognition (ASR), a new telephone speech corpus which contains data from the eleven official languages of South Africa. Because of practical constraints, the amount of speech per language is relatively small compared to major corpora in world languages, and we report on our investigation of the stability of the ASR models derived from the corpus. We also report on phoneme distance measures across languages, and describe initial phone recognisers that were developed using this data. We find that a surprisingly small number of speakers (fewer than 50) and around 10 to 20 h of speech per language are sufficient for the purposes of acceptable phone-based recognition.","Language Resources and Evaluation",2011,"Yes"," speech recognition lwazi corpus resource scarce languages south african languages collecting evaluating speech recognition corpora south african languages describe lwazi corpus automatic speech recognition asr telephone speech corpus data eleven official languages south africa practical constraints amount speech language small compared major corpora world languages report investigation stability asr models derived corpus report phoneme distance measures languages describe initial phone recognisers developed data find surprisingly small number speakers fewer speech language sufficient purposes acceptable phone based recognition",1
"KeywordsTurkish language resources Morphological parser Morphological disambiguation Web corpus ","resources for turkish morphological processing","We present a set of language resources and tools—a morphological parser, a morphological disambiguator, and a text corpus—for exploiting Turkish morphology in natural language processing applications. The morphological parser is a state-of-the-art finite-state transducer-based implementation of Turkish morphology. The disambiguator is based on the averaged perceptron algorithm and has the best accuracy reported for Turkish in the literature. The text corpus has been compiled from the web and contains about 500 million tokens. This is the largest Turkish web corpus published.","Language Resources and Evaluation",2011,"Yes"," turkish language resources morphological parser morphological disambiguation web corpus resources turkish morphological processing present set language resources tools morphological parser morphological disambiguator text corpus exploiting turkish morphology natural language processing applications morphological parser state art finite state transducer based implementation turkish morphology disambiguator based averaged perceptron algorithm accuracy reported turkish literature text corpus compiled web million tokens largest turkish web corpus published",1
"KeywordsInformation extraction Word similarity Comparable corpora Singular value decomposition ","is singular value decomposition useful for word similarity extraction","In this paper, we analyze the behaviour of Singular Value Decomposition in a number of word similarity extraction tasks, namely acquisition of translation equivalents from comparable corpora. Special attention is paid to two different aspects: computational efficiency and extraction quality. The main objective of the paper is to describe several experiments comparing methods based on Singular Value Decomposition (SVD) to other strategies. The results lead us to conclude that SVD makes the extraction less computationally efficient and much less precise than other more basic models for the task of extracting translation equivalents from comparable corpora.","Language Resources and Evaluation",2011,"No"," information extraction word similarity comparable corpora singular decomposition singular decomposition word similarity extraction paper analyze behaviour singular decomposition number word similarity extraction tasks acquisition translation equivalents comparable corpora special attention paid aspects computational efficiency extraction quality main objective paper describe experiments comparing methods based singular decomposition svd strategies results lead conclude svd makes extraction computationally efficient precise basic models task extracting translation equivalents comparable corpora",0
"KeywordsStatistical machine translation Word reordering Statistical classification Automatic evaluation ","recursive alignment block classification technique for word reordering in statistical machine translation","Statistical machine translation (SMT) is based on alignment models which learn from bilingual corpora the word correspondences between source and target language. These models are assumed to be capable of learning reorderings. However, the difference in word order between two languages is one of the most important sources of errors in SMT. In this paper, we show that SMT can take advantage of inductive learning in order to solve reordering problems. Given a word alignment, we identify those pairs of consecutive source blocks (sequences of words) whose translation is swapped, i.e. those blocks which, if swapped, generate a correct monotonic translation. Afterwards, we classify these pairs into groups, following recursively a co-occurrence block criterion, in order to infer reorderings. Inside the same group, we allow new internal combination in order to generalize the reorder to unseen pairs of blocks. Then, we identify the pairs of blocks in the source corpora (both training and test) which belong to the same group. We swap them and we use the modified source training corpora to realign and to build the final translation system. We have evaluated our reordering approach both in alignment and translation quality. In addition, we have used two state-of-the-art SMT systems: a Phrased-based and an Ngram-based. Experiments are reported on the EuroParl task, showing improvements almost over 1 point in the standard MT evaluation metrics (mWER and BLEU).","Language Resources and Evaluation",2011,"No"," statistical machine translation word reordering statistical classification automatic evaluation recursive alignment block classification technique word reordering statistical machine translation statistical machine translation smt based alignment models learn bilingual corpora word correspondences source target language models assumed capable learning reorderings difference word order languages important sources errors smt paper show smt advantage inductive learning order solve reordering problems word alignment identify pairs consecutive source blocks sequences words translation swapped blocks swapped generate correct monotonic translation classify pairs groups recursively occurrence block criterion order infer reorderings inside group internal combination order generalize reorder unseen pairs blocks identify pairs blocks source corpora training test belong group swap modified source training corpora realign build final translation system evaluated reordering approach alignment translation quality addition state art smt systems phrased based ngram based experiments reported europarl task showing improvements point standard mt evaluation metrics mwer bleu",0
"KeywordsStatistical machine translation N-gram-based translation Linguistic knowledge Grammatical categories ","overcoming statistical machine translation limitations error analysis and proposed solutions for the catalanspanish language pair","This work aims to improve an N-gram-based statistical machine translation system between the Catalan and Spanish languages, trained with an aligned Spanish–Catalan parallel corpus consisting of 1.7 million sentences taken from El Periódico newspaper. Starting from a linguistic error analysis above this baseline system, orthographic, morphological, lexical, semantic and syntactic problems are approached using a set of techniques. The proposed solutions include the development and application of additional statistical techniques, text pre- and post-processing tasks, and rules based on the use of grammatical categories, as well as lexical categorization. The performance of the improved system is clearly increased, as is shown in both human and automatic evaluations of the system, with a gain of about 1.1 points BLEU observed in the Spanish-to-Catalan direction of translation, and a gain of about 0.5 points in the reverse direction. The final system is freely available online as a linguistic resource.","Language Resources and Evaluation",2011,"No"," statistical machine translation gram based translation linguistic knowledge grammatical categories overcoming statistical machine translation limitations error analysis proposed solutions catalanspanish language pair work aims improve gram based statistical machine translation system catalan spanish languages trained aligned spanish catalan parallel corpus consisting million sentences el peri dico newspaper starting linguistic error analysis baseline system orthographic morphological lexical semantic syntactic problems approached set techniques proposed solutions include development application additional statistical techniques text pre post processing tasks rules based grammatical categories lexical categorization performance improved system increased shown human automatic evaluations system gain points bleu observed spanish catalan direction translation gain points reverse direction final system freely online linguistic resource",0
"KeywordsTimeML Temporal information Event ordering Temporal expressions Temporal expression recognition Temporal expression normalization Natural Language Processing Annotation schemes Temporal reasoning ","automatic transformation from tides to timeml annotation","Until recently, most systems performing temporal extraction and reasoning from text have focused on recognizing and normalizing temporal expressions alone, for which the TIDES annotation scheme has been adopted. Temporal awareness of a text, however, involves not only identifying the temporal expressions, but the events which these expressions anchor, as well as other events which must be ordered relative to them. Because of these broader concerns, TimeML has been developed as an annotation specification that encompasses not only temporal expressions, but all temporally relevant aspects of a text. The annotation schemes, however, are not interchangeable, resulting in incompatible corpora and accompanying extraction algorithms for each standard. In this paper, we describe an automatic migration process from the  TIMEX2  tags of TIDES to the  TIMEX3  tags of TimeML. This transformation procedure has been implemented and evaluated with two different corpora, obtaining 93.3 and 89.2% overall F-Measure respectively.","Language Resources and Evaluation",2011,"No"," timeml temporal information event ordering temporal expressions temporal expression recognition temporal expression normalization natural language processing annotation schemes temporal reasoning automatic transformation tides timeml annotation recently systems performing temporal extraction reasoning text focused recognizing normalizing temporal expressions tides annotation scheme adopted temporal awareness text involves identifying temporal expressions events expressions anchor events ordered relative broader concerns timeml developed annotation specification encompasses temporal expressions temporally relevant aspects text annotation schemes interchangeable resulting incompatible corpora accompanying extraction algorithms standard paper describe automatic migration process timex tags tides timex tags timeml transformation procedure implemented evaluated corpora obtaining measure ",0
"KeywordsLexical semantics Lexical knowledge bases Wordnet ","methodology and construction of the basque wordnet","Semantic interpretation of language requires extensive and rich lexical knowledge bases (LKB). The Basque WordNet is a LKB based on WordNet and its multilingual counterparts EuroWordNet and the Multilingual Central Repository. This paper reviews the theoretical and practical aspects of the Basque WordNet lexical knowledge base, as well as the steps and methodology followed in its construction. Our methodology is based on the joint development of wordnets and annotated corpora. The Basque WordNet contains 32,456 synsets and 26,565 lemmas, and is complemented by a hand-tagged corpus comprising 59,968 annotations.","Language Resources and Evaluation",2011,"Yes"," lexical semantics lexical knowledge bases wordnet methodology construction basque wordnet semantic interpretation language requires extensive rich lexical knowledge bases lkb basque wordnet lkb based wordnet multilingual counterparts eurowordnet multilingual central repository paper reviews theoretical practical aspects basque wordnet lexical knowledge base steps methodology construction methodology based joint development wordnets annotated corpora basque wordnet synsets lemmas complemented hand tagged corpus comprising annotations",1
"KeywordsCross-language Plagiarism detection Similarity Retrieval model Evaluation ","cross language plagiarism detection","Cross-language plagiarism detection deals with the automatic identification and extraction of plagiarism in a multilingual setting. In this setting, a suspicious document is given, and the task is to retrieve all sections from the document that originate from a large, multilingual document collection. Our contributions in this field are as follows: (1) a comprehensive retrieval process for cross-language plagiarism detection is introduced, highlighting the differences to monolingual plagiarism detection, (2) state-of-the-art solutions for two important subtasks are reviewed, (3) retrieval models for the assessment of cross-language similarity are surveyed, and, (4) the three models CL-CNG, CL-ESA and CL-ASA are compared. Our evaluation is of realistic scale: it relies on 120,000 test documents which are selected from the corpora JRC-Acquis and Wikipedia, so that for each test document highly similar documents are available in all of the six languages English, German, Spanish, French, Dutch, and Polish. The models are employed in a series of ranking tasks, and more than 100 million similarities are computed with each model. The results of our evaluation indicate that CL-CNG, despite its simple approach, is the best choice to rank and compare texts across languages if they are syntactically related. CL-ESA almost matches the performance of CL-CNG, but on arbitrary pairs of languages. CL-ASA works best on “exact” translations but does not generalize well.","Language Resources and Evaluation",2011,"No"," cross language plagiarism detection similarity retrieval model evaluation cross language plagiarism detection cross language plagiarism detection deals automatic identification extraction plagiarism multilingual setting setting suspicious document task retrieve sections document originate large multilingual document collection contributions field comprehensive retrieval process cross language plagiarism detection introduced highlighting differences monolingual plagiarism detection state art solutions important subtasks reviewed retrieval models assessment cross language similarity surveyed models cl cng cl esa cl asa compared evaluation realistic scale relies test documents selected corpora jrc acquis wikipedia test document highly similar documents languages english german spanish french dutch polish models employed series ranking tasks million similarities computed model results evaluation cl cng simple approach choice rank compare texts languages syntactically related cl esa matches performance cl cng arbitrary pairs languages cl asa works exact translations generalize ",0
"KeywordsWeb spam filtering Statistical language models Artificial languages ","filtering artificial texts with statistical machine learning techniques","Fake content is flourishing on the Internet, ranging from basic random word salads to web scraping. Most of this fake content is generated for the purpose of nourishing fake web sites aimed at biasing search engine indexes: at the scale of a search engine, using automatically generated texts render such sites harder to detect than using copies of existing pages. In this paper, we present three methods aimed at distinguishing natural texts from artificially generated ones: the first method uses basic lexicometric features, the second one uses standard language models and the third one is based on a relative entropy measure which captures short range dependencies between words. Our experiments show that lexicometric features and language models are efficient to detect most generated texts, but fail to detect texts that are generated with high order Markov models. By comparison our relative entropy scoring algorithm, especially when trained on a large corpus, allows us to detect these “hard” text generators with a high degree of accuracy.","Language Resources and Evaluation",2011,"No"," web spam filtering statistical language models artificial languages filtering artificial texts statistical machine learning techniques fake content flourishing internet ranging basic random word salads web scraping fake content generated purpose nourishing fake web sites aimed biasing search engine indexes scale search engine automatically generated texts render sites harder detect copies existing pages paper present methods aimed distinguishing natural texts artificially generated method basic lexicometric features standard language models based relative entropy measure captures short range dependencies words experiments show lexicometric features language models efficient detect generated texts fail detect texts generated high order markov models comparison relative entropy scoring algorithm trained large corpus detect hard text generators high degree accuracy",0
"KeywordsSpeech Recognition Machine Translation Statistical Machine Translation Language Technology Human Language Technology ","introduction to the special issue on african language technology","In today’s digital multilingual world, language technology is crucial for providing access to information and opportunities for economic development. With approximately two thousand different languages, Africa is a multilingual continent par excellence, presenting acute challenges for those seeking to promote and use African languages in the areas of business development, education and relief aid. In recent times a number of researchers and institutions, both from Africa and elsewhere, have come forward to share the common goal of developing capabilities in language technology for African languages. In 2009 and 2010, the first two workshops on African Language Technology were organized (De Pauw et al. 2009, 2010a) as a forum to bring together a wide range of researchers working in this domain.","Language Resources and Evaluation",2011,"No"," speech recognition machine translation statistical machine translation language technology human language technology introduction special issue african language technology today digital multilingual world language technology crucial providing access information opportunities economic development approximately thousand languages africa multilingual continent par excellence presenting acute challenges seeking promote african languages areas business development education relief aid recent times number researchers institutions africa forward share common goal developing capabilities language technology african languages workshops african language technology organized de pauw al a forum bring wide range researchers working domain",0
"KeywordsDiacritic restoration Unicodification Under-resourced languages African languages Machine learning ","statistical unicodification of african languages","Many languages in Africa are written using Latin-based scripts, but often with extra diacritics (e.g. dots below in Igbo: \({\d i}, {\d o}, {\d u}\)) or modifications to the letters themselves (e.g. open vowels “e” and “o” in Lingala: ɛ, ɔ). While it is possible to render these characters accurately in Unicode, oftentimes keyboard input methods are not easily accessible or are cumbersome to use, and so the vast majority of electronic texts in many African languages are written in plain ASCII. We call the process of converting an ASCII text to its proper Unicode form unicodification. This paper describes an open-source package which performs automatic unicodification, implementing a variant of an algorithm described in previous work of De Pauw, Wagacha, and de Schryver. We have trained models for more than 100 languages using web data, and have evaluated each language using a range of feature sets.","Language Resources and Evaluation",2011,"No"," diacritic restoration unicodification resourced languages african languages machine learning statistical unicodification african languages languages africa written latin based scripts extra diacritics dots igbo modifications letters open vowels lingala render characters accurately unicode oftentimes keyboard input methods easily accessible cumbersome vast majority electronic texts african languages written plain ascii call process converting ascii text proper unicode form unicodification paper describes open source package performs automatic unicodification implementing variant algorithm previous work de pauw wagacha de schryver trained models languages web data evaluated language range feature sets",0
"KeywordsPlagiarism detection Authorship verification Stylometry One-class classification ","intrinsic plagiarism analysis","Research in automatic text plagiarism detection focuses on algorithms that compare suspicious documents against a collection of reference documents. Recent approaches perform well in identifying copied or modified foreign sections, but they assume a closed world where a reference collection is given. This article investigates the question whether plagiarism can be detected by a computer program if no reference can be provided, e.g., if the foreign sections stem from a book that is not available in digital form. We call this problem class intrinsic plagiarism analysis; it is closely related to the problem of authorship verification. Our contributions are threefold. (1) We organize the algorithmic building blocks for intrinsic plagiarism analysis and authorship verification and survey the state of the art. (2) We show how the meta learning approach of Koppel and Schler, termed “unmasking”, can be employed to post-process unreliable stylometric analysis results. (3) We operationalize and evaluate an analysis chain that combines document chunking, style model computation, one-class classification, and meta learning.","Language Resources and Evaluation",2011,"No"," plagiarism detection authorship verification stylometry class classification intrinsic plagiarism analysis research automatic text plagiarism detection focuses algorithms compare suspicious documents collection reference documents recent approaches perform identifying copied modified foreign sections assume closed world reference collection article investigates question plagiarism detected computer program reference provided foreign sections stem book digital form call problem class intrinsic plagiarism analysis closely related problem authorship verification contributions threefold organize algorithmic building blocks intrinsic plagiarism analysis authorship verification survey state art show meta learning approach koppel schler termed unmasking employed post process unreliable stylometric analysis results operationalize evaluate analysis chain combines document chunking style model computation class classification meta learning",0
"KeywordsVerification Problem Plagiarism Detection Information Retrieval Method Multivariate Discriminant Analysis Statistical Machine Learn ","plagiarism and authorship analysis introduction to the special issue","The Internet has facilitated both the dissemination of anonymous texts as well as easy “borrowing” of ideas and words of others. This has raised a number of important questions regarding authorship. Can we identify the anonymous author of a text by comparing the text with the writings of known authors? Can we determine if a text, or parts of it, has been plagiarized? Such questions are clearly of both academic and commercial importance.","Language Resources and Evaluation",2011,"No"," verification problem plagiarism detection information retrieval method multivariate discriminant analysis statistical machine learn plagiarism authorship analysis introduction special issue internet facilitated dissemination anonymous texts easy borrowing ideas words raised number important questions authorship identify anonymous author text comparing text writings authors determine text parts plagiarized questions academic commercial importance",0
"KeywordsAuthorship attribution Open candidate set Randomized feature set ","authorship attribution in the wild","Most previous work on authorship attribution has focused on the case in which we need to attribute an anonymous document to one of a small set of candidate authors. In this paper, we consider authorship attribution as found in the wild: the set of known candidates is extremely large (possibly many thousands) and might not even include the actual author. Moreover, the known texts and the anonymous texts might be of limited length. We show that even in these difficult cases, we can use similarity-based methods along with multiple randomized feature sets to achieve high precision. Moreover, we show the precise relationship between attribution precision and four parameters: the size of the candidate set, the quantity of known-text by the candidates, the length of the anonymous text and a certain robustness score associated with a attribution.","Language Resources and Evaluation",2011,"No"," authorship attribution open candidate set randomized feature set authorship attribution wild previous work authorship attribution focused case attribute anonymous document small set candidate authors paper authorship attribution found wild set candidates extremely large possibly thousands include actual author texts anonymous texts limited length show difficult cases similarity based methods multiple randomized feature sets achieve high precision show precise relationship attribution precision parameters size candidate set quantity text candidates length anonymous text robustness score attribution",0
"KeywordsSwahili Multilinguality Information extraction Named entity recognition and classification Geo-tagging Quotation recognition Date recognition Subject domain classification News analysis Media monitoring ","expanding a multilingual media monitoring and information extraction tool to a new language swahili","The Europe Media Monitor (EMM) family of applications is a set of multilingual tools that gather, cluster and classify news in currently fifty languages and that extract named entities and quotations (reported speech) from twenty languages. In this paper, we describe the recent effort of adding the African Bantu language Swahili to EMM. EMM is designed in an entirely modular way, allowing plugging in a new language by providing the language-specific resources for that language. We thus describe the type of language-specific resources needed, the effort involved, and ways of boot-strapping the generation of these resources in order to keep the effort of adding a new language to a minimum. The text analysis applications pursued in our efforts include clustering, classification, recognition and disambiguation of named entities (persons, organisations and locations), recognition and normalisation of date expressions, as well as the identification of reported speech quotations by and about people.","Language Resources and Evaluation",2011,"Yes"," swahili multilinguality information extraction named entity recognition classification geo tagging quotation recognition date recognition subject domain classification news analysis media monitoring expanding multilingual media monitoring information extraction tool language swahili europe media monitor emm family applications set multilingual tools gather cluster classify news fifty languages extract named entities quotations reported speech twenty languages paper describe recent effort adding african bantu language swahili emm emm designed modular allowing plugging language providing language specific resources language describe type language specific resources needed effort involved ways boot strapping generation resources order effort adding language minimum text analysis applications pursued efforts include clustering classification recognition disambiguation named entities persons organisations locations recognition normalisation date expressions identification reported speech quotations people",1
NA,"j pittermann a pittermann and w minker handling emotions in humancomputer dialogues","“Handling emotions in human–computer dialogues”, written by Pittermann, Pittermann and Minker, is a complete and interesting book about affective computing in spoken dialogue systems. Dialogue systems are an integrated part of our daily life. They generally mean simplicity, time saving and safety. For example, when driving, hand-free operations are necessary and therefore the possibility of giving commands through speech is a necessity. However, to implement more flexible dialogue systems, it is important that these systems can adapt to the speaker. In particular, it seems very important that dialogue systems should be able to recognize and cope with our emotions. To human, the emotions recognition process occurs in an automatic, unconscious, and effortless fashion. Emotions explicitly affect our autonomic nervous system (e.g., cardiovascular and skin conductance changes) and our somatic nervous system (motor expression in face, voice and body). We usually don’t have many...","Language Resources and Evaluation",2011,"No"," pittermann pittermann minker handling emotions humancomputer dialogues handling emotions human computer dialogues written pittermann pittermann minker complete interesting book affective computing spoken dialogue systems dialogue systems integrated part daily life generally simplicity time saving safety driving hand free operations possibility giving commands speech necessity implement flexible dialogue systems important systems adapt speaker important dialogue systems recognize cope emotions human emotions recognition process occurs automatic unconscious effortless fashion emotions explicitly affect autonomic nervous system cardiovascular skin conductance somatic nervous system motor expression face voice body don ",0
"KeywordsTechnology audit Human language technology Language resources BLaRK Language audit Language resource infrastructure Resource-scarce languages ","the south african human language technology audit","Human language technology (HLT) has been identified as a priority area by the South African government. However, despite efforts by government and the research and development (R&D) community, South Africa has not yet been able to maximise the opportunities of HLT and create a thriving HLT industry. One of the key challenges is the fact that there is insufficient codified knowledge about the current South African HLT components, their attributes and existing relationships. Hence a technology audit was conducted for the South African HLT landscape, to create a systematic and detailed inventory of the status of the HLT components across the eleven official languages. Based on the Basic Language Resource Kit (BLaRK) framework Krauwer (ELRA Newslett 3(2), 1998), we used various data collection methods (such as focus groups, questionnaires and personal consultations with HLT experts) to gather detailed information. The South African HLT landscape is analysed using a number of complementary approaches and based on the interpretations of the results, recommendations are made on how to accelerate HLT development in South Africa, as well as on how to conduct similar audits in other countries and contexts.","Language Resources and Evaluation",2011,"No"," technology audit human language technology language resources blark language audit language resource infrastructure resource scarce languages south african human language technology audit human language technology hlt identified priority area south african government efforts government research development community south africa maximise opportunities hlt create thriving hlt industry key challenges fact insufficient codified knowledge current south african hlt components attributes existing relationships technology audit conducted south african hlt landscape create systematic detailed inventory status hlt components eleven official languages based basic language resource kit blark framework krauwer elra newslett data collection methods focus groups questionnaires personal consultations hlt experts gather detailed information south african hlt landscape analysed number complementary approaches based interpretations results recommendations made accelerate hlt development south africa conduct similar audits countries contexts",0
"KeywordsMultiword expressions Document retrieval Endogenous resources ","an efficient any language approach for the integration of phrases in document retrieval","In this paper, we address the problem of the exploitation of text phrases in a multilingual context. We propose a technique to benefit from multi-word units in adhoc document retrieval, whatever the language of the document collection. We present principles to optimize the performance improvement obtained through this approach. The work is validated through retrieval experiments conducted on Chinese, Japanese, Korean and English.","Language Resources and Evaluation",2010,"No"," multiword expressions document retrieval endogenous resources efficient language approach integration phrases document retrieval paper address problem exploitation text phrases multilingual context propose technique benefit multi word units adhoc document retrieval language document collection present principles optimize performance improvement obtained approach work validated retrieval experiments conducted chinese japanese korean english",0
NA,"marianne hund nadja nesselhauf and carolin biewer corpus linguistics and the web","In 2003, the special issue of “Computational linguistics” (September, 29, 3) dedicated to the Web as corpus and edited by Adam Kilgarriff and Gregory Grefenstette was a landmark event for a promising field of study. Today, this book makes for a fine update, even if it is more limited in scope than its predecessor and less recent in its content than its date of publication would lead to believe. The articles included are in fact partially “based on papers presented at the symposium Corpus linguistics—Perspectives for the Future held (…) in Heidelberg in October 2004” (p. 4). However, the editors state that some of the articles were commissioned later, and many of the texts have in fact been brought up to date to take recent developments into account.","Language Resources and Evaluation",2010,"No"," marianne hund nadja nesselhauf carolin biewer corpus linguistics web special issue computational linguistics september dedicated web corpus edited adam kilgarriff gregory grefenstette landmark event promising field study today book makes fine update limited scope predecessor recent content date publication lead articles included fact partially based papers presented symposium corpus linguistics perspectives future held heidelberg october editors state articles commissioned texts fact brought date recent developments account",0
"KeywordsSpoken dialogue corpora Spoken dialogue systems Cognitive ageing Annotation Information states Speech acts User simulations Speech recognition ","the match corpus a corpus of older and younger users interactions with spoken dialogue systems","We present the MATCH corpus, a unique data set of 447 dialogues in which 26 older and 24 younger adults interact with nine different spoken dialogue systems. The systems varied in the number of options presented and the confirmation strategy used. The corpus also contains information about the users’ cognitive abilities and detailed usability assessments of each dialogue system. The corpus, which was collected using a Wizard-of-Oz methodology, has been fully transcribed and annotated with dialogue acts and “Information State Update” (ISU) representations of dialogue context. Dialogue act and ISU annotations were performed semi-automatically. In addition to describing the corpus collection and annotation, we present a quantitative analysis of the interaction behaviour of older and younger users and discuss further applications of the corpus. We expect that the corpus will provide a key resource for modelling older people’s interaction with spoken dialogue systems.","Language Resources and Evaluation",2010,"Yes"," spoken dialogue corpora spoken dialogue systems cognitive ageing annotation information states speech acts user simulations speech recognition match corpus corpus older younger users interactions spoken dialogue systems present match corpus unique data set dialogues older younger adults interact spoken dialogue systems systems varied number options presented confirmation strategy corpus information users cognitive abilities detailed usability assessments dialogue system corpus collected wizard oz methodology fully transcribed annotated dialogue acts information state update isu representations dialogue context dialogue act isu annotations performed semi automatically addition describing corpus collection annotation present quantitative analysis interaction behaviour older younger users discuss applications corpus expect corpus provide key resource modelling older people interaction spoken dialogue systems",1
"KeywordsLinguistic annotation Language resources Discourse  Prosody Semantics Spoken dialogue ","the nxt format switchboard corpus a rich resource for investigating the syntax semantics pragmatics and prosody of dialogue","This paper describes a recently completed common resource for the study of spoken discourse, the NXT-format Switchboard Corpus. Switchboard is a long-standing corpus of telephone conversations (Godfrey et al. in SWITCHBOARD: Telephone speech corpus for research and development. In Proceedings of ICASSP-92, pp. 517–520, 1992). We have brought together transcriptions with existing annotations for syntax, disfluency, speech acts, animacy, information status, coreference, and prosody; along with substantial new annotations of focus/contrast, more prosody, syllables and phones. The combined corpus uses the format of the NITE XML Toolkit, which allows these annotations to be browsed and searched as a coherent set (Carletta et al. in Lang Resour Eval J 39(4):313–334, 2005). The resulting corpus is a rich resource for the investigation of the linguistic features of dialogue and how they interact. As well as describing the corpus itself, we discuss our approach to overcoming issues involved in such a data integration project, relevant to both users of the corpus and others in the language resource community undertaking similar projects.","Language Resources and Evaluation",2010,"Yes"," linguistic annotation language resources discourse prosody semantics spoken dialogue nxt format switchboard corpus rich resource investigating syntax semantics pragmatics prosody dialogue paper describes recently completed common resource study spoken discourse nxt format switchboard corpus switchboard long standing corpus telephone conversations godfrey al switchboard telephone speech corpus research development proceedings icassp pp brought transcriptions existing annotations syntax disfluency speech acts animacy information status coreference prosody substantial annotations focuscontrast prosody syllables phones combined corpus format nite xml toolkit annotations browsed searched coherent set carletta al lang resour eval resulting corpus rich resource investigation linguistic features dialogue interact describing corpus discuss approach overcoming issues involved data integration project relevant users corpus language resource community undertaking similar projects",1
"KeywordsPhonetic corpus Phonetic transcription Transcription granularity Mexican Spanish Acoustic models ","the corpus dimex100 transcription and evaluation","In this paper the transcription and evaluation of the corpus DIMEx100 for Mexican Spanish is presented. First we describe the corpus and explain the linguistic and computational motivation for its design and collection process; then, the phonetic antecedents and the alphabet adopted for the transcription task are presented; the corpus has been transcribed at three different granularity levels, which are also specified in detail. The corpus statistics for each transcription level are also presented. A set of phonetic rules describing phonetic context observed empirically in spontaneous conversation is also validated with the transcription. The corpus has been used for the construction of acoustic models and a phonetic dictionary for the construction of a speech recognition system. Initial performance results suggest that the data can be used to train good quality acoustic models.","Language Resources and Evaluation",2010,"Yes"," phonetic corpus phonetic transcription transcription granularity mexican spanish acoustic models corpus dimex transcription evaluation paper transcription evaluation corpus dimex mexican spanish presented describe corpus explain linguistic computational motivation design collection process phonetic antecedents alphabet adopted transcription task presented corpus transcribed granularity levels detail corpus statistics transcription level presented set phonetic rules describing phonetic context observed empirically spontaneous conversation validated transcription corpus construction acoustic models phonetic dictionary construction speech recognition system initial performance results suggest data train good quality acoustic models",1
"KeywordsCoreference Anaphora Corpus annotation Annotation scheme Reliability study ","ancora co coreferentially annotated corpora for spanish and catalan","This article describes the enrichment of the AnCora corpora of Spanish and Catalan (400 k each) with coreference links between pronouns (including elliptical subjects and clitics), full noun phrases (including proper nouns), and discourse segments. The coding scheme distinguishes between identity links, predicative relations, and discourse deixis. Inter-annotator agreement on the link types is 85–89% above chance, and we provide an analysis of the sources of disagreement. The resulting corpora make it possible to train and test learning-based algorithms for automatic coreference resolution, as well as to carry out bottom-up linguistic descriptions of coreference relations as they occur in real data.","Language Resources and Evaluation",2010,"Yes"," coreference anaphora corpus annotation annotation scheme reliability study ancora coreferentially annotated corpora spanish catalan article describes enrichment ancora corpora spanish catalan coreference links pronouns including elliptical subjects clitics full noun phrases including proper nouns discourse segments coding scheme distinguishes identity links predicative relations discourse deixis inter annotator agreement link types chance provide analysis sources disagreement resulting corpora make train test learning based algorithms automatic coreference resolution carry bottom linguistic descriptions coreference relations occur real data",1
"KeywordsMultiword expressions Spoken language Transcription Pronunciation reduction Identification ","analyzing and identifying multiword expressions in spoken language","The present paper investigates multiword expressions (MWEs) in spoken language and possible ways of identifying MWEs automatically in speech corpora. Two MWEs that emerged from previous studies and that occur frequently in Dutch are analyzed to study their pronunciation characteristics and compare them to those of other utterances in a large speech corpus. The analyses reveal that these MWEs display extreme pronunciation variation and reduction, i.e., many phonemes and even syllables are deleted. Several measures of pronunciation reduction are calculated for these two MWEs and for all other utterances in the corpus. Five of these measures are more than twice as high for the MWEs, thus indicating considerable reduction. One overall measure of pronunciation deviation is then calculated and used to automatically identify MWEs in a large speech corpus. The results show that neither this overall measure, nor frequency of co-occurrence alone are suitable for identifying MWEs. The best results are obtained by using a metric that combines overall pronunciation reduction with weighted frequency. In this way, recurring “islands of pronunciation reduction” that contain (potential) MWEs can be identified in a large speech corpus.","Language Resources and Evaluation",2010,"No"," multiword expressions spoken language transcription pronunciation reduction identification analyzing identifying multiword expressions spoken language present paper investigates multiword expressions mwes spoken language ways identifying mwes automatically speech corpora mwes emerged previous studies occur frequently dutch analyzed study pronunciation characteristics compare utterances large speech corpus analyses reveal mwes display extreme pronunciation variation reduction phonemes syllables deleted measures pronunciation reduction calculated mwes utterances corpus measures high mwes indicating considerable reduction measure pronunciation deviation calculated automatically identify mwes large speech corpus results show measure frequency occurrence suitable identifying mwes results obtained metric combines pronunciation reduction weighted frequency recurring islands pronunciation reduction potential mwes identified large speech corpus",0
"KeywordsAutomatic identification Word alignment Machine translation Terminology Multiword expressions Lexical acquisition Statistical methods ","alignment based extraction of multiword expressions","Due to idiosyncrasies in their syntax, semantics or frequency, Multiword Expressions (MWEs) have received special attention from the NLP community, as the methods and techniques developed for the treatment of simplex words are not necessarily suitable for them. This is certainly the case for the automatic acquisition of MWEs from corpora. A lot of effort has been directed to the task of automatically identifying them, with considerable success. In this paper, we propose an approach for the identification of MWEs in a multilingual context, as a by-product of a word alignment process, that not only deals with the identification of possible MWE candidates, but also associates some multiword expressions with semantics. The results obtained indicate the feasibility and low costs in terms of tools and resources demanded by this approach, which could, for example, facilitate and speed up lexicographic work.","Language Resources and Evaluation",2010,"No"," automatic identification word alignment machine translation terminology multiword expressions lexical acquisition statistical methods alignment based extraction multiword expressions due idiosyncrasies syntax semantics frequency multiword expressions mwes received special attention nlp community methods techniques developed treatment simplex words necessarily suitable case automatic acquisition mwes corpora lot effort directed task automatically identifying considerable success paper propose approach identification mwes multilingual context product word alignment process deals identification mwe candidates associates multiword expressions semantics results obtained feasibility low costs terms tools resources demanded approach facilitate speed lexicographic work",0
"KeywordsTerminology mining Comparable corpora Lexical alignment Compositional translation ","compositionality and lexical alignment of multi word terms","The automatic compilation of bilingual lists of terms from specialized comparable corpora using lexical alignment has been successful for single-word terms (SWTs), but remains disappointing for multi-word terms (MWTs). The low frequency and the variability of the syntactic structures of MWTs in the source and the target languages are the main reported problems. This paper defines a general framework dedicated to the lexical alignment of MWTs from comparable corpora that includes a compositional translation process and the standard lexical context analysis. The compositional method which is based on the translation of lexical items being restrictive, we introduce an extended compositional method that bridges the gap between MWTs of different syntactic structures through morphological links. We experimented with the two compositional methods for the French–Japanese alignment task. The results show a significant improvement for the translation of MWTs and advocate further morphological analysis in lexical alignment.","Language Resources and Evaluation",2010,"No"," terminology mining comparable corpora lexical alignment compositional translation compositionality lexical alignment multi word terms automatic compilation bilingual lists terms specialized comparable corpora lexical alignment successful single word terms swts remains disappointing multi word terms mwts low frequency variability syntactic structures mwts source target languages main reported problems paper defines general framework dedicated lexical alignment mwts comparable corpora includes compositional translation process standard lexical context analysis compositional method based translation lexical items restrictive introduce extended compositional method bridges gap mwts syntactic structures morphological links experimented compositional methods french japanese alignment task results show significant improvement translation mwts advocate morphological analysis lexical alignment",0
"KeywordsEstonian language Idioms Multi-word expressions Particle verbs Support verb constructions ","the variability of multi word verbal expressions in estonian","This article focuses on the variability of one of the subtypes of multi-word expressions, namely those consisting of a verb and a particle or a verb and its complement(s). We build on evidence from Estonian, an agglutinative language with free word order, analysing the behaviour of verbal multi-word expressions (opaque and transparent idioms, support verb constructions and particle verbs). Using this data we analyse such phenomena as the order of the components of a multi-word expression, lexical substitution and morphosyntactic flexibility.","Language Resources and Evaluation",2010,"No"," estonian language idioms multi word expressions particle verbs support verb constructions variability multi word verbal expressions estonian article focuses variability subtypes multi word expressions consisting verb particle verb complement build evidence estonian agglutinative language free word order analysing behaviour verbal multi word expressions opaque transparent idioms support verb constructions particle verbs data analyse phenomena order components multi word expression lexical substitution morphosyntactic flexibility",0
"KeywordsLexical association measures Collocations Multiword expressions Evaluation ","lexical association measures and collocation extraction","We present an extensive empirical evaluation of collocation extraction methods based on lexical association measures and their combination. The experiments are performed on three sets of collocation candidates extracted from the Prague Dependency Treebank with manual morphosyntactic annotation and from the Czech National Corpus with automatically assigned lemmas and part-of-speech tags. The collocation candidates were manually labeled as collocational or non-collocational. The evaluation is based on measuring the quality of ranking the candidates according to their chance to form collocations. Performance of the methods is compared by precision-recall curves and mean average precision scores. The work is focused on two-word (bigram) collocations only. We experiment with bigrams extracted from sentence dependency structure as well as from surface word order. Further, we study the effect of corpus size on the performance of the individual methods and their combination.","Language Resources and Evaluation",2010,"No"," lexical association measures collocations multiword expressions evaluation lexical association measures collocation extraction present extensive empirical evaluation collocation extraction methods based lexical association measures combination experiments performed sets collocation candidates extracted prague dependency treebank manual morphosyntactic annotation czech national corpus automatically assigned lemmas part speech tags collocation candidates manually labeled collocational collocational evaluation based measuring quality ranking candidates chance form collocations performance methods compared precision recall curves average precision scores work focused word bigram collocations experiment bigrams extracted sentence dependency structure surface word order study effect corpus size performance individual methods combination",0
"KeywordsAnnotation Guidelines Spatial language Geography Information extraction Evaluation Adaptation ","spatialml annotation scheme resources and evaluation","SpatialML is an annotation scheme for marking up references to places in natural language. It covers both named and nominal references to places, grounding them where possible with geo-coordinates, and characterizes relationships among places in terms of a region calculus. A freely available annotation editor has been developed for SpatialML, along with several annotated corpora. Inter-annotator agreement on SpatialML extents is 91.3 F-measure on a corpus of SpatialML-annotated ACE documents released by the Linguistic Data Consortium. Disambiguation agreement on geo-coordinates on ACE is 87.93 F-measure. An automatic tagger for SpatialML extents scores 86.9 F on ACE, while a disambiguator scores 93.0 F on it. Results are also presented for two other corpora. In adapting the extent tagger to new domains, merging the training data from the ACE corpus with annotated data in the new domain provides the best performance.","Language Resources and Evaluation",2010,"No"," annotation guidelines spatial language geography information extraction evaluation adaptation spatialml annotation scheme resources evaluation spatialml annotation scheme marking references places natural language covers named nominal references places grounding geo coordinates characterizes relationships places terms region calculus freely annotation editor developed spatialml annotated corpora inter annotator agreement spatialml extents measure corpus spatialml annotated ace documents released linguistic data consortium disambiguation agreement geo coordinates ace measure automatic tagger spatialml extents scores ace disambiguator scores results presented corpora adapting extent tagger domains merging training data ace corpus annotated data domain performance",0
"KeywordsDutch Lexicon Multiword expressions ","duelme a dutch electronic lexicon of multiword expressions","This article describes the design and implementation of a Dutch Electronic Lexicon of Multiword Expressions (DuELME). DuELME describes the core properties of over 5,000 Dutch multiword expressions. This article gives an overview of the decisions made in order to come to a standard lexical representation and discusses the description fields this representation comprises. We discuss the approach taken, which is innovative since it is based on the Equivalence Class Method (ECM). It is shown that introducing parameters to the ECM optimizes the method. The selection of the lexical entries and their properties is corpus-based. We describe the extraction of candidate expressions from corpora and discuss the selection criteria of the lexical entries. Moreover, we present the results of an evaluation of the standard representation in Alpino, a Dutch dependency parser.","Language Resources and Evaluation",2010,"Yes"," dutch lexicon multiword expressions duelme dutch electronic lexicon multiword expressions article describes design implementation dutch electronic lexicon multiword expressions duelme duelme describes core properties dutch multiword expressions article overview decisions made order standard lexical representation discusses description fields representation comprises discuss approach innovative based equivalence class method ecm shown introducing parameters ecm optimizes method selection lexical entries properties corpus based describe extraction candidate expressions corpora discuss selection criteria lexical entries present results evaluation standard representation alpino dutch dependency parser",1
"KeywordsMultilingual morphology Loanwords Assimilation Neologism ","google the verb","The verb google is intriguing for the study of morphology, loanwords, assimilation, language contrast and neologisms. We present data for it for nineteen languages from nine language families.","Language Resources and Evaluation",2010,"No"," multilingual morphology loanwords assimilation neologism google verb verb google intriguing study morphology loanwords assimilation language contrast neologisms present data nineteen languages language families",0
"KeywordsVerb-particle construction Multiword expression Identification ","how to pick out token instances of english verb particle constructions","We propose a method for automatically identifying individual instances of English verb-particle constructions (VPCs) in raw text. Our method employs the RASP parser and analysis of the sentential context of each VPC candidate to differentiate VPCs from simple combinations of a verb and prepositional phrase. We show that our proposed method has an F-score of 0.974 at VPC identification over the Brown Corpus and Wall Street Journal.","Language Resources and Evaluation",2010,"No"," verb particle construction multiword expression identification pick token instances english verb particle constructions propose method automatically identifying individual instances english verb particle constructions vpcs raw text method employs rasp parser analysis sentential context vpc candidate differentiate vpcs simple combinations verb prepositional phrase show proposed method score vpc identification brown corpus wall street journal",0
"KeywordsNatural Language Processing Natural Language Processing Application Multilingual Context Multiword Expression Natural Language Processing Community ","multiword expressions hard going or plain sailing","Over the past two decades or so, Multi-Word Expressions (MWEs; also called Multi-word Units) have been an increasingly important concern for Computational Linguistics and Natural Language Processing (NLP). The term MWE has been used to refer to various types of linguistic units and expressions, including idioms, noun compounds, phrasal verbs, light verbs and other habitual collocations. However, while there is no universally agreed definition for MWE as yet, most researchers use the term to refer to those frequently occurring phrasal units which are subject to certain level of semantic opaqueness, or non-compositionality. Non-compositional MWEs pose tough challenges for automatic analysis because their interpretation cannot be achieved by directly combining the semantics of their constituents, thereby causing the “pain in the neck of NLP” (Sag et al. 2001).","Language Resources and Evaluation",2010,"No"," natural language processing natural language processing application multilingual context multiword expression natural language processing community multiword expressions hard plain sailing past decades multi word expressions mwes called multi word units increasingly important concern computational linguistics natural language processing nlp term mwe refer types linguistic units expressions including idioms noun compounds phrasal verbs light verbs habitual collocations universally agreed definition mwe researchers term refer frequently occurring phrasal units subject level semantic opaqueness compositionality compositional mwes pose tough challenges automatic analysis interpretation achieved directly combining semantics constituents causing pain neck nlp sag al ",0
"KeywordsWord sense disambiguation Knowledge sources Inductive logic programming ","assessing the contribution of shallow and deep knowledge sources for word sense disambiguation","Corpus-based techniques have proved to be very beneficial in the development of efficient and accurate approaches to word sense disambiguation (WSD) despite the fact that they generally represent relatively shallow knowledge. It has always been thought, however, that WSD could also benefit from deeper knowledge sources. We describe a novel approach to WSD using inductive logic programming to learn theories from first-order logic representations that allows corpus-based evidence to be combined with any kind of background knowledge. This approach has been shown to be effective over several disambiguation tasks using a combination of deep and shallow knowledge sources. Is it important to understand the contribution of the various knowledge sources used in such a system. This paper investigates the contribution of nine knowledge sources to the performance of the disambiguation models produced for the SemEval-2007 English lexical sample task. The outcome of this analysis will assist future work on WSD in concentrating on the most useful knowledge sources.","Language Resources and Evaluation",2010,"No"," word sense disambiguation knowledge sources inductive logic programming assessing contribution shallow deep knowledge sources word sense disambiguation corpus based techniques proved beneficial development efficient accurate approaches word sense disambiguation wsd fact generally represent shallow knowledge thought wsd benefit deeper knowledge sources describe approach wsd inductive logic programming learn theories order logic representations corpus based evidence combined kind background knowledge approach shown effective disambiguation tasks combination deep shallow knowledge sources important understand contribution knowledge sources system paper investigates contribution knowledge sources performance disambiguation models produced semeval english lexical sample task outcome analysis assist future work wsd concentrating knowledge sources",0
NA,"steven bird ewan klein and edward loper natural language processing with python analyzing text with the natural language toolkit","Natural Language Processing (NLP) is experiencing rapid growth as its theories and methods are more and more deployed in a wide range of different fields. In the humanities, the work on corpora is gaining increasing prominence. Within industry, people need NLP for market analysis, web software development to name a few examples. For this reason it is important for many people to have some working knowledge of NLP. The book “Natural Language Processing with Python” by Steven Bird, Ewan Klein and Edward Loper is a recent contribution to cover this demand. It introduces the freely available Natural Language Toolkit (NLTK) 1—a project by the same authors—that was designed with the following goals: simplicity, consistency, extensibility and modularity.","Language Resources and Evaluation",2010,"No"," steven bird ewan klein edward loper natural language processing python analyzing text natural language toolkit natural language processing nlp experiencing rapid growth theories methods deployed wide range fields humanities work corpora gaining increasing prominence industry people nlp market analysis web software development examples reason important people working knowledge nlp book natural language processing python steven bird ewan klein edward loper recent contribution cover demand introduces freely natural language toolkit nltk project authors designed goals simplicity consistency extensibility modularity",0
"Keywordsafst uima Annotation-based analytics development Pattern matching over annotations Annotation lattices High density annotation repositories Finite-state transduction Corpus analysis ","a framework for traversing dense annotation lattices","Pattern matching, or querying, over annotations is a general purpose paradigm for inspecting, navigating, mining, and transforming annotation repositories—the common representation basis for modern pipelined text processing architectures. The open-ended nature of these architectures and expressiveness of feature structure-based annotation schemes account for the natural tendency of such annotation repositories to become very dense, as multiple levels of analysis get encoded as layered annotations. This particular characteristic presents challenges for the design of a pattern matching framework capable of interpreting ‘flat’ patterns over arbitrarily dense annotation lattices. We present an approach where a finite state device applies (compiled) pattern grammars over what is, in effect, a linearized ‘projection’ of a particular route through the lattice. The route is derived by a mix of static grammar analysis and runtime interpretation of navigational directives within an extended grammar formalism; it selects just the annotations sequence appropriate for the patterns at hand. For expressive and efficient pattern matching in dense annotations stores, our implemented approach achieves a mix of lattice traversal and finite state scanning by exposing a language which, to its user, provides constructs for specifying sequential, structural, and configurational constraints among annotations.","Language Resources and Evaluation",2010,"No"," afst uima annotation based analytics development pattern matching annotations annotation lattices high density annotation repositories finite state transduction corpus analysis framework traversing dense annotation lattices pattern matching querying annotations general purpose paradigm inspecting navigating mining transforming annotation repositories common representation basis modern pipelined text processing architectures open ended nature architectures expressiveness feature structure based annotation schemes account natural tendency annotation repositories dense multiple levels analysis encoded layered annotations characteristic presents challenges design pattern matching framework capable interpreting flat patterns arbitrarily dense annotation lattices present approach finite state device applies compiled pattern grammars effect linearized projection route lattice route derived mix static grammar analysis runtime interpretation navigational directives extended grammar formalism selects annotations sequence patterns hand expressive efficient pattern matching dense annotations stores implemented approach achieves mix lattice traversal finite state scanning exposing language user constructs sequential structural configurational constraints annotations",0
"KeywordsMultiword expressions Treebanks Annotation Inter-annotator agreement Named entities ","annotation of multiword expressions in the prague dependency treebank","We describe annotation of multiword expressions (MWEs) in the Prague dependency treebank, using several automatic pre-annotation steps. We use subtrees of the tectogrammatical tree structures of the Prague dependency treebank to store representations of the MWEs in the dictionary and pre-annotate following occurrences automatically. We also show a way to measure reliability of this type of annotation.","Language Resources and Evaluation",2010,"Yes"," multiword expressions treebanks annotation inter annotator agreement named entities annotation multiword expressions prague dependency treebank describe annotation multiword expressions mwes prague dependency treebank automatic pre annotation steps subtrees tectogrammatical tree structures prague dependency treebank store representations mwes dictionary pre annotate occurrences automatically show measure reliability type annotation",1
"KeywordsRemote text-to-speech synthesis evaluation Text-to-speech synthesis modules ECESS consortium ","remote based text to speech modules evaluation framework the res framework","The ECESS consortium (European Center of Excellence in Speech Synthesis) aims to speed up progress in speech synthesis technology, by providing an appropriate evaluation framework. The key element of the evaluation framework is based on the partition of a text-to-speech synthesis system into distributed TTS modules. A text processing, prosody generation, and an acoustic synthesis module have been specified currently. A split into various modules has the advantage that the developers of an institution active in ECESS, can concentrate its efforts on a single module, and test its performance in a complete system using missing modules from the developers of other institutions. In this way, complete TTS systems can be built using high performance modules from different institutions. In order to evaluate the modules and to connect modules efficiently, a remote evaluation platform—the Remote Evaluation System (RES) based on the existing internet infrastructure—has been developed within ECESS. The RES is based on client–server architecture. It consists of RES module servers, which encapsulate the modules of the developers, a RES client, which sends data to and receives data from the RES module servers, and a RES server, which connects the RES module servers, and organizes the flow of information. RES can be used by developers for selecting RES module from the internet, which contains a missing TTS module needed to test and improve the performances of their own modules. Finally, the RES allows for the evaluation of TTS modules running at different institutions worldwide. When using the RES client, the institution performing the evaluation is able to set-up and performs various evaluation tasks by sending test data via the RES client and receiving results from the RES module servers. Currently ELDA www.elda.org is setting-up an evaluation using the RES client, which will then be extended to an evaluation client specializing in the envisaged evaluation tasks.","Language Resources and Evaluation",2010,"No"," remote text speech synthesis evaluation text speech synthesis modules ecess consortium remote based text speech modules evaluation framework res framework ecess consortium european center excellence speech synthesis aims speed progress speech synthesis technology providing evaluation framework key element evaluation framework based partition text speech synthesis system distributed tts modules text processing prosody generation acoustic synthesis module split modules advantage developers institution active ecess concentrate efforts single module test performance complete system missing modules developers institutions complete tts systems built high performance modules institutions order evaluate modules connect modules efficiently remote evaluation platform remote evaluation system res based existing internet infrastructure developed ecess res based client server architecture consists res module servers encapsulate modules developers res client sends data receives data res module servers res server connects res module servers organizes flow information res developers selecting res module internet missing tts module needed test improve performances modules finally res evaluation tts modules running institutions worldwide res client institution performing evaluation set performs evaluation tasks sending test data res client receiving results res module servers elda wwweldaorg setting evaluation res client extended evaluation client specializing envisaged evaluation tasks",0
"KeywordsMultimodal Corpus annotation Audio ","woz acoustic data collection for interactive tv","This paper describes a multichannel acoustic data collection recorded under the European DICIT project, during Wizard of Oz (WOZ) experiments carried out at FAU and FBK-irst laboratories. The application of interest in DICIT is a distant-talking interface for control of interactive TV working in a typical living room, with many interfering devices. The objective of the experiments was to collect a database supporting efficient development and tuning of acoustic processing algorithms for signal enhancement. In DICIT, techniques for sound source localization, multichannel acoustic echo cancellation, blind source separation, speech activity detection, speaker identification and verification as well as beamforming are combined to achieve a maximum possible reduction of the user speech impairments typical of distant-talking interfaces. The collected database permitted to simulate at preliminary stage a realistic scenario and to tailor the involved algorithms to the observed user behaviors. In order to match the project requirements, the WOZ experiments were recorded in three languages: English, German and Italian. Besides the user inputs, the database also contains non-speech related acoustic events, room impulse response measurements and video data, the latter used to compute three-dimensional positions of each subject. Sessions were manually transcribed and segmented at word level, introducing also specific labels for acoustic events.","Language Resources and Evaluation",2010,"Yes"," multimodal corpus annotation audio woz acoustic data collection interactive tv paper describes multichannel acoustic data collection recorded european dicit project wizard oz woz experiments carried fau fbk irst laboratories application interest dicit distant talking interface control interactive tv working typical living room interfering devices objective experiments collect database supporting efficient development tuning acoustic processing algorithms signal enhancement dicit techniques sound source localization multichannel acoustic echo cancellation blind source separation speech activity detection speaker identification verification beamforming combined achieve maximum reduction user speech impairments typical distant talking interfaces collected database permitted simulate preliminary stage realistic scenario tailor involved algorithms observed user behaviors order match project requirements woz experiments recorded languages english german italian user inputs database speech related acoustic events room impulse response measurements video data compute dimensional positions subject sessions manually transcribed segmented word level introducing specific labels acoustic events",1
"KeywordsDialog system Speech understanding Corpus Annotation Evaluation ","media a semantically annotated corpus of task oriented dialogs in french","The aim of the French Media project was to define a protocol for the evaluation of speech understanding modules for dialog systems. Accordingly, a corpus of 1,257 real spoken dialogs related to hotel reservation and tourist information was recorded, transcribed and semantically annotated, and a semantic attribute-value representation was defined in which each conceptual relationship was represented by the names of the attributes. Two semantic annotation levels are distinguished in this approach. At the first level, each utterance is considered separately and the annotation represents the meaning of the statement without taking into account the dialog context. The second level of annotation then corresponds to the interpretation of the meaning of the statement by taking into account the dialog context; in this way a semantic representation of the dialog context is defined. This paper discusses the data collection, the detailed definition of both annotation levels, and the annotation scheme. Then the paper comments on both evaluation campaigns which were carried out during the project and discusses some results.","Language Resources and Evaluation",2009,"Yes"," dialog system speech understanding corpus annotation evaluation media semantically annotated corpus task oriented dialogs french aim french media project define protocol evaluation speech understanding modules dialog systems corpus real spoken dialogs related hotel reservation tourist information recorded transcribed semantically annotated semantic attribute representation defined conceptual relationship represented names attributes semantic annotation levels distinguished approach level utterance considered separately annotation represents meaning statement taking account dialog context level annotation corresponds interpretation meaning statement taking account dialog context semantic representation dialog context defined paper discusses data collection detailed definition annotation levels annotation scheme paper comments evaluation campaigns carried project discusses results",1
"KeywordsEvent factuality Modality Certainty Subjectivity analysis Corpus creation TimeBank ","factbank a corpus annotated with event factuality","Recent work in computational linguistics points out the need for systems to be sensitive to the veracity or factuality of events as mentioned in text; that is, to recognize whether events are presented as corresponding to actual situations in the world, situations that have not happened, or situations of uncertain interpretation. Event factuality is an important aspect of the representation of events in discourse, but the annotation of such information poses a representational challenge, largely because factuality is expressed through the interaction of numerous linguistic markers and constructions. Many of these markers are already encoded in existing corpora, albeit in a somewhat fragmented way. In this article, we present FactBank, a corpus annotated with information concerning the factuality of events. Its annotation has been carried out from a descriptive framework of factuality grounded on both theoretical findings and data analysis. FactBank is built on top of TimeBank, adding to it an additional level of semantic information.","Language Resources and Evaluation",2009,"Yes"," event factuality modality certainty subjectivity analysis corpus creation timebank factbank corpus annotated event factuality recent work computational linguistics points systems sensitive veracity factuality events mentioned text recognize events presented actual situations world situations happened situations uncertain interpretation event factuality important aspect representation events discourse annotation information poses representational challenge largely factuality expressed interaction numerous linguistic markers constructions markers encoded existing corpora albeit fragmented article present factbank corpus annotated information factuality events annotation carried descriptive framework factuality grounded theoretical findings data analysis factbank built top timebank adding additional level semantic information",1
"KeywordsAnnotated corpora Corpus construction General-purpose linguistic resources English German Italian Web as corpus WaCky! ","the wacky wide web a collection of very large linguistically processed web crawled corpora","This article introduces ukWaC, deWaC and itWaC, three very large corpora of English, German, and Italian built by web crawling, and describes the methodology and tools used in their construction. The corpora contain more than a billion words each, and are thus among the largest resources for the respective languages. The paper also provides an evaluation of their suitability for linguistic research, focusing on ukWaC and itWaC. A comparison in terms of lexical coverage with existing resources for the languages of interest produces encouraging results. Qualitative evaluation of ukWaC versus the British National Corpus was also conducted, so as to highlight differences in corpus composition (text types and subject matters). The article concludes with practical information about format and availability of corpora and tools.","Language Resources and Evaluation",2009,"Yes"," annotated corpora corpus construction general purpose linguistic resources english german italian web corpus wacky wacky wide web collection large linguistically processed web crawled corpora article introduces ukwac dewac itwac large corpora english german italian built web crawling describes methodology tools construction corpora billion words largest resources respective languages paper evaluation suitability linguistic research focusing ukwac itwac comparison terms lexical coverage existing resources languages interest produces encouraging results qualitative evaluation ukwac versus british national corpus conducted highlight differences corpus composition text types subject matters article concludes practical information format availability corpora tools",1
"KeywordsLarge comparable corpora Translation equivalents Multiword expressions Distributional similarity ","irrefragable answers using comparable corpora to retrieve translation equivalents","In this paper we present a tool that uses comparable corpora to find appropriate translation equivalents for expressions that are considered by translators as difficult. For a phrase in the source language the tool identifies a range of possible expressions used in similar contexts in target language corpora and presents them to the translator as a list of suggestions. In the paper we discuss the method and present results of human evaluation of the performance of the tool, which highlight its usefulness when dictionary solutions are lacking.","Language Resources and Evaluation",2009,"No"," large comparable corpora translation equivalents multiword expressions distributional similarity irrefragable answers comparable corpora retrieve translation equivalents paper present tool comparable corpora find translation equivalents expressions considered translators difficult phrase source language tool identifies range expressions similar contexts target language corpora presents translator list suggestions paper discuss method present results human evaluation performance tool highlight usefulness dictionary solutions lacking",0
"KeywordsJapanese idiom Corpus Idiom identification Language resources ","compilation of an idiom example database for supervised idiom identification","Some phrases can be interpreted in their context either idiomatically (figuratively) or literally. The precise identification of idioms is essential in order to achieve full-fledged natural language processing. Because of this, the authors of this paper have created an idiom corpus for Japanese. This paper reports on the corpus itself and the results of an idiom identification experiment conducted using the corpus. The corpus targeted 146 ambiguous idioms, and consists of 102,856 examples, each of which is annotated with a literal/idiomatic label. All sentences were collected from the World Wide Web. For idiom identification, 90 out of the 146 idioms were targeted and a word sense disambiguation (WSD) method was adopted using both common WSD features and idiom-specific features. The corpus and the experiment are both, as far as can be determined, the largest of their kinds. It was discovered that a standard supervised WSD method works well for idiom identification and it achieved accuracy levels of 89.25 and 88.86%, with and without idiom-specific features, respectively. It was also found that the most effective idiom-specific feature is the one that involves the adjacency of idiom constituents.","Language Resources and Evaluation",2009,"Yes"," japanese idiom corpus idiom identification language resources compilation idiom database supervised idiom identification phrases interpreted context idiomatically figuratively literally precise identification idioms essential order achieve full fledged natural language processing authors paper created idiom corpus japanese paper reports corpus results idiom identification experiment conducted corpus corpus targeted ambiguous idioms consists examples annotated literalidiomatic label sentences collected world wide web idiom identification idioms targeted word sense disambiguation wsd method adopted common wsd features idiom specific features corpus experiment determined largest kinds discovered standard supervised wsd method works idiom identification achieved accuracy levels idiom specific features found effective idiom specific feature involves adjacency idiom constituents",1
"KeywordsLanguage Resources Interoperability ","multilingual language resources and interoperability","This article introduces the topic of “Multilingual language resources and interoperability”. We start with a taxonomy and parameters for classifying language resources. Later we provide examples and issues of interoperatability, and resource architectures to solve such issues. Finally we discuss aspects of linguistic formalisms and interoperability.","Language Resources and Evaluation",2009,"No"," language resources interoperability multilingual language resources interoperability article introduces topic multilingual language resources interoperability start taxonomy parameters classifying language resources provide examples issues interoperatability resource architectures solve issues finally discuss aspects linguistic formalisms interoperability",0
"KeywordsWordnet Dictionary Lexical semantics Semantic relations Hyponymy Nouns Verbs ","dannet the challenge of compiling a wordnet for danish by reusing a monolingual dictionary","This paper is a contribution to the discussion on compiling computational lexical resources from conventional dictionaries. It describes the theoretical as well as practical problems that are encountered when reusing a conventional dictionary for compiling a lexical-semantic resource in terms of a wordnet. More specifically, it describes the methodological issues of compiling a wordnet for Danish, DanNet, from a monolingual basis, and not—as is often seen—by applying the translational expansion method with Princeton WordNet as the English source. Thus, we apply as our basis a large, corpus-based printed dictionary of modern Danish. Using this approach, we discuss the issues of readjusting inconsistent and/or underspecified hyponymy hierarchies taken from the conventional dictionary, sense distinctions as opposed to the synonym sets of wordnets, generating semantic wordnet relations on the basis of sense definitions, and finally, supplementing missing or implicit information.","Language Resources and Evaluation",2009,"Yes"," wordnet dictionary lexical semantics semantic relations hyponymy nouns verbs dannet challenge compiling wordnet danish reusing monolingual dictionary paper contribution discussion compiling computational lexical resources conventional dictionaries describes theoretical practical problems encountered reusing conventional dictionary compiling lexical semantic resource terms wordnet specifically describes methodological issues compiling wordnet danish dannet monolingual basis applying translational expansion method princeton wordnet english source apply basis large corpus based printed dictionary modern danish approach discuss issues readjusting inconsistent underspecified hyponymy hierarchies conventional dictionary sense distinctions opposed synonym sets wordnets generating semantic wordnet relations basis sense definitions finally supplementing missing implicit information",1
"KeywordsTimeML Temporal annotation Temporal relations Information extraction Evaluation Corpus creation ","the tempeval challenge identifying temporal relations in text","TempEval is a framework for evaluating systems that automatically annotate texts with temporal relations. It was created in the context of the SemEval 2007 workshop and uses the TimeML annotation language. The evaluation consists of three subtasks of temporal annotation: anchoring an event to a time expression in the same sentence, anchoring an event to the document creation time, and ordering main events in consecutive sentences. In this paper we describe the TempEval task and the systems that participated in the evaluation. In addition, we describe how further task decomposition can bring even more structure to the evaluation of temporal relations.","Language Resources and Evaluation",2009,"No"," timeml temporal annotation temporal relations information extraction evaluation corpus creation tempeval challenge identifying temporal relations text tempeval framework evaluating systems automatically annotate texts temporal relations created context semeval workshop timeml annotation language evaluation consists subtasks temporal annotation anchoring event time expression sentence anchoring event document creation time ordering main events consecutive sentences paper describe tempeval task systems participated evaluation addition describe task decomposition bring structure evaluation temporal relations",0
"KeywordsCollocation extraction Evaluation Hybrid methods Multilingual issues Syntactic parsing ","multilingual collocation extraction with a syntactic parser","An impressive amount of work was devoted over the past few decades to collocation extraction. The state of the art shows that there is a sustained interest in the morphosyntactic preprocessing of texts in order to better identify candidate expressions; however, the treatment performed is, in most cases, limited (lemmatization, POS-tagging, or shallow parsing). This article presents a collocation extraction system based on the full parsing of source corpora, which supports four languages: English, French, Spanish, and Italian. The performance of the system is compared against that of the standard mobile-window method. The evaluation experiment investigates several levels of the significance lists, uses a fine-grained annotation schema, and covers all the languages supported. Consistent results were obtained for these languages: parsing, even if imperfect, leads to a significant improvement in the quality of results, in terms of collocational precision (between 16.4 and 29.7%, depending on the language; 20.1% overall), MWE precision (between 19.9 and 35.8%; 26.1% overall), and grammatical precision (between 47.3 and 67.4%; 55.6% overall). This positive result bears a high importance, especially in the perspective of the subsequent integration of extraction results in other NLP applications.","Language Resources and Evaluation",2009,"No"," collocation extraction evaluation hybrid methods multilingual issues syntactic parsing multilingual collocation extraction syntactic parser impressive amount work devoted past decades collocation extraction state art shows sustained interest morphosyntactic preprocessing texts order identify candidate expressions treatment performed cases limited lemmatization pos tagging shallow parsing article presents collocation extraction system based full parsing source corpora supports languages english french spanish italian performance system compared standard mobile window method evaluation experiment investigates levels significance lists fine grained annotation schema covers languages supported consistent results obtained languages parsing imperfect leads significant improvement quality results terms collocational precision depending language mwe precision grammatical precision positive result bears high importance perspective subsequent integration extraction results nlp applications",0
"KeywordsWord sense disambiguation Sense granularity Maximum entropy Linguistically motivated features Linear regression ","improving english verb sense disambiguation performance with linguistically motivated features and clear sense distinction boundaries","This paper presents a high-performance broad-coverage supervised word sense disambiguation (WSD) system for English verbs that uses linguistically motivated features and a smoothed maximum entropy machine learning model. We describe three specific enhancements to our system’s treatment of linguistically motivated features which resulted in the best published results on SENSEVAL-2 verbs. We then present the results of training our system on OntoNotes data, both the SemEval-2007 task and additional data. OntoNotes data is designed to provide clear sense distinctions, based on using explicit syntactic and semantic criteria to group WordNet senses, with sufficient examples to constitute high quality, broad coverage training data. Using similar syntactic and semantic features for WSD, we achieve performance comparable to that of human taggers, and competitive with the top results for the SemEval-2007 task. Empirical analysis of our results suggests that clarifying sense boundaries and/or increasing the number of training instances for certain verbs could further improve system performance.","Language Resources and Evaluation",2009,"No"," word sense disambiguation sense granularity maximum entropy linguistically motivated features linear regression improving english verb sense disambiguation performance linguistically motivated features clear sense distinction boundaries paper presents high performance broad coverage supervised word sense disambiguation wsd system english verbs linguistically motivated features smoothed maximum entropy machine learning model describe specific enhancements system treatment linguistically motivated features resulted published results senseval verbs present results training system ontonotes data semeval task additional data ontonotes data designed provide clear sense distinctions based explicit syntactic semantic criteria group wordnet senses sufficient examples constitute high quality broad coverage training data similar syntactic semantic features wsd achieve performance comparable human taggers competitive top results semeval task empirical analysis results suggests clarifying sense boundaries increasing number training instances verbs improve system performance",0
"KeywordsLexical substitution Word sense disambiguation SemEval-2007 ","the english lexical substitution task","Since the inception of the Senseval series there has been a great deal of debate in the word sense disambiguation (WSD) community on what the right sense distinctions are for evaluation, with the consensus of opinion being that the distinctions should be relevant to the intended application. A solution to the above issue is lexical substitution, i.e. the replacement of a target word in context with a suitable alternative substitute. In this paper, we describe the English lexical substitution task and report an exhaustive evaluation of the systems participating in the task organized at SemEval-2007. The aim of this task is to provide an evaluation where the sense inventory is not predefined and where performance on the task would bode well for applications. The task not only reflects WSD capabilities, but also can be used to compare lexical resources, whether man-made or automatically created, and has the potential to benefit several natural-language applications.","Language Resources and Evaluation",2009,"No"," lexical substitution word sense disambiguation semeval english lexical substitution task inception senseval series great deal debate word sense disambiguation wsd community sense distinctions evaluation consensus opinion distinctions relevant intended application solution issue lexical substitution replacement target word context suitable alternative substitute paper describe english lexical substitution task report exhaustive evaluation systems participating task organized semeval aim task provide evaluation sense inventory predefined performance task bode applications task reflects wsd capabilities compare lexical resources man made automatically created potential benefit natural language applications",0
"KeywordsMetonymy Selectional restrictions Shared task evaluation ","data and models for metonymy resolution","We describe the first shared task for figurative language resolution, which was organised within SemEval-2007 and focused on metonymy. The paper motivates the linguistic principles of data sampling and annotation and shows the task’s feasibility via human agreement. The five participating systems mainly used supervised approaches exploiting a variety of features, of which grammatical relations proved to be the most useful. We compare the systems’ performance to automatic baselines as well as to a manually simulated approach based on selectional restriction violations, showing some limitations of this more traditional approach to metonymy recognition. The main problem supervised systems encountered is data sparseness, since metonymies in general tend to occur more rarely than literal uses. Also, within metonymies, the reading distribution is skewed towards a few frequent metonymy types. Future task developments should focus on addressing this issue.","Language Resources and Evaluation",2009,"No"," metonymy selectional restrictions shared task evaluation data models metonymy resolution describe shared task figurative language resolution organised semeval focused metonymy paper motivates linguistic principles data sampling annotation shows task feasibility human agreement participating systems supervised approaches exploiting variety features grammatical relations proved compare systems performance automatic baselines manually simulated approach based selectional restriction violations showing limitations traditional approach metonymy recognition main problem supervised systems encountered data sparseness metonymies general tend occur rarely literal metonymies reading distribution skewed frequent metonymy types future task developments focus addressing issue",0
"KeywordsVerb valence extraction EM algorithm Co-occurrence matrices Polish language ","valence extraction using em selection and co occurrence matrices","This paper discusses two new procedures for extracting verb valences from raw texts, with an application to the Polish language. The first novel technique, the EM selection algorithm, performs unsupervised disambiguation of valence frame forests, obtained by applying a non-probabilistic deep grammar parser and some post-processing to the text. The second new idea concerns filtering of incorrect frames detected in the parsed text and is motivated by an observation that verbs which take similar arguments tend to have similar frames. This phenomenon is described in terms of newly introduced co-occurrence matrices. Using co-occurrence matrices, we split filtering into two steps. The list of valid arguments is first determined for each verb, whereas the pattern according to which the arguments are combined into frames is computed in the following stage. Our best extracted dictionary reaches an F-score of 45%, compared to an F-score of 39% for the standard frame-based BHT filtering.","Language Resources and Evaluation",2009,"No"," verb valence extraction em algorithm occurrence matrices polish language valence extraction em selection occurrence matrices paper discusses procedures extracting verb valences raw texts application polish language technique em selection algorithm performs unsupervised disambiguation valence frame forests obtained applying probabilistic deep grammar parser post processing text idea concerns filtering incorrect frames detected parsed text motivated observation verbs similar arguments tend similar frames phenomenon terms newly introduced occurrence matrices occurrence matrices split filtering steps list valid arguments determined verb pattern arguments combined frames computed stage extracted dictionary reaches score compared score standard frame based bht filtering",0
"KeywordsTerm extraction Term weighting Maximal marginal relevance Document re-ranking Information retrieval ","chinese document re ranking based on automatically acquired term resource","In this paper, we address the problem of document re-ranking in information retrieval, which is usually conducted after initial retrieval to improve rankings of relevant documents. To deal with this problem, we propose a method which automatically constructs a term resource specific to the document collection and then applies the resource to document re-ranking. The term resource includes a list of terms extracted from the documents as well as their weighting and correlations computed after initial retrieval. The term weighting based on local and global distribution ensures the re-ranking not sensitive to different choices of pseudo relevance, while the term correlation helps avoid any bias to certain specific concept embedded in queries. Experiments with NTCIR3 data show that the approach can not only improve performance of initial retrieval, but also make significant contribution to standard query expansion.","Language Resources and Evaluation",2009,"No"," term extraction term weighting maximal marginal relevance document ranking information retrieval chinese document ranking based automatically acquired term resource paper address problem document ranking information retrieval conducted initial retrieval improve rankings relevant documents deal problem propose method automatically constructs term resource specific document collection applies resource document ranking term resource includes list terms extracted documents weighting correlations computed initial retrieval term weighting based local global distribution ensures ranking sensitive choices pseudo relevance term correlation helps avoid bias specific concept embedded queries experiments ntcir data show approach improve performance initial retrieval make significant contribution standard query expansion",0
"KeywordsThesauri Controlled vocabularies Manual translation process ","a cost effective lexical acquisition process for large scale thesaurus translation","Thesauri and controlled vocabularies facilitate access to digital collections by explicitly representing the underlying principles of organization. Translation of such resources into multiple languages is an important component for providing multilingual access. However, the specificity of vocabulary terms in most thesauri precludes fully-automatic translation using general-domain lexical resources. In this paper, we present an efficient process for leveraging human translations to construct domain-specific lexical resources. This process is illustrated on a thesaurus of 56,000 concepts used to catalog a large archive of oral histories. We elicited human translations on a small subset of concepts, induced a probabilistic phrase dictionary from these translations, and used the resulting resource to automatically translate the rest of the thesaurus. Two separate evaluations demonstrate the acceptability of the automatic translations and the cost-effectiveness of our approach.","Language Resources and Evaluation",2009,"No"," thesauri controlled vocabularies manual translation process cost effective lexical acquisition process large scale thesaurus translation thesauri controlled vocabularies facilitate access digital collections explicitly representing underlying principles organization translation resources multiple languages important component providing multilingual access specificity vocabulary terms thesauri precludes fully automatic translation general domain lexical resources paper present efficient process leveraging human translations construct domain specific lexical resources process illustrated thesaurus concepts catalog large archive oral histories elicited human translations small subset concepts induced probabilistic phrase dictionary translations resulting resource automatically translate rest thesaurus separate evaluations demonstrate acceptability automatic translations cost effectiveness approach",0
"KeywordsSemantic relations Nominals Classification SemEval ","classification of semantic relations between nominals","The NLP community has shown a renewed interest in deeper semantic analyses, among them automatic recognition of semantic relations in text. We present the development and evaluation of a semantic analysis task: automatic recognition of relations between pairs of nominals in a sentence. The task was part of SemEval-2007, the fourth edition of the semantic evaluation event previously known as SensEval. Apart from the observations we have made, the long-lasting effect of this task may be a framework for comparing approaches to the task. We introduce the problem of recognizing relations between nominals, and in particular the process of drafting and refining the definitions of the semantic relations. We show how we created the training and test data, list and briefly describe the 15 participating systems, discuss the results, and conclude with the lessons learned in the course of this exercise.","Language Resources and Evaluation",2009,"No"," semantic relations nominals classification semeval classification semantic relations nominals nlp community shown renewed interest deeper semantic analyses automatic recognition semantic relations text present development evaluation semantic analysis task automatic recognition relations pairs nominals sentence task part semeval fourth edition semantic evaluation event previously senseval observations made long lasting effect task framework comparing approaches task introduce problem recognizing relations nominals process drafting refining definitions semantic relations show created training test data list briefly describe participating systems discuss results conclude lessons learned exercise",0
"LEXICON, LINGUISTICS, COMPUTATIONAL linguistics, MULTILINGUALISM, MONOLINGUALISM, DATABASES, Explanatory combinatorial lexicology, Graph model, Lexical database, Lexical function","lexical systems graph models of natural language lexicons","We introduce a new type of lexical structure called lexical system, an interoperable model that can feed both monolingual and multilingual language resources. We begin with a formal characterization of lexical systems as simple directed graphs, solely made up of nodes corresponding to lexical entities and links. To illustrate our approach, we present data borrowed from a lexical system that has been generated from the French DiCo database. We later explain how the compilation of the original dictionary-like database into a net-like one has been made possible. Finally, we discuss the potential of the proposed lexical structure for designing multilingual lexical resources. [ABSTRACT FROM AUTHOR], Copyright of Language Resources & Evaluation is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual )","Language Resources and Evaluation",2009,"No","lexicon linguistics computational linguistics multilingualism monolingualism databases explanatory combinatorial lexicology graph model lexical database lexical function lexical systems graph models natural language lexicons introduce type lexical structure called lexical system interoperable model feed monolingual multilingual language resources begin formal characterization lexical systems simple directed graphs solely made nodes lexical entities links illustrate approach present data borrowed lexical system generated french dico database explain compilation original dictionary database net made finally discuss potential proposed lexical structure designing multilingual lexical resources abstract author copyright language resources evaluation property springer nature content copied emailed multiple sites posted listserv copyright holder express written permission users print download email articles individual ",0
"KeywordsAgreement Annotation Conceptual information Evaluation Lexical information Mapping Metaphor Resource creation ","the hamburg metaphor database project issues in resource creation","This paper concerns metaphor resource creation. It provides an account of methods used, problems discovered, and insights gained at the Hamburg Metaphor Database project, intended to inform similar resource creation initiatives, as well as future metaphor processing algorithms. After introducing the project, the theoretical underpinnings that motivate the subdivision of represented information into a conceptual and a lexical level are laid out. The acquisition of metaphor attestations from electronic corpora is explained, and annotation practices as well as database contents are evaluated. The paper concludes with an overview of related projects and an outline of possible future work.","Language Resources and Evaluation",2008,"Yes"," agreement annotation conceptual information evaluation lexical information mapping metaphor resource creation hamburg metaphor database project issues resource creation paper concerns metaphor resource creation account methods problems discovered insights gained hamburg metaphor database project intended inform similar resource creation initiatives future metaphor processing algorithms introducing project theoretical underpinnings motivate subdivision represented information conceptual lexical level laid acquisition metaphor attestations electronic corpora explained annotation practices database contents evaluated paper concludes overview related projects outline future work",1
"KeywordsWeb as corpus News corpus Web-based tagged Bengali news corpus Named entity Named entity recognition ","a web based bengali news corpus for named entity recognition","The rapid development of language resources and tools using machine learning techniques for less computerized languages requires appropriately tagged corpus. A tagged Bengali news corpus has been developed from the web archive of a widely read Bengali newspaper. A web crawler retrieves the web pages in Hyper Text Markup Language (HTML) format from the news archive. At present, the corpus contains approximately 34 million wordforms. Named Entity Recognition (NER) systems based on pattern based shallow parsing with or without using linguistic knowledge have been developed using a part of this corpus. The NER system that uses linguistic knowledge has performed better yielding highest F-Score values of 75.40%, 72.30%, 71.37%, and 70.13% for person, location, organization, and miscellaneous names, respectively.","Language Resources and Evaluation",2008,"Yes"," web corpus news corpus web based tagged bengali news corpus named entity named entity recognition web based bengali news corpus named entity recognition rapid development language resources tools machine learning techniques computerized languages requires appropriately tagged corpus tagged bengali news corpus developed web archive widely read bengali newspaper web crawler retrieves web pages hyper text markup language html format news archive present corpus approximately million wordforms named entity recognition ner systems based pattern based shallow parsing linguistic knowledge developed part corpus ner system linguistic knowledge performed yielding highest score values person location organization miscellaneous names ",1
"KeywordsLexical semantics Verb classes Semantic resources Semantic features Resource linking ","comparing and combining semantic verb classifications","In this article, we address the task of comparing and combining different semantic verb classifications within one language. We present a methodology for the manual analysis of individual resources on the level of semantic features. The resulting representations can be aligned across resources, and allow a contrastive analysis of these resources. In a case study on the Manner of Motion domain across four German verb classifications, we find that some features are used in all resources, while others reflect individual emphases on specific meaning aspects. We also provide evidence that feature representations can ultimately provide the basis for linking verb classes themselves across resources, which allows us to combine their coverage and descriptive detail.","Language Resources and Evaluation",2008,"No"," lexical semantics verb classes semantic resources semantic features resource linking comparing combining semantic verb classifications article address task comparing combining semantic verb classifications language present methodology manual analysis individual resources level semantic features resulting representations aligned resources contrastive analysis resources case study manner motion domain german verb classifications find features resources reflect individual emphases specific meaning aspects provide evidence feature representations ultimately provide basis linking verb classes resources combine coverage descriptive detail",0
"KeywordsNumeral classifier Classifier ontology Semantic representation Ontological relations Natural language processing (NLP) techniques Human language technology (HLT) ","semantic representation of korean numeral classifier and its ontology building for hlt applications","The complexity of Korean numeral classifiers demands semantic as well as computational approaches that employ natural language processing (NLP) techniques. The classifier is a universal linguistic device, having the two functions of quantifying and classifying nouns in noun phrase constructions. Many linguistic studies have focused on the fact that numeral classifiers afford decisive clues to categorizing nouns. However, few studies have dealt with the semantic categorization of classifiers and their semantic relations to the nouns they quantify and categorize in building ontologies. In this article, we propose the semantic recategorization of the Korean numeral classifiers in the context of classifier ontology based on large corpora and KorLex Noun 1.5 (Korean wordnet; Korean Lexical Semantic Network), considering its high applicability in the NLP domain. In particular, the classifier can be effectively used to predict the semantic characteristics of nouns and to process them appropriately in NLP. The major challenge is to make such semantic classification and the attendant NLP techniques efficient. Accordingly, a Korean numeral classifier ontology (KorLexClas 1.0), including semantic hierarchies and relations to nouns, was constructed.","Language Resources and Evaluation",2008,"No"," numeral classifier classifier ontology semantic representation ontological relations natural language processing nlp techniques human language technology hlt semantic representation korean numeral classifier ontology building hlt applications complexity korean numeral classifiers demands semantic computational approaches employ natural language processing nlp techniques classifier universal linguistic device functions quantifying classifying nouns noun phrase constructions linguistic studies focused fact numeral classifiers afford decisive clues categorizing nouns studies dealt semantic categorization classifiers semantic relations nouns quantify categorize building ontologies article propose semantic recategorization korean numeral classifiers context classifier ontology based large corpora korlex noun korean wordnet korean lexical semantic network high applicability nlp domain classifier effectively predict semantic characteristics nouns process appropriately nlp major challenge make semantic classification attendant nlp techniques efficient korean numeral classifier ontology korlexclas including semantic hierarchies relations nouns constructed",0
"KeywordsLanguage resources Hebrew Corpora Lexicon Morphological processing WordNet ","language resources for hebrew","We describe a suite of standards, resources and tools for computational encoding and processing of Modern Hebrew texts. These include an array of XML schemas for representing linguistic resources; a variety of text corpora, raw, automatically processed and manually annotated; lexical databases, including a broad-coverage monolingual lexicon, a bilingual dictionary and a WordNet; and morphological processors which can analyze, generate and disambiguate Hebrew word forms. The resources are developed under centralized supervision, so that they are compatible with each other. They are freely available and many of them have already been used for several applications, both academic and industrial.","Language Resources and Evaluation",2008,"Yes"," language resources hebrew corpora lexicon morphological processing wordnet language resources hebrew describe suite standards resources tools computational encoding processing modern hebrew texts include array xml schemas representing linguistic resources variety text corpora raw automatically processed manually annotated lexical databases including broad coverage monolingual lexicon bilingual dictionary wordnet morphological processors analyze generate disambiguate hebrew word forms resources developed centralized supervision compatible freely applications academic industrial",1
"KeywordsFacial Expression Annotation Scheme Virtual Character Multimodal Interface Virtual Agent ","introduction to the special issue on multimodal corpora for modeling human multimodal behavior","There is an increasing interest in multimodal communication as suggested by several national and international projects (ISLE, HUMAINE, SIMILAR, CHIL, AMI, CALO, VACE, CALLAS), the attention devoted to the topic by well-known institutions and organizations (the National Institute of Standards and Technology, the Linguistic Data Consortium), and the success of conferences related to multimodal communication (ICMI, IVA, Gesture, Measuring Behavior, Nordic Symposium on Multimodal Communication, LREC Workshops on Multimodal Corpora).","Language Resources and Evaluation",2008,"No"," facial expression annotation scheme virtual character multimodal interface virtual agent introduction special issue multimodal corpora modeling human multimodal behavior increasing interest multimodal communication suggested national international projects isle humaine similar chil ami calo vace callas attention devoted topic institutions organizations national institute standards technology linguistic data consortium success conferences related multimodal communication icmi iva gesture measuring behavior nordic symposium multimodal communication lrec workshops multimodal corpora",0
"KeywordsChinese chat language Phonetic mapping Chat language Modelling Chat term normalization Natural language processing ","normalization of chinese chat language","Real-time communication platforms such as ICQ, MSN and online chat rooms are getting more popular than ever on the Internet. There are, however, real risks where criminals and terrorists can perpetrate illegal and criminal abuses. This highlights the security significance of accurate detection and translation of the chat language to its stand language counterpart. The language used on these platforms differs significantly from the standard language. This language, referred to as chat language, is comparatively informal, anomalous and dynamic. Such features render conventional language resources such as dictionaries, and processing tools such as parsers ineffective. In this paper, we present the NIL corpus, a chat language text collection annotated to facilitate training and testing of chat language processing algorithms. We analyse the NIL corpus to study the linguistic characteristics and contextual behaviour of a chat language. First we observe that majority of the chat terms, i.e. informal words in a chat text, is formed by phonetic mapping. We then propose the eXtended Source Channel Model (XSCM) for the normalization of the chat language, which is a process to convert messages expressed in a chat language to its standard language counterpart. Experimental results indicate that the performance of XSCM in terms of chat term recognition and normalization accuracy is superior to its Source Channel Model (SCM) counterparts, and is also more consistent over time.","Language Resources and Evaluation",2008,"No"," chinese chat language phonetic mapping chat language modelling chat term normalization natural language processing normalization chinese chat language real time communication platforms icq msn online chat rooms popular internet real risks criminals terrorists perpetrate illegal criminal abuses highlights security significance accurate detection translation chat language stand language counterpart language platforms differs significantly standard language language referred chat language comparatively informal anomalous dynamic features render conventional language resources dictionaries processing tools parsers ineffective paper present nil corpus chat language text collection annotated facilitate training testing chat language processing algorithms analyse nil corpus study linguistic characteristics contextual behaviour chat language observe majority chat terms informal words chat text formed phonetic mapping propose extended source channel model xscm normalization chat language process convert messages expressed chat language standard language counterpart experimental results performance xscm terms chat term recognition normalization accuracy superior source channel model scm counterparts consistent time",0
"KeywordsThai ontology learning Lexico-syntactic patterns Taxonomic list ","automatic building of an ontology on the basis of text corpora in thai","This paper presents a methodology for automatic learning of ontologies from Thai text corpora, by extraction of terms and relations. A shallow parser is used to chunk texts on which we identify taxonomic relations with the help of cues: lexico-syntactic patterns and item lists. The main advantage of the approach is that it simplify the task of concept and relation labeling since cues help for identifying the ontological concept and hinting their relation. However, these techniques pose certain problems, i.e. cue word ambiguity, item list identification, and numerous candidate terms. We also propose the methodology to solve these problems by using lexicon and co-occurrence features and weighting them with information gain. The precision, recall and F-measure of the system are 0.74, 0.78 and 0.76, respectively.","Language Resources and Evaluation",2008,"No"," thai ontology learning lexico syntactic patterns taxonomic list automatic building ontology basis text corpora thai paper presents methodology automatic learning ontologies thai text corpora extraction terms relations shallow parser chunk texts identify taxonomic relations cues lexico syntactic patterns item lists main advantage approach simplify task concept relation labeling cues identifying ontological concept hinting relation techniques pose problems cue word ambiguity item list identification numerous candidate terms propose methodology solve problems lexicon occurrence features weighting information gain precision recall measure system ",0
"KeywordsEvaluation methodology Information extraction Machine learning ","evaluation of machine learning based information extraction algorithms criticisms and recommendations","We survey the evaluation methodology adopted in information extraction (IE), as defined in a few different efforts applying machine learning (ML) to IE. We identify a number of critical issues that hamper comparison of the results obtained by different researchers. Some of these issues are common to other NLP-related tasks: e.g., the difficulty of exactly identifying the effects on performance of the data (sample selection and sample size), of the domain theory (features selected), and of algorithm parameter settings. Some issues are specific to IE: how leniently to assess inexact identification of filler boundaries, the possibility of multiple fillers for a slot, and how the counting is performed. We argue that, when specifying an IE task, these issues should be explicitly addressed, and a number of methodological characteristics should be clearly defined. To empirically verify the practical impact of the issues mentioned above, we perform a survey of the results of different algorithms when applied to a few standard datasets. The survey shows a serious lack of consensus on these issues, which makes it difficult to draw firm conclusions on a comparative evaluation of the algorithms. Our aim is to elaborate a clear and detailed experimental methodology and propose it to the IE community. Widespread agreement on this proposal should lead to future IE comparative evaluations that are fair and reliable. To demonstrate the way the methodology is to be applied we have organized and run a comparative evaluation of ML-based IE systems (the Pascal Challenge on ML-based IE) where the principles described in this article are put into practice. In this article we describe the proposed methodology and its motivations. The Pascal evaluation is then described and its results presented.","Language Resources and Evaluation",2008,"No"," evaluation methodology information extraction machine learning evaluation machine learning based information extraction algorithms criticisms recommendations survey evaluation methodology adopted information extraction defined efforts applying machine learning ml identify number critical issues hamper comparison results obtained researchers issues common nlp related tasks difficulty identifying effects performance data sample selection sample size domain theory features selected algorithm parameter settings issues specific leniently assess inexact identification filler boundaries possibility multiple fillers slot counting performed argue task issues explicitly addressed number methodological characteristics defined empirically verify practical impact issues mentioned perform survey results algorithms applied standard datasets survey shows lack consensus issues makes difficult draw firm conclusions comparative evaluation algorithms aim elaborate clear detailed experimental methodology propose community widespread agreement proposal lead future comparative evaluations fair reliable demonstrate methodology applied organized run comparative evaluation ml based systems pascal challenge ml based principles article put practice article describe proposed methodology motivations pascal evaluation results presented",0
"KeywordsMachine Translation Name Entity Recognition Language Resource Asian Language Ontology Building ","asian language resources the state of the art","This special issue of Language Resources and Evaluation, entitled “New Frontiers in Asian Language Resources”, complements the earlier special double issue on Asian Language Processing: State of the Art Resources and Processing (Huang et al. 2006) by presenting eight papers describing specific Asian language resources. As Bird and Simons (2003) explain, research on language resources must deal with how the resources can be acquired and documented as well as how the resources can be accessed and used. Among the eight papers in this issue, the first four papers focus on resources, while the latter four target specific application tasks and describe resource building in the contexts of these applications.","Language Resources and Evaluation",2008,"No"," machine translation entity recognition language resource asian language ontology building asian language resources state art special issue language resources evaluation entitled frontiers asian language resources complements earlier special double issue asian language processing state art resources processing huang al presenting papers describing specific asian language resources bird simons explain research language resources deal resources acquired documented resources accessed papers issue papers focus resources target specific application tasks describe resource building contexts applications",0
"KeywordsAudio-visual database Dyadic interaction Emotion Emotional assessment Motion capture system ","iemocap interactive emotional dyadic motion capture database","Since emotions are expressed through a combination of verbal and non-verbal channels, a joint analysis of speech and gestures is required to understand expressive human communication. To facilitate such investigations, this paper describes a new corpus named the “interactive emotional dyadic motion capture database” (IEMOCAP), collected by the Speech Analysis and Interpretation Laboratory (SAIL) at the University of Southern California (USC). This database was recorded from ten actors in dyadic sessions with markers on the face, head, and hands, which provide detailed information about their facial expressions and hand movements during scripted and spontaneous spoken communication scenarios. The actors performed selected emotional scripts and also improvised hypothetical scenarios designed to elicit specific types of emotions (happiness, anger, sadness, frustration and neutral state). The corpus contains approximately 12 h of data. The detailed motion capture information, the interactive setting to elicit authentic emotions, and the size of the database make this corpus a valuable addition to the existing databases in the community for the study and modeling of multimodal and expressive human communication.","Language Resources and Evaluation",2008,"Yes"," audio visual database dyadic interaction emotion emotional assessment motion capture system iemocap interactive emotional dyadic motion capture database emotions expressed combination verbal verbal channels joint analysis speech gestures required understand expressive human communication facilitate investigations paper describes corpus named interactive emotional dyadic motion capture database iemocap collected speech analysis interpretation laboratory sail university southern california usc database recorded ten actors dyadic sessions markers face head hands provide detailed information facial expressions hand movements scripted spontaneous spoken communication scenarios actors performed selected emotional scripts improvised hypothetical scenarios designed elicit specific types emotions happiness anger sadness frustration neutral state corpus approximately data detailed motion capture information interactive setting elicit authentic emotions size database make corpus valuable addition existing databases community study modeling multimodal expressive human communication",1
"KeywordsCorpora Language resources Language tools Lexicon Machine translation Morphology ","building language resources for a multi engine english filipino machine translation system","In this paper, we present the building of various language resources for a multi-engine bi-directional English-Filipino Machine Translation (MT) system. Since linguistics information on Philippine languages are available, but as of yet, the focus has been on theoretical linguistics and little is done on the computational aspects of these languages, attempts are reported here on the manual construction of these language resources such as the grammar, lexicon, morphological information, and the corpora which were literally built from almost non-existent digital forms. Due to the inherent difficulties of manual construction, we also discuss our experiments on various technologies for automatic extraction of these resources to handle the intricacies of the Filipino language, designed with the intention of using them for the MT system. To implement the different MT engines and to ensure the improvement of translation quality, other language tools (such as the morphological analyzer and generator, and the part of speech tagger) were developed.","Language Resources and Evaluation",2008,"Yes"," corpora language resources language tools lexicon machine translation morphology building language resources multi engine english filipino machine translation system paper present building language resources multi engine bi directional english filipino machine translation mt system linguistics information philippine languages focus theoretical linguistics computational aspects languages attempts reported manual construction language resources grammar lexicon morphological information corpora literally built existent digital forms due inherent difficulties manual construction discuss experiments technologies automatic extraction resources handle intricacies filipino language designed intention mt system implement mt engines ensure improvement translation quality language tools morphological analyzer generator part speech tagger developed",1
"KeywordsPartial cognates Word sense disambiguation Monolingual bootstrapping Bilingual bootstrapping ","disambiguation of partial cognates","Partial cognates are pairs of words in two languages that have the same meaning in some, but not all contexts. Detecting the actual meaning of a partial cognate in context can be useful for Machine Translation tools and for Computer-Assisted Language Learning tools. We propose a supervised and a semi-supervised method to disambiguate partial cognates between two languages: French and English. The methods use only automatically-labeled data; therefore they can be applied to other pairs of languages as well. The aim of our work is to automatically detect the meaning of a French partial cognate word in a specific context.","Language Resources and Evaluation",2008,"No"," partial cognates word sense disambiguation monolingual bootstrapping bilingual bootstrapping disambiguation partial cognates partial cognates pairs words languages meaning contexts detecting actual meaning partial cognate context machine translation tools computer assisted language learning tools propose supervised semi supervised method disambiguate partial cognates languages french english methods automatically labeled data applied pairs languages aim work automatically detect meaning french partial cognate word specific context",0
"KeywordsInformation extraction Product named entity recognition Hierarchical hidden Markov model ","product named entity recognition in chinese text","There are many expressive and structural differences between product names and general named entities such as person names, location names and organization names. To date, there has been little research on product named entity recognition (NER), which is crucial and valuable for information extraction in the field of market intelligence. This paper focuses on product NER (PRO NER) in Chinese text. First, we describe our efforts on data annotation, including well-defined specifications, data analysis and development of a corpus with annotated product named entities. Second, a hierarchical hidden Markov model-based approach to PRO NER is proposed and evaluated. Extensive experiments show that the proposed method outperforms the cascaded maximum entropy model and obtains promising results on the data sets of two different electronic product domains (digital and cell phone).","Language Resources and Evaluation",2008,"No"," information extraction product named entity recognition hierarchical hidden markov model product named entity recognition chinese text expressive structural differences product names general named entities person names location names organization names date research product named entity recognition ner crucial valuable information extraction field market intelligence paper focuses product ner pro ner chinese text describe efforts data annotation including defined specifications data analysis development corpus annotated product named entities hierarchical hidden markov model based approach pro ner proposed evaluated extensive experiments show proposed method outperforms cascaded maximum entropy model obtains promising results data sets electronic product domains digital cell phone",0
"KeywordsInformation extraction OWL reasoning Ontologies ","semanticlean","In our research on using information extraction to help populate semantic web resources, we have encountered significant obstacles to interoperability between the technologies. We believe these obstacles to be endemic to the basic paradigms and not quirks of the specific implementations we have worked with. In particular, we identify five dimensions of interoperability that must be addressed to successfully employ information extraction systems to populate semantic web resources that are suitable for reasoning. We call the task of transforming IE data into knowledge-based resources knowledge integration and we report results of experiments in which the knowledge integration process uses the deeper semantics of OWL ontologies to improve by between 8% and 13% the precision of relation extraction from text.","Language Resources and Evaluation",2008,"No"," information extraction owl reasoning ontologies semanticlean research information extraction populate semantic web resources encountered significant obstacles interoperability technologies obstacles endemic basic paradigms quirks specific implementations worked identify dimensions interoperability addressed successfully employ information extraction systems populate semantic web resources suitable reasoning call task transforming data knowledge based resources knowledge integration report results experiments knowledge integration process deeper semantics owl ontologies improve precision relation extraction text",0
"KeywordsJapanese Treebank Sensebank HPSG Ontology ","the hinoki syntactic and semantic treebank of japanese","In this paper we describe the current state of a new Japanese lexical resource: the Hinoki treebank. The treebank is built from dictionary definitions, examples and news text, and uses an HPSG based Japanese grammar to encode both syntactic and semantic information. It is combined with an ontology based on the definition sentences to give a detailed sense level description of the most familiar 28,000 words of Japanese.","Language Resources and Evaluation",2008,"Yes"," japanese treebank sensebank hpsg ontology hinoki syntactic semantic treebank japanese paper describe current state japanese lexical resource hinoki treebank treebank built dictionary definitions examples news text hpsg based japanese grammar encode syntactic semantic information combined ontology based definition sentences give detailed sense level description familiar words japanese",1
"KeywordsDialogue act tagsets Conversational corpora Tagset dimensionality ","dimensionality of dialogue act tagsets","This article compares one-dimensional and multi-dimensional dialogue act tagsets used for automatic labeling of utterances. The influence of tagset dimensionality on tagging accuracy is first discussed theoretically, then based on empirical data from human and automatic annotations of large scale resources, using four existing tagsets: damsl, swbd-damsl, icsi-mrda and maltus. The Dominant Function Approximation proposes that automatic dialogue act taggers could focus initially on finding the main dialogue function of each utterance, which is empirically acceptable and has significant practical relevance.","Language Resources and Evaluation",2008,"No"," dialogue act tagsets conversational corpora tagset dimensionality dimensionality dialogue act tagsets article compares dimensional multi dimensional dialogue act tagsets automatic labeling utterances influence tagset dimensionality tagging accuracy discussed theoretically based empirical data human automatic annotations large scale resources existing tagsets damsl swbd damsl icsi mrda maltus dominant function approximation proposes automatic dialogue act taggers focus initially finding main dialogue function utterance empirically acceptable significant practical relevance",0
"KeywordsQuality Assessment Validation Evaluation Language resources Spoken language resources ","validation of spoken language resources an overview of basic aspects","Spoken language resources (SLRs) are essential for both research and application development. In this article we clarify the concept of SLR validation. We define validation and how it differs from evaluation. Further, relevant principles of SLR validation are outlined. We argue that the best way to validate SLRs is to implement validation throughout SLR production and have it carried out by an external and experienced institute. We address which tasks should be carried out by the validation institute, and which not. Further, we list the basic issues that validation criteria for SLR should address. A standard validation protocol is shown, illustrating how validation can prove its value throughout the production phase in terms of pre-validation, full validation and pre-release validation.","Language Resources and Evaluation",2008,"No"," quality assessment validation evaluation language resources spoken language resources validation spoken language resources overview basic aspects spoken language resources slrs essential research application development article clarify concept slr validation define validation differs evaluation relevant principles slr validation outlined argue validate slrs implement validation slr production carried external experienced institute address tasks carried validation institute list basic issues validation criteria slr address standard validation protocol shown illustrating validation prove production phase terms pre validation full validation pre release validation",0
"KeywordsLexical classification Lexical resources Computational linguistics ","a large scale classification of english verbs","Lexical classifications have proved useful in supporting various natural language processing (NLP) tasks. The largest verb classification for English is Levin’s (1993) work which defines groupings of verbs based on syntactic and semantic properties. VerbNet (VN) (Kipper et al. 2000; Kipper-Schuler 2005)—an extensive computational verb lexicon for English—provides detailed syntactic-semantic descriptions of Levin classes. While the classes included are extensive enough for some NLP use, they are not comprehensive. Korhonen and Briscoe (2004) have proposed a significant extension of Levin’s classification which incorporates 57 novel classes for verbs not covered (comprehensively) by Levin. Korhonen and Ryant (unpublished) have recently proposed another extension including 53 additional classes. This article describes the integration of these two extensions into VN. The result is a comprehensive Levin-style classification for English verbs providing over 90% token coverage of the Proposition Bank data (Palmer et al. 2005) and thus can be highly useful for practical applications.","Language Resources and Evaluation",2008,"Yes"," lexical classification lexical resources computational linguistics large scale classification english verbs lexical classifications proved supporting natural language processing nlp tasks largest verb classification english levin work defines groupings verbs based syntactic semantic properties verbnet vn kipper al kipper schuler extensive computational verb lexicon english detailed syntactic semantic descriptions levin classes classes included extensive nlp comprehensive korhonen briscoe proposed significant extension levin classification incorporates classes verbs covered comprehensively levin korhonen ryant unpublished recently proposed extension including additional classes article describes integration extensions vn result comprehensive levin style classification english verbs providing token coverage proposition bank data palmer al highly practical applications",1
"KeywordsTree Adjoining Grammar LTAG-spinal Treebank Dependency parsing ","ltag spinal and the treebank","We introduce LTAG-spinal, a novel variant of traditional Lexicalized Tree Adjoining Grammar (LTAG) with desirable linguistic, computational and statistical properties. Unlike in traditional LTAG, subcategorization frames and the argument–adjunct distinction are left underspecified in LTAG-spinal. LTAG-spinal with adjunction constraints is weakly equivalent to LTAG. The LTAG-spinal formalism is used to extract an LTAG-spinal Treebank from the Penn Treebank with Propbank annotation. Based on Propbank annotation, predicate coordination and LTAG adjunction structures are successfully extracted. The LTAG-spinal Treebank makes explicit semantic relations that are implicit or absent from the original PTB. LTAG-spinal provides a very desirable resource for statistical LTAG parsing, incremental parsing, dependency parsing, and semantic parsing. This treebank has been successfully used to train an incremental LTAG-spinal parser and a bidirectional LTAG dependency parser.","Language Resources and Evaluation",2008,"No"," tree adjoining grammar ltag spinal treebank dependency parsing ltag spinal treebank introduce ltag spinal variant traditional lexicalized tree adjoining grammar ltag desirable linguistic computational statistical properties unlike traditional ltag subcategorization frames argument adjunct distinction left underspecified ltag spinal ltag spinal adjunction constraints weakly equivalent ltag ltag spinal formalism extract ltag spinal treebank penn treebank propbank annotation based propbank annotation predicate coordination ltag adjunction structures successfully extracted ltag spinal treebank makes explicit semantic relations implicit absent original ptb ltag spinal desirable resource statistical ltag parsing incremental parsing dependency parsing semantic parsing treebank successfully train incremental ltag spinal parser bidirectional ltag dependency parser",0
"KeywordsDocumentation Lexical types Linguistic grammar Treebank ","semi automatic documentation of an implemented linguistic grammar augmented with a treebank","We have constructed a large scale and detailed database of lexical types in Japanese from a treebank that includes detailed linguistic information. The database helps treebank annotators and grammar developers to share precise knowledge about the grammatical status of words that constitute the treebank, allowing for consistent large-scale treebanking and grammar development. In addition, it clarifies what lexical types are needed for precise Japanese NLP on the basis of the treebank. In this paper, we report on the motivation and methodology of the database construction.","Language Resources and Evaluation",2008,"No"," documentation lexical types linguistic grammar treebank semi automatic documentation implemented linguistic grammar augmented treebank constructed large scale detailed database lexical types japanese treebank includes detailed linguistic information database helps treebank annotators grammar developers share precise knowledge grammatical status words constitute treebank allowing consistent large scale treebanking grammar development addition clarifies lexical types needed precise japanese nlp basis treebank paper report motivation methodology database construction",0
NA,"adding phonetic similarity data to a lexical database","As part of a project to construct an interactive program which would encourage children to play with language by building jokes, we developed a lexical database, starting from WordNet. To the existing information about part of speech, synonymy, hyponymy, etc., we have added phonetic representations and phonetic similarity ratings for pairs of words/phrases. [ABSTRACT FROM AUTHOR], Copyright of Language Resources & Evaluation is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","Language Resources and Evaluation",2008,"Yes"," adding phonetic similarity data lexical database part project construct interactive program encourage children play language building jokes developed lexical database starting wordnet existing information part speech synonymy hyponymy added phonetic representations phonetic similarity ratings pairs wordsphrases abstract author copyright language resources evaluation property springer nature content copied emailed multiple sites posted listserv copyright holder express written permission users print download email articles individual abstract abridged warranty accuracy copy users refer original published version material full abstract copyright applies abstracts",1
"KeywordsEmotional ECA synthesis Expressivity features Facial features extraction Gesture analysis Virtual agents ","virtual agent multimodal mimicry of humans","This work is about multimodal and expressive synthesis on virtual agents, based on the analysis of actions performed by human users. As input we consider the image sequence of the recorded human behavior. Computer vision and image processing techniques are incorporated in order to detect cues needed for expressivity features extraction. The multimodality of the approach lies in the fact that both facial and gestural aspects of the user’s behavior are analyzed and processed. The mimicry consists of perception, interpretation, planning and animation of the expressions shown by the human, resulting not in an exact duplicate rather than an expressive model of the user’s original behavior.","Language Resources and Evaluation",2007,"No"," emotional eca synthesis expressivity features facial features extraction gesture analysis virtual agents virtual agent multimodal mimicry humans work multimodal expressive synthesis virtual agents based analysis actions performed human users input image sequence recorded human behavior computer vision image processing techniques incorporated order detect cues needed expressivity features extraction multimodality approach lies fact facial gestural aspects user behavior analyzed processed mimicry consists perception interpretation planning animation expressions shown human resulting exact duplicate expressive model user original behavior",0
"KeywordsCommunicative embodied feedback Contact Perception Understanding Emotions Multimodal Embodied communication ","the analysis of embodied communicative feedback in multimodal corpora a prerequisite for behavior simulation","Communicative feedback refers to unobtrusive (usually short) vocal or bodily expressions whereby a recipient of information can inform a contributor of information about whether he/she is able and willing to communicate, perceive the information, and understand the information. This paper provides a theory for embodied communicative feedback, describing the different dimensions and features involved. It also provides a corpus analysis part, describing a first data coding and analysis method geared to find the features postulated by the theory. The corpus analysis part describes different methods and statistical procedures and discusses their applicability and the possible insights gained with these methods.","Language Resources and Evaluation",2007,"No"," communicative embodied feedback contact perception understanding emotions multimodal embodied communication analysis embodied communicative feedback multimodal corpora prerequisite behavior simulation communicative feedback refers unobtrusive short vocal bodily expressions recipient information inform contributor information communicate perceive information understand information paper theory embodied communicative feedback describing dimensions features involved corpus analysis part describing data coding analysis method geared find features postulated theory corpus analysis part describes methods statistical procedures discusses applicability insights gained methods",0
"KeywordsAnnotated corpora Meetings Discourse annotation ","unleashing the killer corpus experiences in creating the multi everything ami meeting corpus","The AMI Meeting Corpus contains 100 h of meetings captured using many synchronized recording devices, and is designed to support work in speech and video processing, language engineering, corpus linguistics, and organizational psychology. It has been transcribed orthographically, with annotated subsets for everything from named entities, dialogue acts, and summaries to simple gaze and head movement. In this written version of an LREC conference keynote address, I describe the data and how it was created. If this is “killer” data, that presupposes a platform that it will “sell”; in this case, that is the NITE XML Toolkit, which allows a distributed set of users to create, store, browse, and search annotations for the same base data that are both time-aligned against signal and related to each other structurally.","Language Resources and Evaluation",2007,"Yes"," annotated corpora meetings discourse annotation unleashing killer corpus experiences creating multi ami meeting corpus ami meeting corpus meetings captured synchronized recording devices designed support work speech video processing language engineering corpus linguistics organizational psychology transcribed orthographically annotated subsets named entities dialogue acts summaries simple gaze head movement written version lrec conference keynote address describe data created killer data presupposes platform sell case nite xml toolkit distributed set users create store browse search annotations base data time aligned signal related structurally",1
"KeywordsData-driven generation Embodied conversational agents Evaluation of generated output Multimodal corpora ","corpus based generation of head and eyebrow motion for an embodied conversational agent","Humans are known to use a wide range of non-verbal behaviour while speaking. Generating naturalistic embodied speech for an artificial agent is therefore an application where techniques that draw directly on recorded human motions can be helpful. We present a system that uses corpus-based selection strategies to specify the head and eyebrow motion of an animated talking head. We first describe how a domain-specific corpus of facial displays was recorded and annotated, and outline the regularities that were found in the data. We then present two different methods of selecting motions for the talking head based on the corpus data: one that chooses the majority option in all cases, and one that makes a weighted choice among all of the options. We compare these methods to each other in two ways: through cross-validation against the corpus, and by asking human judges to rate the output. The results of the two evaluation studies differ: the cross-validation study favoured the majority strategy, while the human judges preferred schedules generated using weighted choice. The judges in the second study also showed a preference for the original corpus data over the output of either of the generation strategies.","Language Resources and Evaluation",2007,"No"," data driven generation embodied conversational agents evaluation generated output multimodal corpora corpus based generation head eyebrow motion embodied conversational agent humans wide range verbal behaviour speaking generating naturalistic embodied speech artificial agent application techniques draw directly recorded human motions helpful present system corpus based selection strategies head eyebrow motion animated talking head describe domain specific corpus facial displays recorded annotated outline regularities found data present methods selecting motions talking head based corpus data chooses majority option cases makes weighted choice options compare methods ways cross validation corpus human judges rate output results evaluation studies differ cross validation study favoured majority strategy human judges preferred schedules generated weighted choice judges study showed preference original corpus data output generation strategies",0
"KeywordsBehaviour analysis Small groups Meetings Multimodality ","a multimodal annotated corpus of consensus decision making meetings","In this paper we present an annotated audio–video corpus of multi-party meetings. The multimodal corpus provides for each subject involved in the experimental sessions six annotation dimensions referring to group dynamics; speech activity and body activity. The corpus is based on 11 audio and video recorded sessions which took place in a lab setting appropriately equipped with cameras and microphones. Our main concern in collecting this multimodal corpus was to explore the possibility of providing feedback services to facilitate group processes and to enhance self awareness among small groups engaged in meetings. We therefore introduce a coding scheme for annotating relevant functional roles that appear in a small group interaction. We also discuss the reliability of the coding scheme and we present the first results for automatic classification.","Language Resources and Evaluation",2007,"Yes"," behaviour analysis small groups meetings multimodality multimodal annotated corpus consensus decision making meetings paper present annotated audio video corpus multi party meetings multimodal corpus subject involved experimental sessions annotation dimensions referring group dynamics speech activity body activity corpus based audio video recorded sessions place lab setting appropriately equipped cameras microphones main concern collecting multimodal corpus explore possibility providing feedback services facilitate group processes enhance awareness small groups engaged meetings introduce coding scheme annotating relevant functional roles small group interaction discuss reliability coding scheme present results automatic classification",1
"KeywordsMutlimodal Corpus Annotation Evaluation Audio Video ","the chil audiovisual corpus for lecture and meeting analysis inside smart rooms","The analysis of lectures and meetings inside smart rooms has recently attracted much interest in the literature, being the focus of international projects and technology evaluations. A key enabler for progress in this area is the availability of appropriate multimodal and multi-sensory corpora, annotated with rich human activity information during lectures and meetings. This paper is devoted to exactly such a corpus, developed in the framework of the European project CHIL, “Computers in the Human Interaction Loop”. The resulting data set has the potential to drastically advance the state-of-the-art, by providing numerous synchronized audio and video streams of real lectures and meetings, captured in multiple recording sites over the past 4 years. It particularly overcomes typical shortcomings of other existing databases that may contain limited sensory or monomodal data, exhibit constrained human behavior and interaction patterns, or lack data variability. The CHIL corpus is accompanied by rich manual annotations of both its audio and visual modalities. These provide a detailed multi-channel verbatim orthographic transcription that includes speaker turns and identities, acoustic condition information, and named entities, as well as video labels in multiple camera views that provide multi-person 3D head and 2D facial feature location information. Over the past 3 years, the corpus has been crucial to the evaluation of a multitude of audiovisual perception technologies for human activity analysis in lecture and meeting scenarios, demonstrating its utility during internal evaluations of the CHIL consortium, as well as at the recent international CLEAR and Rich Transcription evaluations. The CHIL corpus is publicly available to the research community.","Language Resources and Evaluation",2007,"Yes"," mutlimodal corpus annotation evaluation audio video chil audiovisual corpus lecture meeting analysis inside smart rooms analysis lectures meetings inside smart rooms recently attracted interest literature focus international projects technology evaluations key enabler progress area availability multimodal multi sensory corpora annotated rich human activity information lectures meetings paper devoted corpus developed framework european project chil computers human interaction loop resulting data set potential drastically advance state art providing numerous synchronized audio video streams real lectures meetings captured multiple recording sites past years overcomes typical shortcomings existing databases limited sensory monomodal data exhibit constrained human behavior interaction patterns lack data variability chil corpus accompanied rich manual annotations audio visual modalities provide detailed multi channel verbatim orthographic transcription includes speaker turns identities acoustic condition information named entities video labels multiple camera views provide multi person d head d facial feature location information past years corpus crucial evaluation multitude audiovisual perception technologies human activity analysis lecture meeting scenarios demonstrating utility internal evaluations chil consortium recent international clear rich transcription evaluations chil corpus publicly research community",1
"KeywordsDiscourse markers Speech corpora Annotating Conversation Discourse analysis Speech-to-speech translation Spontaneous speech Slovenian language ","annotating discourse markers in spontaneous speech corpora on an example for the slovenian language","Speech-to-speech translation technology has difficulties processing elements of spontaneity in conversation. We propose a discourse marker attribute in speech corpora to help overcome some of these problems. There have already been some attempts to annotate discourse markers in speech corpora. However, as there is no consistency on what expressions count as discourse markers, we have to reconsider how to set a framework for annotating, and, in order to better understand what we gain by introducing a discourse marker category, we have to analyse their characteristics and functions in discourse. This is especially important for languages such as Slovenian where no or little research on the topic of discourse markers has been carried out. The aims of this paper are to present a scheme for annotating discourse markers based on the analysis of a corpus of telephone conversations in the tourism domain in the Slovenian language, and to give some additional arguments based on the characteristics and functions of discourse markers that confirm their special status in conversation.","Language Resources and Evaluation",2007,"No"," discourse markers speech corpora annotating conversation discourse analysis speech speech translation spontaneous speech slovenian language annotating discourse markers spontaneous speech corpora slovenian language speech speech translation technology difficulties processing elements spontaneity conversation propose discourse marker attribute speech corpora overcome problems attempts annotate discourse markers speech corpora consistency expressions count discourse markers reconsider set framework annotating order understand gain introducing discourse marker category analyse characteristics functions discourse important languages slovenian research topic discourse markers carried aims paper present scheme annotating discourse markers based analysis corpus telephone conversations tourism domain slovenian language give additional arguments based characteristics functions discourse markers confirm special status conversation",0
"KeywordsTimeML TimeBank Corpus analysis Temporal information extraction ","timebank evolution as a community resource for timeml parsing","TimeBank is the only reference corpus for TimeML, an expressive language for annotating complex temporal information. It is a rich resource for a broad range of research into various aspects of the expression of time and temporally related events. This paper traces the development of TimeBank from its initial—and somewhat noisy—version (1.1) to a substantially revised release (1.2), now available via the Linguistic Data Consortium. The development path is motivated by the encouraging empirical results of TimeML-compliant annotators developed on the basis of TimeBank 1.1, and is informed by a detailed study of the characteristics of that initial release, which guides a clean-up process turning TimeBank 1.2 into a consistent and robust community resource.","Language Resources and Evaluation",2007,"Yes"," timeml timebank corpus analysis temporal information extraction timebank evolution community resource timeml parsing timebank reference corpus timeml expressive language annotating complex temporal information rich resource broad range research aspects expression time temporally related events paper traces development timebank initial noisy version substantially revised release linguistic data consortium development path motivated encouraging empirical results timeml compliant annotators developed basis timebank informed detailed study characteristics initial release guides clean process turning timebank consistent robust community resource",1
"KeywordsBroad phonetic transcriptions Validation Automatic speech recognition ","validation of phonetic transcriptions in the context of automatic speech recognition","Some of the speech databases and large spoken language corpora that have been collected during the last fifteen years have been (at least partly) annotated with a broad phonetic transcription. Such phonetic transcriptions are often validated in terms of their resemblance to a handcrafted reference transcription. However, there are at least two methodological issues questioning this validation method. First, no reference transcription can fully represent the phonetic truth. This calls into question the status of such a transcription as a single reference for the quality of other phonetic transcriptions. Second, phonetic transcriptions are often generated to serve various purposes, none of which are considered when the transcriptions are compared to a reference transcription that was not made with the same purpose in mind. Since phonetic transcriptions are often used for the development of automatic speech recognition (ASR) systems, and since the relationship between ASR performance and a transcription’s resemblance to a reference transcription does not seem to be straightforward, we verified whether phonetic transcriptions that are to be used for ASR development can be justifiably validated in terms of their similarity to a purpose-independent reference transcription. To this end, we validated canonical representations and manually verified broad phonetic transcriptions of read speech and spontaneous telephone dialogues in terms of their resemblance to a handcrafted reference transcription on the one hand, and in terms of their suitability for ASR development on the other hand. Whereas the manually verified phonetic transcriptions resembled the reference transcription much closer than the canonical representations, the use of both transcription types yielded similar recognition results. The difference between the outcomes of the two validation methods has two implications. First, ASR developers can save themselves the effort of collecting expensive reference transcriptions in order to validate phonetic transcriptions of speech databases or spoken language corpora. Second, phonetic transcriptions should preferably be validated in terms of the application they will serve because a higher resemblance to a purpose-independent reference transcription is no guarantee for a transcription to be better suited for ASR development.","Language Resources and Evaluation",2007,"No"," broad phonetic transcriptions validation automatic speech recognition validation phonetic transcriptions context automatic speech recognition speech databases large spoken language corpora collected fifteen years partly annotated broad phonetic transcription phonetic transcriptions validated terms resemblance handcrafted reference transcription methodological issues questioning validation method reference transcription fully represent phonetic truth calls question status transcription single reference quality phonetic transcriptions phonetic transcriptions generated serve purposes considered transcriptions compared reference transcription made purpose mind phonetic transcriptions development automatic speech recognition asr systems relationship asr performance transcription resemblance reference transcription straightforward verified phonetic transcriptions asr development justifiably validated terms similarity purpose independent reference transcription end validated canonical representations manually verified broad phonetic transcriptions read speech spontaneous telephone dialogues terms resemblance handcrafted reference transcription hand terms suitability asr development hand manually verified phonetic transcriptions resembled reference transcription closer canonical representations transcription types yielded similar recognition results difference outcomes validation methods implications asr developers save effort collecting expensive reference transcriptions order validate phonetic transcriptions speech databases spoken language corpora phonetic transcriptions preferably validated terms application serve higher resemblance purpose independent reference transcription guarantee transcription suited asr development",0
"KeywordsUnicode Font Devanagari South Asian languages/scripts Legacy text Encoding Conversion Virama Conjunct consonant Vowel diacritic ","from legacy encodings to unicode the graphical and logical principles in the scripts of south asia","Much electronic text in the languages of South Asia has been published on the Internet. However, while Unicode has emerged as the favoured encoding system of corpus and computational linguists, most South Asian language data on the web uses one of a wide range of non-standard legacy encodings. This paper describes the difficulties inherent in converting text in these encodings to Unicode. Among the various legacy encodings for South Asian scripts, the most problematic are 8-bit fonts based on graphical principles (as opposed to the logical principles of Unicode). Graphical fonts typically encode several features in ways highly incompatible with Unicode. For instance, half-form glyphs used to construct conjunct consonants are typically separate code points in 8-bit fonts; in Unicode they are represented by the full consonant followed by virama. There are many more such cases. The solution described here is an approach to text conversion based on mapping rules. A small number of generalised rules (plus the capacity for more specialised rules) captures the behaviour of each character in a font, building up a conversion algorithm for that encoding. This system is embedded in a font-mapping program, outputting CES-compliant SGML Unicode. This program, a generalised text-conversion tool, has been employed extensively in corpus-building for South Asian languages.","Language Resources and Evaluation",2007,"Yes"," unicode font devanagari south asian languagesscripts legacy text encoding conversion virama conjunct consonant vowel diacritic legacy encodings unicode graphical logical principles scripts south asia electronic text languages south asia published internet unicode emerged favoured encoding system corpus computational linguists south asian language data web wide range standard legacy encodings paper describes difficulties inherent converting text encodings unicode legacy encodings south asian scripts problematic bit fonts based graphical principles opposed logical principles unicode graphical fonts typically encode features ways highly incompatible unicode instance half form glyphs construct conjunct consonants typically separate code points bit fonts unicode represented full consonant virama cases solution approach text conversion based mapping rules small number generalised rules capacity specialised rules captures behaviour character font building conversion algorithm encoding system embedded font mapping program outputting ces compliant sgml unicode program generalised text conversion tool employed extensively corpus building south asian languages",1
"KeywordsLexical acquisition Corpus-based statistical measures Verb semantics Multiword predicates Light verb constructions ","automatically learning semantic knowledge about multiword predicates","Highly frequent and highly polysemous verbs, such as give, take, and make, pose a challenge to automatic lexical acquisition methods. These verbs widely participate in multiword predicates (such as light verb constructions, or LVCs), in which they contribute a broad range of figurative meanings that must be recognized. Here we focus on two properties that are key to the computational treatment of LVCs. First, we consider the degree of figurativeness of the semantic contribution of such a verb to the various LVCs it participates in. Second, we explore the patterns of acceptability of LVCs, and their productivity over semantically related combinations. To assess these properties, we develop statistical measures of figurativeness and acceptability that draw on linguistic properties of LVCs. We demonstrate that these corpus-based measures correlate well with human judgments of the relevant property. We also use the acceptability measure to estimate the degree to which a semantic class of nouns can productively form LVCs with a given verb. The linguistically-motivated measures outperform a standard measure for capturing the strength of collocation of these multiword expressions.","Language Resources and Evaluation",2007,"No"," lexical acquisition corpus based statistical measures verb semantics multiword predicates light verb constructions automatically learning semantic knowledge multiword predicates highly frequent highly polysemous verbs give make pose challenge automatic lexical acquisition methods verbs widely participate multiword predicates light verb constructions lvcs contribute broad range figurative meanings recognized focus properties key computational treatment lvcs degree figurativeness semantic contribution verb lvcs participates explore patterns acceptability lvcs productivity semantically related combinations assess properties develop statistical measures figurativeness acceptability draw linguistic properties lvcs demonstrate corpus based measures correlate human judgments relevant property acceptability measure estimate degree semantic class nouns productively form lvcs verb linguistically motivated measures outperform standard measure capturing strength collocation multiword expressions",0
"KeywordsMultimodal corpora Embodied conversational agents Gesture generation Human–computer interaction ","an annotation scheme for conversational gestures how to economically capture timing and form","The empirical investigation of human gesture stands at the center of multiple research disciplines, and various gesture annotation schemes exist, with varying degrees of precision and required annotation effort. We present a gesture annotation scheme for the specific purpose of automatically generating and animating character-specific hand/arm gestures, but with potential general value. We focus on how to capture temporal structure and locational information with relatively little annotation effort. The scheme is evaluated in terms of how accurately it captures the original gestures by re-creating those gestures on an animated character using the annotated data. This paper presents our scheme in detail and compares it to other approaches.","Language Resources and Evaluation",2007,"No"," multimodal corpora embodied conversational agents gesture generation human computer interaction annotation scheme conversational gestures economically capture timing form empirical investigation human gesture stands center multiple research disciplines gesture annotation schemes exist varying degrees precision required annotation effort present gesture annotation scheme specific purpose automatically generating animating character specific handarm gestures potential general focus capture temporal structure locational information annotation effort scheme evaluated terms accurately captures original gestures creating gestures animated character annotated data paper presents scheme detail compares approaches",0
"KeywordsCorrection ranking Soundex Shapex Spelling error correction Urdu ","a novel approach for ranking spelling error corrections for urdu","This paper presents a scheme for ranking of spelling error corrections for Urdu. Conventionally spell-checking techniques do not provide any explicit ranking mechanism. Ranking is either implicit in the correction algorithm or corrections are not ranked at all. The research presented in this paper shows that for Urdu, phonetic similarity between the corrections and the erroneous word can serve as a useful parameter for ranking the corrections. This combined with a new technique Shapex that uses visual similarity of characters for ranking gives an improvement of 23% in the accuracy of the one-best match compared to the result obtained when the ranking is done on the basis of word frequencies only.","Language Resources and Evaluation",2007,"No"," correction ranking soundex shapex spelling error correction urdu approach ranking spelling error corrections urdu paper presents scheme ranking spelling error corrections urdu conventionally spell checking techniques provide explicit ranking mechanism ranking implicit correction algorithm corrections ranked research presented paper shows urdu phonetic similarity corrections erroneous word serve parameter ranking corrections combined technique shapex visual similarity characters ranking improvement accuracy match compared result obtained ranking basis word frequencies ",0
"KeywordsMultimodal annotation Feedback Hand and facial gestures ","the mumin coding scheme for the annotation of feedback turn management and sequencing phenomena","This paper deals with a multimodal annotation scheme dedicated to the study of gestures in interpersonal communication, with particular regard to the role played by multimodal expressions for feedback, turn management and sequencing. The scheme has been developed under the framework of the MUMIN network and tested on the analysis of multimodal behaviour in short video clips in Swedish, Finnish and Danish. The preliminary results obtained in these studies show that the reliability of the categories defined in the scheme is acceptable, and that the scheme as a whole constitutes a versatile analysis tool for the study of multimodal communication behaviour.","Language Resources and Evaluation",2007,"No"," multimodal annotation feedback hand facial gestures mumin coding scheme annotation feedback turn management sequencing phenomena paper deals multimodal annotation scheme dedicated study gestures interpersonal communication regard role played multimodal expressions feedback turn management sequencing scheme developed framework mumin network tested analysis multimodal behaviour short video clips swedish finnish danish preliminary results obtained studies show reliability categories defined scheme acceptable scheme constitutes versatile analysis tool study multimodal communication behaviour",0
"KeywordsCollaboration Computer-mediated human interaction Interactive explanation Gaze Gesture Multimodal communication ","the importance of gaze and gesture in interactive multimodal explanation","The objective of this research is twofold. Firstly, we argue that gaze and gesture play an essential part in interactive explanation and that it is thus a multimodal phenomenon. Two corpora are analyzed: (1) a group of teacher novices and experts and (2) a student teacher dyad, both of whom construct explanations of students’ reasoning after viewing videos of student dyads who are solving physics problems. We illustrate roles of gaze in explanations constructed within a group and roles of gesture in explanation constructed within a dyad. Secondly, we show how the analysis of such knowledge-rich empirical data pinpoints particular difficulties in designing human–computer interfaces that can support explanation between humans, or a fortiori, that can support explanation between a human and a computer.","Language Resources and Evaluation",2007,"No"," collaboration computer mediated human interaction interactive explanation gaze gesture multimodal communication importance gaze gesture interactive multimodal explanation objective research twofold firstly argue gaze gesture play essential part interactive explanation multimodal phenomenon corpora analyzed group teacher novices experts student teacher dyad construct explanations students reasoning viewing videos student dyads solving physics problems illustrate roles gaze explanations constructed group roles gesture explanation constructed dyad show analysis knowledge rich empirical data pinpoints difficulties designing human computer interfaces support explanation humans fortiori support explanation human computer",0
"KeywordsUrdu Deep grammars Grammer engineering Parallel grammar development LFG ","urdu in a parallel grammar development environment","In this paper, we report on the role of the Urdu grammar in the Parallel Grammar (ParGram) project (Butt, M., King, T. H., Niño, M.-E., & Segond, F. (1999). A grammar writer’s cookbook. CSLI Publications; Butt, M., Dyvik, H., King, T. H., Masuichi, H., & Rohrer, C. (2002). ‘The parallel grammar project’. In: Proceedings of COLING 2002, Workshop on grammar engineering and evaluation, pp. 1–7). The Urdu grammar was able to take advantage of standards in analyses set by the original grammars in order to speed development. However, novel constructions, such as correlatives and extensive complex predicates, resulted in expansions of the analysis feature space as well as extensions to the underlying parsing platform. These improvements are now available to all the project grammars.","Language Resources and Evaluation",2007,"No"," urdu deep grammars grammer engineering parallel grammar development lfg urdu parallel grammar development environment paper report role urdu grammar parallel grammar pargram project butt king ni segond grammar writer cookbook csli publications butt dyvik king masuichi rohrer parallel grammar project proceedings coling workshop grammar engineering evaluation pp urdu grammar advantage standards analyses set original grammars order speed development constructions correlatives extensive complex predicates resulted expansions analysis feature space extensions underlying parsing platform improvements project grammars",0
"KeywordsInfectious disease surveillance Multilingual ontology Text mining ","a multilingual ontology for infectious disease surveillance rationale design and challenges","A lack of surveillance system infrastructure in the Asia-Pacific region is seen as hindering the global control of rapidly spreading infectious diseases such as the recent avian H5N1 epidemic. As part of improving surveillance in the region, the BioCaster project aims to develop a system based on text mining for automatically monitoring Internet news and other online sources in several regional languages. At the heart of the system is an application ontology which serves the dual purpose of enabling advanced searches on the mined facts and of allowing the system to make intelligent inferences for assessing the priority of events. However, it became clear early on in the project that existing classification schemes did not have the necessary language coverage or semantic specificity for our needs. In this article we present an overview of our needs and explore in detail the rationale and methods for developing a new conceptual structure and multilingual terminological resource that focusses on priority pathogens and the diseases they cause. The ontology is made freely available as an online database and downloadable OWL file.","Language Resources and Evaluation",2007,"No"," infectious disease surveillance multilingual ontology text mining multilingual ontology infectious disease surveillance rationale design challenges lack surveillance system infrastructure asia pacific region hindering global control rapidly spreading infectious diseases recent avian hn epidemic part improving surveillance region biocaster project aims develop system based text mining automatically monitoring internet news online sources regional languages heart system application ontology serves dual purpose enabling advanced searches mined facts allowing system make intelligent inferences assessing priority events clear early project existing classification schemes language coverage semantic specificity article present overview explore detail rationale methods developing conceptual structure multilingual terminological resource focusses priority pathogens diseases ontology made freely online database downloadable owl file",0
NA,"wordnet then and now","We briefly discuss the origin and development of WordNet, a large lexical database for English. We outline its design and contents as well as its usefulness for Natural Language Processing. Finally, we discuss crosslinguistic WordNets and complementary lexical resources. [ABSTRACT FROM AUTHOR], Copyright of Language Resources & Evaluation is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","Language Resources and Evaluation",2007,"Yes"," wordnet briefly discuss origin development wordnet large lexical database english outline design contents usefulness natural language processing finally discuss crosslinguistic wordnets complementary lexical resources abstract author copyright language resources evaluation property springer nature content copied emailed multiple sites posted listserv copyright holder express written permission users print download email articles individual abstract abridged warranty accuracy copy users refer original published version material full abstract copyright applies abstracts",1
"KeywordsLexicon Linguistic resources Part-of-speech Standardization Syntactic description Vietnamese ","a lexicon for vietnamese language processing","Only very recently have Vietnamese researchers begun to be involved in the domain of Natural Language Processing (NLP). As there does not exist any published work in formal linguistics nor any recognizable standard for Vietnamese word definition and word categories, the fundamental tasks for automatic Vietnamese language processing, such as part-of-speech tagging, parsing, etc., are very difficult tasks for computer scientists. The fact that all necessary linguistic resources have to be built from scratch by each research team is a real obstacle to the development of Vietnamese language processing. The aim of our projects is thus to build a common linguistic database that is freely and easily exploitable for the automatic processing of Vietnamese. In this paper, we present our work on creating a Vietnamese lexicon for NLP applications. We emphasize the standardization aspect of the lexicon representation. We especially propose an extensible set of Vietnamese syntactic descriptions that can be used for tagset definition and morphosyntactic analysis. These descriptors are established in such a way as to be a reference set proposal for Vietnamese in the context of ISO subcommittee TC 37/SC 4 (Language Resource Management).","Language Resources and Evaluation",2006,"Yes"," lexicon linguistic resources part speech standardization syntactic description vietnamese lexicon vietnamese language processing recently vietnamese researchers begun involved domain natural language processing nlp exist published work formal linguistics recognizable standard vietnamese word definition word categories fundamental tasks automatic vietnamese language processing part speech tagging parsing difficult tasks computer scientists fact linguistic resources built scratch research team real obstacle development vietnamese language processing aim projects build common linguistic database freely easily exploitable automatic processing vietnamese paper present work creating vietnamese lexicon nlp applications emphasize standardization aspect lexicon representation propose extensible set vietnamese syntactic descriptions tagset definition morphosyntactic analysis descriptors established reference set proposal vietnamese context iso subcommittee tc sc language resource management",1
"KeywordsNatural Language Processing Machine Translation Query Expansion Word Sense Statistical Machine Translation ","asian language processing current state of the art","Asian language processing presents formidable challenges to achieving multilingualism and multiculturalism in our society. One of the first and most obvious challenges is the multitude and diversity of languages: more than 2,000 languages are listed as languages in Asia by Ethnologue (Gordon 2005), representing four major language families: Austronesian, Trans-New Guinea, Indo-European, and Sino-Tibetan. 1The challenge is made more formidable by the fact that as a whole, Asian languages range from the language with most speakers in the world (Mandarin Chinese, close to 900 million native speakers) to the more than 70 nearly extinct languages (e.g. Pazeh in Taiwan, one speaker). As a result, there are vast differences in the level of language processing capability and the number of sharable resources available for individual languages. Major Asian languages such as Mandarin Chinese, Hindi, Japanese, Korean, and Thai have benefited...","Language Resources and Evaluation",2006,"No"," natural language processing machine translation query expansion word sense statistical machine translation asian language processing current state art asian language processing presents formidable challenges achieving multilingualism multiculturalism society obvious challenges multitude diversity languages languages listed languages asia ethnologue gordon representing major language families austronesian trans guinea indo european sino tibetan the challenge made formidable fact asian languages range language speakers world mandarin chinese close million native speakers extinct languages pazeh taiwan speaker result vast differences level language processing capability number sharable resources individual languages major asian languages mandarin chinese hindi japanese korean thai benefited",0
"KeywordsCorpus linguistics Lexicography Computational linguistics Natural language processing Dictionaries Irish Gaelic Hiberno-English Language technology ","efficient corpus development for lexicography building the new corpus for ireland","In a 12-month project we have developed a new, register-diverse, 55-million-word bilingual corpus—the New Corpus for Ireland (NCI)—to support the creation of a new English-to-Irish dictionary. The paper describes the strategies we employed, and the solutions to problems encountered. We believe we have a good model for corpus creation for lexicography, and others may find it useful as a blueprint. The corpus has two parts, one Irish, the other Hiberno-English (English as spoken in Ireland). We describe its design, collection and encoding.","Language Resources and Evaluation",2006,"Yes"," corpus linguistics lexicography computational linguistics natural language processing dictionaries irish gaelic hiberno english language technology efficient corpus development lexicography building corpus ireland month project developed register diverse million word bilingual corpus corpus ireland nci support creation english irish dictionary paper describes strategies employed solutions problems encountered good model corpus creation lexicography find blueprint corpus parts irish hiberno english english spoken ireland describe design collection encoding",1
"KeywordsComplex predicates Wordnet Ontology Noun incorporation Compound verbs Verb hierarchy ","complex predicates in indian languages and wordnets","Wordnets, which are repositories of lexical semantic knowledge containing semantically linked synsets and lexically linked words, are indispensable for work on computational linguistics and natural language processing. While building wordnets for Hindi and Marathi, two major Indo-European languages, we observed that the verb hierarchy in the Princeton Wordnet was rather shallow. We set to constructing a verb knowledge base for Hindi, which arranges the Hindi verbs in a hierarchy of is-a (hypernymy) relation. We realized that there are unique Indian language phenomena that bear upon the lexicalization vs. syntactically derived choice. One such example is the occurrence of conjunct and compound verbs (called Complex Predicates) which are found in all Indian languages. This paper presents our experience in the construction of lexical knowledge bases for Indian languages with special attention to Hindi. The question of storing versus deriving complex predicates has been dealt with linguistically and computationally. We have constructed empirical tests to decide if a combination of two words, the second of which is a verb, is a complex predicate or not. Such tests provide a principled way of deciding the status of complex predicates in Indian language wordnets.","Language Resources and Evaluation",2006,"No"," complex predicates wordnet ontology noun incorporation compound verbs verb hierarchy complex predicates indian languages wordnets wordnets repositories lexical semantic knowledge semantically linked synsets lexically linked words indispensable work computational linguistics natural language processing building wordnets hindi marathi major indo european languages observed verb hierarchy princeton wordnet shallow set constructing verb knowledge base hindi arranges hindi verbs hierarchy hypernymy relation realized unique indian language phenomena bear lexicalization syntactically derived choice occurrence conjunct compound verbs called complex predicates found indian languages paper presents experience construction lexical knowledge bases indian languages special attention hindi question storing versus deriving complex predicates dealt linguistically computationally constructed empirical tests decide combination words verb complex predicate tests provide principled deciding status complex predicates indian language wordnets",0
"KeywordsChinese PropBank Frameset Frame Alternation Semantic roles ","a chinese semantic lexicon of senses and roles","We describe a Chinese lexical semantic resource that consists of 11,765 predicates (mostly verbs and their nominalizations) analyzed with coarse-grained senses and semantic roles. We show that distinguishing senses at a coarse-grained level is a necessary part of specifying the semantic roles and describe our strategies for sense determination for purposes of predicate-argument structure specification. The semantic roles are postulated to account for syntactic variations, the different ways in which the semantic roles of a predicate are realized. The immediate purpose for this lexical semantic resource is to support the annotation of the Chinese PropBank, but we believe it can also serve as stepping stone for higher-level semantic generalizations.","Language Resources and Evaluation",2006,"Yes"," chinese propbank frameset frame alternation semantic roles chinese semantic lexicon senses roles describe chinese lexical semantic resource consists predicates verbs nominalizations analyzed coarse grained senses semantic roles show distinguishing senses coarse grained level part semantic roles describe strategies sense determination purposes predicate argument structure specification semantic roles postulated account syntactic variations ways semantic roles predicate realized purpose lexical semantic resource support annotation chinese propbank serve stepping stone higher level semantic generalizations",1
"KeywordsAdaptation Evaluation ","adaptation of an automotive dialogue system to users expertise and evaluation of the system","Spoken dialogue systems (SDSs) can be used to operate devices, e.g. in the automotive environment. People using these systems usually have different levels of experience. However, most systems do not take this into account. In this paper, we present a method to build a dialogue system in an automotive environment that automatically adapts to the user’s experience with the system. We implemented the adaptation in a prototype and carried out exhaustive tests. Our usability tests show that adaptation increases both user performance and user satisfaction. We describe the tests that were performed, and the methods used to assess the test results. One of these methods is a modification of PARADISE, a framework for evaluating the performance of SDSs [Walker MA, Litman DJ, Kamm CA, Abella A (Comput Speech Lang 12(3):317–347, 1998)]. We discuss its drawbacks for the evaluation of SDSs like ours, the modifications we have carried out, and the test results.","Language Resources and Evaluation",2006,"No"," adaptation evaluation adaptation automotive dialogue system users expertise evaluation system spoken dialogue systems sdss operate devices automotive environment people systems levels experience systems account paper present method build dialogue system automotive environment automatically adapts user experience system implemented adaptation prototype carried exhaustive tests usability tests show adaptation increases user performance user satisfaction describe tests performed methods assess test results methods modification paradise framework evaluating performance sdss walker ma litman dj kamm ca abella comput speech lang discuss drawbacks evaluation sdss modifications carried test results",0
"KeywordsAddressing Multi-party dialogues Multimodal corpora Annotation schemas Reliability analysis ","a corpus for studying addressing behaviour in multi party dialogues","This paper describes a multi-modal corpus of hand-annotated meeting dialogues that was designed for studying addressing behaviour in face-to-face conversations. The corpus contains annotated dialogue acts, addressees, adjacency pairs and gaze direction. First, we describe the corpus design where we present the meetings collection, annotation scheme and annotation tools. Then, we present the analysis of the reproducibility and stability of the annotation scheme.","Language Resources and Evaluation",2006,"Yes"," addressing multi party dialogues multimodal corpora annotation schemas reliability analysis corpus studying addressing behaviour multi party dialogues paper describes multi modal corpus hand annotated meeting dialogues designed studying addressing behaviour face face conversations corpus annotated dialogue acts addressees adjacency pairs gaze direction describe corpus design present meetings collection annotation scheme annotation tools present analysis reproducibility stability annotation scheme",1
"KeywordsInformation Extraction Evaluation Message understanding conferences ","fact distribution in information extraction","Several recent Information Extraction (IE) systems have been restricted to the identification facts which are described within a single sentence. It is not clear what effect this has on the difficulty of the extraction task or how the performance of systems which consider only single sentences should be compared with those which consider multiple sentences. This paper compares three IE evaluation corpora, from the Message Understanding Conferences, and finds that a significant proportion of the facts mentioned therein are not described within a single sentence. Therefore systems which are evaluated only on facts described within single sentences are being tested against a limited portion of the relevant information in the text and it is difficult to compare their performance with other systems. Further analysis demonstrates that anaphora resolution and world knowledge are required to combine information described across multiple sentences. This result has implications for the development and evaluation of IE systems.","Language Resources and Evaluation",2006,"No"," information extraction evaluation message understanding conferences fact distribution information extraction recent information extraction systems restricted identification facts single sentence clear effect difficulty extraction task performance systems single sentences compared multiple sentences paper compares evaluation corpora message understanding conferences finds significant proportion facts mentioned single sentence systems evaluated facts single sentences tested limited portion relevant information text difficult compare performance systems analysis demonstrates anaphora resolution world knowledge required combine information multiple sentences result implications development evaluation systems",0
"KeywordsLanguage model Spoken dialogue systems User simulation Example-based generation ","automatic induction of language model data for a spoken dialogue system","In this paper, we address the issue of generating in-domain language model training data when little or no real user data are available. The two-stage approach taken begins with a data induction phase whereby linguistic constructs from out-of-domain sentences are harvested and integrated with artificially constructed in-domain phrases. After some syntactic and semantic filtering, a large corpus of synthetically assembled user utterances is induced. In the second stage, two sampling methods are explored to filter the synthetic corpus to achieve a desired probability distribution of the semantic content, both on the sentence level and on the class level. The first method utilizes user simulation technology, which obtains the probability model via an interplay between a probabilistic user model and the dialogue system. The second method synthesizes novel dialogue interactions from the raw data by modelling after a small set of dialogues produced by the developers during the course of system refinement. Evaluation is conducted on recognition performance in a restaurant information domain. We show that a partial match to usage-appropriate semantic content distribution can be achieved via user simulations. Furthermore, word error rate can be reduced when limited amounts of in-domain training data are augmented with synthetic data derived by our methods.","Language Resources and Evaluation",2006,"No"," language model spoken dialogue systems user simulation based generation automatic induction language model data spoken dialogue system paper address issue generating domain language model training data real user data stage approach begins data induction phase linguistic constructs domain sentences harvested integrated artificially constructed domain phrases syntactic semantic filtering large corpus synthetically assembled user utterances induced stage sampling methods explored filter synthetic corpus achieve desired probability distribution semantic content sentence level class level method utilizes user simulation technology obtains probability model interplay probabilistic user model dialogue system method synthesizes dialogue interactions raw data modelling small set dialogues produced developers system refinement evaluation conducted recognition performance restaurant information domain show partial match usage semantic content distribution achieved user simulations word error rate reduced limited amounts domain training data augmented synthetic data derived methods",0
"KeywordsLexical acquisition Subcategorisation Verb classes Catalan Clustering ","automatic acquisition of syntactic verb classes with basic resources","This paper describes a methodology aimed at grouping Catalan verbs according to their syntactic behavior. Our goal is to acquire a small number of basic classes with a high level of accuracy, using minimal resources. Information on syntactic class, expensive and slow to compile by hand, is useful for any NLP task requiring specific lexical information. We show that it is possible to acquire this kind of information using only a POS-tagged corpus. We perform two clustering experiments. The first one aims at classifying verbs into transitive, intransitive and verbs alternating with a se-construction. Our system achieves an average 0.84 F-score, for a task with a 0.33 baseline. The second experiment aims at further distinguishing among pure intransitives and verbs bearing a prepositional object. The baseline for the task is 0.51 and the upperbound 0.98. The system achieves an average 0.88 F-score.","Language Resources and Evaluation",2006,"No"," lexical acquisition subcategorisation verb classes catalan clustering automatic acquisition syntactic verb classes basic resources paper describes methodology aimed grouping catalan verbs syntactic behavior goal acquire small number basic classes high level accuracy minimal resources information syntactic class expensive slow compile hand nlp task requiring specific lexical information show acquire kind information pos tagged corpus perform clustering experiments aims classifying verbs transitive intransitive verbs alternating se construction system achieves average score task baseline experiment aims distinguishing pure intransitives verbs bearing prepositional object baseline task upperbound system achieves average score",0
"KeywordsMorphological parsing Word segmentation Data annotation Unsupervised learning Asian language processing Bengali ","unsupervised morphological parsing of bengali","Unsupervised morphological analysis is the task of segmenting words into prefixes, suffixes and stems without prior knowledge of language-specific morphotactics and morpho-phonological rules. This paper introduces a simple, yet highly effective algorithm for unsupervised morphological learning for Bengali, an Indo–Aryan language that is highly inflectional in nature. When evaluated on a set of 4,110 human-segmented Bengali words, our algorithm achieves an F-score of 83%, substantially outperforming Linguistica, one of the most widely-used unsupervised morphological parsers, by about 23%.","Language Resources and Evaluation",2006,"No"," morphological parsing word segmentation data annotation unsupervised learning asian language processing bengali unsupervised morphological parsing bengali unsupervised morphological analysis task segmenting words prefixes suffixes stems prior knowledge language specific morphotactics morpho phonological rules paper introduces simple highly effective algorithm unsupervised morphological learning bengali indo aryan language highly inflectional nature evaluated set human segmented bengali words algorithm achieves score substantially outperforming linguistica widely unsupervised morphological parsers ",0
"KeywordsCombination of taggers Integration of taggers Linguistically motivated rules Simple voting Tagging accuracy ","tagging icelandic text an experiment with integrations and combinations of taggers","We use integrations and combinations of taggers to improve the tagging accuracy of Icelandic text. The accuracy of the best performing integrated tagger, which consists of our linguistic rule-based tagger for initial disambiguation and a trigram tagger for full disambiguation, is 91.80%. Combining five different taggers, using simple voting, results in 93.34% accuracy. By adding two linguistically motivated rules to the combined tagger, we obtain an accuracy of 93.48%. This method reduces the error rate by 20.5%, with respect to the best performing tagger in the combination pool.","Language Resources and Evaluation",2006,"No"," combination taggers integration taggers linguistically motivated rules simple voting tagging accuracy tagging icelandic text experiment integrations combinations taggers integrations combinations taggers improve tagging accuracy icelandic text accuracy performing integrated tagger consists linguistic rule based tagger initial disambiguation trigram tagger full disambiguation combining taggers simple voting results accuracy adding linguistically motivated rules combined tagger obtain accuracy method reduces error rate respect performing tagger combination pool",0
"KeywordsMarkov Decision Process Data Resource Dialogue System Dialogue Model Influence Diagram ","introduction to special issue on data resources evaluation and dialogue interaction","This special issue on Data Resources, Evaluation, and Dialogue Interaction is based on five thoroughly revised and extended papers from the sixth SIGdial Workshop held in Lisbon, Portugal, in September 2005. SIGdial is a special interest group on discourse and dialogue whose parent organisations are the Association for Computational Linguistics (ACL) and the International Speech Communication Association (ISCA). SIGdial workshops accommodate a broad range of topics related to discourse and dialogue. Among these topics are data resources, evaluation, and dialogue interaction. The papers selected for this special issue have in common that they all deal with aspects of these topics and each paper has its focus on at least one of them.","Language Resources and Evaluation",2006,"No"," markov decision process data resource dialogue system dialogue model influence diagram introduction special issue data resources evaluation dialogue interaction special issue data resources evaluation dialogue interaction based revised extended papers sixth sigdial workshop held lisbon portugal september sigdial special interest group discourse dialogue parent organisations association computational linguistics acl international speech communication association isca sigdial workshops accommodate broad range topics related discourse dialogue topics data resources evaluation dialogue interaction papers selected special issue common deal aspects topics paper focus ",0
"KeywordsAbbreviation Atomic abbreviation Single character recovery model ","mining atomic chinese abbreviations with a probabilistic single character recovery model","An HMM-based single character recovery (SCR) model is proposed in this paper to extract a large set of atomic abbreviations and their full forms from a text corpus. By an “atomic abbreviation,” it refers to an abbreviated word consisting of a single Chinese character. This task is important since Chinese abbreviations cannot be enumerated exhaustively but the abbreviation process for compound words seems to be compositional. One can often decode an abbreviated word character by character to its full form. With a large atomic abbreviation dictionary, one may be able to handle multiple character abbreviation problems more easily based on the compositional property of abbreviations.","Language Resources and Evaluation",2006,"No"," abbreviation atomic abbreviation single character recovery model mining atomic chinese abbreviations probabilistic single character recovery model hmm based single character recovery scr model proposed paper extract large set atomic abbreviations full forms text corpus atomic abbreviation refers abbreviated word consisting single chinese character task important chinese abbreviations enumerated exhaustively abbreviation process compound words compositional decode abbreviated word character character full form large atomic abbreviation dictionary handle multiple character abbreviation problems easily based compositional property abbreviations",0
"KeywordsTense/aspect/modality Support vector machine Machine translation system On the market ","japanese to english translations of tense aspect and modality using machine learning methods and comparison with machine translation systems on market","This paper describes experiments carried out utilizing a variety of machine-learning methods (the k-nearest neighborhood, decision list, maximum entropy, and support vector machine), and using six machine-translation (MT) systems available on the market for translating tense, aspect, and modality. We found that all these, including the simple string-matching-based k-nearest neighborhood used in a previous study, obtained higher accuracy rates than the MT systems currently available on the market. We also found that the support vector machine obtained the best accuracy rates (98.8%) of these methods. Finally, we analyzed errors against the machine-learning methods and commercially available MT systems and obtained error patterns that should be useful for making future improvements.","Language Resources and Evaluation",2006,"No"," tenseaspectmodality support vector machine machine translation system market japanese english translations tense aspect modality machine learning methods comparison machine translation systems market paper describes experiments carried utilizing variety machine learning methods nearest neighborhood decision list maximum entropy support vector machine machine translation mt systems market translating tense aspect modality found including simple string matching based nearest neighborhood previous study obtained higher accuracy rates mt systems market found support vector machine obtained accuracy rates methods finally analyzed errors machine learning methods commercially mt systems obtained error patterns making future improvements",0
"KeywordsChinese processing Copy detection Ferret Plagiarism Word definition ","copy detection in chinese documents using ferret","The Ferret copy detector has been used since 2001 to find plagiarism in large collections of students’ coursework in English. This article reports on extending its application to Chinese, with experiments on corpora of coursework collected from two Chinese universities. Our experiments show that Ferret can find both artificially constructed plagiarism and actually occurring, previously undetected plagiarism. We discuss issues of representation, focus on the effectiveness of a sub-symbolic approach, and show that Ferret does not need to find word boundaries first.","Language Resources and Evaluation",2006,"No"," chinese processing copy detection ferret plagiarism word definition copy detection chinese documents ferret ferret copy detector find plagiarism large collections students coursework english article reports extending application chinese experiments corpora coursework collected chinese universities experiments show ferret find artificially constructed plagiarism occurring previously undetected plagiarism discuss issues representation focus effectiveness symbolic approach show ferret find word boundaries ",0
"KeywordsDependency structure Parsing accuracy Parsing time Sentence segmentation Speech corpus Speech understanding Spoken language Stochastic parsing Syntactically annotated corpus ","dependency parsing of japanese monologue using clause boundaries","Spoken monologues feature greater sentence length and structural complexity than spoken dialogues. To achieve high-parsing performance for spoken monologues, simplifying the structure by dividing a sentence into suitable language units could prove effective. This paper proposes a method for dependency parsing of Japanese spoken monologues based on sentence segmentation. In this method, dependency parsing is executed in two stages: at the clause level and the sentence level. First, dependencies within a clause are identified by dividing a sentence into clauses and executing stochastic dependency parsing for each clause. Next, dependencies across clause boundaries are identified stochastically, and the dependency structure of the entire sentence is thus completed. An experiment using a spoken monologue corpus shows the effectiveness of this method for efficient dependency parsing of Japanese monologue sentences.","Language Resources and Evaluation",2006,"No"," dependency structure parsing accuracy parsing time sentence segmentation speech corpus speech understanding spoken language stochastic parsing syntactically annotated corpus dependency parsing japanese monologue clause boundaries spoken monologues feature greater sentence length structural complexity spoken dialogues achieve high parsing performance spoken monologues simplifying structure dividing sentence suitable language units prove effective paper proposes method dependency parsing japanese spoken monologues based sentence segmentation method dependency parsing executed stages clause level sentence level dependencies clause identified dividing sentence clauses executing stochastic dependency parsing clause dependencies clause boundaries identified stochastically dependency structure entire sentence completed experiment spoken monologue corpus shows effectiveness method efficient dependency parsing japanese monologue sentences",0
"KeywordsFeature selection MDL Clustering Word senses Text processing ","word sense learning based on feature selection and mdl principle","In this paper, we propose a word sense learning algorithm which is capable of unsupervised feature selection and cluster number identification. Feature selection for word sense learning is built on an entropy-based filter and formalized as a constraint optimization problem, the output of which is a set of important features. Cluster number identification is built on a Gaussian mixture model with a MDL-based criterion, and the optimal model order is inferred by minimizing the criterion. To evaluate closeness between the learned sense clusters with the ground-truth classes, we introduce a kind of weighted F-measure to model the effort needed to reconstruct the classes from the clusters. Experiments show that the algorithm can retrieve important features, roughly estimate the class numbers automatically and outperforms other algorithms in terms of the weighted F-measure. In addition, we also try to apply the algorithm to a specific task of adding new words into a Chinese thesaurus.","Language Resources and Evaluation",2006,"No"," feature selection mdl clustering word senses text processing word sense learning based feature selection mdl principle paper propose word sense learning algorithm capable unsupervised feature selection cluster number identification feature selection word sense learning built entropy based filter formalized constraint optimization problem output set important features cluster number identification built gaussian mixture model mdl based criterion optimal model order inferred minimizing criterion evaluate closeness learned sense clusters ground truth classes introduce kind weighted measure model effort needed reconstruct classes clusters experiments show algorithm retrieve important features roughly estimate class numbers automatically outperforms algorithms terms weighted measure addition apply algorithm specific task adding words chinese thesaurus",0
"KeywordsWord sense disambiguation Idiom detection Linguistic knowledge ","detecting japanese idioms with a linguistically rich dictionary","Detecting idioms in a sentence is important to sentence understanding. This paper discusses the linguistic knowledge for idiom detection. The challenges are that idioms can be ambiguous between literal and idiomatic meanings, and that they can be “transformed” when expressed in a sentence. However, there has been little research on Japanese idiom detection with its ambiguity and transformations taken into account. We propose a set of linguistic knowledge for idiom detection that is implemented in an idiom dictionary. We evaluated the linguistic knowledge by measuring the performance of an idiom detector that exploits the dictionary. As a result, more than 90% of the idioms are detected with 90% accuracy.","Language Resources and Evaluation",2006,"No"," word sense disambiguation idiom detection linguistic knowledge detecting japanese idioms linguistically rich dictionary detecting idioms sentence important sentence understanding paper discusses linguistic knowledge idiom detection challenges idioms ambiguous literal idiomatic meanings transformed expressed sentence research japanese idiom detection ambiguity transformations account propose set linguistic knowledge idiom detection implemented idiom dictionary evaluated linguistic knowledge measuring performance idiom detector exploits dictionary result idioms detected accuracy",0
"KeywordsAnnotation tool Treebank Minimal human intervention Parsing ","a segment based annotation tool for korean treebanks with minimal human intervention","In this paper, we propose a segment-based annotation tool providing appropriate interactivity between a human annotator and an automatic parser. The proposed annotation tool provides the preview of a complete sentence structure suggested by the parser, and updates the preview whenever the annotator cancels or selects each segmentation point. Thus, the annotator can select the proper sentence segments maximizing parsing accuracy and minimizing human intervention. Experimental results show that the proposed tool allows the annotator to be able to reduce human intervention by approximately 39% compared with manual annotation. Sejong Korean treebank, one of the large scale treebanks, was constructed with the proposed annotation tool.","Language Resources and Evaluation",2006,"Yes"," annotation tool treebank minimal human intervention parsing segment based annotation tool korean treebanks minimal human intervention paper propose segment based annotation tool providing interactivity human annotator automatic parser proposed annotation tool preview complete sentence structure suggested parser updates preview annotator cancels selects segmentation point annotator select proper sentence segments maximizing parsing accuracy minimizing human intervention experimental results show proposed tool annotator reduce human intervention approximately compared manual annotation sejong korean treebank large scale treebanks constructed proposed annotation tool",1
"KeywordsKanji in web search Japanese web search queries Query processing Query substitution Query reformulation ","automatically generating related queries in japanese","Web searchers reformulate their queries, as they adapt to search engine behavior, learn more about a topic, or simply correct typing errors. Automatic query rewriting can help user web search, by augmenting a user’s query, or replacing the query with one likely to retrieve better results. One example of query-rewriting is spell-correction. We may also be interested in changing words to synonyms or other related terms. For Japanese, the opportunities for improving results are greater than for languages with a single character set, since documents may be written in multiple character sets, and a user may express the same meaning using different character sets. We give a description of the characteristics of Japanese search query logs and manual query reformulations carried out by Japanese web searchers. We use characteristics of Japanese query reformulations to extend previous work on automatic query rewriting in English, taking into account the Japanese writing system. We introduce several new features for building models resulting from this difference and discuss their impact on automatic query rewriting. We also examine enhancements in the form of rules which block conversion between some character sets, to address Japanese homophones. The precision/recall curves show significant improvement with the new feature set and blocking rules, and are often better than the English counterpart.","Language Resources and Evaluation",2006,"No"," kanji web search japanese web search queries query processing query substitution query reformulation automatically generating related queries japanese web searchers reformulate queries adapt search engine behavior learn topic simply correct typing errors automatic query rewriting user web search augmenting user query replacing query retrieve results query rewriting spell correction interested changing words synonyms related terms japanese opportunities improving results greater languages single character set documents written multiple character sets user express meaning character sets give description characteristics japanese search query logs manual query reformulations carried japanese web searchers characteristics japanese query reformulations extend previous work automatic query rewriting english taking account japanese writing system introduce features building models resulting difference discuss impact automatic query rewriting examine enhancements form rules block conversion character sets address japanese homophones precisionrecall curves show significant improvement feature set blocking rules english counterpart",0
"Keywordsinternational cooperation language resources language technology language technology evaluation ","developing language technologies with the support of language resources and evaluation programs","The role of language resources and language technology evaluation is now recognized as being crucial for the development of written and spoken language processing systems. Given the increasing challenge of multilingualism in Europe, the development of language technologies requires a more internationally distributed effort. This paper first describes several recent and on-going activities in France aimed at the development of language resources and evaluation. We then outline a new project intended to enhance collaboration, cooperation, and resource sharing among the international language processing research community.","Language Resources and Evaluation",2005,"No"," international cooperation language resources language technology language technology evaluation developing language technologies support language resources evaluation programs role language resources language technology evaluation recognized crucial development written spoken language processing systems increasing challenge multilingualism europe development language technologies requires internationally distributed effort paper describes recent activities france aimed development language resources evaluation outline project intended enhance collaboration cooperation resource sharing international language processing research community",0
"KeywordsWord meaning Lexicography Lexical universe Amour French Corneille ","how to measure the meanings of words amour in corneilles work","We present a new method to describe the contextual meaning of a key word in a corpus. The vocabulary of the sentences containing this word is compared to that of the entire corpus in order to highlight the words which are significantly overutilized in the neighbourhood of this key word (they are associated in the author’s mind) and the ones which are significantly underutilized (they are mutually exclusive). This method provides an interesting tool for lexicography and literary studies as is shown by applying it to the word amour (love) in the work of Pierre Corneille, the most famous French playwright of the 17th century.","Language Resources and Evaluation",2005,"No"," word meaning lexicography lexical universe amour french corneille measure meanings words amour corneilles work present method describe contextual meaning key word corpus vocabulary sentences word compared entire corpus order highlight words significantly overutilized neighbourhood key word author mind significantly underutilized mutually exclusive method interesting tool lexicography literary studies shown applying word amour love work pierre corneille famous french playwright th century",0
"Keywordsontology terminology text mining thesaurus ","thesaurus or logical ontology which one do we need for text mining","Ontologies are recognised as important tools, not only for effective and efficient information sharing, but also for information extraction and text mining. In the biomedical domain, the need for a common ontology for information sharing has long been recognised, and several ontologies are now widely used. However, there is confusion among researchers concerning the type of ontology that is needed for text mining , and how it can be used for effective knowledge management, sharing, and integration in biomedicine. We argue that there are several different ways to define an ontology and that, while the logical view is popular for some applications, it may be neither possible nor necessary for text mining. We propose a text-centered approach for knowledge sharing, as an alternative to formal ontologies. We argue that a thesaurus (i.e. an organised collection of terms enriched with relations) is more useful for text mining applications than formal ontologies.","Language Resources and Evaluation",2005,"No"," ontology terminology text mining thesaurus thesaurus logical ontology text mining ontologies recognised important tools effective efficient information sharing information extraction text mining biomedical domain common ontology information sharing long recognised ontologies widely confusion researchers type ontology needed text mining effective knowledge management sharing integration biomedicine argue ways define ontology logical view popular applications text mining propose text centered approach knowledge sharing alternative formal ontologies argue thesaurus organised collection terms enriched relations text mining applications formal ontologies",0
"Key wordsLinguistic annotation Multi-modal language corpora Software tools ","the nite xml toolkit data model and query language","The NITE XML Toolkit (NXT) is open source software for working with language corpora, with particular strengths for multimodal and heavily cross-annotated data sets. In NXT, annotations are described by types and attribute value pairs, and can relate to signal via start and end times, to representations of the external environment, and to each other via either an arbitrary graph structure or a multi-rooted tree structure characterized by both temporal and structural orderings. Simple queries in NXT express variable bindings for n-tuples of objects, optionally constrained by type, and give a set of conditions on the n-tuples combined with boolean operators. The defined operators for the condition tests allow full access to the timing and structural properties of the data model. A complex query facility passes variable bindings from one query to another for filtering, returning a tree structure. In addition to describing NXTȁ9s core data handling and search capabilities, we explain the stand-off XML data storage format that it employs and illustrate its use with examples from an early adopter of the technology.","Language Resources and Evaluation",2005,"No","key wordslinguistic annotation multi modal language corpora software tools nite xml toolkit data model query language nite xml toolkit nxt open source software working language corpora strengths multimodal heavily cross annotated data sets nxt annotations types attribute pairs relate signal start end times representations external environment arbitrary graph structure multi rooted tree structure characterized temporal structural orderings simple queries nxt express variable bindings tuples objects optionally constrained type give set conditions tuples combined boolean operators defined operators condition tests full access timing structural properties data model complex query facility passes variable bindings query filtering returning tree structure addition describing nxt s core data handling search capabilities explain stand xml data storage format employs illustrate examples early adopter technology",0
"Keywordsalignment error rate bilingual evaluation gold standard manual alignment parallel corpus precision recall word alignment ","guidelines for word alignment evaluation and manual alignment","The purpose of this paper is to provide guidelines for building a word alignment evaluation scheme. The notion of word alignment quality depends on the application: here we review standard scoring metrics for full text alignment and give explanations on how to use them better. We discuss strategies to build a reference corpus, and show that the ratio between ambiguous and unambiguous links in the reference has a great impact on scores measured with these metrics. In particular, automatically computed alignments with higher precision or higher recall can be favoured depending on the value of this ratio. Finally, we suggest a strategy to build a reference corpus particularly adapted to applications where recall plays a significant role, like in machine translation. The manually aligned corpus we built for the Spanish-English European Parliament corpus is also described. This corpus is freely available.","Language Resources and Evaluation",2005,"Yes"," alignment error rate bilingual evaluation gold standard manual alignment parallel corpus precision recall word alignment guidelines word alignment evaluation manual alignment purpose paper provide guidelines building word alignment evaluation scheme notion word alignment quality depends application review standard scoring metrics full text alignment give explanations discuss strategies build reference corpus show ratio ambiguous unambiguous links reference great impact scores measured metrics automatically computed alignments higher precision higher recall favoured depending ratio finally suggest strategy build reference corpus adapted applications recall plays significant role machine translation manually aligned corpus built spanish english european parliament corpus corpus freely ",1
"Keywordsaffect attitudes corpus annotation emotion natural language processing opinions sentiment subjectivity ","annotating expressions of opinions and emotions in language","This paper describes a corpus annotation project to study issues in the manual annotation of opinions, emotions, sentiments, speculations, evaluations and other private states in language. The resulting corpus annotation scheme is described, as well as examples of its use. In addition, the manual annotation process and the results of an inter-annotator agreement study on a 10,000-sentence corpus of articles drawn from the world press are presented.","Language Resources and Evaluation",2005,"Yes"," affect attitudes corpus annotation emotion natural language processing opinions sentiment subjectivity annotating expressions opinions emotions language paper describes corpus annotation project study issues manual annotation opinions emotions sentiments speculations evaluations private states language resulting corpus annotation scheme examples addition manual annotation process results inter annotator agreement study sentence corpus articles drawn world press presented",1
"Keywordsannotation inference temporal closure temporal information TimeML ","the role of inference in the temporal annotation and analysis of text","In this paper we argue for the importance of doing inference over the information expressed by the annotations of temporally annotated corpora. We describe the process of inferential closure which can be applied to determine the full temporal content that follows from an annotation. We illustrate the importance of temporal inference and temporal closure in relation to three tasks, which are: (a) the comparison of different temporal annotations, (b) facilitating the manual annotation process needed to create temporally annotated corpora and (c) empirical investigations done over temporally annotated data.","Language Resources and Evaluation",2005,"No"," annotation inference temporal closure temporal information timeml role inference temporal annotation analysis text paper argue importance inference information expressed annotations temporally annotated corpora describe process inferential closure applied determine full temporal content annotation illustrate importance temporal inference temporal closure relation tasks comparison temporal annotations facilitating manual annotation process needed create temporally annotated corpora empirical investigations temporally annotated data",0
"KeywordsComputational Linguistic Question Answering ","introduction to special issue on advances in question answering",NA,"Language Resources and Evaluation",2005,"No"," computational linguistic question answering introduction special issue advances question answering na",0
"Keywordsemotion neuro-psychology non-verbal speech paralinguistic information speech technology ","getting to the heart of the matter speech as the expression of affect rather than just text or language","This paper addresses the current needs for so-called emotion in speech, but points out that the issue is better described as the expression of relationships and attitudes rather than the currently held raw (or big-six) emotional states. From an analysis of more than three years of daily conversational speech, we find the direct expression of emotion to be extremely rare, and contend that when speech technologists say that what we need now is more ‘emotion’ in speech, what they really mean is that the current technologies are too text-based, and that more expression of speaker attitude, affect, and discourse relationships is required.","Language Resources and Evaluation",2005,"No"," emotion neuro psychology verbal speech paralinguistic information speech technology heart matter speech expression affect text language paper addresses current called emotion speech points issue expression relationships attitudes held raw big emotional states analysis years daily conversational speech find direct expression emotion extremely rare contend speech technologists emotion speech current technologies text based expression speaker attitude affect discourse relationships required",0
"KeywordsComputational Linguistic ","some of my best friends are linguists",NA,"Language Resources and Evaluation",2005,"No"," computational linguistic friends linguists na",0
"Keywordsevaluation architecture MT architecture Machine Translation ","hybrid architectures for machine translation systems","Although some progress has been made on the quality of Machine Translation in recent years, there is still a significant potential for quality improvement. There has also been a shift in paradigm of machine translation, from “classical” rule-based systems like METAL or LMT1 towards example-based or statistical MT.2 It seems to be time now to evaluate the progress and compare the results of these efforts, and draw conclusions for further improvements of MT quality.","Language Resources and Evaluation",2005,"No"," evaluation architecture mt architecture machine translation hybrid architectures machine translation systems progress made quality machine translation recent years significant potential quality improvement shift paradigm machine translation classical rule based systems metal lmt based statistical mt time evaluate progress compare results efforts draw conclusions improvements mt quality",0
"KeywordsComputational Linguistic Temporal Closure Annotation Environment ","temporal closure in an annotation environment",NA,"Language Resources and Evaluation",2005,"No"," computational linguistic temporal closure annotation environment temporal closure annotation environment na",0
"Keywordsquestion answering temporal ordering annotation events modality temporal expressions ","temporal and event information in natural language text","In this paper, we discuss the role that temporal information plays in natural language text, specifically in the context of question answering systems. We define a descriptive framework with which we can examine the temporally sensitive aspects of natural language queries. We then investigate broadly what properties a general specification language would need, in order to mark up temporal and event information in text. We present a language, TimeML, which attempts to capture the richness of temporal and event related information in language, while demonstrating how it can play an important part in the development of more robust question answering systems.","Language Resources and Evaluation",2005,"No"," question answering temporal ordering annotation events modality temporal expressions temporal event information natural language text paper discuss role temporal information plays natural language text specifically context question answering systems define descriptive framework examine temporally sensitive aspects natural language queries investigate broadly properties general specification language order mark temporal event information text present language timeml attempts capture richness temporal event related information language demonstrating play important part development robust question answering systems",0
"Keywordsdialogue systems evaluation machine learning ","can we talk methods for evaluation and training of spoken dialogue systems","There is a strong relationship between evaluation and methods for automatically training language processing systems, where generally the same resource and metrics are used both to train system components and to evaluate them. To date, in dialogue systems research, this general methodology is not typically applied to the dialogue manager and spoken language generator. However, any metric for evaluating system performance can be used as a feedback function for automatically training the system. This approach is motivated with examples of the application of reinforcement learning to dialogue manager optimization, and the use of boosting to train the spoken language generator.","Language Resources and Evaluation",2005,"No"," dialogue systems evaluation machine learning talk methods evaluation training spoken dialogue systems strong relationship evaluation methods automatically training language processing systems generally resource metrics train system components evaluate date dialogue systems research general methodology typically applied dialogue manager spoken language generator metric evaluating system performance feedback function automatically training system approach motivated examples application reinforcement learning dialogue manager optimization boosting train spoken language generator",0
"KeywordsAntonio Zampolli language resources and evaluation ","introduction to the special inaugural issue","This first issue of Language Resources and Evaluation is dedicated to the memory of Antonio Zampolli, whom few would dispute is the one person who has led the way in promoting and establishing the development of language resources (LR) of all kinds for the past four decades. In this inaugural issue, we have attempted to bring together articles by major figures in the field in order to provide an overview of the history, state of the art, and the future of the creation, annotation, exploitation, evaluation, and distribution of LR. Hopefully, this collection of articles will serve not only as a tribute to Antonio, but also as a framework out of which this journal – which almost certainly would not have existed were it not for him – can grow.","Language Resources and Evaluation",2005,"No"," antonio zampolli language resources evaluation introduction special inaugural issue issue language resources evaluation dedicated memory antonio zampolli dispute person led promoting establishing development language resources lr kinds past decades inaugural issue attempted bring articles major figures field order provide overview history state art future creation annotation exploitation evaluation distribution lr collection articles serve tribute antonio framework journal existed grow",0
"Keywordsauthorship attribution Herdan’s Vm lexical statistics vocabulary repetition vocabulary richness Yule’s K ","yules characteristic k revisited","The measure of lexical repetition constitutes one of the variables used to determine the lexical richness of literary texts, a value further employed in authorship attribution studies. Although most of the constants for lexical richness actually depend on text length, Yule’s characteristic is considered to be highly reliable for being text length independent. It is not the aim of this paper questioning the validity of K to measure the lexical repeat-rate, nor to evaluate its usefulness in authorship studies, but to review the most accurate procedure to calculate its value in the light of the lack of standardization found in the specific literature. At the same time, the peculiar calculation of Yule’s K by TACT is explained. Our study suggests that standardization will certainly help improve the studies where K is employed.","Language Resources and Evaluation",2005,"No"," authorship attribution herdan vm lexical statistics vocabulary repetition vocabulary richness yule yules characteristic revisited measure lexical repetition constitutes variables determine lexical richness literary texts employed authorship attribution studies constants lexical richness depend text length yule characteristic considered highly reliable text length independent aim paper questioning validity measure lexical repeat rate evaluate usefulness authorship studies review accurate procedure calculate light lack standardization found specific literature time peculiar calculation yule tact explained study suggests standardization improve studies employed",0
"Keywordsevaluation language resources production standards validation ","elra european language resources association background recent developments and future perspectives","The European Language Resources Association (ELRA) was founded in 1995 with the mission of providing language resources (LR) to European research institutions and companies. In this paper we describe the background, the mission and the major activities since then.","Language Resources and Evaluation",2005,"No"," evaluation language resources production standards validation elra european language resources association background recent developments future perspectives european language resources association elra founded mission providing language resources lr european research institutions companies paper describe background mission major activities ",0
"EuroWordNet OIL methodology ontology semantic relation semantic Web SIMPLE top ontology ","semantic roles as slots in oil ontologies","The purpose of our research is to consider how the paradigms of EuroWordNet and SIMPLE linguistic projects on the one hand and the OIL methodology on the other hand may affect each other. OIL (Ontology Inference Layer) aims at implementing the ``semantic'' Web idea and is based on the notion of ontology, which is also employed in EuroWordNet and SIMPLE. In both latter projects the meanings of words are partially described by means of the finite sets of relations to other meanings of words, whereas in OIL the user is free to define the arbitrary relations of this kind.The relations considered in EuroWordNet and SIMPLE were defined on the basis of a careful observation of the large linguistic area, andt hey aim at reflecting the meaning as precisely as possible, therefore it seems useful to merge them with OIL. Moreover, the valuable feature of OIL is its formal language with precisely defined semantics. All things considered, we suggest how certain EuroWordNet and SIMPLE definitions may be expressed in OIL.","Computers and the Humanities",2004,"No","eurowordnet oil methodology ontology semantic relation semantic web simple top ontology semantic roles slots oil ontologies purpose research paradigms eurowordnet simple linguistic projects hand oil methodology hand affect oil ontology inference layer aims implementing semantic web idea based notion ontology employed eurowordnet simple projects meanings words partially means finite sets relations meanings words oil user free define arbitrary relations kind relations considered eurowordnet simple defined basis careful observation large linguistic area andt hey aim reflecting meaning precisely merge oil valuable feature oil formal language precisely defined semantics things considered suggest eurowordnet simple definitions expressed oil",0
"Keywordscorpus hypernymy pattern semantic variation terminology thesaurus ","automatic acquisition and expansion of hypernym links","Recent developments in computational terminology call for the design of multiple and complementary tools for the acquisition, the structuring and the exploitation of terminological data. This paper proposes to bridge the gap between term acquisition and thesaurus construction by offering a framework for automatic structuring of multi-word candidate terms with the help of corpus-based links between single-word terms. First, we present a system for corpus-based acquisition of terminological relationships through discursive patterns. This system is built on previous work on automatic extraction of hyponymy links through shallow parsing. Second, we show how hypernym links between single-word terms can be extended to semantic links between multi-word terms through corpus-based extraction of semantic variants. The induced hierarchy is incomplete but provides an automatic generalization of single-word terms relations to multi-word terms that are pervasive in technical thesauri and corpora.","Computers and the Humanities",2004,"No"," corpus hypernymy pattern semantic variation terminology thesaurus automatic acquisition expansion hypernym links recent developments computational terminology call design multiple complementary tools acquisition structuring exploitation terminological data paper proposes bridge gap term acquisition thesaurus construction offering framework automatic structuring multi word candidate terms corpus based links single word terms present system corpus based acquisition terminological relationships discursive patterns system built previous work automatic extraction hyponymy links shallow parsing show hypernym links single word terms extended semantic links multi word terms corpus based extraction semantic variants induced hierarchy incomplete automatic generalization single word terms relations multi word terms pervasive technical thesauri corpora",0
"author identification coherence computational linguistics content analysis corpus linguistics idiolect latent semantic analysis literary period sociolect ","semantic variation in idiolect and sociolect corpus linguistic evidence from literary texts","Idiolects are person-dependent similarities in language use. They imply that texts by one author show more similarities in language use than texts between authors. Sociolects, on the other hand, are group-dependent similarities in language use. They imply that texts by a group of authors, for instance in terms of gender or time period, share more similarities within a group than between groups. Although idiolects and sociolects are commonly used terms in the humanities, they have not been investigated a great deal from corpus and computational linguistic points of view. To test several idiolect and sociolect hypotheses a factorial combination was used of time period (Modernism, Realism), gender of author (male, female) and author (Eliot, Dickens, Woolf, Joyce) totaling 16 corresponding literary texts. In a series of corpus linguistic studies using Boolean and vector models, no conclusive evidence was found for the selected idiolect and sociolect hypotheses. In final analyses testing the semantics within each literary text, this lack of evidence was explained by the low homogeneity within a literary text.","Computers and the Humanities",2004,"No","author identification coherence computational linguistics content analysis corpus linguistics idiolect latent semantic analysis literary period sociolect semantic variation idiolect sociolect corpus linguistic evidence literary texts idiolects person dependent similarities language imply texts author show similarities language texts authors sociolects hand group dependent similarities language imply texts group authors instance terms gender time period share similarities group groups idiolects sociolects commonly terms humanities investigated great deal corpus computational linguistic points view test idiolect sociolect hypotheses factorial combination time period modernism realism gender author male female author eliot dickens woolf joyce totaling literary texts series corpus linguistic studies boolean vector models conclusive evidence found selected idiolect sociolect hypotheses final analyses testing semantics literary text lack evidence explained low homogeneity literary text",0
"authorship attribution collaboration federalist papers statistics ","detecting collaborations in text comparing the authors rhetorical language choices in the federalist papers","In author attribution studies function words or lexical measures areoften used to differentiate the authors' textual fingerprints. Thesestudies can be thought of as quantifying the texts, representing thetext with measured variables that stand for specific textual features.The resulting quantifications, while proven useful for statisticallydifferentiating among the texts, bear no resemblance to the understanding a human reader – even an astute one – would develop whilereading the texts. In this paper we present an attribution study that,instead, characterizes the texts according to the representationallanguage choices of the authors, similar to a way we believe close humanreaders come to know a text and distinguish its rhetorical purpose. Fromour automated quantification of The Federalist papers, it isclear why human readers find it impossible to distinguish the authorshipof the disputed papers. Our findings suggest that changes occur in theprocesses of rhetorical invention when undertaken in collaborativesituations. This points to a need to re-evaluate the premise ofautonomous authorship that has informed attribution studies of The Federalist case.","Computers and the Humanities",2004,"No","authorship attribution collaboration federalist papers statistics detecting collaborations text comparing authors rhetorical language choices federalist papers author attribution studies function words lexical measures areoften differentiate authors textual fingerprints thesestudies thought quantifying texts representing thetext measured variables stand specific textual features resulting quantifications proven statisticallydifferentiating texts bear resemblance understanding human reader astute develop whilereading texts paper present attribution study characterizes texts representationallanguage choices authors similar close humanreaders text distinguish rhetorical purpose fromour automated quantification federalist papers isclear human readers find impossible distinguish authorshipof disputed papers findings suggest occur theprocesses rhetorical invention undertaken collaborativesituations points evaluate premise ofautonomous authorship informed attribution studies federalist case",0
"alignment copyright greedy string tiling journalism n-grams plagiarism text rewriting ","on the ownership of text","The paper explores the notions of text ownership and its partial inverse, plagiarism, and asks how close or different they are from a procedural point of view that might seek to establish either of these properties. The emphasis is on procedures rather than on the conventional subject division of authorship studies, plagiarism detection etc. We use, as a particular example, our research on the notion of computational detection of text rewriting, in the benign sense of a standard journalist's adaptation of the Press Association newsfeed. The conclusion is that, whatever may be the case in copyright law, procedural detection and establishment of the ownership is a complex and vexed matter. Behind the paper is an unspoken appeal to return to an earlier historical phase, one where texts were normally rewritten and rewritten again and the ownership of text by an individual was a less clear matter than in historically recent times.","Computers and the Humanities",2004,"No","alignment copyright greedy string tiling journalism grams plagiarism text rewriting ownership text paper explores notions text ownership partial inverse plagiarism asks close procedural point view seek establish properties emphasis procedures conventional subject division authorship studies plagiarism detection research notion computational detection text rewriting benign sense standard journalist adaptation press association newsfeed conclusion case copyright law procedural detection establishment ownership complex vexed matter paper unspoken appeal return earlier historical phase texts rewritten rewritten ownership text individual clear matter historically recent times",0
"Keywordsagreement between transcribers χ2 analysis corpus research effect size log odds power of a test sequential dependence unit dependence ","pitfalls in corpus research",". This paper discusses some pitfalls in corpus research and suggests solutions on the basis of examples and computer simulations. We first address reliability problems in language transcriptions, agreement between transcribers, and how disagreements can be dealt with. We then show that the frequencies of occurrence obtained from a corpus cannot always be analyzed with the traditional χ2 test, as corpus data are often not sequentially independent and unit independent. Next, we stress the relevance of the power of statistical tests, and the sizes of statistically significant effects. Finally, we point out that a t-test based on log odds often provides a better alternative to a χ2 analysis based on frequency counts.","Computers and the Humanities",2004,"No"," agreement transcribers analysis corpus research effect size log odds power test sequential dependence unit dependence pitfalls corpus research paper discusses pitfalls corpus research suggests solutions basis examples computer simulations address reliability problems language transcriptions agreement transcribers disagreements dealt show frequencies occurrence obtained corpus analyzed traditional test corpus data sequentially independent unit independent stress relevance power statistical tests sizes statistically significant effects finally point test based log odds alternative analysis based frequency counts",0
"alignment evaluation lemmatization tagging translation equivalence ","extracting multilingual lexicons from parallel corpora","The paper describes our recent developments in automatic extraction of translation equivalents from parallel corpora. We describe three increasingly complex algorithms: a simple baseline iterative method, and two non-iterative more elaborated versions. While the baseline algorithm is mainly described for illustrative purposes, the non-iterative algorithms outline the use of different working hypotheses which may be motivated by different kinds of applications and to some extent by the languages concerned. The first two algorithms rely on cross-lingual POS preservation, while with the third one POS invariance is not an extraction condition. The evaluation of the algorithms was conducted on three different corpora and several pairs of languages.","Computers and the Humanities",2004,"No","alignment evaluation lemmatization tagging translation equivalence extracting multilingual lexicons parallel corpora paper describes recent developments automatic extraction translation equivalents parallel corpora describe increasingly complex algorithms simple baseline iterative method iterative elaborated versions baseline algorithm illustrative purposes iterative algorithms outline working hypotheses motivated kinds applications extent languages concerned algorithms rely cross lingual pos preservation pos invariance extraction condition evaluation algorithms conducted corpora pairs languages",0
"alignment bilingual document generation bitext parallel corpus segmentation SGML TEI translation memories ","bitext generation through rich markup","This paper reports on a method for exploiting a bitext as the primary linguistic information source for the design of a generation environment for specialized bilingual documentation. The paper discusses such issues as Text Encoding Initiative (TEI), proposals for specialized corpus tagging, text segmentation and alignment of translation units and their allocation into translation memories, Document Type Definition (DTD), abstraction from tagged texts, and DTD deployment for bilingual text generation. The parallel corpus used for experimentation has two main features:","Computers and the Humanities",2004,"No","alignment bilingual document generation bitext parallel corpus segmentation sgml tei translation memories bitext generation rich markup paper reports method exploiting bitext primary linguistic information source design generation environment specialized bilingual documentation paper discusses issues text encoding initiative tei proposals specialized corpus tagging text segmentation alignment translation units allocation translation memories document type definition dtd abstraction tagged texts dtd deployment bilingual text generation parallel corpus experimentation main features",0
"automatic term recognition special languages special language subcorpora terms term extraction verb subcategorisation patterns ","an analysis of verb subcategorization frames in three special language corpora with a view towards automatic term recognition","Current term recognition algorithms havecentred mostly on the notion of term based onthe assumption that terms are monoreferentialand as such independent of context. Thecharacteristics and behaviour of terms in realtexts are however far removed from this idealbecause factors such as text type orcommunicative situation greatly influence thelinguistic realisation of a concept. Context,therefore, is important for the correctidentification of terms (Dubuc and Lauriston,1997). Based on this assumption, we haveshifted our emphasis from terms towardssurrounding linguistic context, namely verbs,as verbs are considered the central elements inthe sentence. More specifically, we have setout to examine whether verbs and verbal syntaxin particular, could help us towards the taskof automatic term recognition. Our findingssuggest that term occurrence variessignificantly in different argument structuresand different syntactic positions. Additionally, deviant grammatical structureshave proved rich environments for terms. Theanalysis was carried out in three differentspecialised subcorpora in order to explore howthe effectiveness of verbal syntax as apotential indicator of term occurrence can beconstrained by factors such as subject matterand text type.","Computers and the Humanities",2004,"No","automatic term recognition special languages special language subcorpora terms term extraction verb subcategorisation patterns analysis verb subcategorization frames special language corpora view automatic term recognition current term recognition algorithms havecentred notion term based onthe assumption terms monoreferentialand independent context thecharacteristics behaviour terms realtexts removed idealbecause factors text type orcommunicative situation greatly influence thelinguistic realisation concept context important correctidentification terms dubuc lauriston based assumption haveshifted emphasis terms towardssurrounding linguistic context verbs verbs considered central elements inthe sentence specifically setout examine verbs verbal syntaxin taskof automatic term recognition findingssuggest term occurrence variessignificantly argument structuresand syntactic positions additionally deviant grammatical structureshave proved rich environments terms theanalysis carried differentspecialised subcorpora order explore howthe effectiveness verbal syntax apotential indicator term occurrence beconstrained factors subject matterand text type",0
"Keywordsarabic automatic diacritics generation envelope geographic names N-gram romanization security stochastic transliteration ","stochastic models for automatic diacritics generation of arabic names","In this paper, two new models for generating diacritics for Arabic names are proposed. The first proposed model is called N-gram model. It is a stochastic model that is based on generating a corpus database of N-grams extracted from a large database of names with their corresponding probability according to an N-gram position in a text (Bhal et al., 1983). i.e., the probability that an N-gram has happened in a position x, where x can be the first, second,... or ith position in the text. Replacing the N-grams with their patterns extends the first model to the second proposed stochastic model. It is called the Envelope model. These two proposed models are unique in being the first attempt to solve the problem in Arabic text diacritics generation using linguistic constraints stochastic approaches that are neither grammatical nor pure lexical based (Merialdo, 1991; Ney and Essen, 1991; Schukat-Talamazzini et al., 1992; Witschel and Niedermair, 1992). This methodology helps in reducing size and complexity of software implementation of the proposed models and makes it easier to update and port across different platforms.","Computers and the Humanities",2004,"No"," arabic automatic diacritics generation envelope geographic names gram romanization security stochastic transliteration stochastic models automatic diacritics generation arabic names paper models generating diacritics arabic names proposed proposed model called gram model stochastic model based generating corpus database grams extracted large database names probability gram position text bhal al probability gram happened position ith position text replacing grams patterns extends model proposed stochastic model called envelope model proposed models unique attempt solve problem arabic text diacritics generation linguistic constraints stochastic approaches grammatical pure lexical based merialdo ney essen schukat talamazzini al witschel niedermair methodology helps reducing size complexity software implementation proposed models makes easier update port platforms",0
"Keywordscombination mapping WordNet word sense disambiguation ","multiple heuristics and their combination for automatic wordnet mapping","This paper presents an automatic construction of Korean WordNet from pre-existing lexical resources. We develop a set of automatic word sense disambiguation techniques to link a Korean word sense collected from a bilingual machine-readable dictionary to a single corresponding English WordNet synset. We show how individual links provided by each word sense disambiguation method can be non-linearly combined to produce a Korean WordNet from existing English WordNet for nouns.","Computers and the Humanities",2004,"Yes"," combination mapping wordnet word sense disambiguation multiple heuristics combination automatic wordnet mapping paper presents automatic construction korean wordnet pre existing lexical resources develop set automatic word sense disambiguation techniques link korean word sense collected bilingual machine readable dictionary single english wordnet synset show individual links provided word sense disambiguation method linearly combined produce korean wordnet existing english wordnet nouns",1
"Keywordslinguistic features self-organized document maps semantic space SENSEVAL-2 word sense disambiguation ","evaluation of linguistic features for word sense disambiguation with self organized document maps","Word sense disambiguation automatically determines the appropriate senses of a word in context. We have previously shown that self-organized document maps have properties similar to a large-scale semantic structure that is useful for word sense disambiguation. This work evaluates the impact of different linguistic features on self-organized document maps for word sense disambiguation. The features evaluated are various qualitative features, e.g. part-of-speech and syntactic labels, and quantitative features, e.g. cut-off levels for word frequency. It is shown that linguistic features help make contextual information explicit. If the training corpus is large even contextually weak features, such as base forms, will act in concert to produce sense distinctions in a statistically significant way. However, the most important features are syntactic dependency relations and base forms annotated with part of speech or syntactic labels. We achieve 62.9% ± 0.73% correct results on the fine grained lexical task of the English SENSEVAL-2 data. On the 96.7% of the test cases which need no back-off to the most frequent sense we achieve 65.7% correct results.","Computers and the Humanities",2004,"No"," linguistic features organized document maps semantic space senseval word sense disambiguation evaluation linguistic features word sense disambiguation organized document maps word sense disambiguation automatically determines senses word context previously shown organized document maps properties similar large scale semantic structure word sense disambiguation work evaluates impact linguistic features organized document maps word sense disambiguation features evaluated qualitative features part speech syntactic labels quantitative features cut levels word frequency shown linguistic features make contextual information explicit training corpus large contextually weak features base forms act concert produce sense distinctions statistically significant important features syntactic dependency relations base forms annotated part speech syntactic labels achieve correct results fine grained lexical task english senseval data test cases back frequent sense achieve correct results",0
"agglutinative languages authorship attribution statistical analysis stylochoronometry stylometry Turkish ","change of writing style with time","This study investigates the writing stylechange of two Turkish authors, Çetin Altanand Yaşar Kemal, in their old and newworks using respectively their newspapercolumns and novels. The style markers are thefrequencies of word lengths in both text andvocabulary, and the rate of usage of mostfrequent words. For both authors, t-tests andlogistic regressions show that the length ofthe words in new works is significantly longerthan that of the old. The principal componentanalyses graphically illustrate the separationbetween old and new texts. The works arecorrectly categorized as old or new with 75 to100% accuracy and 92% average accuracy usingdiscriminant analysis-based cross validation. The results imply higher time gap may havepositive impact in separation andcategorization. For Altan a regressionanalysis demonstrates a decrease in averageword length as the age of his column increases. One interesting observation is that for oneword each author has similar preference changesover time.","Computers and the Humanities",2004,"No","agglutinative languages authorship attribution statistical analysis stylochoronometry stylometry turkish change writing style time study investigates writing stylechange turkish authors etin altanand ya ar kemal newworks newspapercolumns novels style markers thefrequencies word lengths text andvocabulary rate usage mostfrequent words authors tests andlogistic regressions show length ofthe words works significantly longerthan principal componentanalyses graphically illustrate separationbetween texts works arecorrectly categorized to accuracy average accuracy usingdiscriminant analysis based cross validation results imply higher time gap havepositive impact separation andcategorization altan regressionanalysis demonstrates decrease averageword length age column increases interesting observation oneword author similar preference changesover time",0
"automated scoring content-based scoring short answer scoring ","c rater automated scoring of short answer questions","C-rater is an automated scoringengine that has been developed to scoreresponses to content-based short answerquestions. It is not simply a stringmatching program – instead it uses predicateargument structure, pronominal reference,morphological analysis and synonyms to assignfull or partial credit to a short answerquestion. C-rater has been used in two studies:National Assessment for Educational Progress(NAEP) and a statewide assessment in Indiana.In both studies, c-rater agreed with humangraders about 84% of the time.","Computers and the Humanities",2003,"No","automated scoring content based scoring short answer scoring rater automated scoring short answer questions rater automated scoringengine developed scoreresponses content based short answerquestions simply stringmatching program predicateargument structure pronominal referencemorphological analysis synonyms assignfull partial credit short answerquestion rater studiesnational assessment educational progressnaep statewide assessment indiana studies rater agreed humangraders time",0
"KeywordsComputational Linguistic Questionnaire Data Rural Dialect ","the were subjunctive in british rural dialects marrying corpus and questionnaire data",NA,"Computers and the Humanities",2003,"No"," computational linguistic questionnaire data rural dialect subjunctive british rural dialects marrying corpus questionnaire data na",0
"education gender language proficiency lexical statistics parts of speech socioeconomic background speech vocabulary vocabulary richness ","vocabulary in interviews as related to respondent characteristics","Responses in personalinterviews about education and career with 415Swedish men and women (age 34) forms the basisof a speech corpus with 1.8 million words. Thevocabulary is described by means of two sets ofvariables. One is based on the number of tokensand types, word length and sectioning of therunning text. The other set divides the corpusinto grammatical categories. Both sets ofvariables are related to a number of backgroundvariables such as gender, socioeconomicbackground, education, and indicators of verbalproficiency at age 13 and 32. This possibilityto study the relationship between vocabularyand a broad set of respondent characteristicsis a unique feature of this corpus.","Computers and the Humanities",2003,"No","education gender language proficiency lexical statistics parts speech socioeconomic background speech vocabulary vocabulary richness vocabulary interviews related respondent characteristics responses personalinterviews education career swedish men women age forms basisof speech corpus million words thevocabulary means sets ofvariables based number tokensand types word length sectioning therunning text set divides corpusinto grammatical categories sets ofvariables related number backgroundvariables gender socioeconomicbackground education indicators verbalproficiency age possibilityto study relationship vocabularyand broad set respondent characteristicsis unique feature corpus",0
"aggregate methods association measures multidimensional scaling profile-based analysis variational linguistics ","profile based linguistic uniformity as a generic method for comparing language varieties","In this text we present``profile-based linguistic uniformity'', a methoddesigned to compare language varieties on thebasis of a wide range of potentiallyheterogeneous linguistic variables. In manyrespects a parallel can be drawn with currentmethods in dialectometry (for an overview, see,Nerbonne and Heeringa, 2001; Heeringa, Nerbonneand Kleiweg, 2002): in both casesdissimilarities between varieties on the basisof individual variables are summarized inglobal dissimilarities, and a series oflanguage varieties are subsequently clusteredor charted using multivariate techniques suchas cluster analysis or multidimensionalscaling. This global similarity between themethods makes it possible to compare them andto investigate the implications of notabledifferences. In this text we specifically focuson, and defend one characteristic of ourmethodology, its profile-based nature.","Computers and the Humanities",2003,"No","aggregate methods association measures multidimensional scaling profile based analysis variational linguistics profile based linguistic uniformity generic method comparing language varieties text presentprofile based linguistic uniformity methoddesigned compare language varieties thebasis wide range potentiallyheterogeneous linguistic variables manyrespects parallel drawn currentmethods dialectometry overview nerbonne heeringa heeringa nerbonneand kleiweg casesdissimilarities varieties basisof individual variables summarized inglobal dissimilarities series oflanguage varieties subsequently clusteredor charted multivariate techniques suchas cluster analysis multidimensionalscaling global similarity themethods makes compare andto investigate implications notabledifferences text specifically focuson defend characteristic ourmethodology profile based nature",0
"discourse processing finite state methods machine translation speech act assignment ","modeling task oriented dialogue","A common tool for improving theperformance quality of natural languageprocessing systems is the use of contextualinformation for disambiguation. Here I describethe use of a finite state machine (FSM) todisambiguate speech acts in a machinetranslation system. The FSM has two layers thatmodel, respectively, the global and localstructures found in naturally-occurringconversations. The FSM has been modeled on acorpus of task-oriented dialogues in a travelplanning situation. In the dialogues, one ofthe interactants is a travel agent or hotelclerk, and the other a client requestinginformation or services. A discourse processorbased on the FSM was implemented in order toprocess contextual information in a machinetranslation system. Evaluation results showthat the discourse processor is able todisambiguate and improve the quality of thedialogue translation. Other applicationsinclude human-computer interaction andcomputer-assisted language learning.","Computers and the Humanities",2003,"No","discourse processing finite state methods machine translation speech act assignment modeling task oriented dialogue common tool improving theperformance quality natural languageprocessing systems contextualinformation disambiguation describethe finite state machine fsm todisambiguate speech acts machinetranslation system fsm layers thatmodel global localstructures found naturally occurringconversations fsm modeled acorpus task oriented dialogues travelplanning situation dialogues ofthe interactants travel agent hotelclerk client requestinginformation services discourse processorbased fsm implemented order toprocess contextual information machinetranslation system evaluation results showthat discourse processor todisambiguate improve quality thedialogue translation applicationsinclude human computer interaction andcomputer assisted language learning",0
"automatic text categorization text analysis text classification ","categorisation techniques in computer assisted reading and analysis of texts carat in the humanities","There are two important strategies incomputer-assisted reading and analysis of text(CARAT). The first relates to theclassification process, and the second pertainsto the categorisation process. These twooften-interrelated operations have beenregularly recognised as essential components oftext analysis. However, the two operations arehighly time-consuming. A possible solution tothis problem calls upon more inductive orbottom-up strategies that are numerical andstatistical in nature. In our own research, wehave been exploring a few of these techniquesand their combination. We now know, through ourown past research and others' work, that theclassification methods allow a good empiricalthematic exploration of a corpus. Morespecifically, in this paper we shallconcentrate on the problem of assisting theautomatic categorisation of small segments of aphilosophical text into a set of thematiccategories.","Computers and the Humanities",2003,"No","automatic text categorization text analysis text classification categorisation techniques computer assisted reading analysis texts carat humanities important strategies incomputer assisted reading analysis textcarat relates theclassification process pertainsto categorisation process twooften interrelated operations beenregularly recognised essential components oftext analysis operations arehighly time consuming solution tothis problem calls inductive orbottom strategies numerical andstatistical nature research wehave exploring techniquesand combination ourown past research work theclassification methods good empiricalthematic exploration corpus morespecifically paper shallconcentrate problem assisting theautomatic categorisation small segments aphilosophical text set thematiccategories",0
"information theory KL-distance language change linguistic distance mathematics of language ","the time course of language change","This paper presents a numeric and information theoretic model for themeasuring of language change, without specifying the particular type ofchange. It is shown that this measurement is intuitively plausibleand that meaningful measurements canbe made from as few as 1000 characters. This measurement techniqueis extended to the task of determining the ``rate'' of language changebased on an examination of brief excerpts from the NationalGeographic Magazine and determining both their linguistic distancefrom one another as well as the number of years of temporal separation.A statistical analysis of these results shows, first, that language changecan be measured, and second, that the rate of languagechange has not been uniform, and that in particular, the period 1939-;1948had particularly slow change, while 1949-;1958 and 1959-;1968 hadparticularly rapid changes.","Computers and the Humanities",2003,"No","information theory kl distance language change linguistic distance mathematics language time language change paper presents numeric information theoretic model themeasuring language change type ofchange shown measurement intuitively plausibleand meaningful measurements canbe made characters measurement techniqueis extended task determining rate language changebased examination excerpts nationalgeographic magazine determining linguistic distancefrom number years temporal separation statistical analysis results shows language changecan measured rate languagechange uniform period had slow change hadparticularly rapid ",0
"KeywordsComputational Linguistic ","questions of authorship attribution and beyond a lecture delivered on the occasion of the roberto busa award ach allc 2001 new york",NA,"Computers and the Humanities",2003,"No"," computational linguistic questions authorship attribution lecture delivered occasion roberto busa award ach allc york na",0
NA,"contents of volume 37",NA,"Computers and the Humanities",2003,"No"," contents volume na",0
"cognates dialects features phonetic alignment phonetic similarity ","phonetic alignment and similarity","The computation of the optimal phonetic alignment andthe phonetic similarity between wordsis an important step in many applications in computational phonology,including dialectometry.After discussing several related algorithms,I present a novel approach to the problem that employsa scoring scheme for computing phonetic similarity between phonetic segmentson the basis of multivalued articulatory phonetic features.The scheme incorporates the key concept of feature salience,which is necessary to properly balance the importance of various features.The new algorithm combines several techniquesdeveloped for sequence comparison:an extended set of edit operations,local and semiglobal modes of alignment,and the capability of retrieving a set of near-optimal alignments.On a set of 82 cognate pairs,it performs better than comparable algorithms reported in the literature.","Computers and the Humanities",2003,"No","cognates dialects features phonetic alignment phonetic similarity phonetic alignment similarity computation optimal phonetic alignment andthe phonetic similarity wordsis important step applications computational phonologyincluding dialectometry discussing related algorithms present approach problem employsa scoring scheme computing phonetic similarity phonetic segmentson basis multivalued articulatory phonetic features scheme incorporates key concept feature salience properly balance importance features algorithm combines techniquesdeveloped sequence comparison extended set edit operationslocal semiglobal modes alignment capability retrieving set optimal alignments set cognate pairs performs comparable algorithms reported literature",0
"digital library letter publishing literature ","putting the dialogue back together re creating structure in letter publishing","In this paper, we will present a publication system in which selectedmaterial from letter collections is presented as dialogues between twopersons.","Computers and the Humanities",2003,"No","digital library letter publishing literature putting dialogue back creating structure letter publishing paper present publication system selectedmaterial letter collections presented dialogues twopersons",0
"authorship attribution lexical statistics stylistics vocabulary richness ","another perspective on vocabulary richness","This article examines the usefulness ofvocabulary richness for authorship attributionand tests the assumption that appropriatemeasures of vocabulary richness can capture anauthor's distinctive style or identity. Afterbriefly discussing perceived and actualvocabulary richness, I show that doubling andcombining texts affects some measures incomputationally predictable but conceptuallysurprising ways. I discuss some theoretical andempirical problems with some measures anddevelop simple methods to test how wellvocabulary richness distinguishes texts bydifferent authors. These methods show thatvocabulary richness is ineffective for largegroups of texts because of the extremevariability within and among them. I concludethat vocabulary richness is of marginal valuein stylistic and authorship studies because thebasic assumption that it constitutes awordprint for authors is false.","Computers and the Humanities",2003,"No","authorship attribution lexical statistics stylistics vocabulary richness perspective vocabulary richness article examines usefulness ofvocabulary richness authorship attributionand tests assumption appropriatemeasures vocabulary richness capture anauthor distinctive style identity afterbriefly discussing perceived actualvocabulary richness show doubling andcombining texts affects measures incomputationally predictable conceptuallysurprising ways discuss theoretical andempirical problems measures anddevelop simple methods test wellvocabulary richness distinguishes texts bydifferent authors methods show thatvocabulary richness ineffective largegroups texts extremevariability concludethat vocabulary richness marginal valuein stylistic authorship studies thebasic assumption constitutes awordprint authors false",0
"Canterbury Tales Chaucer gene order phylogenetic analysis stemmatology ","analyzing the order of items in manuscripts of the canterbury tales","Chaucer's CanterburyTales consists of loosely-connected stories,appearing in many different orders in extantmanuscripts. Differences in order result fromrearrangements by scribes during copying, andmay reveal relationships among manuscripts. Identifying these relationships is analogous todetermining evolutionary relationships amongorganisms from the order of genes on a genome. We use gene order analysis to construct astemma for the Canterbury Tales. Thisstemma shows relationships predicted by earlierscholars, reveals new relationships, and sharesfeatures with a word variation stemma. Ourresults support the idea that there was noestablished order when the first manuscriptswere written.","Computers and the Humanities",2003,"No","canterbury tales chaucer gene order phylogenetic analysis stemmatology analyzing order items manuscripts canterbury tales chaucer canterburytales consists loosely connected storiesappearing orders extantmanuscripts differences order result fromrearrangements scribes copying andmay reveal relationships manuscripts identifying relationships analogous todetermining evolutionary relationships amongorganisms order genes genome gene order analysis construct astemma canterbury tales thisstemma shows relationships predicted earlierscholars reveals relationships sharesfeatures word variation stemma ourresults support idea noestablished order manuscriptswere written",0
"cluster analysis dialectometry Finnish dialects idiolectal variation transitional dialects ","neighbours or enemies competing variants causing differences in transitional dialects","The aim of this study is to show how clusteranalysis can shed light on very complexvariation in a transitional dialect zone ineastern Finland. In the course of history thisarea has been on the border between Sweden andRussia and the population has clearly been oftwo kinds: the Savo people and the Karelians.It is a well-known fact that there is variationamong these dialects, but the spread and extentof the variation has not been demonstrated previously.The idiolects of the area were studied in thelight of ten phonological and morphologicalfeatures. The material consisted of recordingsof 198 idiolects, totalling around 195 hoursand representing 19 parishes. The variation wasanalysed using hierarchical cluster analysis.While the analysis showed the extent of thevariation between idiolects and parishes, italso demonstrated how the effects of the oldparishes, borders and settlements are stillvisible in the dialects. On the parish level,the data formed clear clusters that correspondwith the main dialects in the area and itssurroundings. On the idiolect level, however,the speakers from the surrounding areas formedfairly homogenous clusters but the idiolectsfrom the Savonlinna area were spread acrossalmost all clusters.","Computers and the Humanities",2003,"No","cluster analysis dialectometry finnish dialects idiolectal variation transitional dialects neighbours enemies competing variants causing differences transitional dialects aim study show clusteranalysis shed light complexvariation transitional dialect zone ineastern finland history thisarea border sweden andrussia population oftwo kinds savo people karelians fact variationamong dialects spread extentof variation demonstrated previously idiolects area studied thelight ten phonological morphologicalfeatures material consisted recordingsof idiolects totalling hoursand representing parishes variation wasanalysed hierarchical cluster analysis analysis showed extent thevariation idiolects parishes italso demonstrated effects oldparishes borders settlements stillvisible dialects parish level data formed clear clusters correspondwith main dialects area itssurroundings idiolect level speakers surrounding areas formedfairly homogenous clusters idiolectsfrom savonlinna area spread acrossalmost clusters",0
NA,"instructions for authors",NA,"Computers and the Humanities",2003,"No"," instructions authors na",0
"language resources metadata open archives ","extending dublin core metadata to support the description and discovery of language resources","As language data and associatedtechnologies proliferate and as the languageresources community expands, it is becomingincreasingly difficult to locate and reuse existingresources. Are there any lexical resources forsuch-and-such a language? What tool workswith transcripts in this particular format?What is a good format to use for linguisticdata of this type? Questions like these dominate manymailing lists, since web search engines are anunreliable way to find language resources. Thispaper reports on a new digital infrastructurefor discovering language resources beingdeveloped by the Open Language Archives Community(OLAC). At the core of OLAC is its metadataformat, which is designed to facilitatedescription and discovery of all kinds oflanguage resources, including data, tools, oradvice. The paper describes OLAC metadata, itsrelationship to Dublin Core metadata, and itsdissemination using the metadata harvesting protocol of the Open Archives Initiative.","Computers and the Humanities",2003,"Yes","language resources metadata open archives extending dublin core metadata support description discovery language resources language data associatedtechnologies proliferate languageresources community expands becomingincreasingly difficult locate reuse existingresources lexical resources forsuch language tool workswith transcripts format good format linguisticdata type questions dominate manymailing lists web search engines anunreliable find language resources thispaper reports digital infrastructurefor discovering language resources beingdeveloped open language archives communityolac core olac metadataformat designed facilitatedescription discovery kinds oflanguage resources including data tools oradvice paper describes olac metadata itsrelationship dublin core metadata itsdissemination metadata harvesting protocol open archives initiative",1
"authorship lexical richness Lewis Carroll pastiche ","authorship attribution and pastiche","This paper considers the question of authorship attribution techniques whenfaced with a pastiche. We ask whether the techniques can distinguish the real thing from the fake, or can the author fool the computer? If the latter, is this because the pastiche is good, or because the technique is faulty? Using a number of mainly vocabulary-based techniques, Gilbert Adair's pastiche of Lewis Carroll, Alice Through the Needle's Eye, is compared with the original `Alice' books. Standard measures of lexical richness, Yule's K andOrlov's Z both distinguish Adair from Carroll, though Z also distinguishesthe two originals. A principal component analysis based on word frequenciesfinds that the main differences are not due to authorship. A discriminantanalysis based on word usage and lexical richness successfully distinguishes thepastiche from the originals. Weighted cusum tests were also unable to distinguish the two authors in a majority of cases. As a cross-validation, wemade similar comparisons with control texts: another children's story from thesame era, and other work by Carroll and Adair. The implications of thesefindings are discussed.","Computers and the Humanities",2003,"No","authorship lexical richness lewis carroll pastiche authorship attribution pastiche paper considers question authorship attribution techniques whenfaced pastiche techniques distinguish real thing fake author fool computer pastiche good technique faulty number vocabulary based techniques gilbert adair pastiche lewis carroll alice needle eye compared original alice books standard measures lexical richness yule andorlov distinguish adair carroll distinguishesthe originals principal component analysis based word frequenciesfinds main differences due authorship discriminantanalysis based word usage lexical richness successfully distinguishes thepastiche originals weighted cusum tests unable distinguish authors majority cases cross validation wemade similar comparisons control texts children story thesame era work carroll adair implications thesefindings discussed",0
"morphology analyzer parser part of speech proper nouns tokenizer ","extracting an arabic lexicon from arabic newspaper text","We describe how to build a largecomprehensive, integrated Arabic lexicon byautomatic parsing of newspaper text. We havebuilt a parser system to read Arabic newspaperarticles, isolate the tokens from them, findthe part of speech, and the features for eachtoken. To achieve this goal we designed a setof algorithms, we generated several sets ofrules, and we developed a set of techniques,and a set of components to carry out thesetechniques. As each sentence is processed, newwords and features are added to the lexicon, sothat it grows continuously as the system runs.To test the system we have used 100 articles(80,444 words) from the Al-Raya newspaper.The system consists of several modules: thetokenizer module to isolate the tokens, the type findersystem to find the part of speech of eachtoken, the proper noun phrase parser module tomark the proper nouns and to discover someinformation about them and the feature findermodule to find the features of the words.","Computers and the Humanities",2002,"Yes","morphology analyzer parser part speech proper nouns tokenizer extracting arabic lexicon arabic newspaper text describe build largecomprehensive integrated arabic lexicon byautomatic parsing newspaper text havebuilt parser system read arabic newspaperarticles isolate tokens findthe part speech features eachtoken achieve goal designed setof algorithms generated sets ofrules developed set techniques set components carry thesetechniques sentence processed newwords features added lexicon sothat grows continuously system runs test system articles words al raya newspaper system consists modules thetokenizer module isolate tokens type findersystem find part speech eachtoken proper noun phrase parser module tomark proper nouns discover someinformation feature findermodule find features words",1
"KeywordsGeneral Purpose Specific Factor Base Form Related Issue Computational Linguistic ","on the corpus size needed for compiling a comprehensive computational lexicon by automatic lexical acquisition","Comprehensive computational lexicons areessential to practical natural languageprocessing (NLP). To compile such computationallexicons by automatically acquiring lexicalinformation, however, we previously requiresufficiently large corpora. This study aims atpredicting the ideal size of suchautomatic-lexical-acquisition oriented corpora,focusing on six specific factors: (1) specificversus general purpose prediction, (2)variation among corpora, (3) base forms versus inflected forms, (4) open class items,(5) homographs, and (6) unknown words.Another important and related issue withregard to predictability has something to dowith data sparseness. Research using theTOTAL Corpus reveals serious datasparseness in this corpus. This, again, pointstowards the importance and necessity ofreducing data sparseness to a satisfactorylevel for the automatic lexical acquisition andreliable corpus predictions. The functions ofpredicting the number of tokens and lemmas in acorpus are based on the piecewisecurve-fitting algorithm. Unfortunately, thepredicted size of a corpus for automaticlexical acquisition is too astronomicalto compile it by using presently existingcompiling strategies. Therefore, we suggest apractical and efficient alternative method. Weare confident that this study will shed newlight on issues such as corpus predictability,compiling strategies and linguisticcomprehensiveness.","Computers and the Humanities",2002,"No"," general purpose specific factor base form related issue computational linguistic corpus size needed compiling comprehensive computational lexicon automatic lexical acquisition comprehensive computational lexicons areessential practical natural languageprocessing nlp compile computationallexicons automatically acquiring lexicalinformation previously requiresufficiently large corpora study aims atpredicting ideal size suchautomatic lexical acquisition oriented corporafocusing specific factors specificversus general purpose prediction variation corpora base forms versus inflected forms open class items homographs unknown words important related issue withregard predictability dowith data sparseness research thetotal corpus reveals datasparseness corpus pointstowards importance necessity ofreducing data sparseness satisfactorylevel automatic lexical acquisition andreliable corpus predictions functions ofpredicting number tokens lemmas acorpus based piecewisecurve fitting algorithm thepredicted size corpus automaticlexical acquisition astronomicalto compile presently existingcompiling strategies suggest apractical efficient alternative method weare confident study shed newlight issues corpus predictabilitycompiling strategies linguisticcomprehensiveness",0
"commercial text expert systems French theory industrial text literary criticism literary text SATOR structuralism WinBrill ","industrial text and french neo structuralism","Parallel to, and to some degree inreaction to French poststructuralisttheorization (as championed by Derrida,Foucault, and Lacan, among others) is a Frenchneo-structuralism built directly on theachievements of structuralism using electronicmeans. This paper examines some exemplaryapproaches to text analysis in thisneo-structuralist vein: SATOR's topoidictionary, the WinBrill POS tagger andFrançois Rastier's interpretativesemantics. I consider how a computer-assisted``Wissenschaft'' accumulation of expertisecomplements the neo-structuralist approach.Ultimately, electronic critical studies will bedefined by their strategic position at theintersection of the two chief technologiesshaping our society: the new informationprocessing technology of computers and therepresentational techniques that haveaccumulated for centuries in texts.Understanding how these two informationmanagement paradigms complement each other is akey issue for the humanities, for computerscience, and vital to industry, even beyond thenarrow realm of the language industries. Thedirection of critical studies, a small planetlong orbiting in only rarefied academiccircles, will be radically altered by the sheersize of the economic stakes implied by a newkind of text, the industrial text, thetechnological heart of an information society.","Computers and the Humanities",2002,"No","commercial text expert systems french theory industrial text literary criticism literary text sator structuralism winbrill industrial text french neo structuralism parallel degree inreaction french poststructuralisttheorization championed derridafoucault lacan frenchneo structuralism built directly theachievements structuralism electronicmeans paper examines exemplaryapproaches text analysis thisneo structuralist vein sator topoidictionary winbrill pos tagger andfran ois rastier interpretativesemantics computer assistedwissenschaft accumulation expertisecomplements neo structuralist approachultimately electronic critical studies bedefined strategic position theintersection chief technologiesshaping society informationprocessing technology computers therepresentational techniques haveaccumulated centuries textsunderstanding informationmanagement paradigms complement akey issue humanities computerscience vital industry thenarrow realm language industries thedirection critical studies small planetlong orbiting rarefied academiccircles radically altered sheersize economic stakes implied newkind text industrial text thetechnological heart information society",0
"KeywordsComputational Linguistic ","dialogue and interpretation at the interface of man and machine reflections on textuality and a proposal for an experiment in machine reading",NA,"Computers and the Humanities",2002,"No"," computational linguistic dialogue interpretation interface man machine reflections textuality proposal experiment machine reading na",0
"chronology hapax prediction vocabulary Yuless K ","stylistic constancy and change across literary corpora using measures of lexical richness to date works","The measure of the lexical richness of literary texts as a tool in thecomparative analysis of literary style has been hampered by the problem ofthe inequality of text lengths within and between literary corpora. Thispaper proposes an empirical method of description of lexical richness byaveraging measures on multiple chunks of text of a standard lengthwithin a literary work or corpus. A workss average vocabulary richness,average portion of hapax legomenaof the corpus from which it derives,and average repetition of frequently appearing vocabulary may thencharacterize that work relative to other works partitioned along withit. This method reveals the possibility of significant variance of thesemeasures of vocabulary among works of a single authorss corpus and warnsagainst the notion of some absolute authorial stylistic character. Weapply this method of vocabulary averaging to the corpora of threeplaywrights from classical antiquity whose works are chronologicallyrankable: Euripides, Aristophanes, and Terence. We look for trends in vocabulary richness over time, which we posit functions as anindicator of progressively changing authorial ability or inclination. This method then holds the potential of predicting datesfor undateable or tenuously dated works within a corpus of otherwisesecurely dated texts. From the results derived, a relatively late date forthe composition of the redrafted version ofAristophaness Clouds appearslikely; we predict an early composition date for the redraft of TerencessHecyra (and thus are inclined to think that the playwright did verylittle redrafting); and finally we findEuripidess Electra andSupplices exhibiting vocabulary characteristics of extremely latecomposition and we predict dates much later than those assigned based onmetrical considerations.","Computers and the Humanities",2002,"No","chronology hapax prediction vocabulary yuless stylistic constancy change literary corpora measures lexical richness date works measure lexical richness literary texts tool thecomparative analysis literary style hampered problem ofthe inequality text lengths literary corpora thispaper proposes empirical method description lexical richness byaveraging measures multiple chunks text standard lengthwithin literary work corpus workss average vocabulary richnessaverage portion hapax legomenaof corpus derives average repetition frequently appearing vocabulary thencharacterize work relative works partitioned withit method reveals possibility significant variance thesemeasures vocabulary works single authorss corpus warnsagainst notion absolute authorial stylistic character weapply method vocabulary averaging corpora threeplaywrights classical antiquity works chronologicallyrankable euripides aristophanes terence trends vocabulary richness time posit functions anindicator progressively changing authorial ability inclination method holds potential predicting datesfor undateable tenuously dated works corpus otherwisesecurely dated texts results derived late date forthe composition redrafted version ofaristophaness clouds appearslikely predict early composition date redraft terencesshecyra inclined playwright verylittle redrafting finally findeuripidess electra andsupplices exhibiting vocabulary characteristics extremely latecomposition predict dates assigned based onmetrical considerations",0
"agglutinative languages morphological disambiguation n-gram language models statistical natural language processing Turkish ","statistical morphological disambiguation for agglutinative languages","We present statistical models for morphological disambiguation in agglutinative languages, with a specific application to Turkish. Turkish presents an interesting problem for statistical models as the potential tag set size is very large because of the productive derivational morphology. We propose to handle this by breaking up the morhosyntactic tags into inflectional groups, each of which contains the inflectional features for each (intermediate) derived form. Our statistical models score the probability of each morhosyntactic tag by considering statistics over the individual inflectional groups and surface roots in trigram models. Among the four models that we have developed and tested, the simplest model ignoring the local morphotactics within words performs the best. Our best trigram model performs with 93.95% accuracy on our test data getting all the morhosyntactic and semantic features correct. If we are just interested in syntactically relevant features and ignore a very small set of semantic features, then the accuracy increases to 95.07%.","Computers and the Humanities",2002,"No","agglutinative languages morphological disambiguation gram language models statistical natural language processing turkish statistical morphological disambiguation agglutinative languages present statistical models morphological disambiguation agglutinative languages specific application turkish turkish presents interesting problem statistical models potential tag set size large productive derivational morphology propose handle breaking morhosyntactic tags inflectional groups inflectional features intermediate derived form statistical models score probability morhosyntactic tag statistics individual inflectional groups surface roots trigram models models developed tested simplest model ignoring local morphotactics words performs trigram model performs accuracy test data morhosyntactic semantic features correct interested syntactically relevant features ignore small set semantic features accuracy increases ",0
"GATE infrastructure language engineering software architecture ","gate a general architecture for text engineering","This paper presents the design, implementation and evaluation of GATE, a General Architecture for Text Engineering.GATE lies at the intersection of human language computation and software engineering, and constitutes aninfrastructural system supporting research and development of languageprocessing software.","Computers and the Humanities",2002,"No","gate infrastructure language engineering software architecture gate general architecture text engineering paper presents design implementation evaluation gate general architecture text engineeringgate lies intersection human language computation software engineering constitutes aninfrastructural system supporting research development languageprocessing software",0
NA,"contents of volume 36",NA,"Computers and the Humanities",2002,"No"," contents volume na",0
"electronic texts literary criticism reception theory versioning ","computer mediated texts and textuality theory and practice","The majority of humanities computingprojects within the discipline of literaturehave been conceived more as digital librariesthan monographs which utilise the medium as asite of interpretation. The impetus to conceiveelectronic research in this way comes from theunderlying philosophy of texts and textualityimplicit in SGML and its instantiation for thehumanities, the TEI, which was conceived as ``amarkup system intended for representing alreadyexisting literary texts''. This article exploresthe most common theories used to conceiveelectronic research in literature, such ashypertext theory, OCHO (Ordered Hierarchy ofContent Objects), and Jerome J. McGann's``noninformational'' forms of textuality. It alsoargues that as our understanding of electronictexts and textuality deepens, and as advancesin technology progresses, other theories, suchas Reception Theory and Versioning, may well beadapted to serve as a theoretical basis forconceiving research more akin to an electronicmonograph than a digital library.","Computers and the Humanities",2002,"No","electronic texts literary criticism reception theory versioning computer mediated texts textuality theory practice majority humanities computingprojects discipline literaturehave conceived digital librariesthan monographs utilise medium asite interpretation impetus conceiveelectronic research theunderlying philosophy texts textualityimplicit sgml instantiation thehumanities tei conceived amarkup system intended representing alreadyexisting literary texts article exploresthe common theories conceiveelectronic research literature ashypertext theory ocho ordered hierarchy ofcontent objects jerome mcgannnoninformational forms textuality alsoargues understanding electronictexts textuality deepens advancesin technology progresses theories suchas reception theory versioning beadapted serve theoretical basis forconceiving research akin electronicmonograph digital library",0
"CCG KCCG Korean Morpho-Syntactic modeling parsing ","korean combinatory categorial grammar and statistical parsing","Korean Combinatory Categorial Grammar (KCCG) is an extendedcombinatory categorial grammar formalism to capture thesyntax and interpretation of a relative freess word order, longdistance scrambling, and other specific characteristics of Korean.KCCG formalism can uniformly handle word order variations amongarguments and adjuncts within a clause, as well as in complexclauses and across clause boundaries, i.e. long distancescrambling. The approach we develop takes advantage of the ability of CCGfor type raising and composition along with the ability of variablecategories and unordered argument modeling for relatively freeword order treatment (Lee et al., 1994; Lee et al., 1997).We apply a probability model and heuristics using Koreancharacteristics to our KCCG parser.Results of the experiments on varioustext genre show that the KCCG parser performsat 87.67/87.03% constituent precision/recall.","Computers and the Humanities",2002,"No","ccg kccg korean morpho syntactic modeling parsing korean combinatory categorial grammar statistical parsing korean combinatory categorial grammar kccg extendedcombinatory categorial grammar formalism capture thesyntax interpretation relative freess word order longdistance scrambling specific characteristics koreankccg formalism uniformly handle word order variations amongarguments adjuncts clause complexclauses clause boundaries long distancescrambling approach develop takes advantage ability ccgfor type raising composition ability variablecategories unordered argument modeling freeword order treatment lee al lee al apply probability model heuristics koreancharacteristics kccg parserresults experiments varioustext genre show kccg parser performsat constituent precisionrecall",0
"betagraphy codicology database image watermarked paper ","profil an iconographic database for modern watermarked papers","The database Profil has been set up tooffer readers studying modern literarymanuscripts a reference tool to identifywatermarked papers. In the study of writers'drafts as in artists' sketches, the differentkinds of papers used provide valuableinformation on the genesis of a work of art andwatermarks, when they exist, are the bestvisible hint allowing us to identify paper. Amultimedia database, with digitized images moreprecise than usual traced design, seems to beappropriate to register, visualize, and comparemodern watermarked papers. Besides itsusefulness for specialists, such a databasebearing on modern manuscripts should also beconceived in a didactic perspective, as it isoriented towards literary scholars who are notparticularly familiar with the history of modern paper. In this paper we present the database Profilwhich includes a set of digitized images from acollection of betagraphies made by thereproduction service of the National FrenchLibrary. Then we explain problems of databasenormalization when human sciences areinvolved.","Computers and the Humanities",2002,"No","betagraphy codicology database image watermarked paper profil iconographic database modern watermarked papers database profil set tooffer readers studying modern literarymanuscripts reference tool identifywatermarked papers study writersdrafts artists sketches differentkinds papers provide valuableinformation genesis work art andwatermarks exist bestvisible hint allowing identify paper amultimedia database digitized images moreprecise usual traced design beappropriate register visualize comparemodern watermarked papers itsusefulness specialists databasebearing modern manuscripts beconceived didactic perspective isoriented literary scholars notparticularly familiar history modern paper paper present database profilwhich includes set digitized images acollection betagraphies made thereproduction service national frenchlibrary explain problems databasenormalization human sciences areinvolved",0
"browsing and navigation in large hypermedia image-based humanities computing TEI text/image coupling text encoding and rendering transcription/editing tools XML ","text image coupling for editing literary sources","Users need more sophisticatedtools to handle the growing numberof image-based documents availablein databases. In this paper, wepresent a system devoted to theediting and browsing of complexliterary hypermedia includingoriginal manuscript documents andother handwritten sources. Editingcapabilities allow the user totranscribe manuscript images in aninteractive way and to encode theresulting textual representationby means of a logical markuplanguage (based on the XML/TEIspecification). Bothrepresentations (image andstructured text) are tightlylinked to facilitate the readingand the interpretation ofdocuments. This text/imagecoupling scheme is an attempt tounify several layers ofinformation in order to providethe user with a global vision ofthe work. Our system also suppliestools capable of processing andrelating information stored bothin images and structured texts.Finally, application-specificvisualization techniques have beendeveloped in order to provideusers with a way to identifyrelationships between sourcedocuments and help them tonavigate.","Computers and the Humanities",2002,"No","browsing navigation large hypermedia image based humanities computing tei textimage coupling text encoding rendering transcriptionediting tools xml text image coupling editing literary sources users sophisticatedtools handle growing numberof image based documents availablein databases paper wepresent system devoted theediting browsing complexliterary hypermedia includingoriginal manuscript documents andother handwritten sources editingcapabilities user totranscribe manuscript images aninteractive encode theresulting textual representationby means logical markuplanguage based xmlteispecification bothrepresentations image andstructured text tightlylinked facilitate readingand interpretation ofdocuments textimagecoupling scheme attempt tounify layers ofinformation order providethe user global vision ofthe work system suppliestools capable processing andrelating information stored bothin images structured textsfinally application specificvisualization techniques beendeveloped order provideusers identifyrelationships sourcedocuments tonavigate",0
"KeywordsComputational Linguistic British Library ","the reappearances of st basil the great in british library ms cotton otho b x",NA,"Computers and the Humanities",2002,"No"," computational linguistic british library reappearances st basil great british library ms cotton otho na",0
"KeywordsComputational Linguistic Humanity Computing Select Resource ","select resources for image based humanities computing",NA,"Computers and the Humanities",2002,"No"," computational linguistic humanity computing select resource select resources image based humanities computing na",0
"KeywordsComputational Linguistic ","the place of images in a world of text",NA,"Computers and the Humanities",2002,"No"," computational linguistic place images world text na",0
"adaptive narrative electronic literature media studies mutability ","mutability medium and character","Looking specifically at the genre ofadaptive narrative, this article explores thefuture of literature created for and withcomputer technology, focusing primarily on thetrope of mutability as it is played out withnew media. Some of the questions askedare: What can the medium of a work ofliterature, that is its material aspect, tellus about the text? About character? What canit possibly matter if narrative is recounted onpapyrus, retold on parchment and rag, and thenremediated in pixels? Isn't it the messagecarried by the medium we are most concernedwith, stable or unstable throughout the processof inscription, reinscription, encoding anddecoding, translation and remediation? Thispaper speculates about possibilities ratherthan attempts to answer these questions, butthe structuring and mean-making componentsconsidered here stand as examples of some wemay want to think about when developing futuretheories about literature – and all types ofwriting – generated by and for electronicenvironments.","Computers and the Humanities",2002,"No","adaptive narrative electronic literature media studies mutability mutability medium character specifically genre ofadaptive narrative article explores thefuture literature created withcomputer technology focusing primarily thetrope mutability played withnew media questions askedare medium work ofliterature material aspect tellus text character canit possibly matter narrative recounted onpapyrus retold parchment rag thenremediated pixels messagecarried medium concernedwith stable unstable processof inscription reinscription encoding anddecoding translation remediation thispaper speculates possibilities ratherthan attempts answer questions butthe structuring making componentsconsidered stand examples wemay developing futuretheories literature types ofwriting generated electronicenvironments",0
"corpus search parsing syntactic annotation SGML computational linguistics psycholinguistics ","finding syntactic structure in unparsed corpora the gsearch corpus query system","The Gsearch system allows the selection of sentences by syntacticcriteria from text corpora, even when these corpora contain no priorsyntactic markup. This is achieved by means of a fast chart parser,which takes as input a grammar and a search expression specified by theuser. Gsearch features a modular architecture that can be extendedstraightforwardly to give access to new corpora. The Gsearcharchitecture also allows interfacing with external linguistic resources(such as taggers and lexical databases). Gsearch can be used withgraphical tools for visualizing the results of a query.","Computers and the Humanities",2001,"No","corpus search parsing syntactic annotation sgml computational linguistics psycholinguistics finding syntactic structure unparsed corpora gsearch corpus query system gsearch system selection sentences syntacticcriteria text corpora corpora priorsyntactic markup achieved means fast chart parser takes input grammar search expression theuser gsearch features modular architecture extendedstraightforwardly give access corpora gsearcharchitecture interfacing external linguistic resources taggers lexical databases gsearch withgraphical tools visualizing results query",0
NA,"index of key words of volume 35",NA,"Computers and the Humanities",2001,"No"," index key words volume na",0
"KeywordsSystem Performance Information Retrieval Major Topic Technical Document Computational Linguistic ","japaneseenglish cross language information retrieval exploration of query translation and transliteration","Cross-language information retrieval (CLIR), where queriesand documents are in different languages, has of late become one ofthe major topics within the information retrieval community. Thispaper proposes a Japanese/English CLIR system, where we combine aquery translation and retrieval modules. We currently target theretrieval of technical documents, and therefore the performance of oursystem is highly dependent on the quality of the translation oftechnical terms. However, the technical term translation is stillproblematic in that technical terms are often compound words, and thusnew terms are progressively created by combining existing basewords. In addition, Japanese often represents loanwords based on itsspecial phonogram. Consequently, existing dictionaries find itdifficult to achieve sufficient coverage. To counter the firstproblem, we produce a Japanese/English dictionary for base words, andtranslate compound words on a word-by-word basis. We also use aprobabilistic method to resolve translation ambiguity. For the secondproblem, we use a transliteration method, which corresponds wordsunlisted in the base word dictionary to their phonetic equivalents inthe target language. We evaluate our system using a test collectionfor CLIR, and show that both the compound word translation andtransliteration methods improve the system performance.","Computers and the Humanities",2001,"No"," system performance information retrieval major topic technical document computational linguistic japaneseenglish cross language information retrieval exploration query translation transliteration cross language information retrieval clir queriesand documents languages late ofthe major topics information retrieval community thispaper proposes japaneseenglish clir system combine aquery translation retrieval modules target theretrieval technical documents performance oursystem highly dependent quality translation oftechnical terms technical term translation stillproblematic technical terms compound words thusnew terms progressively created combining existing basewords addition japanese represents loanwords based itsspecial phonogram existing dictionaries find itdifficult achieve sufficient coverage counter firstproblem produce japaneseenglish dictionary base words andtranslate compound words word word basis aprobabilistic method resolve translation ambiguity secondproblem transliteration method corresponds wordsunlisted base word dictionary phonetic equivalents inthe target language evaluate system test collectionfor clir show compound word translation andtransliteration methods improve system performance",0
"KeywordsComputational Linguistic Important Approach Common Word Newspaper Corpus Authorship Attribution ","computer based authorship attribution without lexical measures","The most important approaches to computer-assistedauthorship attribution are exclusively based onlexical measures that either represent the vocabularyrichness of the author or simply comprise frequenciesof occurrence of common words. In this paper wepresent a fully-automated approach to theidentification of the authorship of unrestricted textthat excludes any lexical measure. Instead we adapt aset of style markers to the analysis of the textperformed by an already existing natural languageprocessing tool using three stylometric levels, i.e.,token-level, phrase-level, and analysis-levelmeasures. The latter represent the way in which thetext has been analyzed. The presented experiments ona Modern Greek newspaper corpus show that the proposedset of style markers is able to distinguish reliablythe authors of a randomly-chosen group and performsbetter than a lexically-based approach. However, thecombination of these two approaches provides the mostaccurate solution (i.e., 87% accuracy). Moreover, wedescribe experiments on various sizes of the trainingdata as well as tests dealing with the significance ofthe proposed set of style markers.","Computers and the Humanities",2001,"No"," computational linguistic important approach common word newspaper corpus authorship attribution computer based authorship attribution lexical measures important approaches computer assistedauthorship attribution exclusively based onlexical measures represent vocabularyrichness author simply comprise frequenciesof occurrence common words paper wepresent fully automated approach theidentification authorship unrestricted textthat excludes lexical measure adapt aset style markers analysis textperformed existing natural languageprocessing tool stylometric levels token level phrase level analysis levelmeasures represent thetext analyzed presented experiments ona modern greek newspaper corpus show proposedset style markers distinguish reliablythe authors randomly chosen group performsbetter lexically based approach thecombination approaches mostaccurate solution accuracy wedescribe experiments sizes trainingdata tests dealing significance ofthe proposed set style markers",0
"KeywordsPattern Recognition Relative Weight Successful Implementation Computational Linguistic Error Criterion ","perceptual issues in music pattern recognition complexity of rhythm and key finding","We consider several perceptual issues in the context of machine recognition ofmusic patterns. It is argued that a successful implementation of a musicrecognition system must incorporate perceptual information and error criteria.We discuss several measures of rhythm complexity which are used fordetermining relative weights of pitch and rhythm errors. Then, a new methodfor determining a localized tonal context is proposed. This method is based onempirically derived key distances. The generated key assignments are then usedto construct the perceptual pitch error criterion which is based on noterelatedness ratings obtained from experiments with human listeners.","Computers and the Humanities",2001,"No"," pattern recognition relative weight successful implementation computational linguistic error criterion perceptual issues music pattern recognition complexity rhythm key finding perceptual issues context machine recognition ofmusic patterns argued successful implementation musicrecognition system incorporate perceptual information error criteria discuss measures rhythm complexity fordetermining relative weights pitch rhythm errors methodfor determining localized tonal context proposed method based onempirically derived key distances generated key assignments usedto construct perceptual pitch error criterion based noterelatedness ratings obtained experiments human listeners",0
"α-cover collocations convergence correlation interrupted bigram randomness ","automatic extraction of collocations from korean text","In this paper, we propose a statistical method to automaticallyextract collocations from Korean POS-tagged corpus. Since a large portion of language is represented by collocation patterns, the collocational knowledge provides a valuable resource for NLP applications. One difficulty of collocation extraction is that Korean has a partially free word order, which also appears in collocations. In this work, we exploit four statistics, ‘frequency’,‘randomness’, ‘convergence’, and ‘correlation' in order to take into account the flexible word order of Korean collocations. We separate meaningful bigrams using an evaluation function based on the four statistics and extend the bigrams to n-gram collocations using a fuzzy relation. Experiments show that this method works well for Korean collocations.","Computers and the Humanities",2001,"No"," cover collocations convergence correlation interrupted bigram randomness automatic extraction collocations korean text paper propose statistical method automaticallyextract collocations korean pos tagged corpus large portion language represented collocation patterns collocational knowledge valuable resource nlp applications difficulty collocation extraction korean partially free word order appears collocations work exploit statistics frequency randomness convergence correlation order account flexible word order korean collocations separate meaningful bigrams evaluation function based statistics extend bigrams gram collocations fuzzy relation experiments show method works korean collocations",0
"database data mining data warehouse Defters historical analysis serial documents ","data mining and serial documents","This paper is concerned with the investigation of the relevance and suitability of the data mining approach to serial documents. Conceptually the paper is divided into three parts. The first part presents the salient features of data mining and its symbiotic relationship to data warehousing. In the second part of the paper, historical serial documents are introduced, and the Ottoman Tax Registers (Defters) are taken as a case study. Their conformance to the data mining approach is established in terms of structure, analysis and results. A high-level conceptual model for the Defters is also presented. The final part concludes with a brief consideration of the implication of data mining for historical research.","Computers and the Humanities",2001,"No","database data mining data warehouse defters historical analysis serial documents data mining serial documents paper concerned investigation relevance suitability data mining approach serial documents conceptually paper divided parts part presents salient features data mining symbiotic relationship data warehousing part paper historical serial documents introduced ottoman tax registers defters case study conformance data mining approach established terms structure analysis results high level conceptual model defters presented final part concludes consideration implication data mining historical research",0
"KeywordsDetailed Analysis Publication Date Computational Linguistic Quantitative Inquiry Newspaper Text ","computing historical consciousness a quantitative inquiry into the presence of the past in newspaper texts","In this paper, some electronically gathered data arepresented and analyzed about the presence of the pastin newspaper texts. In ten large text corpora of sixdifferent languages, all dates in the form of yearsbetween 1930 and 1990 were counted. For six of thesecorpora this was done for all the years between 1200and 1993. Depicting these frequencies on the timeline,we find an underlying regularly declining curve,deviations at regular places and culturally determinedpeaks at irregular points. These three phenomena areanalyzed.","Computers and the Humanities",2001,"No"," detailed analysis publication date computational linguistic quantitative inquiry newspaper text computing historical consciousness quantitative inquiry presence past newspaper texts paper electronically gathered data arepresented analyzed presence pastin newspaper texts ten large text corpora sixdifferent languages dates form yearsbetween counted thesecorpora years and depicting frequencies timeline find underlying regularly declining curvedeviations regular places culturally determinedpeaks irregular points phenomena areanalyzed",0
"corpus contemporary de electronic texts Golden-Age language que Spanish word frequency y ","spanish word frequency a historical surprise","This article compares the word frequencies of the few most commonwords in Spanish as revealed by a modern corpus of over fivethousand words with a corpus of Golden-Age Spanish texts of overa million words, and finds that although de is by far themost common word in contemporary Spanish, in the 16thand 17th Centuries it was considerably less frequent, and in many texts was less frequent than y, or quefor which shared very similar frequency figures. It is arguedthat this significant change in the Spanish language comes aboutin the 20th Century.","Computers and the Humanities",2001,"No","corpus contemporary de electronic texts golden age language spanish word frequency spanish word frequency historical surprise article compares word frequencies commonwords spanish revealed modern corpus fivethousand words corpus golden age spanish texts overa million words finds de themost common word contemporary spanish thand th centuries considerably frequent texts frequent quefor shared similar frequency figures arguedthat significant change spanish language aboutin th century",0
"browsing support cross-language information retrieval partial translation term list ","a method for supporting document selection in cross language information retrieval and its evaluation","It is important to give useful clues for selecting desiredcontent from a number of retrieval results obtained (usually) from avague search request. Compared with monolingual retrieval, such asupport framework is inevitable and much more significant for filteringgiven translingual retrieval results. This paper describes an attempt toprovide appropriate translation of major keywords in each document in across-language information retrieval (CLIR) result, as a browsingsupport for users. Our idea of determining appropriate translation ofmajor keywords is based on word co-occurrence distribution in thetranslation target language, considering the actual situation of WWWcontent where it is difficult to obtain aligned parallel (multilingual)corpora. The proposed method provides higher quality of keywordtranslation to yield a more effective support in identifying the targetdocuments in the retrieval result. We report the advantage of thisbrowsing support technique through evaluation experiments includingcomparison with conditions of referring to a translated documentsummary, and discuss related issues to be examined towards moreeffective cross-language information extraction.","Computers and the Humanities",2001,"No","browsing support cross language information retrieval partial translation term list method supporting document selection cross language information retrieval evaluation important give clues selecting desiredcontent number retrieval results obtained avague search request compared monolingual retrieval asupport framework inevitable significant filteringgiven translingual retrieval results paper describes attempt toprovide translation major keywords document language information retrieval clir result browsingsupport users idea determining translation ofmajor keywords based word occurrence distribution thetranslation target language actual situation wwwcontent difficult obtain aligned parallel multilingualcorpora proposed method higher quality keywordtranslation yield effective support identifying targetdocuments retrieval result report advantage thisbrowsing support technique evaluation experiments includingcomparison conditions referring translated documentsummary discuss related issues examined moreeffective cross language information extraction",0
"authorship New York Tribune Stephen Crane stylometry ","stephen crane and the new york tribune a case study in traditional and non traditional authorship attribution","This paper describes how traditional andnon-traditional methods were used to identifyseventeen previously unknown articles that webelieve to be by Stephen Crane, published inthe New-York Tribune between 1889 and1892. The articles, printed without byline inwhat was at the time New York City's mostprestigious newspaper, report on activities ina string of summer resort towns on New Jersey'snorthern shore. Scholars had previouslyidentified fourteen shore reports as Crane's;these possible attributions more than doublethat corpus. The seventeen articles confirmhow remarkably early Stephen Crane set hisdistinctive writing style and artistic agenda. In addition, the sheer quantity of the articlesfrom the summer of 1892 reveals how vigorouslythe twenty-year-old Crane sought to establishhimself in the role of professional writer. Finally, our discovery of an article about theNew Jersey National Guard's summer encampmentreveals another way in which Crane immersedhimself in nineteenth-century military cultureand help to explain how a young man who hadnever seen a battle could write so convincinglyof war in his soon-to-come masterpiece,The Red Badge of Courage. We argue that thejoint interdisciplinary approach employed inthis paper should be the way in whichattributional research is conducted.","Computers and the Humanities",2001,"No","authorship york tribune stephen crane stylometry stephen crane york tribune case study traditional traditional authorship attribution paper describes traditional andnon traditional methods identifyseventeen previously unknown articles webelieve stephen crane published inthe york tribune and articles printed byline inwhat time york city mostprestigious newspaper report activities ina string summer resort towns jerseysnorthern shore scholars previouslyidentified fourteen shore reports crane attributions doublethat corpus seventeen articles confirmhow remarkably early stephen crane set hisdistinctive writing style artistic agenda addition sheer quantity articlesfrom summer reveals vigorouslythe twenty year crane sought establishhimself role professional writer finally discovery article thenew jersey national guard summer encampmentreveals crane immersedhimself nineteenth century military cultureand explain young man hadnever battle write convincinglyof war masterpiece red badge courage argue thejoint interdisciplinary approach employed inthis paper whichattributional research conducted",0
NA,"contents of volume 35",NA,"Computers and the Humanities",2001,"No"," contents volume na",0
NA,"the ach page",NA,"Computers and the Humanities",2001,"No"," ach page na",0
"KeywordsGood Predictor Economic Factor Computational Linguistic Important Descriptor Emotional Variable ","the times and the man as predictors of emotion and style in the inaugural addresses of us presidents","Intercorrelations among stylistic and emotional variables and constructvalidity deduced from relationships to other ratings of U.S. presidentssuggest that power language (language that is linguistically simple,emotionally evocative, highly imaged, and rich in references to Americanvalues) is an important descriptor of inaugural addresses. Attempts topredict the use of power language in inaugural addresses from variablesrepresenting the times (year, media, economic factors) and the man(presidential personality) lead to the conclusion that time-basedfactors are the best predictors of the use of such language (81%prediction of variance in the criterion) while presidential personalityadds at most a small amount of prediction to the model. Changes in powerlanguage are discussed as the outcome of a tendency to opt for breadthof communication over depth.","Computers and the Humanities",2001,"No"," good predictor economic factor computational linguistic important descriptor emotional variable times man predictors emotion style inaugural addresses presidents intercorrelations stylistic emotional variables constructvalidity deduced relationships ratings presidentssuggest power language language linguistically simpleemotionally evocative highly imaged rich references americanvalues important descriptor inaugural addresses attempts topredict power language inaugural addresses variablesrepresenting times year media economic factors manpresidential personality lead conclusion time basedfactors predictors language prediction variance criterion presidential personalityadds small amount prediction model powerlanguage discussed outcome tendency opt breadthof communication depth",0
"optical music recognition musical data acquisition document image analysis pattern recognition ","the challenge of optical music recognition","This article describes the challenges posed by optical musicrecognition – a topic in computer science that aims to convert scannedpages of music into an on-line format. First, the problem is described;then a generalised framework for software is presented that emphasises keystages that must be solved: staff line identification, musical objectlocation, musical feature classification, and musical semantics. Next,significant research projects in the area are reviewed, showing how eachfits the generalised framework. The article concludes by discussingperhaps the most open question in the field: how to compare the accuracy and success of rival systems, highlighting certain steps thathelp ease the task.","Computers and the Humanities",2001,"No","optical music recognition musical data acquisition document image analysis pattern recognition challenge optical music recognition article describes challenges posed optical musicrecognition topic computer science aims convert scannedpages music line format problem generalised framework software presented emphasises keystages solved staff line identification musical objectlocation musical feature classification musical semantics significant research projects area reviewed showing eachfits generalised framework article concludes discussingperhaps open question field compare accuracy success rival systems highlighting steps thathelp ease task",0
"cross-language information retrieval information access Japanese-English machine translation probabilistic retrieval ","a framework for cross language information access application to english and japanese","Internet search engines allow access to online information from all over the world. However, there is currently a general assumption that users are fluent in the languages of all documentsthat they might search for. This has for historical reasons usually been a choice between English and the locally supported language. Given the rapidly growing size of the Internet, it is likely that future users will need to access information in languages in which they are not fluent or have no knowledge of at all. This papershows how information retrieval and machine translation can becombined in a cross-language information access frameworkto help overcome the language barrier. We presentencouraging preliminary experimental results using English queries toretrieve documents from the standard Japanese language BMIR-J2retrieval test collection. We outline the scope and purpose ofcross-language information access and provide an example applicationto suggest that technology already exists to provide effective andpotentially useful applications.","Computers and the Humanities",2001,"No","cross language information retrieval information access japanese english machine translation probabilistic retrieval framework cross language information access application english japanese internet search engines access online information world general assumption users fluent languages documentsthat search historical reasons choice english locally supported language rapidly growing size internet future users access information languages fluent knowledge papershows information retrieval machine translation becombined cross language information access frameworkto overcome language barrier presentencouraging preliminary experimental results english queries toretrieve documents standard japanese language bmir jretrieval test collection outline scope purpose ofcross language information access provide applicationto suggest technology exists provide effective andpotentially applications",0
"KeywordsProcessing Technique Processing Algorithm Computational Linguistic Pattern Processing Processing Problem ","pattern processing in melodic sequences challenges caveats and prospects","In this paper a number of issues relating to theapplication of string processing techniques on musicalsequences are discussed. A brief survey of somemusical string processing algorithms is given and someissues of melodic representation, abstraction,segmentation and categorisation are presented. Thispaper is not intended to provide solutions tostring processing problems but rather tohighlight possible stumbling-block areas andraise awareness of primarily music‐elatedparticularities that can cause problems in matchingapplications.","Computers and the Humanities",2001,"No"," processing technique processing algorithm computational linguistic pattern processing processing problem pattern processing melodic sequences challenges caveats prospects paper number issues relating theapplication string processing techniques musicalsequences discussed survey somemusical string processing algorithms someissues melodic representation abstractionsegmentation categorisation presented thispaper intended provide solutions tostring processing problems tohighlight stumbling block areas andraise awareness primarily music elatedparticularities problems matchingapplications",0
"ambiguity Arabic definite clause grammar heuristics parser single-parse syntax analysis ","identifying syntactic ambiguities in single parse arabic sentence","The aim of this paper is to describe a technique for identifying the sourcesof several types of syntactic ambiguity in Arabic Sentences with a singleparse only. Normally, any sentence with two or more structuralrepresentations is said to be syntactically ambiguous. However, Arabicsentences with only one structural representation may be ambiguous. Ourtechnique for identifying Syntactic Ambiguity in Single-Parse ArabicSentences (SASPAS) analyzes each sentence and verifies the conditionsthat govern the existence of certain types of syntactic ambiguities in Arabicsentences. SASPAS is integrated with the syntactic parser, which is basedon Definite Clause Grammar (DCG) formalism. The system accepts Arabicsentences in their original script.","Computers and the Humanities",2001,"No","ambiguity arabic definite clause grammar heuristics parser single parse syntax analysis identifying syntactic ambiguities single parse arabic sentence aim paper describe technique identifying sourcesof types syntactic ambiguity arabic sentences singleparse sentence structuralrepresentations syntactically ambiguous arabicsentences structural representation ambiguous ourtechnique identifying syntactic ambiguity single parse arabicsentences saspas analyzes sentence verifies conditionsthat govern existence types syntactic ambiguities arabicsentences saspas integrated syntactic parser basedon definite clause grammar dcg formalism system accepts arabicsentences original script",0
"LINGUISTICS, LANGUAGE & languages, DATABASES, INFORMATION storage & retrieval systems, ELECTRONIC data processing, COMPUTERS, CW, IR, lexical database, Reuters, SEMCOR, TC, WordNet, WSD","integrating linguistic resources in tc through wsd","Information access methods must be improved to overcome the information overload that most professionals face nowadays. Text classification tasks, like Text Categorization, help the users to access to the great amount of text they find in the Internet and their organizations. TC is the classification of documents into a predefined set of categories. Most approaches to automatic TC are based on the utilization of a training collection, which is a set of manually classified documents. Other linguistic resources that are emerging, like lexical databases, can also be used for classification tasks. This article describes an approach to TC based on the integration of a training collection (Reuters-21578) and a lexical database (WordNet 1.6) as knowledge sources. Lexical databases accumulate information on the lexical items of one or several languages. This information must be filtered in order to make an effective use of it in our model of TC. This filtering process is a Word Sense Disambig)","Computers and the Humanities",2001,"No","linguistics language languages databases information storage retrieval systems electronic data processing computers cw ir lexical database reuters semcor tc wordnet wsd integrating linguistic resources tc wsd information access methods improved overcome information overload professionals face nowadays text classification tasks text categorization users access great amount text find internet organizations tc classification documents predefined set categories approaches automatic tc based utilization training collection set manually classified documents linguistic resources emerging lexical databases classification tasks article describes approach tc based integration training collection reuters lexical database wordnet knowledge sources lexical databases accumulate information lexical items languages information filtered order make effective model tc filtering process word sense disambig",0
"generalised dictionary heterogeneous dictionary databases query language TEI tag definitions ","an architecture and query language for a federation ofheterogeneous dictionary databases","An architecture for federating heterogeneousdictionary databases is described. It proposes acommon description language and query language toprovide for the exchange of information betweendatabases with different organizations, on differentplatforms and in different DBMSs. The common querylanguage has an SQL like structure. The first versionof the description language follows the TEI standardtag definitions for dictionaries with the expectationthat the description language will be expanded in thefuture. A practical implementation of the proposalsusing WWW technology for two multi-lingualdictionaries is described.","Computers and the Humanities",2000,"No","generalised dictionary heterogeneous dictionary databases query language tei tag definitions architecture query language federation ofheterogeneous dictionary databases architecture federating heterogeneousdictionary databases proposes acommon description language query language toprovide exchange information betweendatabases organizations differentplatforms dbmss common querylanguage sql structure versionof description language tei standardtag definitions dictionaries expectationthat description language expanded thefuture practical implementation proposalsusing www technology multi lingualdictionaries ",0
"evaluation SENSEVAL word sense disambiguation ","framework and results for english senseval","Senseval was the first open, community-based evaluation exercisefor Word Sense Disambiguation programs. It adopted the quantitativeapproach to evaluation developed in MUC and other ARPA evaluationexercises. It took place in 1998. In this paper we describe thestructure, organisation and results of the SENSEVAL exercise forEnglish. We present and defend various design choices for theexercise, describe the data and gold-standard preparation, considerissues of scoring strategies and baselines, and present the resultsfor the 18 participating systems. The exercise identifies thestate-of-the-art for fine-grained word sense disambiguation, wheretraining data is available, as 74–78% correct, with a number ofalgorithms approaching this level of performance. For systems thatdid not assume the availability of training data, performance wasmarkedly lower and also more variable. Human inter-tagger agreementwas high, with the gold standard taggings being around 95%replicable.","Computers and the Humanities",2000,"No","evaluation senseval word sense disambiguation framework results english senseval senseval open community based evaluation exercisefor word sense disambiguation programs adopted quantitativeapproach evaluation developed muc arpa evaluationexercises place paper describe thestructure organisation results senseval exercise forenglish present defend design choices theexercise describe data gold standard preparation considerissues scoring strategies baselines present resultsfor participating systems exercise identifies thestate art fine grained word sense disambiguation wheretraining data correct number ofalgorithms approaching level performance systems thatdid assume availability training data performance wasmarkedly lower variable human inter tagger agreementwas high gold standard taggings replicable",0
"parallel corpora sense disambiguation translation ","cross lingual sense determination can it work","This article reports the results of apreliminary analysis of translation equivalents infour languages from different language families,extracted from an on-line parallel corpus of GeorgeOrwell's Nineteen Eighty-Four. The goal ofthe study is to determine the degree to whichtranslation equivalents for different meanings of apolysemous word in English are lexicalized differentlyacross a variety of languages, and to determinewhether this information can be used to structure orcreate a set of sense distinctions useful in naturallanguage processing applications. A coherenceindex is computed that measures the tendency fordifferent senses of the same English word to belexicalized differently, and from this data aclustering algorithm is used to create sensehierarchies.","Computers and the Humanities",2000,"No","parallel corpora sense disambiguation translation cross lingual sense determination work article reports results apreliminary analysis translation equivalents infour languages language familiesextracted line parallel corpus georgeorwell nineteen eighty goal ofthe study determine degree whichtranslation equivalents meanings apolysemous word english lexicalized differentlyacross variety languages determinewhether information structure orcreate set sense distinctions naturallanguage processing applications coherenceindex computed measures tendency fordifferent senses english word belexicalized differently data aclustering algorithm create sensehierarchies",0
"selectional preferences ","word sense disambiguation using automatically acquired verbal preferences","The selectional preferences of verbal predicates are an importantcomponent of a computational lexicon. They have frequently been citedas being useful for WSD, alongside other sources ofknowledge. We evaluate automatically acquired selectional preferenceson the level playing field provided by SENSEVAL to examine towhat extent they help in WSD.","Computers and the Humanities",2000,"No","selectional preferences word sense disambiguation automatically acquired verbal preferences selectional preferences verbal predicates importantcomponent computational lexicon frequently citedas wsd alongside sources ofknowledge evaluate automatically acquired selectional preferenceson level playing field provided senseval examine towhat extent wsd",0
"KeywordsComputational Linguistic Word Sense ","ginger ii an example driven word sense disambiguator",NA,"Computers and the Humanities",2000,"No"," computational linguistic word sense ginger ii driven word sense disambiguator na",0
"evaluation ambiguity resolution WSD inter-annotator agreement ","tagger evaluation given hierarchical tag sets","We present methods for evaluating human and automatictaggers that extend current practice in three ways. First, we show howto evaluate taggers that assign multiple tags to each test instance,even if they do not assign probabilities. Second, we show how toaccommodate a common property of manually constructed ``gold standards''that are typically used for objective evaluation, namely that there isoften more than one correct answer. Third, we show how to measureperformance when the set of possible tags is tree-structured in an IS-Ahierarchy. To illustrate how our methods can be used to measureinter-annotator agreement, we show how to compute the kappa coefficientover hierarchical tag sets.","Computers and the Humanities",2000,"No","evaluation ambiguity resolution wsd inter annotator agreement tagger evaluation hierarchical tag sets present methods evaluating human automatictaggers extend current practice ways show howto evaluate taggers assign multiple tags test instance assign probabilities show toaccommodate common property manually constructed gold standards typically objective evaluation isoften correct answer show measureperformance set tags tree structured ahierarchy illustrate methods measureinter annotator agreement show compute kappa coefficientover hierarchical tag sets",0
"disambiguation Senseval Bayesian classifier ","a topicallocal classifier for word sense identification","TLC is a supervised training (S) system that uses a Bayesianstatistical model and features of a word's context to identifyword sense. We describe the classifier's operation and how itcan be configured to use only topical context cues, only localcues, or a combination of both. Our results on Senseval'sfinal run are presented along with a comparison to theperformance of the best S system and the average for S systems.We discuss ways to improve TLC by enriching its featureset and by substituting other decision procedures for the Bayesianmodel. Future development of supervised training classifiers willdepend on the availability of tagged training data. TLC canassist in the hand-tagging effort by helping human taggers locateinfrequent senses of polysemous words.","Computers and the Humanities",2000,"No","disambiguation senseval bayesian classifier topicallocal classifier word sense identification tlc supervised training system bayesianstatistical model features word context identifyword sense describe classifier operation itcan configured topical context cues localcues combination results sensevalsfinal run presented comparison theperformance system average systems discuss ways improve tlc enriching featureset substituting decision procedures bayesianmodel future development supervised training classifiers willdepend availability tagged training data tlc canassist hand tagging effort helping human taggers locateinfrequent senses polysemous words",0
"KeywordsComputational Linguistic Word Sense Word Sense Disambiguation Case Library ","word sense disambiguation with a similarity smoothed case library",NA,"Computers and the Humanities",2000,"No"," computational linguistic word sense word sense disambiguation case library word sense disambiguation similarity smoothed case library na",0
"word-sense disambiguation Senseval dictionary software analysis of parsing output ","senseval the cl research experience","The CL Research Senseval system wasthe highest performing system among the ``All-words''systems, with an overall fine-grained score of 61.6percent for precision and 60.5 percent for recall on98 percent of the 8,448 texts on the revisedsubmission (up by almost 6 and 9 percent from thefirst). The results were achieved with an almostcomplete reliance on syntactic behavior, using (1) arobust and fast ATN-style parser producing parse treeswith annotations on nodes, (2) DIMAP dictionarycreation and maintenance software (after conversion ofthe Hector dictionary files) to hold dictionaryentries, and (3) a strategy for analyzing the parsetrees in concert with the dictionary data. Furtherconsiderable improvements are possible in the parser,exploitation of the Hector data (and representation ofdictionary entries), and the analysis strategy, stillwith syntactic and collocational data. The Sensevaldata (the dictionary entries and the corpora) providean excellent testbed for understanding the sources offailures and for evaluating changes in the CL Researchsystem.","Computers and the Humanities",2000,"Yes","word sense disambiguation senseval dictionary software analysis parsing output senseval cl research experience cl research senseval system wasthe highest performing system wordssystems fine grained score percent precision percent recall on percent texts revisedsubmission percent thefirst results achieved almostcomplete reliance syntactic behavior arobust fast atn style parser producing parse treeswith annotations nodes dimap dictionarycreation maintenance software conversion ofthe hector dictionary files hold dictionaryentries strategy analyzing parsetrees concert dictionary data furtherconsiderable improvements parserexploitation hector data representation ofdictionary entries analysis strategy stillwith syntactic collocational data sensevaldata dictionary entries corpora providean excellent testbed understanding sources offailures evaluating cl researchsystem",1
"semantic classification trees SENSEVAL word sense disambiguation WSD evaluation ","using semantic classification trees for wsd","This paper describes the evaluation of a WSD method withinSENSEVAL. This method is based on Semantic Classification Trees (SCTs)and short context dependencies between nouns and verbs. The trainingprocedure creates a binary tree for each word to be disambiguated. SCTsare easy to implement and yield some promising results. The integrationof linguistic knowledge could lead to substantial improvement.","Computers and the Humanities",2000,"No","semantic classification trees senseval word sense disambiguation wsd evaluation semantic classification trees wsd paper describes evaluation wsd method withinsenseval method based semantic classification trees scts short context dependencies nouns verbs trainingprocedure creates binary tree word disambiguated sctsare easy implement yield promising results integrationof linguistic knowledge lead substantial improvement",0
"KeywordsComputational Linguistic Argument Structure Consistent Criterion Sense Distinction Syntactic Frame ","consistent criteria for sense distinctions","This paper specifically addresses the question of polysemy with respect toverbs, and whether or not the sense distinctions that are made in on-linelexical resources such as WordNet are appropriate for computational lexicons.The use of sets of related syntactic frames and verb classes are examined as ameans of simplifying the task of defining different senses, and the importanceof concrete criteria such as different predicate argument structures, semanticclass constraints and lexical co-occurrences is emphasized.","Computers and the Humanities",2000,"No"," computational linguistic argument structure consistent criterion sense distinction syntactic frame consistent criteria sense distinctions paper specifically addresses question polysemy respect toverbs sense distinctions made linelexical resources wordnet computational lexicons sets related syntactic frames verb classes examined ameans simplifying task defining senses importanceof concrete criteria predicate argument structures semanticclass constraints lexical occurrences emphasized",0
"KeywordsComputational Linguistic Word Meaning Meaning Exist ","do word meanings exist",NA,"Computers and the Humanities",2000,"No"," computational linguistic word meaning meaning exist word meanings exist na",0
NA,"editorial computers in humanities teaching and research",NA,"Computers and the Humanities",2000,"No"," editorial computers humanities teaching research na",0
"CMC conferencing discourse ethics Habermas pedagogy Rawls ","wag the dog online conferencing and teaching","Web-accessible conferencing softwareand ``conversational ethics'' drawn from Habermas andRawls have successfully brought together on-lineparticipants separated by geography and viewpoint, andoccasionally resulted in consensus regarding otherwisedivisive issues such as abortion. The author describessuccesses, limitations, and costs of incorporatingthese technologies and discourse ethics in a religiousstudies class. Results are striking, but thepedagogical benefits involve technical risks and highlabor and time costs. This experience, coupled withrecent research, suggests that electronic pedagogies,like other teaching strategies, work for some, but notall students: this argues that we take up electronicteaching as one approach among many.","Computers and the Humanities",2000,"No","cmc conferencing discourse ethics habermas pedagogy rawls wag dog online conferencing teaching web accessible conferencing softwareand conversational ethics drawn habermas andrawls successfully brought lineparticipants separated geography viewpoint andoccasionally resulted consensus otherwisedivisive issues abortion author describessuccesses limitations costs incorporatingthese technologies discourse ethics religiousstudies class results striking thepedagogical benefits involve technical risks highlabor time costs experience coupled withrecent research suggests electronic pedagogies teaching strategies work notall students argues electronicteaching approach ",0
"semantic tagging word sense disambiguation WSDS evaluation inter-annotator agreement Italian corpus annotation ","sensevalromanseval the framework for italian","In this paper we present some observations concerning an experiment of (manual/automatic) semantic tagging of a small Italian corpus performed within the framework of the SENSEVAL/ROMANSEVAL initiative. Themain goal of the initiative was to set up a framework for evaluation of Word Sense Disambiguation systems (WSDS) through the comparative analysis of their performance on the same type of data. In this experiment there are two aspects which are of relevance: first, the preparation of the reference annotated corpus, and, second, the evaluation of the systems against it. In both aspects we are mainly interested here in the analysis of the linguistic side which can lead to a better understanding of the problem of semantic annotation of a corpus, be itmanual or automatic annotation. In particular, we will investigate, firstly, the reasons for disagreement between human annotators, secondly, some linguistically relevant aspects of the performance of the Italian WSDS and, finally, the lessons learned from the present experiment.","Computers and the Humanities",2000,"No","semantic tagging word sense disambiguation wsds evaluation inter annotator agreement italian corpus annotation sensevalromanseval framework italian paper present observations experiment manualautomatic semantic tagging small italian corpus performed framework sensevalromanseval initiative themain goal initiative set framework evaluation word sense disambiguation systems wsds comparative analysis performance type data experiment aspects relevance preparation reference annotated corpus evaluation systems aspects interested analysis linguistic side lead understanding problem semantic annotation corpus itmanual automatic annotation investigate firstly reasons disagreement human annotators linguistically relevant aspects performance italian wsds finally lessons learned present experiment",0
"KeywordsComputational Linguistic ","framework and results for french",NA,"Computers and the Humanities",2000,"No"," computational linguistic framework results french na",0
"context/kwd> corpus evaluation lexicography part-of-speech tagging word sense disambiguation sense-tagging ","peeling an onion the lexicographers experience ofmanual sense tagging","SENSEVAL set itself the task of evaluating automaticword sense disambiguation programs (see Kilgarriff andRosenzweig, this volume, for an overview of theframework and results). In order to do this, it wasnecessary to provide a `gold standard' dataset of `correct' answers. This paper will describe thelexicographic part of the process involved in creatingthat dataset. The primary objective was for a group oflexicographers to manually examine keywords in a largenumber of corpus contexts, and assign to each contexta sense-tag for the keyword, taken from the Hectordictionary. Corpus contexts also had to be manuallypart-of-speech (POS) tagged. Various observationsmade and insights gained by the lexicographers duringthis process will be presented, including a critiqueof the resources and the methodology.","Computers and the Humanities",2000,"No","contextkwd corpus evaluation lexicography part speech tagging word sense disambiguation sense tagging peeling onion lexicographers experience ofmanual sense tagging senseval set task evaluating automaticword sense disambiguation programs kilgarriff androsenzweig volume overview theframework results order wasnecessary provide gold standard dataset correct answers paper describe thelexicographic part process involved creatingthat dataset primary objective group oflexicographers manually examine keywords largenumber corpus contexts assign contexta sense tag keyword hectordictionary corpus contexts manuallypart speech pos tagged observationsmade insights gained lexicographers duringthis process presented including critiqueof resources methodology",0
"word sense disambiguation evaluation SENSEVAL ","introduction to the special issue on senseval","Senseval was the first open, community-based evaluation exercise for WordSense Disambiguation programs. It took place in the summer of 1998,with tasks for English, French and Italian. There were participating systems from 23 researchgroups. This special issueis an account of the exercise. In addition to describing the contentsof the volume, this introduction considers how the exercise has shedlight on some general questions about wordsenses and evaluation.","Computers and the Humanities",2000,"No","word sense disambiguation evaluation senseval introduction special issue senseval senseval open community based evaluation exercise wordsense disambiguation programs place summer tasks english french italian participating systems researchgroups special issueis account exercise addition describing contentsof volume introduction considers exercise shedlight general questions wordsenses evaluation",0
"KeywordsDevelopment Time Information Source Computational Linguistic Close Match Fast Development ","memory based word sense disambiguation","We describe a memory-based classification architecture for word sense disambiguation and its application to the SENSEVAL evaluationtask. For each ambiguous word, a semantic word expert isautomatically trained using a memory-based approach. In each expert,selecting the correct sense of a word in a new context is achieved byfinding the closest match to stored examples of this task. Advantagesof the approach include (i) fast development time for word experts,(ii) easy and elegant automatic integration of information sources,(iii) use of all available data for training the experts, and (iv)relatively high accuracy with minimal linguistic engineering.","Computers and the Humanities",2000,"No"," development time information source computational linguistic close match fast development memory based word sense disambiguation describe memory based classification architecture word sense disambiguation application senseval evaluationtask ambiguous word semantic word expert isautomatically trained memory based approach expertselecting correct sense word context achieved byfinding closest match stored examples task advantagesof approach include fast development time word expertsii easy elegant automatic integration information sourcesiii data training experts iv high accuracy minimal linguistic engineering",0
"combining knowledge sources word sense disambiguation ","combining supervised and unsupervised lexical knowledge methods for word sense disambiguation","This work combines a set of available techniques – whichcould be further extended – to perform noun sense disambiguation. We use several unsupervised techniques (Rigau et al., 1997) that draw knowledge from a variety of sources. In addition, we also apply a supervised technique in order to show that supervised and unsupervised methods can be combined to obtain better results. This paper tries to prove that using an appropriate method to combine those heuristics we can disambiguate words in free running text with reasonable precision.","Computers and the Humanities",2000,"No","combining knowledge sources word sense disambiguation combining supervised unsupervised lexical knowledge methods word sense disambiguation work combines set techniques whichcould extended perform noun sense disambiguation unsupervised techniques rigau al draw knowledge variety sources addition apply supervised technique order show supervised unsupervised methods combined obtain results paper prove method combine heuristics disambiguate words free running text reasonable precision",0
"analogy-based NLP semantic similarity word sense disambiguation ","romanseval results for italian by sense","The paper describes SENSE, a word sense disambiguation system thatmakes use of different types of cues to infer the most likelysense of a word given its context. Architecture and functioning ofthe system are briefly illustrated. Results are given for theROMANSEVAL Italian test corpus of verbs.","Computers and the Humanities",2000,"No","analogy based nlp semantic similarity word sense disambiguation romanseval results italian sense paper describes sense word sense disambiguation system thatmakes types cues infer likelysense word context architecture functioning ofthe system briefly illustrated results theromanseval italian test corpus verbs",0
"KeywordsComputational Linguistic ","lexicography and disambiguation the size of the problem",NA,"Computers and the Humanities",2000,"No"," computational linguistic lexicography disambiguation size problem na",0
"word sense disambiguation information filtering SENSEVAL ","word sense disambiguation by information filtering and extraction","We describe a simple approach to word sensedisambiguation using information filtering andextraction. The method fully exploits and extends theinformation available in the Hector dictionary. Thealgorithm proceeds by the application of severalfilters to prune the candidate set of word sensesreturning the most frequent if more than one remains.The experimental methodology and its implication arealso discussed.","Computers and the Humanities",2000,"No","word sense disambiguation information filtering senseval word sense disambiguation information filtering extraction describe simple approach word sensedisambiguation information filtering andextraction method fully exploits extends theinformation hector dictionary thealgorithm proceeds application severalfilters prune candidate set word sensesreturning frequent remains experimental methodology implication arealso discussed",0
"Senseval statistical WSD word sense disambiguation ","simple word sense discrimination","Wisdom is a system for performing word sense disambiguation (WSD)using a limited number of linguistic features and a simplesupervised learning algorithm. The most likely sense tag for aword is determined by calculating co-occurrence statistics forwords appearing within a small window. This paper gives abrief description of the components in the Wisdom system and thealgorithm used to predict the correct sense tag. Some results forWisdom from the Senseval competition are presented, and directionsfor future work are also explored.","Computers and the Humanities",2000,"No","senseval statistical wsd word sense disambiguation simple word sense discrimination wisdom system performing word sense disambiguation wsd limited number linguistic features simplesupervised learning algorithm sense tag aword determined calculating occurrence statistics forwords appearing small window paper abrief description components wisdom system thealgorithm predict correct sense tag results forwisdom senseval competition presented directionsfor future work explored",0
"KeywordsComputational Linguistic ","dictionary driven semantic look up",NA,"Computers and the Humanities",2000,"No"," computational linguistic dictionary driven semantic na",0
"KeywordsHybrid System Specific Task Similar System Computational Linguistic Knowledge Source ","large scale wsd using learning applied to senseval","A word sense disambiguation system which is going to be used aspart of a NLP system needs to be large scale, able to beoptimised towards a specific task and above all accurate. This paperdescribes the knowledge sources used in a disambiguation system able toachieve all three of these criteria. It is a hybrid system combining sub-symbolic, stochastic and rule-based learning. The paper reportsthe results achieved in Senseval and analyses them to show the system'sstrengths and weaknesses relative to other similar systems.","Computers and the Humanities",2000,"No"," hybrid system specific task similar system computational linguistic knowledge source large scale wsd learning applied senseval word sense disambiguation system aspart nlp system large scale beoptimised specific task accurate paperdescribes knowledge sources disambiguation system toachieve criteria hybrid system combining symbolic stochastic rule based learning paper reportsthe results achieved senseval analyses show systemsstrengths weaknesses relative similar systems",0
"word sense disambiguation decision lists supervised machine learning lexical ambiguity resolution SENSEVAL ","hierarchical decision lists for word sense disambiguation","This paper describes a supervised algorithm for word sensedisambiguation based on hierarchies of decision lists. This algorithmsupports a useful degree of conditional branching while minimizing thetraining data fragmentation typical of decision trees. Classificationsare based on a rich set of collocational, morphological and syntacticcontextual features, extracted automatically from training data andweighted sensitive to the nature of the feature and feature class. Thealgorithm is evaluated comprehensively in the SENSEVAL framework,achieving the top performance of all participating supervised systems onthe 36 test words where training data is available.","Computers and the Humanities",2000,"No","word sense disambiguation decision lists supervised machine learning lexical ambiguity resolution senseval hierarchical decision lists word sense disambiguation paper describes supervised algorithm word sensedisambiguation based hierarchies decision lists algorithmsupports degree conditional branching minimizing thetraining data fragmentation typical decision trees classificationsare based rich set collocational morphological syntacticcontextual features extracted automatically training data andweighted sensitive nature feature feature class thealgorithm evaluated comprehensively senseval frameworkachieving top performance participating supervised systems onthe test words training data ",0
"Classification Information Model classification information word sense disambiguation ","word sense disambiguation using the classification information model","A Classification Information Model is a pattern classification model.The model decides the proper class of an input instance by integrating individual decisions, each of which is made with each feature in the pattern.Each individual decision is weighted according to the distributional property of the feature deriving the decision. An individual decision and its weight are represented as classification information which is extracted from the training instances.In the word sense disambiguation based on the model, the proper sense of an input instance is determined by the weighted sum of whole individual decisions derived from the features contained in the instance.","Computers and the Humanities",2000,"No","classification information model classification information word sense disambiguation word sense disambiguation classification information model classification information model pattern classification model model decides proper class input instance integrating individual decisions made feature pattern individual decision weighted distributional property feature deriving decision individual decision weight represented classification information extracted training instances word sense disambiguation based model proper sense input instance determined weighted sum individual decisions derived features contained instance",0
"Word Sense Disambiguation lexical tuning part of speech tagging lexical rules vagueness ","is word sense disambiguation just one more nlp task","The paper examines the task of Word Sense Disambiguation (WSD) criticallyand compares it with Part of Speech (POS) tagging, arguing that the abilityof a writer to create new senses distinguishes the tasks and makes it moreproblematic to test WSD by the mark-up-and-model paradigm, because newsenses cannot be marked up against dictionaries. This serves to set WSDapart and puts limits on its effectiveness as an independent NLP task.Moreover, it is argued that current WSD methods based on very small wordsamples are also potentially misleading because they may or may not scaleup. Since all-word WSD methods are now available and are producing figurescomparable to the smaller scale tasks, it is argued that we shouldconcentrate on the former and find ways of bootstrapping test materialsfor such tests in the future.","Computers and the Humanities",2000,"No","word sense disambiguation lexical tuning part speech tagging lexical rules vagueness word sense disambiguation nlp task paper examines task word sense disambiguation wsd criticallyand compares part speech pos tagging arguing abilityof writer create senses distinguishes tasks makes moreproblematic test wsd mark model paradigm newsenses marked dictionaries serves set wsdapart puts limits effectiveness independent nlp task argued current wsd methods based small wordsamples potentially misleading scaleup word wsd methods producing figurescomparable smaller scale tasks argued shouldconcentrate find ways bootstrapping test materialsfor tests future",0
"KeywordsComputational Linguistic Teaching Literature ","technology in teaching literature and culturesome reflections",NA,"Computers and the Humanities",2000,"No"," computational linguistic teaching literature technology teaching literature culturesome reflections na",0
"KeywordsProbability Model Computational Linguistic Probabilistic Classifier Representational Power Decomposable Model ","selecting decomposable models for word sense disambiguation thegrling sdm system","This paper describes the grling-sdm system, which is asupervised probabilistic classifier that participated in the 1998SENSEVAL competition for word-sense disambiguation. This systemuses model search to select decomposable probability models describingthe dependencies among the feature variables.These types of models have been found to be advantageous in terms ofefficiency and representational power. Performance on the SENSEVALevaluation data is discussed.","Computers and the Humanities",2000,"No"," probability model computational linguistic probabilistic classifier representational power decomposable model selecting decomposable models word sense disambiguation thegrling sdm system paper describes grling sdm system asupervised probabilistic classifier participated senseval competition word sense disambiguation systemuses model search select decomposable probability models describingthe dependencies feature variables types models found advantageous terms ofefficiency representational power performance sensevalevaluation data discussed",0
"Bible computational linguistics parallel corpora Corpus Encoding Standard translation lexicons ","the bible as a parallel corpus annotating the book of 2000 tongues","We report on a project to annotate biblical texts in order to create an aligned multilingual Bible corpus for linguistic research, particularly computational linguistics, including automatically creating and evaluating translation lexicons and semantically tagged texts. The output of this project will enable researchers to take advantage of parallel translations across a wider number of languages than previously available, providing, with relatively little effort, a corpus that contains careful translations and reliable alignment at the near-sentence level. We discuss the nature of the text, our annotation process, preliminary and planned uses for the corpus, and relevant aspects of the Corpus Encoding Standard (CES) with respect to this corpus. We also present a quantitative comparison with dictionary and corpus resources for modern-day English, confirming the relevance of this corpus for research on present day language.","Computers and the Humanities",1999,"Yes","bible computational linguistics parallel corpora corpus encoding standard translation lexicons bible parallel corpus annotating book tongues report project annotate biblical texts order create aligned multilingual bible corpus linguistic research computational linguistics including automatically creating evaluating translation lexicons semantically tagged texts output project enable researchers advantage parallel translations wider number languages previously providing effort corpus careful translations reliable alignment sentence level discuss nature text annotation process preliminary planned corpus relevant aspects corpus encoding standard ces respect corpus present quantitative comparison dictionary corpus resources modern day english confirming relevance corpus research present day language",1
"linguistic resources TEI text encoding header ","silfide a system for open access and distributed delivery of tei encoded documents","This paper presents some aspects of the Silfide server, a system dedicated to the delivery of linguistic resources on the web. After presenting the main issues behind the design of such a system, we focus on the editorial choices related to the use of the Text Encoding Initiative to represent our textual documents. In particular, we focus on the accommodations we have had to carry with regards to the TEI header and address the trade-off between extensive enrichment and genericity of the primary data when one wants to precisely mark-up a given document content. As a whole, we show how essential the TEI has proven to be for a project such as ours both from a practical and conceptual point of view.","Computers and the Humanities",1999,"Yes","linguistic resources tei text encoding header silfide system open access distributed delivery tei encoded documents paper presents aspects silfide server system dedicated delivery linguistic resources web presenting main issues design system focus editorial choices related text encoding initiative represent textual documents focus accommodations carry tei header address trade extensive enrichment genericity primary data precisely mark document content show essential tei proven project practical conceptual point view",1
"KeywordsProper Noun Lexical Knowledge Generative Lexicon Corpus Processing Subcategorization Frame ","branimir boguraev and james pustejovsky corpus processing for lexical acquisition",NA,"Computers and the Humanities",1999,"No"," proper noun lexical knowledge generative lexicon corpus processing subcategorization frame branimir boguraev james pustejovsky corpus processing lexical acquisition na",0
"KeywordsObject Model Computational Linguistic Database Schema Architectural Processing Architectural Form ","using architectural forms to map tei data into an object oriented database","This paper develops a solution to the problem of importing existing TEI data into an existing object-oriented database schema without changing the TEI data or the database schema. The solution is based on architectural processing. Two meta-DTDs are used, one to define the architectural forms for the object model and another to map the existing SGML data onto those forms. A full example using a critical text in TEI markup is developed.","Computers and the Humanities",1999,"No"," object model computational linguistic database schema architectural processing architectural form architectural forms map tei data object oriented database paper develops solution problem importing existing tei data existing object oriented database schema changing tei data database schema solution based architectural processing meta dtds define architectural forms object model map existing sgml data forms full critical text tei markup developed",0
"clustering semantic acquisition noun phrase extraction ","elementary dependency trees for identifying corpus specific semantic classes","Elementary dependency relationships between words within parse trees produced by robust analyzers on a corpus help automate the discovery of semantic classes relevant for the underlying domain. We introduce two methods for extracting elementary syntactic dependencies from normalized parse trees. The groupings which are obtained help identify coarse-grain semantic categories and isolate lexical idiosyncrasies belonging to a specific sublanguage. A comparison shows a satisfactory overlapping with an existing nomenclature for medical language processing. This symbolic approach is efficient on medium size corpora which resist to statistical clustering methods but seems more appropriate for specialized texts.","Computers and the Humanities",1999,"No","clustering semantic acquisition noun phrase extraction elementary dependency trees identifying corpus specific semantic classes elementary dependency relationships words parse trees produced robust analyzers corpus automate discovery semantic classes relevant underlying domain introduce methods extracting elementary syntactic dependencies normalized parse trees groupings obtained identify coarse grain semantic categories isolate lexical idiosyncrasies belonging specific sublanguage comparison shows satisfactory overlapping existing nomenclature medical language processing symbolic approach efficient medium size corpora resist statistical clustering methods specialized texts",0
"AEDI AI-Strata indexing modeling scholarly use of audio-visual documents standardization ","managing full indexed audiovisual documents a new perspective for the humanities","The digitization of library documents and archives increasingly extends to audiovisual (AV) document repositories. As a consequence, new computer-aided techniques are being devised, providing opportunities for new uses of AV documents. As scholars work mainly by reading, annotating, reusing, and producing documents they are directly concerned by these changes. The first part of this article describes AV document use in the humanities, as well as the current and future influence computers might have on evolving practices. After establishing that “full-indexing” (indexing of the content for random access to any segment of an AV document) is a necessary condition if scholars are to develop new practices in using AV material, we will focus on the specific problems raised by AV indexing as opposed to text indexing, followed by a discussion of related AV indexing projects as well as standardization issues. The third part will propose a representation model for the description of AV material (AI-Strata) and an exchange format of AV annotations (AEDI), based on a free segmentation approach. An example of annotation is also provided. The last part is devoted to a discussion regarding potential long-term influences of digital AV indexing techniques on scholarly uses of AV documents.","Computers and the Humanities",1999,"No","aedi ai strata indexing modeling scholarly audio visual documents standardization managing full indexed audiovisual documents perspective humanities digitization library documents archives increasingly extends audiovisual av document repositories consequence computer aided techniques devised providing opportunities av documents scholars work reading annotating reusing producing documents directly concerned part article describes av document humanities current future influence computers evolving practices establishing full indexing indexing content random access segment av document condition scholars develop practices av material focus specific problems raised av indexing opposed text indexing discussion related av indexing projects standardization issues part propose representation model description av material ai strata exchange format av annotations aedi based free segmentation approach annotation provided part devoted discussion potential long term influences digital av indexing techniques scholarly av documents",0
"Ben Jonson idiolects Principal Components Analysis ","contrast and change in the idiolects of ben jonson characters","The paper presents the results of a series of Principal Components Analyses of the frequencies of very common words in the dialogue of characters in plays by Ben Jonson. The first Principal Component in the data, the most important axis of differentiation, proves in each case to be a spectrum from elaborate, authoritative pronouncements to a dialogue style of reaction and interchange. Reference to other quantitative studies, literary and otherwise, suggests that a version of this axis may often be among the most important in stylistic difference generally. In Jonson it has a chronological aspect -- there is a shift over his career from one end to the other -- and there is often significant change within the idiolects of his characters as well. Successive segments of Volpone and Mosca's parts (they are protagonist and antagonist of Volpone, perhaps Jonson's best-known comedy) change markedly along this axis, beginning far apart but coming by the end of the play to resemble each other very closely on this measure.","Computers and the Humanities",1999,"No","ben jonson idiolects principal components analysis contrast change idiolects ben jonson characters paper presents results series principal components analyses frequencies common words dialogue characters plays ben jonson principal component data important axis differentiation proves case spectrum elaborate authoritative pronouncements dialogue style reaction interchange reference quantitative studies literary suggests version axis important stylistic difference generally jonson chronological aspect shift career end significant change idiolects characters successive segments volpone mosca parts protagonist antagonist volpone jonson comedy change markedly axis beginning coming end play resemble closely measure",0
"Canada Canadian digitization microreproductions research ","attitudes of the canadian research community toward creating and accessing digitized facsimile collections of historical documents","A study commissioned by the Canadian Institute for Historical Microreproductions produced some interesting secondary findings about the attitudes of the Canadian research community towards digitized facsimile collections. In written responses to a questionnaire designed primarily to elicit advice about the subject content and focus of future projects, and in structured follow-up interviews, many respondents demonstrated a marked ambivalence towards the concept of digitized collections. Furthermore, if faced with a choice between fully searchable text and digitized facsimile images with traditional points of access (subject, author, title, etc.), there appears to be a preference for the latter means of access.","Computers and the Humanities",1999,"No","canada canadian digitization microreproductions research attitudes canadian research community creating accessing digitized facsimile collections historical documents study commissioned canadian institute historical microreproductions produced interesting secondary findings attitudes canadian research community digitized facsimile collections written responses questionnaire designed primarily elicit advice subject content focus future projects structured follow interviews respondents demonstrated marked ambivalence concept digitized collections faced choice fully searchable text digitized facsimile images traditional points access subject author title appears preference means access",0
"content-based retrieval image databases image indexing image retrieval ","access to pictorial material a review of current research and future prospects","Rapid expansion in the digitization of image and image collections has vastly increased the numbers of images available to scholars and researchers through electronic means. This research review will familiarize the reader with current research applicable to the development of image retrieval systems and provides additional material for exploring the topic further, both in print and online. The discussion will cover several broad areas, among them classification and indexing systems used for describing image collections and research initiatives into image access focusing on image attributes, users, queries, tasks, and cognitive aspects of searching. Prospects for the future of image access, including an outline of future research initiatives, are discussed. Further research in each of these areas will provide basic data which will inform and enrich image access system design and will hopefully provide a richer, more flexible, and satisfactory environment for searching for and discovering images. Harnessing the true power of the digital image environment will only be possible when image retrieval systems are coherently designed from principles derived from the fullest range of applicable disciplines, rather than from isolated or fragmented perspectives.","Computers and the Humanities",1999,"No","content based retrieval image databases image indexing image retrieval access pictorial material review current research future prospects rapid expansion digitization image image collections vastly increased numbers images scholars researchers electronic means research review familiarize reader current research applicable development image retrieval systems additional material exploring topic print online discussion cover broad areas classification indexing systems describing image collections research initiatives image access focusing image attributes users queries tasks cognitive aspects searching prospects future image access including outline future research initiatives discussed research areas provide basic data inform enrich image access system design provide richer flexible satisfactory environment searching discovering images harnessing true power digital image environment image retrieval systems coherently designed principles derived fullest range applicable disciplines isolated fragmented perspectives",0
"automatic document encoding corpus linguistics TEI World Wide Web ","taking snapshots of the web with a tei camera","Electronic texts are claimed to exhibit features distinct from their more tangible cousins. The Snapshot project aims to observe and capture language usage in an electronic medium by creating an open corpus of World Wide Web documents. These documents are re-encoded using the TEI guidelines to create a flexible, persistent and portable data repository. This report gives an overview of the decisions made with respect to the re-encoding of HTML documents, and with the structuring the overall corpus.","Computers and the Humanities",1999,"No","automatic document encoding corpus linguistics tei world wide web taking snapshots web tei camera electronic texts claimed exhibit features distinct tangible cousins snapshot project aims observe capture language usage electronic medium creating open corpus world wide web documents documents encoded tei guidelines create flexible persistent portable data repository report overview decisions made respect encoding html documents structuring corpus",0
"text encoding syntactic tagging syntactic tagset ","tei encoding and syntactic tagging of an old french text","This paper report on some of the concrete outcomes of a larger research project on the study of syntactic change. In this part of the project, we are collecting and encoding historical texts and tagging them for syntactic analysis. We have so far produced a TEI-conformant version of an Old French text, La Vie de Saint Louis written by Jehan de Joinville around 1305, and we are in the process of adding syntactic tags to this text. Those syntactic tags are derived from the Penn-Helsinki coding scheme, which had been devised for the syntactic encoding of Middle English texts, and have been translated into TEI.","Computers and the Humanities",1999,"No","text encoding syntactic tagging syntactic tagset tei encoding syntactic tagging french text paper report concrete outcomes larger research project study syntactic change part project collecting encoding historical texts tagging syntactic analysis produced tei conformant version french text la vie de saint louis written jehan de joinville process adding syntactic tags text syntactic tags derived penn helsinki coding scheme devised syntactic encoding middle english texts translated tei",0
"KeywordsLanguage Acquisition Selectional Constraint Minimalist Program Language Change Distributional Regularity ","michael r brent computational approaches to language acquisition",NA,"Computers and the Humanities",1999,"No"," language acquisition selectional constraint minimalist program language change distributional regularity michael brent computational approaches language acquisition na",0
"corpus processing electronic dictionaries finite state technology natural language processing ","text indexation with intex","INTEX is a linguistic development environment that includes large-coverage dictionaries and grammars, and parses texts of several million words in real time. INTEX has tools to create and maintain large-coverage lexical resources as well as morphological and syntactic grammars. Dictionaries and grammars are applied to texts in order to locate morphological, lexical and syntactic patterns, remove ambiguities, and tag simple and compound words. INTEX can build lemmatized concordances and indices of large texts with respect to all types of Finite State patterns. INTEX is used as a corpus processor, to analyze literary, journalistic and technical texts. I describe here the subset of tools used to perform advanced search requests on large texts.","Computers and the Humanities",1999,"Yes","corpus processing electronic dictionaries finite state technology natural language processing text indexation intex intex linguistic development environment includes large coverage dictionaries grammars parses texts million words real time intex tools create maintain large coverage lexical resources morphological syntactic grammars dictionaries grammars applied texts order locate morphological lexical syntactic patterns remove ambiguities tag simple compound words intex build lemmatized concordances indices large texts respect types finite state patterns intex corpus processor analyze literary journalistic technical texts describe subset tools perform advanced search requests large texts",1
"KeywordsRepresentation System Current System Knowledge Representation Computational Linguistic Description Logic ","using the right tools enhancing retrieval from marked up documents","We are experimenting with the representation of a DTD and associated documents (i.e., documents conformant to the DTD) in a knowledge representation (KR) system, in order to provide more sophisticated query and retrieval from TEI documents than current systems provide. We are using CLASSIC, a frame-based representation system developed at AT&T Bell Laboratories. Like many KR systems, CLASSIC enables the definition of structured concepts/frames, their organization into taxonomies, the creation and manipulation of individual instances of such concepts, and inference such as inheritance, relation transitivity, inverses, etc. In addition, CLASSIC provides for the key inferences of subsumption and classification. By representing a document as an individual instance of a hierarchy of concepts derived from the DTD, and by allowing the creation of additional user-defined concepts and relations, sophisticated query and retrieval operations can be performed. This paper describes CLASSIC and the formalism of description logic that underlies it, and demonstrates how it can be used for enhanced retrieval from richly encoded documents.","Computers and the Humanities",1999,"No"," representation system current system knowledge representation computational linguistic description logic tools enhancing retrieval marked documents experimenting representation dtd documents documents conformant dtd knowledge representation kr system order provide sophisticated query retrieval tei documents current systems provide classic frame based representation system developed bell laboratories kr systems classic enables definition structured conceptsframes organization taxonomies creation manipulation individual instances concepts inference inheritance relation transitivity inverses addition classic key inferences subsumption classification representing document individual instance hierarchy concepts derived dtd allowing creation additional user defined concepts relations sophisticated query retrieval operations performed paper describes classic formalism description logic underlies demonstrates enhanced retrieval richly encoded documents",0
"XML SGML TEI markup languages ","xml and the tei","Electronic texts are claimed to exhibit features distinct from their more tangible cousins. The Snapshot project aims to observe and capture language usage in an electronic medium by creating an open corpus of World Wide Web documents. These documents are re-encoded using the TEI guidelines to create a flexible, persistent and portable data repository. This report gives an overview of the decisions made with respect to the re-encoding of HTML documents, and with the structuring the overall corpus.","Computers and the Humanities",1999,"No","xml sgml tei markup languages xml tei electronic texts claimed exhibit features distinct tangible cousins snapshot project aims observe capture language usage electronic medium creating open corpus world wide web documents documents encoded tei guidelines create flexible persistent portable data repository report overview decisions made respect encoding html documents structuring corpus",0
"TEI10 SGML TEI markup conference research communities ","the text encoding initiative at 10 not just an interchange format anymore but a new research community","Mylonas and Renear introduce a volume of selected papers from The Text Encoding Initiative 10th Anniversary Conference, held at Brown University in November 1997. The Text Encoding Initiative (TEI), was launched in 1987 and sponsored by the Association for Computers and the Humanities, the Association for Literary and Linguistic Computing, and the Association for Computational Linguistics. It had as its original objective the development of an interchange language for textual data. This effort was completely successful and the TEI Guidelines are now widely accepted as the standard interchange format for textual data. Mylonas and Renear also note that the TEI has accomplished two other major achievements: it has produced a powerful new data description language (which is influencing the development of new WWW standards); and, most importantly, it has motivated the development of an entirely new research community, focused on understanding the role of text structure and markup in the use of emerging information technologies in culture, scholarship, and communication.","Computers and the Humanities",1999,"No","tei sgml tei markup conference research communities text encoding initiative interchange format anymore research community mylonas renear introduce volume selected papers text encoding initiative th anniversary conference held brown university november text encoding initiative tei launched sponsored association computers humanities association literary linguistic computing association computational linguistics original objective development interchange language textual data effort completely successful tei guidelines widely accepted standard interchange format textual data mylonas renear note tei accomplished major achievements produced powerful data description language influencing development www standards importantly motivated development research community focused understanding role text structure markup emerging information technologies culture scholarship communication",0
"lexicon semantic network Natural Language Processing ","a semantic network of english the mother of all wordnets","We give a brief outline of the design and contents of the English lexical database WordNet, which serves as a model for similarly conceived wordnets in several European languages. WordNet is a semantic network, in which the meanings of nouns, verbs, adjectives, and adverbs are represented in terms of their links to other (groups of) words via conceptual-semantic and lexical relations. Each part of speech is treated differently reflecting different semantic properties. We briefly discuss polysemy in WordNet, and focus on the case of meaning extensions in the verb lexicon. Finally, we outline the potential uses of WordNet not only for applications in natural language processing, but also for research in stylistic analyses in conjunction with a semantic concordance.","Computers and the Humanities",1998,"Yes","lexicon semantic network natural language processing semantic network english mother wordnets give outline design contents english lexical database wordnet serves model similarly conceived wordnets european languages wordnet semantic network meanings nouns verbs adjectives adverbs represented terms links groups words conceptual semantic lexical relations part speech treated differently reflecting semantic properties briefly discuss polysemy wordnet focus case meaning extensions verb lexicon finally outline potential wordnet applications natural language processing research stylistic analyses conjunction semantic concordance",1
"Base Concept ontology building Top Ontology ","the top down strategy for building eurowordnet vocabulary coverage base concepts and top ontology","This paper describes two fundamental aspects in the process of building of the EuroWordNet database. In EuroWordNet we have chosen for a flexible design in which local wordnets are built relatively independently as language-specific structures, which are linked to an Inter-Lingual-Index (ILI). To ensure compatibility between the wordnets, a core set of common concepts has been defined that has to be covered by every language. Furthermore, these concepts have been classified via the ILI in terms of a Top Ontology of 63 fundamental semantic distinctions used in various semantic theories and paradigms. This paper first discusses the process leading to the definition of the set of Base Concepts, and the structure and the rationale of the Top Ontology.","Computers and the Humanities",1998,"Yes","base concept ontology building top ontology top strategy building eurowordnet vocabulary coverage base concepts top ontology paper describes fundamental aspects process building eurowordnet database eurowordnet chosen flexible design local wordnets built independently language specific structures linked inter lingual index ili ensure compatibility wordnets core set common concepts defined covered language concepts classified ili terms top ontology fundamental semantic distinctions semantic theories paradigms paper discusses process leading definition set base concepts structure rationale top ontology",1
"multilingual lexical semantic database ","introduction to eurowordnet","This paper gives a global introduction to the aims and objectives of the EuroWordNet project, and it provides a general framework for the other papers in this volume. EuroWordNet is an EC project that develops a multilingual database with wordnets in several European languages, structured along the same lines as the Princeton WordNet. Each wordnet represents an autonomous structure of language-specific lexicalizations, which are interconnected via an Inter-Lingual-Index. The wordnets are built at different sites from existing resources, starting from a shared level of basic concepts and extended top-down. The results will be publicly available and will be tested in cross-language information retrieval applications.","Computers and the Humanities",1998,"Yes","multilingual lexical semantic database introduction eurowordnet paper global introduction aims objectives eurowordnet project general framework papers volume eurowordnet ec project develops multilingual database wordnets european languages structured lines princeton wordnet wordnet represents autonomous structure language specific lexicalizations interconnected inter lingual index wordnets built sites existing resources starting shared level basic concepts extended top results publicly tested cross language information retrieval applications",1
"equivalence relations lexical-semantic relations language-internal relations synset ","the linguistic design of the eurowordnet database","In this paper the linguistic design of the database under construction within the EuroWordNet project is described. This is mainly structured along the same lines as the Princeton WordNet, although some changes have been made to the WordNet overall design due to both theoretical and practical reasons. The most important reasons for such changes are the multilinguality of the EuroWordNet database and the fact that it is intended to be used in Language Engineering applications. Thus, i) some relations have been added to those identified in WordNet; ii) some labels have been identified which can be added to the relations in order to make their implications more explicit and precise; iii) some relations, already present in the WordNet design, have been modified in order to specify their role more clearly.","Computers and the Humanities",1998,"Yes","equivalence relations lexical semantic relations language internal relations synset linguistic design eurowordnet database paper linguistic design database construction eurowordnet project structured lines princeton wordnet made wordnet design due theoretical practical reasons important reasons multilinguality eurowordnet database fact intended language engineering applications relations added identified wordnet ii labels identified added relations order make implications explicit precise iii relations present wordnet design modified order role ",1
"overlapping relations and lexical gaps sense differentiation ","compatibility in interpretation of relations in eurowordnet","This paper describes how the Euro WordNet project established a maximum level of consensus in the interpretation of relations, without loosing the possibility of encoding language-specific lexicalizations. Problematic cases arise due to the fact that each site re-used different resources and because the core vocabulary of the wordnets show complex properties. Many of these cases are discussed with respect to language internal and equivalence relations. Possible solutions are given in the form of additional criteria.","Computers and the Humanities",1998,"Yes","overlapping relations lexical gaps sense differentiation compatibility interpretation relations eurowordnet paper describes euro wordnet project established maximum level consensus interpretation relations loosing possibility encoding language specific lexicalizations problematic cases arise due fact site resources core vocabulary wordnets show complex properties cases discussed respect language internal equivalence relations solutions form additional criteria",1
"aligning wordnets equivalence relations multilingual database ","cross linguistic alignment of wordnets with an inter lingual index","This paper discusses the design of the EuroWordNet database, in which semantic databases like WordNet1.5 for several languages are combined via a so-called inter-lingual-index. In this database, language-independent data is shared whilst language-specific properties are maintained. A special interface has been developed to compare the semantic configurations across languages and to track down differences.","Computers and the Humanities",1998,"Yes","aligning wordnets equivalence relations multilingual database cross linguistic alignment wordnets inter lingual index paper discusses design eurowordnet database semantic databases wordnet languages combined called inter lingual index database language independent data shared whilst language specific properties maintained special interface developed compare semantic configurations languages track differences",1
"cross-language text retrieval multilingual lexical resources large-scale ontologies ","applying eurowordnet to cross language text retrieval","We discuss ways in which EuroWordNet (EWN) can be used in multilingual information retrieval activities, focusing on two approaches to Cross-Language Text Retrieval that use the EWN database as a large-scale multilingual semantic resource. The first approach indexes documents and queries in terms of the EuroWordNet Inter-Lingual-Index, thus turning term weighting and query/document matching into language-independent tasks. The second describes how the information in the EWN database could be integrated with a corpus-based technique, thus allowing retrieval of domain-specific terms that may not be present in our multilingual database. Our objective is to show the potential of EuroWordNet as a promising alternative to existing approaches to Cross-Language Text Retrieval.","Computers and the Humanities",1998,"Yes","cross language text retrieval multilingual lexical resources large scale ontologies applying eurowordnet cross language text retrieval discuss ways eurowordnet ewn multilingual information retrieval activities focusing approaches cross language text retrieval ewn database large scale multilingual semantic resource approach indexes documents queries terms eurowordnet inter lingual index turning term weighting querydocument matching language independent tasks describes information ewn database integrated corpus based technique allowing retrieval domain specific terms present multilingual database objective show potential eurowordnet promising alternative existing approaches cross language text retrieval",1
"corpus annotation reusability ","the feasibility of incremental linguistic annotation","This paper examines the feasibility of incremental annotation, i.e. using existing annotation on a text as the basis for further annotation rather than starting the new annotation from scratch. It contains a theoretical component, describing basic methodology and potential obstacles, as well as a practical component, describing an experiment which tests the efficiency of incremental annotation. Apart from guidelines for the execution of such pilot experiments, the experiment demonstrates that incremental annotation is most effective when supported by thorough pre-planning and documentation. Unplanned, opportunistic use of existing annotation is much less effective in its reduction of annotation time and furthermore increases the development time of the annotation software, so that this type of incremental annotation appears only practical for large amounts of heritage data.","Computers and the Humanities",1998,"No","corpus annotation reusability feasibility incremental linguistic annotation paper examines feasibility incremental annotation existing annotation text basis annotation starting annotation scratch theoretical component describing basic methodology potential obstacles practical component describing experiment tests efficiency incremental annotation guidelines execution pilot experiments experiment demonstrates incremental annotation effective supported pre planning documentation unplanned opportunistic existing annotation effective reduction annotation time increases development time annotation software type incremental annotation appears practical large amounts heritage data",0
"Constraint Grammar corpus investigation nominative pronouns in English and Norwegian statistic infrequency Subject position ","tagging and the case of pronouns","Using a corpus to investigate empirically grammatical phenomena prior to writing grammatical rules or constraints for a disambiguating tagger is important. The paper shows how even case distinctions on pronouns are used more diversely than is usually assumed. Both in English and Norwegian nominative pronouns are used in more positions than the expected Subject one. Although the other uses are statistically less frequent, they may be important to the users of the resulting tagged corpus – who are often theoretical linguists. A tagger should therefore tag correctly also the more infrequent constructions. The paper shows how this can be done in a Constraint Grammar type tagger.","Computers and the Humanities",1998,"No","constraint grammar corpus investigation nominative pronouns english norwegian statistic infrequency subject position tagging case pronouns corpus investigate empirically grammatical phenomena prior writing grammatical rules constraints disambiguating tagger important paper shows case distinctions pronouns diversely assumed english norwegian nominative pronouns positions expected subject statistically frequent important users resulting tagged corpus theoretical linguists tagger tag correctly infrequent constructions paper shows constraint grammar type tagger",0
"authorship Elegy by W.S. Shakespeare stylometry ","the professor doth protest too much methinks problems with the foster response","In “Response to Elliott and Valenza, 'And Then There Were None'”, (1996) Donald Foster has taken strenuous issue with our Shakespeare Clinic's final report, which concluded that none of the testable Shakespeare claimants, and none of the Shakespeare Apocrypha poems and plays – including Funeral Elegy by W.S. – match Shakespeare. Though he seems to accept most of our exclusions – notably excepting those of the Elegy and A Lover's Complaint – he believes that our methodology is nonetheless fatally flawed by “worthless figures ... wrong more often than right”, “rigorous cherry–picking”, “playing with a stacked deck”, and “conveniently exil[ing] ... inconvenient data.” He describes our tests as “foul vapor” and “methodological madness.”","Computers and the Humanities",1998,"No","authorship elegy shakespeare stylometry professor doth protest methinks problems foster response response elliott valenza donald foster strenuous issue shakespeare clinic final report concluded testable shakespeare claimants shakespeare apocrypha poems plays including funeral elegy match shakespeare accept exclusions notably excepting elegy lover complaint believes methodology nonetheless fatally flawed worthless figures wrong rigorous cherry picking playing stacked deck conveniently exiling inconvenient data describes tests foul vapor methodological madness ",0
"Chadwyck-Healey electronic publishing English literature SGML ","literature online building a home for english and american literature on the world wide web","Chadwyck-Healey has a long tradition of electronic publishing. Beginning with production of CD-based literary corpora, it has recently moved many of its products to a web-accessible online environment. The article reflects on experiences with both CD and web-based publications.","Computers and the Humanities",1998,"No","chadwyck healey electronic publishing english literature sgml literature online building home english american literature world wide web chadwyck healey long tradition electronic publishing beginning production cd based literary corpora recently moved products web accessible online environment article reflects experiences cd web based publications",0
"Canterbury Tales Chaucer critical editions electronic publishing SGML ","publishing an electronic textual edition the case of the wife of baths prologue on cd rom","The article reports on one of the more sophisticated critical editions ever to be published in electronic format. The Wife of Bath is richly encoded, provides access to literally thousands of manuscript images, and enables users to assess the relationships between the numerous extant manuscript editions. The authors assess the methods used in the edition's development and the lessons learned through its production.","Computers and the Humanities",1998,"No","canterbury tales chaucer critical editions electronic publishing sgml publishing electronic textual edition case wife baths prologue cd rom article reports sophisticated critical editions published electronic format wife bath richly encoded access literally thousands manuscript images enables users assess relationships numerous extant manuscript editions authors assess methods edition development lessons learned production",0
"lexical statistics Monte Carlo methods vocabulary richness ","how variable may a constant be measures of lexical richness in perspective","A well-known problem in the domain of quantitative linguistics and stylistics concerns the evaluation of the lexical richness of texts. Since the most obvious measure of lexical richness, the vocabulary size (the number of different word types), depends heavily on the text length (measured in word tokens), a variety of alternative measures has been proposed which are claimed to be independent of the text length. This paper has a threefold aim. Firstly, we have investigated to what extent these alternative measures are truly textual constants. We have observed that in practice all measures vary substantially and systematically with the text length. We also show that in theory, only three of these measures are truly constant or nearly constant. Secondly, we have studied the extent to which these measures tap into different aspects of lexical structure. We have found that there are two main families of constants, one measuring lexical richness and one measuring lexical repetition. Thirdly, we have considered to what extent these measures can be used to investigate questions of textual similarity between and within authors. We propose to carry out such comparisons by means of the empirical trajectories of texts in the plane spanned by the dimensions of lexical richness and lexical repetition, and we provide a statistical technique for constructing confidence intervals around the empirical trajectories of texts. Our results suggest that the trajectories tap into a considerable amount of authorial structure without, however, guaranteeing that spatial separation implies a difference in authorship.","Computers and the Humanities",1998,"No","lexical statistics monte carlo methods vocabulary richness variable constant measures lexical richness perspective problem domain quantitative linguistics stylistics concerns evaluation lexical richness texts obvious measure lexical richness vocabulary size number word types depends heavily text length measured word tokens variety alternative measures proposed claimed independent text length paper threefold aim firstly investigated extent alternative measures textual constants observed practice measures vary substantially systematically text length show theory measures constant constant studied extent measures tap aspects lexical structure found main families constants measuring lexical richness measuring lexical repetition thirdly considered extent measures investigate questions textual similarity authors propose carry comparisons means empirical trajectories texts plane spanned dimensions lexical richness lexical repetition provide statistical technique constructing confidence intervals empirical trajectories texts results suggest trajectories tap considerable amount authorial structure guaranteeing spatial separation implies difference authorship",0
"KeywordsComputational Linguistic Network Requirement ","dancing to the telephone network requirements and opportunities",NA,"Computers and the Humanities",1998,"No"," computational linguistic network requirement dancing telephone network requirements opportunities na",0
"assessment nominal record linkage nominal retrieval ","assessment of systems for nominal retrieval and historical record linkage","Problems in retrieval of names form large data bases and in nominal record linkage are discussed with respect to computational solutions. The quest for robust methods that can handle the typical variability of historical nominal information is discussed, with some emphasis on probabilistic methods. It is argued that comparison and assessment of different systems used on the same data could enhance our understanding of methodological issues.","Computers and the Humanities",1998,"No","assessment nominal record linkage nominal retrieval assessment systems nominal retrieval historical record linkage problems retrieval names form large data bases nominal record linkage discussed respect computational solutions quest robust methods handle typical variability historical nominal information discussed emphasis probabilistic methods argued comparison assessment systems data enhance understanding methodological issues",0
"business strategies commercial electronic publishing development practicalities multimedia teaching tools SGML textbases ","electronic publishing at routledge","This article describes how an independent commercial academic publisher initiated its electronic publishing programme. It outlines the range of electronic activities under development and some of the issues addressed during the creation of electronic resources. Case studies of two early projects are included: a multimedia teaching too, A Right to Die? The Dax Cowart Case; and an SGML textbase, the Arden Shakespeare CD-ROM. In addition, the Routledge Encyclopedia of Philosophy is discussed as an example of the second generation of electronic projects at Routledge, highlighting lessons learned from previous projects and some of the issues relating to the production of a simultaneous print and electronic resource.","Computers and the Humanities",1998,"No","business strategies commercial electronic publishing development practicalities multimedia teaching tools sgml textbases electronic publishing routledge article describes independent commercial academic publisher initiated electronic publishing programme outlines range electronic activities development issues addressed creation electronic resources case studies early projects included multimedia teaching die dax cowart case sgml textbase arden shakespeare cd rom addition routledge encyclopedia philosophy discussed generation electronic projects routledge highlighting lessons learned previous projects issues relating production simultaneous print electronic resource",0
"ambiguity disambiguation lexicography polysemy word sense ","i dont believe in word senses","Word sense disambiguation assumes word senses. Withinthe lexicography and linguistics literature, they areknown to bevery slippery entities. The first part of the paperlooks at problemswith existing accounts of ‘word sense’ and describesthe various kinds of ways in which a word's meaning candeviate from its coremeaning. An analysis is presented in which wordsenses areabstractions from clusters of corpus citations, inaccordance withcurrent lexicographic practice. The corpus citations,not the wordsenses, are the basic objects in the ontology. Thecorpus citationswill be clustered into senses according to thepurposes of whoever or whatever does the clustering. In theabsence of suchpurposes, word senses do not exist.","Computers and the Humanities",1997,"No","ambiguity disambiguation lexicography polysemy word sense dont word senses word sense disambiguation assumes word senses withinthe lexicography linguistics literature areknown bevery slippery entities part paperlooks problemswith existing accounts word sense describesthe kinds ways word meaning candeviate coremeaning analysis presented wordsenses areabstractions clusters corpus citations inaccordance withcurrent lexicographic practice corpus citations wordsenses basic objects ontology thecorpus citationswill clustered senses thepurposes clustering theabsence suchpurposes word senses exist",0
"computational lexical semantics defining formats formats learning minimal cover pattern matching thesaurus building ","adding new words into a chinese thesaurus","In this paper, we study the problem of adding a large number of new words into a Chinese thesaurus according to their definitions in a Chinese dictionary, while minimizing the effort of hand tagging. To deal with the problem, we first make use of a kind of supervised learning technique to learn a set of defining formats for each class in the thesaurus, which tries to characterize the regularities about the definitions of the words in the class. We then use traditional techniques in Graph theory to derive a minimal subset of the new words to be added into the thesaurus, which meets the following condition: if we add the new words in the subset into the thesaurus by hand, the other new words can be added into the thesaurus automatically by matching their definitions with the defining formats of each class in the thesaurus. The method uses little, if any, language-specific or thesaurus-specific knowledge, and can be applied to the thesauri of other languages.","Computers and the Humanities",1997,"Yes","computational lexical semantics defining formats formats learning minimal cover pattern matching thesaurus building adding words chinese thesaurus paper study problem adding large number words chinese thesaurus definitions chinese dictionary minimizing effort hand tagging deal problem make kind supervised learning technique learn set defining formats class thesaurus characterize regularities definitions words class traditional techniques graph theory derive minimal subset words added thesaurus meets condition add words subset thesaurus hand words added thesaurus automatically matching definitions defining formats class thesaurus method language specific thesaurus specific knowledge applied thesauri languages",1
"lexicon on-line dictionary syntactic dictionary ","comlex syntax a large syntactic dictionary for natural language processing","This article is a detailed account of COMLEX Syntax, an on-line syntactic dictionary of English, developed by the Proteus Project at New York University under the auspices of the Linguistics Data Consortium. This lexicon was intended to be used for a variety of tasks in natural language processing by computer and as such has very detailed classes with a large number of syntactic features and complements for the major parts of speech and is, as far as possible, theory neutral. The dictionary was entered by hand with reference to hard copy dictionaries, an on-line concordance and native speakers‘intuition. Thus it is without prior encumbrances and can be used for both pure research and commercial purposes.","Computers and the Humanities",1997,"Yes","lexicon line dictionary syntactic dictionary comlex syntax large syntactic dictionary natural language processing article detailed account comlex syntax line syntactic dictionary english developed proteus project york university auspices linguistics data consortium lexicon intended variety tasks natural language processing computer detailed classes large number syntactic features complements major parts speech theory neutral dictionary entered hand reference hard copy dictionaries line concordance native speakers intuition prior encumbrances pure research commercial purposes",1
"word sense disambiguation semantics, grammar knowledge representation ","senses and texts","This paper addresses the question of whether it is possible tosense-tag systematically, and on a large scale, and how we shouldassess progress so far. That is to say, how to attach each occurrenceof a word in a text to one and only one sense in a dictionary – aparticular dictionary of course, and that is part of the problem. Thepaper does not propose a solution to the question, though we havereported empirical findings elsewhere (Cowie et al., 1992;Wilks et al., 1996; Wilks and Stevenson, 1997), and intend to continue andrefine that work. The point of this paper is to examine two well-knowncontributions critically: The first (Kilgarriff, 1993), which is widelytaken to show that the task, as defined, cannot be carried outsystematically by humans and, secondly (Yarowsky, 1995), which claimsstrikingly good results at doing exactly that.","Computers and the Humanities",1997,"No","word sense disambiguation semantics grammar knowledge representation senses texts paper addresses question tosense tag systematically large scale shouldassess progress attach occurrenceof word text sense dictionary aparticular dictionary part problem thepaper propose solution question havereported empirical findings cowie al wilks al wilks stevenson intend continue andrefine work point paper examine knowncontributions critically kilgarriff widelytaken show task defined carried outsystematically humans yarowsky claimsstrikingly good results ",0
"character coding standards document editing keyboard input multilingual text textual data interchange ","mtscript a multi lingual text editor","This paper describes the multilingual text editor MtScript developed in the framework of the MULTEXT project.MtScript enables the use of many differentwriting systems in the same document (Latin, Arabic,Cyrillic, Hebrew, Chinese, Japanese, etc.). Editingfunctions enable the insertion or deletion of textzones even if they have opposite writing directions.In addition, the languages in the text can be marked,customized keyboard input rules can be associated witheach language and different character coding systems(one or two bytes) can be combined. MtScript isbased on a portable environment (Tcl/Tk). MtScript.1.1version has been developed underUnix/X-Windows (Solaris, Linux systems) and otherversions are planned to be ported to the Windows andMacintosh environments. The current 1.1 versionpresents several limits that will be fixed in futureversions, such as the justification of bi-directionaltexts, printing support, and text import/exportsupport. Future versions will use SGML and TEI norms,which offer ways of encoding multilingual texts andare to a large extent meant for interchange.","Computers and the Humanities",1997,"No","character coding standards document editing keyboard input multilingual text textual data interchange mtscript multi lingual text editor paper describes multilingual text editor mtscript developed framework multext projectmtscript enables differentwriting systems document latin arabiccyrillic hebrew chinese japanese editingfunctions enable insertion deletion textzones opposite writing directions addition languages text markedcustomized keyboard input rules witheach language character coding systems bytes combined mtscript isbased portable environment tcltk mtscriptversion developed underunix windows solaris linux systems otherversions planned ported windows andmacintosh environments current versionpresents limits fixed futureversions justification bi directionaltexts printing support text importexportsupport future versions sgml tei norms offer ways encoding multilingual texts andare large extent meant interchange",0
"SGML women's writing document type definition (DTD) design content tagging ","sgml and the orland project descriptive markup for an electronic ghistory of womens writing","This paper describes the novel ways in which the Orlando Project, based at the Universities of Alberta and Guelph, is using SGML to create an integrated electronic history of British women's writing in English. Unlike most other SGML-based humanities computing projects which are tagging existing texts, we are researching and writing new material, including biographies, items of historical significance, and many kinds of literary and historical interpretation, all of which incorporates sophisticated SGML encoding for content as well as structure. We have created three DTDs, for biographies, for writing-related activities and publications, and for social, political and other events. A major factor influencing the design of the DTDs was the requirement to be able to merge and restructure the entire text base in many ways in order to retrieve and index it and to reflect multiple views and interpretations. In addition a stable and well-documented system for tagging was deemed essential for a team which involves almost twenty people, including eight graduate students, in two locations.","Computers and the Humanities",1997,"No","sgml women writing document type definition dtd design content tagging sgml orland project descriptive markup electronic ghistory womens writing paper describes ways orlando project based universities alberta guelph sgml create integrated electronic history british women writing english unlike sgml based humanities computing projects tagging existing texts researching writing material including biographies items historical significance kinds literary historical interpretation incorporates sophisticated sgml encoding content structure created dtds biographies writing related activities publications social political events major factor influencing design dtds requirement merge restructure entire text base ways order retrieve index reflect multiple views interpretations addition stable documented system tagging deemed essential team involves twenty people including graduate students locations",0
"semantic mark up corpus-based rule development for lexical acquisition SGML ","marking up in tatoe and exporting to sgml","This paper presents a method for developing limited-context grammar rules in order to mark up text automatically, by attaching specific text segments to a small number of well-defined and application-determined semantic categories. The Text Analysis Tool with Object Encoding (TATOE) was used in order to support the iterative process of developing a set of rules as well as for constructing and managing the lexical resources. The work reported here is part of a real-world application scenario: the automatic semantic mark up of German news messages, as provided by a German press agency, according to the SGML-based standard News Industry Text Format (NITF) to facilitate their further exchange. The implemented export mechanism of the semantic mark up into NITF is also described in the paper. ","Computers and the Humanities",1997,"No","semantic mark corpus based rule development lexical acquisition sgml marking tatoe exporting sgml paper presents method developing limited context grammar rules order mark text automatically attaching specific text segments small number defined application determined semantic categories text analysis tool object encoding tatoe order support iterative process developing set rules constructing managing lexical resources work reported part real world application scenario automatic semantic mark german news messages provided german press agency sgml based standard news industry text format nitf facilitate exchange implemented export mechanism semantic mark nitf paper ",0
"CD-Rom development crossing subject boundaries didactic considerations language learning technologies multi-media ","the potential of multi media for foreign language learning a critical evaluation","Multi-Media does not, just by itself, guarantee accelerated learning and enhanced motivation unless there is a clear pedagogical progression and learning strategy. The authors describe and analyze the didactic dimensions to be considered when designing a multi-media tool, based on their own experience as software authors and language trainers.","Computers and the Humanities",1997,"No","cd rom development crossing subject boundaries didactic considerations language learning technologies multi media potential multi media foreign language learning critical evaluation multi media guarantee accelerated learning enhanced motivation clear pedagogical progression learning strategy authors describe analyze didactic dimensions considered designing multi media tool based experience software authors language trainers",0
"computer implementation Estonian language engineering morphology text corpora ","an estonian morphological analyser and the impact of a corpus on its development","The paper describes a morphological analyser forEstonian and how using a text corpus influenced theprocess of creating it and the resulting programitself. The influence is not limited to the lexicononly, but is also noticeable in the resulting algorithm andimplementation too. When work on the analyser began,there were no computational treatment of Estonianderivatives and compounds. After some cycles ofdevelopment and testing on the corpus, we came up withan acceptable algorithm for their treatment. Both themorphological analyser and the speller based on ithave been successfully marketed.","Computers and the Humanities",1997,"No","computer implementation estonian language engineering morphology text corpora estonian morphological analyser impact corpus development paper describes morphological analyser forestonian text corpus influenced theprocess creating resulting programitself influence limited lexicononly noticeable resulting algorithm andimplementation work analyser began computational treatment estonianderivatives compounds cycles ofdevelopment testing corpus withan acceptable algorithm treatment themorphological analyser speller based ithave successfully marketed",0
"collocation concordance lines language independent software lexical statistics ","language independent statistical software for corpus exploration","In this report two programs for statistical analysis of concordance lines are described. The programs have been developed for analyzing he lexical context of a given word. It is shown how different parameter settings influence the outcome of collocational analysis, and how the concept of collocation can be extended to allow the extraction of lines typical for a word from a set of concordance lines. Even though all the examples are for English, the software is completely language independent and only requires minimal linguistic resources.","Computers and the Humanities",1997,"No","collocation concordance lines language independent software lexical statistics language independent statistical software corpus exploration report programs statistical analysis concordance lines programs developed analyzing lexical context word shown parameter settings influence outcome collocational analysis concept collocation extended extraction lines typical word set concordance lines examples english software completely language independent requires minimal linguistic resources",0
"corpus-based linguistics natural language processing SGML ","using sgml as a basis for data intensive natural language processing","This paper describes the LT NSL system (McKelvie et al., 1996), an architecture for writing corpus processing tools. This system is then compared with two other systems which address similar issues, the GATE system (Cunningham et al., 1995) and the IMS Corpus Workbench (Christ, 1994). In particular we address the advantages and disadvantages of an SGML approach compared with a non-sgml database approach.","Computers and the Humanities",1997,"No","corpus based linguistics natural language processing sgml sgml basis data intensive natural language processing paper describes lt nsl system mckelvie al architecture writing corpus processing tools system compared systems address similar issues gate system cunningham al ims corpus workbench christ address advantages disadvantages sgml approach compared sgml database approach",0
"dictionary entry Korean markup SGML TEI ","modifying the tei dtd the case of korean dictionaries","Dictionary markup is one of the concerns of the Text Encoding Initiative (TEI), an international project for text encoding. In this paper, we investigate ways to use and extend the TEI encoding scheme for the markup of Korean dictionary entries. Since TEI suggestions for dictionary markup are mainly for western language dictionaries, we need to cope with problems to be encountered in encoding Korean dictionary entries. We try to extend and modify the TEI encoding scheme in the way suggested by the TEI. Also, we restrict the content model so that the encoded dictionary might be viewed as a database as well as a computerized, originally printed, dictionary.","Computers and the Humanities",1997,"No","dictionary entry korean markup sgml tei modifying tei dtd case korean dictionaries dictionary markup concerns text encoding initiative tei international project text encoding paper investigate ways extend tei encoding scheme markup korean dictionary entries tei suggestions dictionary markup western language dictionaries cope problems encountered encoding korean dictionary entries extend modify tei encoding scheme suggested tei restrict content model encoded dictionary viewed database computerized originally printed dictionary",0
"archaeological typology ceramics knowledge acquisition machine learning Sudan ","plata an application of legal a machine learning based system to a typology of archaeological ceramics","The authors here show that machine learning techniques can be used for designing an archaeological typology, at an early stage when the classes are not yet well defined. The program (LEGAL, LEarning with GAlois Lattice) is a machine learning system which uses a set of examples and counter-examples in order to discriminate between classes. Results show a good compatibility between the classes such as the yare defined by the system and the archaeological hypotheses.","Computers and the Humanities",1997,"No","archaeological typology ceramics knowledge acquisition machine learning sudan plata application legal machine learning based system typology archaeological ceramics authors show machine learning techniques designing archaeological typology early stage classes defined program legal learning galois lattice machine learning system set examples counter examples order discriminate classes results show good compatibility classes yare defined system archaeological hypotheses",0
"KeywordsMachine Translation Single Authorship Electronic Edition Humanity Computing Attribution Study ","introduction quo vadimus",NA,"Computers and the Humanities",1997,"No"," machine translation single authorship electronic edition humanity computing attribution study introduction quo vadimus na",0
"conflation algorithm Hartlib Papers Collection Latin Patrologia Latina stemming text databases ","retrieval of morphological variants in searches of latin text databases","This paper reports a detailed evaluation of the effectiveness of a system that has been developed for the identification and retrieval of morphological variants in searches of Latin text databases. A user of the retrieval system enters the principal parts of the search term (two parts for a noun or adjective, three parts for a deponent verb, and four parts for other verbs), this enabling the identification of the type of word that is to be processed and of the rules that are to be followed in determining the morphological variants that should be retrieved. Two different search algorithms are described. The algorithms are applied to the Latin portion of the Hartlib Papers Collection and to a range of classical, vulgar and medieval Latin texts drawn from the Patrologia Latina and from the PHI Disk 5.3 datasets. The effectiveness of these searches demonstrates the effectiveness of our procedures in providing access to the full range of classical and post-classical Latin text databases.","Computers and the Humanities",1997,"No","conflation algorithm hartlib papers collection latin patrologia latina stemming text databases retrieval morphological variants searches latin text databases paper reports detailed evaluation effectiveness system developed identification retrieval morphological variants searches latin text databases user retrieval system enters principal parts search term parts noun adjective parts deponent verb parts verbs enabling identification type word processed rules determining morphological variants retrieved search algorithms algorithms applied latin portion hartlib papers collection range classical vulgar medieval latin texts drawn patrologia latina phi disk datasets effectiveness searches demonstrates effectiveness procedures providing access full range classical post classical latin text databases",0
"authorship attribution statistics stylistics ","the state of authorship attribution studies some problems and solutions","The statement, ’’Results of most non-traditional authorship attribution studies are not universally accepted as definitive,'' is explicated. A variety of problems in these studies are listed and discussed: studies governed by expediency; a lack of competent research; flawed statistical techniques; corrupted primary data; lack of expertise in allied fields; a dilettantish approach; inadequate treatment of errors. Various solutions are suggested: construct a correct and complete experimental design; educate the practitioners; study style in its totality; identify and educate the gatekeepers; develop a complete theoretical framework; form an association of practitioners.","Computers and the Humanities",1997,"No","authorship attribution statistics stylistics state authorship attribution studies problems solutions statement results traditional authorship attribution studies universally accepted definitive explicated variety problems studies listed discussed studies governed expediency lack competent research flawed statistical techniques corrupted primary data lack expertise allied fields dilettantish approach inadequate treatment errors solutions suggested construct correct complete experimental design educate practitioners study style totality identify educate gatekeepers develop complete theoretical framework form association practitioners",0
"TEI SGML proper nouns text encoding Women Writers Project CELT ","names proper and improper applying the tei to the classification of proper nouns","This paper discusses the encoding of proper names using the TEI Guidelines, describing the practice of the Women Writers Project at Brown University, and the CELT Project at University College, Cork. We argue that such encoding may be necessary to enable historical and literary research, and that the specific approach taken will depend on the needs of the project and the audience to be served. Because the TEI Guidelines provide a fairly flexible system for the encoding of proper names, we conclude that projects may need to collaborate to determine more specific constraints, to ensure consistency of approach and compatibility of data.","Computers and the Humanities",1997,"No","tei sgml proper nouns text encoding women writers project celt names proper improper applying tei classification proper nouns paper discusses encoding proper names tei guidelines describing practice women writers project brown university celt project university college cork argue encoding enable historical literary research specific approach depend project audience served tei guidelines provide fairly flexible system encoding proper names conclude projects collaborate determine specific constraints ensure consistency approach compatibility data",0
"PhotoCD digitisation World Wide Web medieval manuscript Aberdeen Bestiary ","text and illustration the digitisation of a medieval manuscript","This paper considers the choice of the medieval Aberdeen Bestiaryas the first project in Aberdeen University Library’s digitisationprogramme, and discusses some of the unusual features of themanuscript itself.","Computers and the Humanities",1997,"No","photocd digitisation world wide web medieval manuscript aberdeen bestiary text illustration digitisation medieval manuscript paper considers choice medieval aberdeen bestiaryas project aberdeen university library digitisationprogramme discusses unusual features themanuscript ",0
"KeywordsTextual Feature Computational Linguistic Standard Generalize Intended Purpose Careful Thought ","some problems of tei markup and early printed books","This paper presents two groups of text encodingproblems encountered by the Brown University WomenWriters Project (WWP). The WWP is creating a full-textdatabase of transcriptions of pre-1830 printed bookswritten by women in English. For encoding our texts weuse Standard Generalized Markup Language (SGML),following the Text Encoding Initiative’s Guidelines for Electronic Text Encoding andInterchange. SGML is a powerful text encoding systemfor describing complex textual features, but a fullexpression of these may require very complex encoding,and careful thought about the intended purpose of theencoded text. We present here several possibleapproaches to these encoding problems, and analyze theissues they raise.","Computers and the Humanities",1997,"No"," textual feature computational linguistic standard generalize intended purpose careful thought problems tei markup early printed books paper presents groups text encodingproblems encountered brown university womenwriters project wwp wwp creating full textdatabase transcriptions pre printed bookswritten women english encoding texts weuse standard generalized markup language sgml text encoding initiative guidelines electronic text encoding andinterchange sgml powerful text encoding systemfor describing complex textual features fullexpression require complex encoding careful thought intended purpose theencoded text present possibleapproaches encoding problems analyze theissues raise",0
"Key wordsword frequency distributions lexical conceptual structure lognormality bimodal density estimation ","word frequency distributions and lexical semantics","This paper addresses the relation between meaning, lexical productivity, and frequency of use. Using density estimation as a visualization tool, we show that differences in semantic structure can be reflected in probability density functions estimated for word frequency distributions. We call attention to an example of a bimodal density, and suggest that bimodality arises when distributions of well-entrenched lexical items, which appear to be lognormal, are mixed with distributions of productively created nonce formations.","Computers and the Humanities",1996,"No","key wordsword frequency distributions lexical conceptual structure lognormality bimodal density estimation word frequency distributions lexical semantics paper addresses relation meaning lexical productivity frequency density estimation visualization tool show differences semantic structure reflected probability density functions estimated word frequency distributions call attention bimodal density suggest bimodality arises distributions entrenched lexical items lognormal mixed distributions productively created nonce formations",0
NA,"infornttica y humanidades",NA,"Computers and the Humanities",1996,"No"," infornttica humanidades na",0
"Key wordsstatistical analysis stylistic analysis style Raymond Chandler ","the not so simple art of imitation pastiche literary style and raymond chandler","This analysis extends the tools of statistical analysis to the challenging task of distinguishing between genuine works by an author, the preeminent American writer of mysteries, Raymond Chandler, and deliberate attempts by others to mimic the author's style. Rendering the task all the more challenging, the analysis focuses exclusively on the main elements of Chandler's style rather than on his minor but telling stylistic idiosyncrasies. Statistical analysis establishes that indicators of these stylistic elements can successfully detect the pastiches.","Computers and the Humanities",1996,"No","key wordsstatistical analysis stylistic analysis style raymond chandler simple art imitation pastiche literary style raymond chandler analysis extends tools statistical analysis challenging task distinguishing genuine works author preeminent american writer mysteries raymond chandler deliberate attempts mimic author style rendering task challenging analysis focuses exclusively main elements chandler style minor telling stylistic idiosyncrasies statistical analysis establishes indicators stylistic elements successfully detect pastiches",0
"Key wordsCommon Sense content analysis forcefulness simplicity stylistic analysis Thomas Paine ","the common style of common sense","The extraordinary impact of Thomas Paine's Common Sense has often been attributed to its style — to the simplicity and forcefulness with which Paine expressed ideas that many others before him had expressed. Comparative analysis of Common Sense and other pre-Revolutionary pamphlets suggests that Common Sense was indeed stylistically unique; no other pamphleteer came close to matching Paine's combination of simplicity and forcefulness.","Computers and the Humanities",1996,"No","key wordscommon sense content analysis forcefulness simplicity stylistic analysis thomas paine common style common sense extraordinary impact thomas paine common sense attributed style simplicity forcefulness paine expressed ideas expressed comparative analysis common sense pre revolutionary pamphlets suggests common sense stylistically unique pamphleteer close matching paine combination simplicity forcefulness",0
"Key wordsdiscourse information natural language syntax natural language semantics punctuation ","current approaches to punctuation in computational linguistics","Some recent studies in computational linguistics have aimed to take advantage of various cues presented by punctuation marks. This short survey is intended to summarise these research efforts and additionally, to outline a current perspective for the usage and functions of punctuation marks. We conclude by presenting an information-based framework for punctuation, influenced by treatments of several related phenomena in computational linguistics.","Computers and the Humanities",1996,"No","key wordsdiscourse information natural language syntax natural language semantics punctuation current approaches punctuation computational linguistics recent studies computational linguistics aimed advantage cues presented punctuation marks short survey intended summarise research efforts additionally outline current perspective usage functions punctuation marks conclude presenting information based framework punctuation influenced treatments related phenomena computational linguistics",0
"KeywordsConceptual Modeling Problem Domain Computational Linguistic Computing Technology Recent Introduction ","conceptual modeling versus visual modeling a technological key to building consensus","Debate has long been a hallmark of the academic endeavor. The recent introduction of computers into academic life has not been the deus ex machina to bring sudden resolution to these debates. There is a new computing technology, however, that has some promise in this regard. It is called conceptual modeling. This paper' demonstrates how a computer-based model of a problem domain can lead to consensus when competing approaches to the domain can be encapsulated in different visual models that are applied to the same underlying conceptual model.","Computers and the Humanities",1996,"No"," conceptual modeling problem domain computational linguistic computing technology recent introduction conceptual modeling versus visual modeling technological key building consensus debate long hallmark academic endeavor recent introduction computers academic life deus machina bring sudden resolution debates computing technology promise regard called conceptual modeling paper demonstrates computer based model problem domain lead consensus competing approaches domain encapsulated visual models applied underlying conceptual model",0
"Key wordsstylometry Shakespeare authorship Shakespeare canon Shakespeare Apocrypha Elizabethan poems Elizabethan plays ","and then there were none winnowing the shakespeare claimants","The Shakespeare Clinic has developed 51 computer tests of Shakespeare play authorship and 14 of poem authorship, and applied them to 37 claimed “true Shakespeares,” to 27 plays of the Shakespeare Apocrypha, and to several poems of unknown or disputed authorship. No claimant, and none of the apocryphal plays or poems, matched Shakespeare. Two plays and one poem from the Shakespeare Canon,Titus Andronicus, Henry VI, Part 3, and “A Lover's Complaint,” do not match the others.","Computers and the Humanities",1996,"No","key wordsstylometry shakespeare authorship shakespeare canon shakespeare apocrypha elizabethan poems elizabethan plays winnowing shakespeare claimants shakespeare clinic developed computer tests shakespeare play authorship poem authorship applied claimed true shakespeares plays shakespeare apocrypha poems unknown disputed authorship claimant apocryphal plays poems matched shakespeare plays poem shakespeare canontitus andronicus henry vi part lover complaint match ",0
"Key wordshypertext hypermedia WWW hypertext authoring literature instruction ","current uses of hypertext in teaching literature","Literature instructors are using hypertext to enhance their teaching in a broad variety of ways that includes putting course materials on the WWW; creating online tutorials; using annotated hypertexts in addition to or in lieu of print texts; having students write hypertexts; examining the medium of hypertext as a literary and cultural theme; and studying hypertext fiction in the context of traditional literature classes. The article describes examples of each of these uses of hypertext in teaching literature and provides sources of further examples of and information on using hypertext as a teaching tool in literature classes.","Computers and the Humanities",1996,"No","key wordshypertext hypermedia www hypertext authoring literature instruction current hypertext teaching literature literature instructors hypertext enhance teaching broad variety ways includes putting materials www creating online tutorials annotated hypertexts addition lieu print texts students write hypertexts examining medium hypertext literary cultural theme studying hypertext fiction context traditional literature classes article describes examples hypertext teaching literature sources examples information hypertext teaching tool literature classes",0
"KeywordsComputational Linguistic ","computers and historians past present and future",NA,"Computers and the Humanities",1996,"No"," computational linguistic computers historians past present future na",0
"Key wordsprincipal components analysis Marlowe Shakespeare stylometry style contextual words ","tamburlaine stalks in henry vi","Starting from the accepted premise that Marlowe influenced the young Shakespeare, a selection of strongly contextual words characteristic of Tamburlaine are shown to be reflected in Shakespeare's early history plays. Principal components analysis confirms this. A very similar configuration, however, results when the selected Marlowe-preferred words are non-contextual common words. This feature can not be explained by influence in its conventional sense, particularly when the Shakespeare plays closest to Marlowe are those that share with Marlowe a dearth of selected Shakespeare-preferred common words.","Computers and the Humanities",1996,"No","key wordsprincipal components analysis marlowe shakespeare stylometry style contextual words tamburlaine stalks henry vi starting accepted premise marlowe influenced young shakespeare selection strongly contextual words characteristic tamburlaine shown reflected shakespeare early history plays principal components analysis confirms similar configuration results selected marlowe preferred words contextual common words feature explained influence conventional sense shakespeare plays closest marlowe share marlowe dearth selected shakespeare preferred common words",0
"Key wordswordlist lexical analysis semantic fields collocate display distribution display index display KWIC ","literary studies a computer assisted teaching methodology","We used TACT computer software to teach Joseph Conrad's novel Heart of Darkness to BA (Hops) students at the University of Luton in England. Conrad's novel is one of the texts used in the ‘Language and New Literatures’ modules (units). In these modules we combine analytical approaches to literary texts with linguistic methods. We used TACT to reinforce the understanding of the text of Heart of Darkness achieved through such a combination of methods. An exposure to the computer-based approaches to the text described in this article made the students' interaction with the text a more complex and rewarding experience.","Computers and the Humanities",1996,"No","key wordswordlist lexical analysis semantic fields collocate display distribution display index display kwic literary studies computer assisted teaching methodology tact computer software teach joseph conrad heart darkness ba hops students university luton england conrad texts language literatures modules units modules combine analytical approaches literary texts linguistic methods tact reinforce understanding text heart darkness achieved combination methods exposure computer based approaches text article made students interaction text complex rewarding experience",0
"Key wordshistory and computing historiography ","bringing bacon home the divergent progress of computer aided historical research in europe and the united states","This historiographical article surveys the different developmental trajectories of computer-aided historical research and teaching in Western Europe and in the United States, and seeks synergies which promise to enhance the discipline.","Computers and the Humanities",1996,"No","key wordshistory computing historiography bringing bacon home divergent progress computer aided historical research europe united states historiographical article surveys developmental trajectories computer aided historical research teaching western europe united states seeks synergies promise enhance discipline",0
"Key wordsChrétien de Troyes complex media database electronic archive electronic text hypertext medieval manuscript culture Old French TEI World Wide Web ","the charrette project manipulating text and image in an electronic archive of a medieval manuscript tradition","This paper concerns the Charrette Project, a multimedia electronic archive of a medieval manuscript tradition. In this paper, we argue that the computer's strengths in manipulating complex and varied resources should be an important organizing principle in the conception and construction of electronic text projects. Specifically, we describe the elements of the Charrette archive, its architecture, and its potential for scholarly research and pedagogical applications.","Computers and the Humanities",1996,"Yes","key wordschr tien de troyes complex media database electronic archive electronic text hypertext medieval manuscript culture french tei world wide web charrette project manipulating text image electronic archive medieval manuscript tradition paper concerns charrette project multimedia electronic archive medieval manuscript tradition paper argue computer strengths manipulating complex varied resources important organizing principle conception construction electronic text projects specifically describe elements charrette archive architecture potential scholarly research pedagogical applications",1
"Key wordsneural networks function words authorship attribution The Federalist Papers ","neural network applications in stylometry the federalist papers","Neural Networks have recently been a matter of extensive research and popularity. Their application has increased considerably in areas in which we are presented with a large amount of data and we have to identify an underlying pattern. This paper will look at their application to stylometry. We believe that statistical methods of attributing authorship can be coupled effectively with neural networks to produce a very powerful classification tool. We illustrate this with an example of a famous case of disputed authorship, The Federalist Papers. Our method assigns the disputed papers to Madison, a result which is consistent with previous work on the subject.","Computers and the Humanities",1996,"No","key wordsneural networks function words authorship attribution federalist papers neural network applications stylometry federalist papers neural networks recently matter extensive research popularity application increased considerably areas presented large amount data identify underlying pattern paper application stylometry statistical methods attributing authorship coupled effectively neural networks produce powerful classification tool illustrate famous case disputed authorship federalist papers method assigns disputed papers madison result consistent previous work subject",0
"Key wordscomputer-assisted language learning education IT concepts ","a rationale for teacher education and call the holistic view and its implications","In a paper written in 1987 entitled “Computers and the Humanities Courses: Philosophical Bases and Approaches” Nancy Ide put forward two views on teacher education in humanities computing, the “Expert User's View” and the “Holistic View”.1 Ide's two views are derived from the collective opinions given by members of a workshop on teaching computing and humanities courses. In this article the degree to which Ide's two Views can be substantiated in Computer-Assisted Language Learning (CALL) is explored, through a review of the literature and through an international survey on CALL materials development conducted by the author in 1991 (Levy, 1994). On this basis, and given the scarcity of Holistic courses in CALL, a rationale for a CALL course with a holistic orientation is presented.","Computers and the Humanities",1996,"No","key wordscomputer assisted language learning education concepts rationale teacher education call holistic view implications paper written entitled computers humanities courses philosophical bases approaches nancy ide put forward views teacher education humanities computing expert user view holistic view ide views derived collective opinions members workshop teaching computing humanities courses article degree ide views substantiated computer assisted language learning call explored review literature international survey call materials development conducted author levy basis scarcity holistic courses call rationale call holistic orientation presented",0
"Key wordsencoding TEI dictionaries SGML ","encoding dictionaries","This article describes the major problems in devising a TEI encoding format for dictionaries, which, because of their high degree of structuring and compression of information, are among the most complex text types treated in the TEI. The major problems for this task were (1) the tension between generality of the description, in order to be widely applicable across dictionaries, and descriptive power, that is, the ability to describe with precision the particular structure of any given dictionary; and (2) the need to accommodate different views and uses of the encoded dictionary, for example, as printed object and as a database of information.","Computers and the Humanities",1995,"No","key wordsencoding tei dictionaries sgml encoding dictionaries article describes major problems devising tei encoding format dictionaries high degree structuring compression information complex text types treated tei major problems task tension generality description order widely applicable dictionaries descriptive power ability describe precision structure dictionary accommodate views encoded dictionary printed object database information",0
"Key wordsderivational morphology lexical transformation lexicon extension word-formation ","new words from old a formalism for word formation","Many languages make use of word-formation devices to allow speakers or writers to create new words when the existing vocabulary proves inadequate. In this paper we consider how these devices can be expressed formally, allowing them to be used in word- and sentence-generation, for dictionary expansion, and the like. The paper begins with some typical word-formation rules drawn mostly from French. Attention is drawn to some features of these rules which must be captured in any formal representation. The formal representation of a basic lexical transformation is presented in some detail, along with a number of examples. A computer implementation of the transformation system is described, together with a range of applications. A discussion of static and dynamic generation leads to the concept of an inverted transformation.","Computers and the Humanities",1995,"No","key wordsderivational morphology lexical transformation lexicon extension word formation words formalism word formation languages make word formation devices speakers writers create words existing vocabulary proves inadequate paper devices expressed formally allowing word sentence generation dictionary expansion paper begins typical word formation rules drawn french attention drawn features rules captured formal representation formal representation basic lexical transformation presented detail number examples computer implementation transformation system range applications discussion static dynamic generation leads concept inverted transformation",0
"Key wordsSGML textual criticism electronic text editions text encoding TEI ","encoding textual criticism","This paper chronicles the work of the TEI textual criticism working groups through several phases, documenting how and why the design goals were shaped by the requirements of several distinct user communities and by the nature of the textual evidence itself. Encoding schemes for the representation of physical details of textual witnesses were unified with encoding schemes for critical editing practices when it was observed that the two phenomena were inextricably layered and linked within real texts. Rationale is offered for the development teams' adherence to exceedingly general design principles: (a) the requirement that the encoding notations be neutral in text-theoretic terms; (b) the need to accommodate dramatically different text-transmission phenomena and research goals within diverse text-critical arenas; (c) the need for commensurability of the text-critical markup with encoding notations used in closely related text-analytic research. The paper also assesses the results of the effort in terms of the encoding scheme's adequacy for several scholarly purposes: suggestions are made concerning the need for programmatic testing, for refinement, and for extension of the encoding model to support a broader range of text-transmission phenomena and research objectives.","Computers and the Humanities",1995,"No","key wordssgml textual criticism electronic text editions text encoding tei encoding textual criticism paper chronicles work tei textual criticism working groups phases documenting design goals shaped requirements distinct user communities nature textual evidence encoding schemes representation physical details textual witnesses unified encoding schemes critical editing practices observed phenomena inextricably layered linked real texts rationale offered development teams adherence exceedingly general design principles requirement encoding notations neutral text theoretic terms accommodate dramatically text transmission phenomena research goals diverse text critical arenas commensurability text critical markup encoding notations closely related text analytic research paper assesses results effort terms encoding scheme adequacy scholarly purposes suggestions made programmatic testing refinement extension encoding model support broader range text transmission phenomena research objectives",0
"Key wordsCentury of Prose Corpus COPC historical corpora literary corpora tagging text markup ","the century of prose corpus a half million word historical data base","The Century of Prose Corpus is a historical corpus of British English of the period 1680–1780. It has been designed to provide a resource for students of the language of that era. The COPC is diachronic and may be considered a unit in what will eventually become a series of corpora providing access to the whole of the English language from the oldest specimens to the present. This article describes and explains the various features of the COPC.","Computers and the Humanities",1995,"Yes","key wordscentury prose corpus copc historical corpora literary corpora tagging text markup century prose corpus half million word historical data base century prose corpus historical corpus british english period designed provide resource students language era copc diachronic considered unit eventually series corpora providing access english language oldest specimens present article describes explains features copc",1
"Key wordsauthor attribution content analysis discriminant analysis lexical statistics neural networks The Federalist ","on the utility of content analysis in author attributionthe federalist","In studies of author attribution, measurement of differential use of function words is the most common procedure, though lexical statistics are often used. Content analysis has seldom been employed. We compare the success of lexical statistics, content analysis, and function words in classifying the 12 disputedFederalist papers. Of course, Mosteller and Wallace (1964) have presented overwhelming evidence that all 12 were by James Madison rather than by Alexander Hamilton. Our purpose is not to challenge these attributions but rather to useThe Federalist as a test case. We found lexical statistics to be of no use in classifying the disputed papers. Using both classical canonical discriminant analysis and a neural-network approach, content analytic measures — the Harvard III Psychosociological Dictionary and semantic differential indices — were found to be successful at attributing most of the disputed papers to Madison. However, a function-word approach is more successful. We argue that content analysis can be useful in cases where the function-word approach does not yield compelling conclusions and, perhaps, in preliminary screening in cases where there are a large number of possible authors.","Computers and the Humanities",1995,"No","key wordsauthor attribution content analysis discriminant analysis lexical statistics neural networks federalist utility content analysis author attributionthe federalist studies author attribution measurement differential function words common procedure lexical statistics content analysis seldom employed compare success lexical statistics content analysis function words classifying disputedfederalist papers mosteller wallace presented overwhelming evidence james madison alexander hamilton purpose challenge attributions usethe federalist test case found lexical statistics classifying disputed papers classical canonical discriminant analysis neural network approach content analytic measures harvard iii psychosociological dictionary semantic differential indices found successful attributing disputed papers madison function word approach successful argue content analysis cases function word approach yield compelling conclusions preliminary screening cases large number authors",0
"Key wordsBible English Gospels Greek Hebrew information retrieval latent semantic indexing singular value decomposition ","using latent semantic indexing for multilanguage information retrieval","In this paper, a method for indexing cross-language databases for conceptual query matching is presented. Two languages (Greek and English) are combined by appending a small portion of documents from one language to the identical documents in the other language. The proposed merging strategy duplicates less than 7% of the entire database (made up of different translations of the Gospels). Previous strategies duplicated up to 34% of the initial database in order to perform the merger. The proposed method retrieves a larger number of relevant documents for both languages with higher cosine rankings when Latent Semantic Indexing (LSI) is employed. Using the proposed merge strategies, LSI is shown to be effective in retrieving documents from either language (Greek or English) without requiring any translation of a user's query. An effective Bible search product needs to allow the use of natural language for searching (queries). LSI enables the user to form queries with using natural expressions in the user's own native language. The merging strategy proposed in this study enables LSI to retrieve relevant documents effectively using a minimum of the database in a foreign language.","Computers and the Humanities",1995,"No","key wordsbible english gospels greek hebrew information retrieval latent semantic indexing singular decomposition latent semantic indexing multilanguage information retrieval paper method indexing cross language databases conceptual query matching presented languages greek english combined appending small portion documents language identical documents language proposed merging strategy duplicates entire database made translations gospels previous strategies duplicated initial database order perform merger proposed method retrieves larger number relevant documents languages higher cosine rankings latent semantic indexing lsi employed proposed merge strategies lsi shown effective retrieving documents language greek english requiring translation user query effective bible search product natural language searching queries lsi enables user form queries natural expressions user native language merging strategy proposed study enables lsi retrieve relevant documents effectively minimum database foreign language",0
"Key wordspiano music MIDI computer music etudes piano technique Liszt ","virtuoso pianism from the qwerty keyboard the electronic realization of liszts scores","For a decade or so, Liszt thrilled and astounded audiences at a time when virtuosity (often as an end in itself) was the norm and the piano had rapidly evolved into a form recognisable as a close relative of the instrument we know today. During this period Liszt frequently performed hisGrandes Etudes (1838), which he had developed from his boyhoodEtude en 12 exercices (1826) and which he later revised and technically simplified asEtudes d'Exécution transcendante (1851). Although Liszt's own performances cannot be recreated, procedures for generating electronic realizations, which contain nuances of balance and tempo, are described. All three versions of the eighth of Liszt's set of 12 studies are used for illustration. Contrary to received opinion, it is argued that the 1838 version is more satisfying than the 1851 revision and that this is due to its formal structure.","Computers and the Humanities",1995,"No","key wordspiano music midi computer music etudes piano technique liszt virtuoso pianism qwerty keyboard electronic realization liszts scores decade liszt thrilled astounded audiences time virtuosity end norm piano rapidly evolved form recognisable close relative instrument today period liszt frequently performed hisgrandes etudes developed boyhoodetude en exercices revised technically simplified asetudes cution transcendante liszt performances recreated procedures generating electronic realizations nuances balance tempo versions eighth liszt set studies illustration contrary received opinion argued version satisfying revision due formal structure",0
"Key wordsneural networks stylometric analysis Shakespeare Fletcher discrimination classification ","shakespeare vs fletcher a stylometric analysis by radial basis functions","In this paper we show, for the first time, how Radial Basis Function (RBF) network techniques can be used to explore questions surrounding authorship of historic documents. The paper illustrates the technical and practical aspects of RBF's, using data extracted from works written in the early 17th century by William Shakespeare and his contemporary John Fletcher. We also present benchmark comparisons with other standard techniques for contrast and comparison.","Computers and the Humanities",1995,"No","key wordsneural networks stylometric analysis shakespeare fletcher discrimination classification shakespeare fletcher stylometric analysis radial basis functions paper show time radial basis function rbf network techniques explore questions surrounding authorship historic documents paper illustrates technical practical aspects rbf data extracted works written early th century william shakespeare contemporary john fletcher present benchmark comparisons standard techniques contrast comparison",0
NA,"understanding hyper media required readings",NA,"Computers and the Humanities",1995,"No"," understanding hyper media required readings na",0
"Key wordsbibliographic records electronic texts electronic title page large corpora standard generalized markup language SGML text encoding initiative TEI ","practical considerations in the use of tei headers in a large corpus","Many aspects of the guidelines of the Text Encoding Initiative (TEI) are applicable to corpora and text collections, and to the texts that these contain. As the first large corpus developed using mark-up conforming to the guidelines, the British National Corpus (BNC) is a test-bed for many TEI-developed mechanisms. This is particularly true in the case of the TEI header, which has three intended applications — to describe a corpus, to describe an individual text, and as a free-standing bibliographic record — all of them used by the BNC. This paper describes the application of the TEI header to the BNC. It is intended that this information should, through a description of experience on a practical project, serve as a guide for those wishing to use TEI headers in the documentation and management of other corpora and collections of texts.","Computers and the Humanities",1995,"No","key wordsbibliographic records electronic texts electronic title page large corpora standard generalized markup language sgml text encoding initiative tei practical considerations tei headers large corpus aspects guidelines text encoding initiative tei applicable corpora text collections texts large corpus developed mark conforming guidelines british national corpus bnc test bed tei developed mechanisms true case tei header intended applications describe corpus describe individual text free standing bibliographic record bnc paper describes application tei header bnc intended information description experience practical project serve guide wishing tei headers documentation management corpora collections texts",0
"Key wordsTEI SGML text encoding spoken texts transcription temporal alignment ","the encoding of spoken texts","There is a great deal of variation in the encoding of spoken texts in electronic form, both with respect to the types of features represented and the way particular features are rendered. This paper surveys problems in the electronic representation of speech and presents the solutions proposed by the Text Encoding Initiative. The special tags needed for the encoding of spoken texts are discussed, including a mechanism for temporal alignment. Further work is needed on phonological aspects, parallel representation, and on the development of software which connects the systematic underlying representation with a workable format for input and display.","Computers and the Humanities",1995,"No","key wordstei sgml text encoding spoken texts transcription temporal alignment encoding spoken texts great deal variation encoding spoken texts electronic form respect types features represented features rendered paper surveys problems electronic representation speech presents solutions proposed text encoding initiative special tags needed encoding spoken texts discussed including mechanism temporal alignment work needed phonological aspects parallel representation development software connects systematic underlying representation workable format input display",0
"Key wordstext encoding initiative TEI header header electronic documentation cataloging SGML ","the tei header and the documentation of electronic texts","The article gives a general introduction to the form and function of the TEI header, points out some of the reasoning of the Text Documentation Committee that went into its design, and discusses some of its limitations. The TEI header's major strength is that it gives encoders the ability to document the electronic text itself, its source, its encoding principles, revisions, and characteristics of the text in an interchange format. Its bibliographical descriptions can be loaded into standard remote bibliographic databases, which should make electronic texts as easy to find for researchers as texts in other media, including print. Its major weakness is that it does not yet provide the ability for retrieval across texts in a networked environment, which users may want now or in the future.","Computers and the Humanities",1995,"No","key wordstext encoding initiative tei header header electronic documentation cataloging sgml tei header documentation electronic texts article general introduction form function tei header points reasoning text documentation committee design discusses limitations tei header major strength encoders ability document electronic text source encoding principles revisions characteristics text interchange format bibliographical descriptions loaded standard remote bibliographic databases make electronic texts easy find researchers texts media including print major weakness provide ability retrieval texts networked environment users future",0
"Key wordsTEI electronic texts text encoding encoding standards SGML tagging ","the tei history goals and future","This paper traces the history of the Text Encoding Initiative, through the Vassar Conference and the Poughkeepsie Principles to the publication, in May 1994, of theGuidelines for the Electronic Text Encoding and Interchange. The authors explain the types of questions that were raised, the attempts made to resolve them, the TEI project's aims, the general organization of the TEI committees, and they discuss the project's future.","Computers and the Humanities",1995,"No","key wordstei electronic texts text encoding encoding standards sgml tagging tei history goals future paper traces history text encoding initiative vassar conference poughkeepsie principles publication theguidelines electronic text encoding interchange authors explain types questions raised attempts made resolve tei project aims general organization tei committees discuss project future",0
"Key wordsTEI text encoding encoding schemes electronic text markup language tagging SGML ","the design of the tei encoding scheme","This paper discusses the basic design of the encoding scheme described by the Text Encoding Initiative'sGuidelines for Electronic Text Encoding and Interchange (TEI document number TEI P3, hereafter simplyP3 orthe Guidelines). It first reviews the basic design goals of the TEI project and their development during the course of the project. Next, it outlines some basic notions relevant for the design of any markup language and uses those notions to describe the basic structure of the TEI encoding scheme. It also describes briefly the “core” tag set defined in chapter 6 of P3, and the “default text structure” defined in chapter 7 of that work. The final section of the paper attempts an evaluation of P3 in the light of its original design goals, and outlines areas in which further work is still needed.","Computers and the Humanities",1995,"No","key wordstei text encoding encoding schemes electronic text markup language tagging sgml design tei encoding scheme paper discusses basic design encoding scheme text encoding initiativesguidelines electronic text encoding interchange tei document number tei p simplyp orthe guidelines reviews basic design goals tei project development project outlines basic notions relevant design markup language notions describe basic structure tei encoding scheme describes briefly core tag set defined chapter p default text structure defined chapter work final section paper attempts evaluation p light original design goals outlines areas work needed",0
"Key wordsTEI SGML linguistic tagging feature structure features lexical tagging interlinear text analysis ","a rationale for the tei recommendations for feature structure markup","In this paper, we concentrate on justifying the decisions we made in developing the TEI recommendations for feature structure markup. The first four sections of this paper present the justification for the recommended treatment of feature structures, of features and their values, and of combinations of features or values and of alternations and negations of features and their values. Section 5 departs briefly from the linguistic focus to argue that the markup scheme developed for feature structures is in fact a general-purpose mechanism that can be used for a wide range of applications. Section 6 describes an auxiliary document called a “feature system declaration” that is used to document and validate a system of feature-structure markup. The seventh and final section illustrates the use of the recommended markup scheme with two examples, lexical tagging and interlinear text analysis.","Computers and the Humanities",1995,"No","key wordstei sgml linguistic tagging feature structure features lexical tagging interlinear text analysis rationale tei recommendations feature structure markup paper concentrate justifying decisions made developing tei recommendations feature structure markup sections paper present justification recommended treatment feature structures features values combinations features values alternations negations features values section departs briefly linguistic focus argue markup scheme developed feature structures fact general purpose mechanism wide range applications section describes auxiliary document called feature system declaration document validate system feature structure markup seventh final section illustrates recommended markup scheme examples lexical tagging interlinear text analysis",0
"Key wordsSGML TEI text encoding history historical documents Sasines ","speaking with one voice encoding standards and the prospects for an integrated approach to computing in history","This paper focusses on the types of questions that are raised in the encoding of historical documents. Using the example of a 17th century Scottish Sasine, the authors show how TEI-based encoding can produce a text which will be of major value to a variety of future historical researchers. Firstly, they show how to produce a machine-readable transcription which would be comprehensible to a word-processor as a text stream filled with print and formatting instructions; to a text analysis package as compilation of named text segments of some known structure; and to a statistical package as a set of observations each of which comprises a number of defined and named variables. Secondly, they make provision for a machine-readable transcription where the encoder's research agenda and assumptions are reversible or alterable by secondary analysts who will have access to a maximum amount of information contained in the original source.","Computers and the Humanities",1995,"No","key wordssgml tei text encoding history historical documents sasines speaking voice encoding standards prospects integrated approach computing history paper focusses types questions raised encoding historical documents th century scottish sasine authors show tei based encoding produce text major variety future historical researchers firstly show produce machine readable transcription comprehensible word processor text stream filled print formatting instructions text analysis package compilation named text segments structure statistical package set observations comprises number defined named variables make provision machine readable transcription encoder research agenda assumptions reversible alterable secondary analysts access maximum amount information contained original source",0
"Key wordsdatabases in the humanities NIAM formalism ","making an information system for the humanities","The Documentation Project is a cooperative project between Faculties of Arts in the Norwegian universities. It aims to produce “the Norwegian universities' databases for language and culture” from the paper-based archives at the participating institutions. The project has been active on a national basis since 1992. This paper describes the methodologies involved and ongoing subprojects.","Computers and the Humanities",1994,"No","key wordsdatabases humanities niam formalism making information system humanities documentation project cooperative project faculties arts norwegian universities aims produce norwegian universities databases language culture paper based archives participating institutions project active national basis paper describes methodologies involved ongoing subprojects",0
"Key wordsmetaphors semantics netmet analogy ","netmet a program for generating and interpreting metaphors","Metaphors have computable semantics. A program called NETMET both generates metaphors and produces partial literal interpretations of metaphors. NETMET is based on Kittay's semantic field theory of metaphor and Black's interaction theory of metaphor. Input to NETMET consists of a list of literal propositions. NETMET creates metaphors by finding topic and source semantic fields, producing an analogical map from source to topic, then generating utterances in which terms in the source are identified with or predicated of terms in the topic. Given a metaphor, NETMET utilizes if-then rules to generate the implication complex of that metaphor. The literal leaves of the implication complex comprise a partial literal interpretation.","Computers and the Humanities",1994,"No","key wordsmetaphors semantics netmet analogy netmet program generating interpreting metaphors metaphors computable semantics program called netmet generates metaphors produces partial literal interpretations metaphors netmet based kittay semantic field theory metaphor black interaction theory metaphor input netmet consists list literal propositions netmet creates metaphors finding topic source semantic fields producing analogical map source topic generating utterances terms source identified predicated terms topic metaphor netmet utilizes rules generate implication complex metaphor literal leaves implication complex comprise partial literal interpretation",0
"Key Wordsstylometry authorship vocabulary model multivariate ","authorship attribution","This paper considers the problem of quantifying literary style and looks at several variables which may be used as stylistic “fingerprints” of a writer. A review of work done on the statistical analysis of “change over time” in literary style is then presented, followed by a look at a specific application area, the authorship of Biblical texts.","Computers and the Humanities",1994,"No","key wordsstylometry authorship vocabulary model multivariate authorship attribution paper considers problem quantifying literary style variables stylistic fingerprints writer review work statistical analysis change time literary style presented specific application area authorship biblical texts",0
"Key wordslanguage corpora corpus linguistics representativeness of corpora structure of corpora uses of corpora text encoding ","icame quo vadis reflections on the use of computer corpora in linguistics","The focus of the paper is on the use of computer corpora in language research. The historical background is touched on, with special reference to work within the International Computer Archive of Modern English (ICAME). Developments in the use of corpora are surveyed. Issues taken up include the representativeness and structure of corpora. Special attention is paid to pitfalls in the use of corpora. Corpus compilers must provide adequate documentation on the texts. Corpus users must know the corpus in order to evaluate whether it is appropriate for their research problem and in order to evaluate the results of their studies.","Computers and the Humanities",1994,"No","key wordslanguage corpora corpus linguistics representativeness corpora structure corpora corpora text encoding icame quo vadis reflections computer corpora linguistics focus paper computer corpora language research historical background touched special reference work international computer archive modern english icame developments corpora surveyed issues include representativeness structure corpora special attention paid pitfalls corpora corpus compilers provide adequate documentation texts corpus users corpus order evaluate research problem order evaluate results studies",0
"Key Wordsdatabase querying information retrieval linguistic databases french interrogatives ","a database for linguists intelligent querying and increase of data","What do database (DB) querying and information retrieval imply for linguistics? What are data for the linguist? How can one envisage efficient access to data? I propose that DB querying in language sciences be designed linguistically and directly determined by linguistic data. Linguists from various backgrounds could then use a consensual query tool. An implemented data-directed DB querying system, which was developed for research on French interrogative structures, is presented in detail.","Computers and the Humanities",1994,"No","key wordsdatabase querying information retrieval linguistic databases french interrogatives database linguists intelligent querying increase data database db querying information retrieval imply linguistics data linguist envisage efficient access data propose db querying language sciences designed linguistically directly determined linguistic data linguists backgrounds consensual query tool implemented data directed db querying system developed research french interrogative structures presented detail",0
"Key wordscritique empirical research literary computing reader response criticism textual databases women as readers women writers ","empirical literary research on women and readers","Both traditional and computerized scholars face problems when they attempt empirical research on women writers and women readers using currently available computational tools. This essay discusses some factors that have inhibited empirical research; it develops its examples from work in progress on 18th century English poetry and on reader responses. A number of large linguistic and text databases are almost useless for research on women writers because works by women are either not included or represented by easily accessible, rather than editorially clean, texts. Traditional and contemporary reader response studies are also insufficiently empirical for reasons of sexual bias or flaws in research design.","Computers and the Humanities",1994,"No","key wordscritique empirical research literary computing reader response criticism textual databases women readers women writers empirical literary research women readers traditional computerized scholars face problems attempt empirical research women writers women readers computational tools essay discusses factors inhibited empirical research develops examples work progress th century english poetry reader responses number large linguistic text databases useless research women writers works women included represented easily accessible editorially clean texts traditional contemporary reader response studies insufficiently empirical reasons sexual bias flaws research design",0
"Key Wordssyntax parsing statistical stylistics "," eyeball an interactive system for producing stylistic descriptions and comparisons","We have revised the mainframe EYEBALL, written in the early 1970s and used by several researchers during that decade, to run on microcomputers. This program parses English language texts and provides a statistical description of many linguistic features which are of interest to those involved in stylistics. In order to produce an analytical approach which goes beyond the simple reporting of linguistic data, we have selected a small number of features which characterize noticeable elements of the language used and we have written programs to facilitate statistical comparisons among texts. While EYEBALL is not automatic, the interactive parsing routines are elegant enough that the users spend most of their time confirming accurate guesses, rather than having to determine the structure of each phrase and clause. We hope that the program will be used by humanities scholars who have not previously been inclined to wrestle with the barriers of mainframe computing in the past.","Computers and the Humanities",1994,"No","key wordssyntax parsing statistical stylistics eyeball interactive system producing stylistic descriptions comparisons revised mainframe eyeball written early s researchers decade run microcomputers program parses english language texts statistical description linguistic features interest involved stylistics order produce analytical approach simple reporting linguistic data selected small number features characterize noticeable elements language written programs facilitate statistical comparisons texts eyeball automatic interactive parsing routines elegant users spend time confirming accurate guesses determine structure phrase clause hope program humanities scholars previously inclined wrestle barriers mainframe computing past",0
"Key wordsdatabases thematics Gide faits divers ","andr gides collection offaits divers","André Gide's collection offaits divers spans a period of fifty years and includes more than 650 documents. A database analysis offers an efficient means of handling such extensive material. Our goal is to trace statistically this author's varied interests as they evolved throughout his career.","Computers and the Humanities",1994,"No","key wordsdatabases thematics gide faits divers andr gides collection offaits divers andr gide collection offaits divers spans period fifty years includes documents database analysis offers efficient means handling extensive material goal trace statistically author varied interests evolved career",0
"Key WordsTACT literary analysis text retrieval imagery symbolism structural patterns prosody teaching text analysis ","the computer in literary analysis usingtact with students","TACT, a freeware program from the University of Toronto's Centre for Computing in the Humanities, is a highly sophisticated tool for text retrieval; although written for experienced critics and researchers, it can teach undergraduate students to read literature in new, fresh ways. Without requiring that the user become either a programmer, linguist, mathematician, or statistician,TACT introduces the literature student to the computer as a research tool. Studies of imagery and symbolism, of structural patterns, and of prosody can result from the student's careful tagging of a literary text and can yield significant insights into the work of literature. Students who use the computer as such a tool learn to read literary texts more closely and to think more clearly about literary problems.","Computers and the Humanities",1994,"No","key wordstact literary analysis text retrieval imagery symbolism structural patterns prosody teaching text analysis computer literary analysis usingtact students tact freeware program university toronto centre computing humanities highly sophisticated tool text retrieval written experienced critics researchers teach undergraduate students read literature fresh ways requiring user programmer linguist mathematician statisticiantact introduces literature student computer research tool studies imagery symbolism structural patterns prosody result student careful tagging literary text yield significant insights work literature students computer tool learn read literary texts closely literary problems",0
"Key wordscomputational lexicography natural language processing computational linguistics syntactic parsing lexicon dictionary machine tractable dictionary ","a full and efficient machine tractable dictionary for natural language processing a revised version of the cuvoald","A lexicon is an essential part of any natural language processing system. The size, content and format of the lexicon is crucial in determining the power and sophistication of a natural language processing system. However, a lexicon which provides comprehensive, consistent and accurate lexical information and which is in a format facilitating fast retrieval is not easily available. This paper reports on a project which aims at the development of such a lexicon. The resulting lexicon is actually the modified and extended version of the machine tractable version of the Oxford Advanced Learner's Dictionary. The modification and extension concentrate mainly on the aspects of comprehensiveness, consistency, explicitness, accuracy and the dictionary format. The modified and extended version is considered a desirable source of lexical information for any natural language processing system.","Computers and the Humanities",1994,"No","key wordscomputational lexicography natural language processing computational linguistics syntactic parsing lexicon dictionary machine tractable dictionary full efficient machine tractable dictionary natural language processing revised version cuvoald lexicon essential part natural language processing system size content format lexicon crucial determining power sophistication natural language processing system lexicon comprehensive consistent accurate lexical information format facilitating fast retrieval easily paper reports project aims development lexicon resulting lexicon modified extended version machine tractable version oxford advanced learner dictionary modification extension concentrate aspects comprehensiveness consistency explicitness accuracy dictionary format modified extended version considered desirable source lexical information natural language processing system",0
"Key Wordstroubadours seriation chronology ","musical chronology by seriation","The statistical process of seriation arranges a set of objects along a one-dimensional line so that the distance between each pair of objects reflects the dissimilarity between them. The process has recently been cast into an algorithm which makes it feasible to seriate a few tens of objects efficiently on a personal computer. This algorithm is now used for establishing a chronology of the Troubadours according to certain melodic features in their works. Other musicological uses for seriation are proposed.","Computers and the Humanities",1994,"No","key wordstroubadours seriation chronology musical chronology seriation statistical process seriation arranges set objects dimensional line distance pair objects reflects dissimilarity process recently cast algorithm makes feasible seriate tens objects efficiently personal computer algorithm establishing chronology troubadours melodic features works musicological seriation proposed",0
"KeywordsComputational Linguistic Technical Review ","technical reviews",NA,"Computers and the Humanities",1994,"No"," computational linguistic technical review technical reviews na",0
"Key WordsEnglish romantic renaissance drama tragedy computational stylistics function-words multivariate statistics ","lyrical drama and the turbid mountebanks styles of dialogue in romantic and renaissance tragedy","Critics have condemned English Romantic tragedies as a series of poor imitations of Renaissance tragedy. This paper tests such “literary-critical” questions through statistical comparisons of ten plays from each group. The measures chosen give evidence of a strong and consistent difference between the groups, going beyond historical changes in the language. The Romantic tragedies are more expository; the Renaissance ones include more commonplace interactions between characters. The later plays do not show the marked variations in function-word frequencies of their predecessors. Of the Renaissance plays, Shakespeare's show the closest affinity to the Romantic tragedies, and the most telling contrasts.","Computers and the Humanities",1994,"No","key wordsenglish romantic renaissance drama tragedy computational stylistics function words multivariate statistics lyrical drama turbid mountebanks styles dialogue romantic renaissance tragedy critics condemned english romantic tragedies series poor imitations renaissance tragedy paper tests literary critical questions statistical comparisons ten plays group measures chosen give evidence strong consistent difference groups historical language romantic tragedies expository renaissance include commonplace interactions characters plays show marked variations function word frequencies predecessors renaissance plays shakespeare show closest affinity romantic tragedies telling contrasts",0
"Key wordssound symbolism alliteration Japanese poetry Kokinshû poetics Icon programming language database software ","a computer study of systematic sound symbolism in classical japanese verse","The analysis of sound symbolism in poetry is one of the more promising applications of computational methods. This paper proposes using database software with spreadsheet capabilities to give maximum versatility in the examination of consonant alliteration. In this case the database is drawn from a 10th century anthology of classical Japanese verse called theKokinshû. In recent years scholars have pointed out a few obvious examples of sound symbolism inKokinshû poetry. This study attempts to show that with these few notable exceptions, poetry of the period seems to have striven toward a balance in sound, avoiding techniques such as word initial alliteration which might call attention to itself.","Computers and the Humanities",1994,"No","key wordssound symbolism alliteration japanese poetry kokinsh poetics icon programming language database software computer study systematic sound symbolism classical japanese verse analysis sound symbolism poetry promising applications computational methods paper proposes database software spreadsheet capabilities give maximum versatility examination consonant alliteration case database drawn th century anthology classical japanese verse called thekokinsh recent years scholars pointed obvious examples sound symbolism inkokinsh poetry study attempts show notable exceptions poetry period striven balance sound avoiding techniques word initial alliteration call attention ",0
"Key wordscomputer-based learning foreign language hypermedia applications interactive video instructional technology Latin American culture Spanish language ","metacognitive learning techniques in the user interface advance organizers and captioning","The use of metacognitive strategies of learning and instruction such as content abstracts or previews, subtitles and captioning (on-screen foreign language subtitles) have been recurrent pedagogical tools for facilitating foreign language (L2) instruction. New technology has broadened their scope and multiplied the ways in which they can be used in L2 computer-based applications. A pilot test was carried out using a hypermedia instructional application for Spanish: “Operación Futuro.” The test addresses the question of how two types of metacognitive strategies, written and spoken Advance Organizers (AOs) and verbatim Captioning (CP) may facilitate L2 comprehension and recall.","Computers and the Humanities",1994,"No","key wordscomputer based learning foreign language hypermedia applications interactive video instructional technology latin american culture spanish language metacognitive learning techniques user interface advance organizers captioning metacognitive strategies learning instruction content abstracts previews subtitles captioning screen foreign language subtitles recurrent pedagogical tools facilitating foreign language l instruction technology broadened scope multiplied ways l computer based applications pilot test carried hypermedia instructional application spanish operaci futuro test addresses question types metacognitive strategies written spoken advance organizers aos verbatim captioning cp facilitate l comprehension recall",0
"Key Wordsquantitative studies textual analysis ARTFL Gobineau themes Stendhal André Gide literary criticism text encoding quantitative methods reception studies Riffaterre ","an argument for single author and similar studies using quantitative methods is there safety in numbers","Lack of a critical mass of scholars involved with the computer-assisted analysis of texts (CAAT), coupled with insufficient communication among various sectors of the literary and linguistic disciplines, has led to a skewed notion of computing humanists' work among their colleagues. This paper highlights the gap through examples of misunderstood humanist needs and achievements drawn from both recent media reports and humanities conferences. It suggests that networking and less modesty in manuscript submission can be at least partial solutions. The author cites some of his own published work and work-in-progress on Stendhal and Gobineau in refuting Mark Olsen's thesis that the dominance of single- or dual-author studies must be the cause of CAAT's “failure” to make significant inroads in mainstream literary journals. The author builds a case for the use of both diachronic and synchronic lexico-statistical data in carrying out such studies successfully. He recommends a new “Synthetic Criticism” where relevant quantitative methods would not be absent.","Computers and the Humanities",1993,"No","key wordsquantitative studies textual analysis artfl gobineau themes stendhal andr gide literary criticism text encoding quantitative methods reception studies riffaterre argument single author similar studies quantitative methods safety numbers lack critical mass scholars involved computer assisted analysis texts caat coupled insufficient communication sectors literary linguistic disciplines led skewed notion computing humanists work colleagues paper highlights gap examples misunderstood humanist achievements drawn recent media reports humanities conferences suggests networking modesty manuscript submission partial solutions author cites published work work progress stendhal gobineau refuting mark olsen thesis dominance single dual author studies caat failure make significant inroads mainstream literary journals author builds case diachronic synchronic lexico statistical data carrying studies successfully recommends synthetic criticism relevant quantitative methods absent",0
"Key Wordstext intertextuality discourse interdiscursivity literary theory chaos theory conceptual convergence ","towards the implementation of text and discourse theory in computer assisted textual analysis","Humanities computing (HC) has failed to integrate into its practices many of the key theoretical elements of contemporary text and discourse theory. This has in turn contributed to the marginalization of HC in research and teaching. Outdated theoretical models must be abandoned in order to develop a critical discourse based on the insights of HC. HC projects remain far too attached to micro-analyses and have not developed the theoretical and methodological tools necessary to undertake systemic macro-analyses on the level of discourse. Given that texts are a mixture of determinate and dynamic systems, recent developments in chaos theory may be of help in modelling the interrelationship of these elements at discourse level.","Computers and the Humanities",1993,"No","key wordstext intertextuality discourse interdiscursivity literary theory chaos theory conceptual convergence implementation text discourse theory computer assisted textual analysis humanities computing hc failed integrate practices key theoretical elements contemporary text discourse theory turn contributed marginalization hc research teaching outdated theoretical models abandoned order develop critical discourse based insights hc hc projects remain attached micro analyses developed theoretical methodological tools undertake systemic macro analyses level discourse texts mixture determinate dynamic systems recent developments chaos theory modelling interrelationship elements discourse level",0
"Key WordsMusical database quality control melodic progression Gregorian Chant German folksong ","on the accuracy of musical data with examples from gregorian chant and german folksong","Attention is drawn to the need for controlling (during encoding) and checking (after encoding) the quality or accuracy of musical data. Some large databases of melodies are now becoming available, and methods of control and checking are presented which are specially suited to these. Two applications are discussed in detail: to Gregorian Chant and to German folksong. An effective method in tonal and modal music is found to be the investigation of melodic progressions which remain unusual even after amalgamation by transposition to a central register.","Computers and the Humanities",1993,"No","key wordsmusical database quality control melodic progression gregorian chant german folksong accuracy musical data examples gregorian chant german folksong attention drawn controlling encoding checking encoding quality accuracy musical data large databases melodies methods control checking presented specially suited applications discussed detail gregorian chant german folksong effective method tonal modal music found investigation melodic progressions remain unusual amalgamation transposition central register",0
"Key wordsinterpretive theory meaning computer-assisted research stylistics style cognitive theory private language feminist language theory ","have it your way and mine the theory of styles","Olsen is right to note what can be done with a good theory and the right machine. His particular theory, however, is not transferable to literary studies. If we need a new model, I would suggest that cognitive science can provide a few interesting ones. I have begun to do some work based on David Marr's VISION, in which he hypothesizes two levels of processing within the visual module. My speculation has been on the parallel existence of distinguishable levels of conceptual or language organization which would correspond to the viewer and object centered perspectives Marr describes for vision. I propose to explore the possibility that we may find here the model for the existence of stylistic individualism within overarching historical stylistic generalizations, and even more, that this may be what feminists are searching for when they try to resist being coopted by the masculine language of objectivity.","Computers and the Humanities",1993,"No","key wordsinterpretive theory meaning computer assisted research stylistics style cognitive theory private language feminist language theory mine theory styles olsen note good theory machine theory transferable literary studies model suggest cognitive science provide interesting begun work based david marr vision hypothesizes levels processing visual module speculation parallel existence distinguishable levels conceptual language organization correspond viewer object centered perspectives marr describes vision propose explore possibility find model existence stylistic individualism overarching historical stylistic generalizations feminists searching resist coopted masculine language objectivity",0
"Key Wordsdictionary changing language literary criticism PAT system OED computer research formalist deviation statistics ","literary texts and the state of the language the role of the computer","We should follow Mark Olsen's lead and think with maximum ambition of the role of the computer in supporting literary research of the highest order. Thus the computer enables us to answer one of the great questions of literary criticism: how does a given writer contribute to the changing language? We can now chart the influence of given writers by correlating their words and phrasing with computerized dictionaries so as to produce profiles and histories of the way words have entered the language.","Computers and the Humanities",1993,"No","key wordsdictionary changing language literary criticism pat system oed computer research formalist deviation statistics literary texts state language role computer follow mark olsen lead maximum ambition role computer supporting literary research highest order computer enables answer great questions literary criticism writer contribute changing language chart influence writers correlating words phrasing computerized dictionaries produce profiles histories words entered language",0
"Key Wordscognition semantics riddles fuzzy sets language systems ideation ","the surface of language and humanities computing","Recent articles have noted that humanities computing techniques and methodologies remain marginal to mainstream literary scholarship. Mark Olsen's paper discusses this phenomenon and argues for large scale analyses of text databases that would incorporate a shift in theoretical orientation to include greater stress on intertextuality and sign theory. Part of Olsen's argument revolves on the need to move away from the syntactic and overt grammatical elements of textual language to more subtle semantics and meaning systems. While provocative and important, Olsen's stance remains rooted in literary theoretical constructs. Another level of language, the cognitive, offers equally interesting challenges for humanities computing, though the paradigms for this type of computer-based exploration are derived from disciplines traditionally removed from the humanities. The riddle, a nearly universal genre, offers a window onto some of the cognitive processes involved in deep level language function. By analyzing the riddling process, different methods of computational modelling can be inferred, suggesting new avenues for computing in the humanities.","Computers and the Humanities",1993,"No","key wordscognition semantics riddles fuzzy sets language systems ideation surface language humanities computing recent articles noted humanities computing techniques methodologies remain marginal mainstream literary scholarship mark olsen paper discusses phenomenon argues large scale analyses text databases incorporate shift theoretical orientation include greater stress intertextuality sign theory part olsen argument revolves move syntactic overt grammatical elements textual language subtle semantics meaning systems provocative important olsen stance remains rooted literary theoretical constructs level language cognitive offers equally interesting challenges humanities computing paradigms type computer based exploration derived disciplines traditionally removed humanities riddle universal genre offers window cognitive processes involved deep level language function analyzing riddling process methods computational modelling inferred suggesting avenues computing humanities",0
"Key WordsDuras narrative focalization stylostatistics text topography ","toward a narra topography a pilot study applied to marguerite duras novelmoderato cantabile","Although lexical frequencies are familiar measures of stylistic and thematic analysis, only recently have some stylostatisticians been tempted to investigate the relationship between the frequency and topography of repeated lexical items. In the present paper the authors have turned to the study of the four focal types of discursive narratology, using Marguerite Duras'Moderato Cantabile. Their intent is to uncover aspects of narratological performance which further elucidate the communicative strategies in the story. Part 1 summarizes the problematic between frequency and topography. It describes how a topographical index can be computed for any repeated item and how a Global Topography Index (GTI) can summarize the major topographical characteristics of any text sequence. Part 2 presents a four-cell typology of narrational mode: a segmentation of the verbal chain into narrating and narrated speech acts, with each text sequence tagged according to its discursive function: overt sender intervention for story coherence or comment on the focal level of a narrating present; representation of discrete or unlocalized events on the focal level of a mimeticized past. In Part 3 the focal encodings are displayed in numerical and graphic form, first according to the eight surface chapter divisions and then according to twenty-six subsets of approximately equal length. The fluctuations of the topography indices are reviewed, with particular attention being paid to the manifestation of cluster effects. Although sender interventions predominate, the relativized behavior of each focal type contributes to a climactic unraveling of the intrigue in the final chapters. In conclusion, the authors stress the dichotomy between the calm surface of the chapters and the agitated tensions of the twenty-six subsets.","Computers and the Humanities",1993,"No","key wordsduras narrative focalization stylostatistics text topography narra topography pilot study applied marguerite duras novelmoderato cantabile lexical frequencies familiar measures stylistic thematic analysis recently stylostatisticians tempted investigate relationship frequency topography repeated lexical items present paper authors turned study focal types discursive narratology marguerite durasmoderato cantabile intent uncover aspects narratological performance elucidate communicative strategies story part summarizes problematic frequency topography describes topographical index computed repeated item global topography index gti summarize major topographical characteristics text sequence part presents cell typology narrational mode segmentation verbal chain narrating narrated speech acts text sequence tagged discursive function overt sender intervention story coherence comment focal level narrating present representation discrete unlocalized events focal level mimeticized past part focal encodings displayed numerical graphic form surface chapter divisions twenty subsets approximately equal length fluctuations topography indices reviewed attention paid manifestation cluster effects sender interventions predominate relativized behavior focal type contributes climactic unraveling intrigue final chapters conclusion authors stress dichotomy calm surface chapters agitated tensions twenty subsets",0
"Key Wordsintertextuality Old French scribal practice manuscript traditions Marie de France Chrétien de Troyes ","intertextuality and large corpora a medievalist approach","This paper concurs with Mark Olsen's premise that computer-aided literature studies should take a different direction, one that is more suited to the computer's strength in analyzing large corpora of texts. However, the authors take issue with his conclusion that a reorientation of the notions of textual analysis is necessary in order to exploit the computer's capabilities. Contemporary medieval studies already provides us with models of textual analysis which are well suited to computer development. Though they stem from the particularities of medieval textual production, these models can perhaps be useful in the study of modern literatures.","Computers and the Humanities",1993,"No","key wordsintertextuality french scribal practice manuscript traditions marie de france chr tien de troyes intertextuality large corpora medievalist approach paper concurs mark olsen premise computer aided literature studies direction suited computer strength analyzing large corpora texts authors issue conclusion reorientation notions textual analysis order exploit computer capabilities contemporary medieval studies models textual analysis suited computer development stem particularities medieval textual production models study modern literatures",0
"Key Wordsoptical character recognition scanning off-shore keyboarding efficiency ARTFL cost analysis ","optical character scanning a discussion of efficiency and politics","Optical Character Recognition is shown to be significantly more expensive than keyboarding, using off-shore contractors, for entry of large amounts of text where high accuracy is required. Using large test samples in French and English, the paper indicates that OCR applications which require significant post-scan editing are labor intensive projects that can be accomplished more efficiently by keyboarding. Most OCR systems are still not capable of entering large amounts of text accurately enough to avoid an expensive editing step.","Computers and the Humanities",1993,"No","key wordsoptical character recognition scanning shore keyboarding efficiency artfl cost analysis optical character scanning discussion efficiency politics optical character recognition shown significantly expensive keyboarding shore contractors entry large amounts text high accuracy required large test samples french english paper ocr applications require significant post scan editing labor intensive projects accomplished efficiently keyboarding ocr systems capable entering large amounts text accurately avoid expensive editing step",0
"Key wordstext as vision space dimensions correspondence analysis concordances contexts Kierkegaard ","the multi dimensional concordance a new tool for literary research","This paper describes the use of correspondence analysis to create the “space” of a book, constructs that of Kierkegaard'sFear and Trembling as an illustration, and distinguishes three separate contexts of some of its most important words: thespatial context (where the search word lies in that named and ordered space); theoverall context (the x words closest to the search word in multi-dimensional space); and the “role/sense” context (the words associated with the search word in each of its most important roles, some of which may represent new senses.) It describes the identification of these contexts, discusses their importance and concludes by noting certain respects in which the procedure might perhaps be improved.","Computers and the Humanities",1993,"No","key wordstext vision space dimensions correspondence analysis concordances contexts kierkegaard multi dimensional concordance tool literary research paper describes correspondence analysis create space book constructs kierkegaardsfear trembling illustration distinguishes separate contexts important words thespatial context search word lies named ordered space theoverall context words closest search word multi dimensional space rolesense context words search word important roles represent senses describes identification contexts discusses importance concludes noting respects procedure improved",0
"Key WordsCéline computer-aided literary analysis concordances ","computerizing cline","This article uses recent work on the computer-aided analysis of texts by the French writer Céline as a framework to discuss Olsen's paper on the current state of computer-aided literary analysis. Drawing on analysis of syntactic structures, lexical creativity and use of proper names, it makes two points: (1) given a rich theoretical framework and sufficiently precise models, even simple computer tools such as text editors and concordances can make a valuable contribution to literary scholarship; (2) it is important to view the computer not as a device for finding what we as readers have failed to notice, but rather as a means of focussing more closely on what we have already felt as readers, and of verifying hypotheses we have produced as researchers.","Computers and the Humanities",1993,"No","key wordsc line computer aided literary analysis concordances computerizing cline article recent work computer aided analysis texts french writer line framework discuss olsen paper current state computer aided literary analysis drawing analysis syntactic structures lexical creativity proper names makes points rich theoretical framework sufficiently precise models simple computer tools text editors concordances make valuable contribution literary scholarship important view computer device finding readers failed notice means focussing closely felt readers verifying hypotheses produced researchers",0
"Key WordsMallarmé object on-line poetry semantics semiotics semiology sign text analysis truth ","texts on line","The study of signs is divided between those scholars who use the Saussurian binary sign (semiology) and those who prefer Charles Peirce's tripartite sign (semiotics). The common view of the opposition between the two types of signs does not take into consideration the methodological conditions of applicability of these two types of signs. This is particularly important in the field of literary studies and hence for the preparation of electronic programs for text analysis. The Peircian sign explicitly entails the discovery of a truth of meaning that claims to be universal and not reducible to a collection of opinions based on fragmented information; it also imposes the task of elucidating a transhistorical and universal significantion encoded in a text. Contrary to Peirce's view of the sign, our use of computer programs for text analysis, however, demonstrates that we implicitly treat every literary text as a set of linguistic data (letters, phonemes, syntagmatic segments, etc.) which are reducible to units that can be treated separately. A brief comparison of the results obtained from computer analyses of the French poet Stéphane Mallarmé's text, “Le Cygne,” with those obtained from two Peircian analyses (by Riffaterre and Champigny) of the same text demonstrates that our current methods of computer textual analysis are based on a Saussurian semiology, which is unidimensional and limited, and that these methods are still quite unable to produce a semiotic interpretation based on a totalizing hierarchy of the text's various discursive components.","Computers and the Humanities",1993,"No","key wordsmallarm object line poetry semantics semiotics semiology sign text analysis truth texts line study signs divided scholars saussurian binary sign semiology prefer charles peirce tripartite sign semiotics common view opposition types signs consideration methodological conditions applicability types signs important field literary studies preparation electronic programs text analysis peircian sign explicitly entails discovery truth meaning claims universal reducible collection opinions based fragmented information imposes task elucidating transhistorical universal significantion encoded text contrary peirce view sign computer programs text analysis demonstrates implicitly treat literary text set linguistic data letters phonemes syntagmatic segments reducible units treated separately comparison results obtained computer analyses french poet st phane mallarm text le cygne obtained peircian analyses riffaterre champigny text demonstrates current methods computer textual analysis based saussurian semiology unidimensional limited methods unable produce semiotic interpretation based totalizing hierarchy text discursive components",0
"Key wordspoetry meter rhythm linguistics alexandrine Cornulier Molière Racine Corneille ","toward a theory of rhythm in french poetry computer assisted recognition of rhythmic groups in traditional isometrical alexandrines","Benoît de Cornulier's writings on French poetry concentrate on metrical boundaries, or caesura; however, the the criteria upon which he bases his analyses are useful in studying rhythm, or the relationship between syllables within the alexandrine's twohémistiches. This study focuses on three aspects of rhythm in French poetry: the definition of rhythm following Cornulier; the development of a method using the computer to detect rhythmic patterns in traditional isometrical alexandrines; the results of such a study when applied to three classical seventeenth-century plays which are composed of isometrical alexandrines (Corneille'sPolyeucte, Racine'sPhèdre, and Molière'sLe Tartuffe).","Computers and the Humanities",1993,"No","key wordspoetry meter rhythm linguistics alexandrine cornulier moli racine corneille theory rhythm french poetry computer assisted recognition rhythmic groups traditional isometrical alexandrines beno de cornulier writings french poetry concentrate metrical boundaries caesura criteria bases analyses studying rhythm relationship syllables alexandrine twoh mistiches study focuses aspects rhythm french poetry definition rhythm cornulier development method computer detect rhythmic patterns traditional isometrical alexandrines results study applied classical seventeenth century plays composed isometrical alexandrines corneillespolyeucte racinesph dre moli sle tartuffe",0
"Key WordsEddington humanistic speculation letter frequencies Etaoin Shrdlu cryptology digrams trigrams tries data structures ","an infinite order solution to the eddington problem or getting monkeys to type shakespeare","Could a troupe of monkeys really produce Shakespeare if allowed to bang away at the word processor long enough? This age old question, commonly referred to as the Eddington problem, relates to fundamental issues of probability, and is examined in this article in a new light. Based on earlier research by the physicist William Bennett, Jr., the author describes a data structure which enables the computer to simulate the hypothetical monkeys. Exploiting principles of cryptology, the computer leads the simulated monkeys closer to their goal. Though the intent of the article is to encourage humanistic speculation, the final result proves to be quite practical and may come as a surprise to computer scientists and humanists alike.","Computers and the Humanities",1993,"No","key wordseddington humanistic speculation letter frequencies etaoin shrdlu cryptology digrams trigrams data structures infinite order solution eddington problem monkeys type shakespeare troupe monkeys produce shakespeare allowed bang word processor long age question commonly referred eddington problem relates fundamental issues probability examined article light based earlier research physicist william bennett jr author describes data structure enables computer simulate hypothetical monkeys exploiting principles cryptology computer leads simulated monkeys closer goal intent article encourage humanistic speculation final result proves practical surprise computer scientists humanists alike",0
"Key Wordsnovel theory narrative statistics Genett Sartre ","babies bathwater and the study of literature","Although many scholars in literature currently seem mainly interested in theory, the focus on literary texts is what defines literature studies. Computer technology and the statistical methods it fosters are applicable to both the theoretical and to the interpretative issues which scholars of literature habitually address. Genette's distinction between the homodiegetic and the autodiegetic perspective in first-person narrative can be confirmed statistically. Roquentin's loneliness inLa nausée can be shown to be a formal characteristic of the type of novel he narrates, thus validating his commentary on his society. The computer can be used to deal with standard literary questions in a principled fashion, and a new orientation of literature studies on a cultural history model, which Mark Olsen recommends, is not necessary.","Computers and the Humanities",1993,"No","key wordsnovel theory narrative statistics genett sartre babies bathwater study literature scholars literature interested theory focus literary texts defines literature studies computer technology statistical methods fosters applicable theoretical interpretative issues scholars literature habitually address genette distinction homodiegetic autodiegetic perspective person narrative confirmed statistically roquentin loneliness inla naus shown formal characteristic type narrates validating commentary society computer deal standard literary questions principled fashion orientation literature studies cultural history model mark olsen recommends ",0
"KeywordsComputational Linguistic ","courseware review",NA,"Computers and the Humanities",1993,"No"," computational linguistic courseware review na",0
"Key WordsColumbus Las Casas style Spanish literature dual authorship ","the two authors of columbusdiary","Although Columbus'Diary of the first voyage to America as we know it is largely a transcription of the original diary carried out by Bartolomé de las Casas, commentators and readers often treat it as if it were Columbus' work alone. Editions published to date do not separate the explorer's narrative from that of his transcriber or editor. Since style can influence readers' perceptions of a writer's personality, it is important to determine characteristics of writing attributed to Columbus that may pertain instead to his trascriber. This study employs the computer to explore the style of Las Casas and that of Columbus. Differences in the writing of each “author” emerge with computer assistance by isolating Columbus' words from those of his transcriber and analyzing selected features of vocabulary, sentence length, and syntax.1","Computers and the Humanities",1993,"No","key wordscolumbus las casas style spanish literature dual authorship authors columbusdiary columbusdiary voyage america largely transcription original diary carried bartolom de las casas commentators readers treat columbus work editions published date separate explorer narrative transcriber editor style influence readers perceptions writer personality important determine characteristics writing attributed columbus pertain trascriber study employs computer explore style las casas columbus differences writing author emerge computer assistance isolating columbus words transcriber analyzing selected features vocabulary sentence length syntax",0
"Key Wordsconceptual analysis knowledge representation knowledge engineering semantic networks text analysis ","frame based representation of philosophical systems using a knowledge engineering tool","This article addresses the methodological problem of the non-linear representation of philosophical systems in a computerized knowledge base. It is a problem of knowledge representation as defined in the field of artificial intelligence. Instead of a purely theoretical discussion of the issue, we present selected results of a practical experiment which has in itself some theoretical significance. We show how one can represent different philosophies using CODE, a knowledge engineering system developed by artificial intelligence researchers. The hypothesis is that such a computer based representation of philosophical systems can give insight into their conceptual structure. We argue that computer aided text analysis can apply knowledge representation tools and techniques developed in artificial intelligence and we estimate how philosophers as well as knowledge engineers could gain from this cross-fertilization. This paper should be considered as an experiment report on the use of knowledge representation techniques in computer aided text analysis. It is part of a much broader project on the representation of conceptual structures in an expert system. However, we intentionally avoided technical issues related to either Computer Science or History of Philosophy to focus on the benefit to enhance traditional humanistic studies with tools and methods developed in AI on the one hand and the need to develop more appropriate tools on the other.","Computers and the Humanities",1993,"No","key wordsconceptual analysis knowledge representation knowledge engineering semantic networks text analysis frame based representation philosophical systems knowledge engineering tool article addresses methodological problem linear representation philosophical systems computerized knowledge base problem knowledge representation defined field artificial intelligence purely theoretical discussion issue present selected results practical experiment theoretical significance show represent philosophies code knowledge engineering system developed artificial intelligence researchers hypothesis computer based representation philosophical systems give insight conceptual structure argue computer aided text analysis apply knowledge representation tools techniques developed artificial intelligence estimate philosophers knowledge engineers gain cross fertilization paper considered experiment report knowledge representation techniques computer aided text analysis part broader project representation conceptual structures expert system intentionally avoided technical issues related computer science history philosophy focus benefit enhance traditional humanistic studies tools methods developed ai hand develop tools ",0
"Key Wordsarchives electronic records electronic office systems historical research hypertext HYTIME linked documents ODA SGML tagging TEI textual records textual analysis ","electronically generated records and twentieth century history","The electronic generation of documents in modern offices will trasform the nature of archives, and also the techniques of historical research. Although considerable attention has been directed to developing research methodologies for social and economic history using computerized numeric data, almost no attention has been paid to the impact of machine readable textual records on historical writing. This article considers the advantages and disadvantages for the historian of the shift from paper records to electronic documents, and suggests a number of approaches to historical research made possible by the new technology.","Computers and the Humanities",1993,"No","key wordsarchives electronic records electronic office systems historical research hypertext hytime linked documents oda sgml tagging tei textual records textual analysis electronically generated records twentieth century history electronic generation documents modern offices trasform nature archives techniques historical research considerable attention directed developing research methodologies social economic history computerized numeric data attention paid impact machine readable textual records historical writing article considers advantages disadvantages historian shift paper records electronic documents suggests number approaches historical research made technology",0
"Key wordscomputer-aided literature studies literature literary theory structuralism ","signs symbols and discourses a new direction for computer aided literature studies","Computer-aided literature studies have failed to have a significant impact on the field as a whole. This failure is traced to a concentration on how a text achieves its literary effect by the examination of subtle semantic or grammatical structures in single texts or the works of individual authors. Computer systems have proven to be very poorly suited to such refined analysis of complex language. Adopting such traditional objects of study has tended to discourage researchers from using the tool to ask questions to which it is better adapted, the examination of large amounts of simple linguistic features. Theoreticians such as Barthes, Foucault and Halliday show the importance of determining the linguistic and semantic characteristics of the language used by the author and her/his audience. Current technology, and databases like the TLG or ARTFL, facilitate such wide-spectrum analyses. Computer-aided methods are thus capable of opening up new areas of study, which can potentially transform the way in which literature is studied.","Computers and the Humanities",1993,"No","key wordscomputer aided literature studies literature literary theory structuralism signs symbols discourses direction computer aided literature studies computer aided literature studies failed significant impact field failure traced concentration text achieves literary effect examination subtle semantic grammatical structures single texts works individual authors computer systems proven poorly suited refined analysis complex language adopting traditional objects study tended discourage researchers tool questions adapted examination large amounts simple linguistic features theoreticians barthes foucault halliday show importance determining linguistic semantic characteristics language author audience current technology databases tlg artfl facilitate wide spectrum analyses computer aided methods capable opening areas study potentially transform literature studied",0
"Key Wordsdictionary lexicography word senses polysemy homonymy corpus ","dictionary word sense distinctions an enquiry into their nature","The word senses in a published dictionary are a valuable resource for natural language processing and textual criticism alike. In order that they can be further exploited, their nature must be better understood. Lexicographers have always had to decide where to say a word has one sense, where two. The two studies described here look into their grounds for making distinctions. The first develops a classification scheme to describe the commonly occurring distinction types. The second examines the task of matching the usages of a word from a corpus with the senses a dictionary provides. Finally, a view of the ontological status of dictionary word senses is presented.","Computers and the Humanities",1992,"No","key wordsdictionary lexicography word senses polysemy homonymy corpus dictionary word sense distinctions enquiry nature word senses published dictionary valuable resource natural language processing textual criticism alike order exploited nature understood lexicographers decide word sense studies grounds making distinctions develops classification scheme describe commonly occurring distinction types examines task matching usages word corpus senses dictionary finally view ontological status dictionary word senses presented",0
"Key wordsidiolect genderlect stylistics women's language essay Mexican literature Spanish feminist criticism ","a computer assisted investigation of gender related idiolect in octavio paz and rosario castellanos","This study reports on computer-aided investigation of salient differences in the essay idiolects of the Mexican writers Octavio Paz and Rosario Castellanos and suggests that some of them may be linked to gender. It describes use of ready-made software and computational strategies requiring no tagging and minimal ocular scan. It suggests some parameters that can be searched and in most cases quantified to explore characteristics posited by linguistic and literary scholars, taking into consideration the particular language and culture of the authors.","Computers and the Humanities",1992,"No","key wordsidiolect genderlect stylistics women language essay mexican literature spanish feminist criticism computer assisted investigation gender related idiolect octavio paz rosario castellanos study reports computer aided investigation salient differences essay idiolects mexican writers octavio paz rosario castellanos suggests linked gender describes ready made software computational strategies requiring tagging minimal ocular scan suggests parameters searched cases quantified explore characteristics posited linguistic literary scholars taking consideration language culture authors",0
"Key Wordslinguistics language instruction CALL evaluation ","developing and evaluating language courseware","The paper sets out twenty proposals for the development and evaluation of Computer Assisted Language Learning (CALL) programs. These proposals emerge from special characteristics of language instruction and of the use of computers to assist in language instruction. We combine theoretically-based assumptions with empirical findings drawn from investigation of language courseware for Hebrew speakers in Israel. We first list four unique features of language instruction: (1) the object-language-meta-language distinction; (2) computer as written medium vs. language as primary spoken medium; (3) teaching of second language skills vs. linguistics; (4) the computer as an electronic tool vs. the computer as a cognitive entity simulating the speaker. We then show how these unique characteristics of language instruction (mother-tongue and foreign language) impose special proposals on language courseware. These proposals should be observed in the development of language courseware and in the evaluation of such programs. Clearly, these proposals integrate with general courseware proposals.","Computers and the Humanities",1992,"No","key wordslinguistics language instruction call evaluation developing evaluating language courseware paper sets twenty proposals development evaluation computer assisted language learning call programs proposals emerge special characteristics language instruction computers assist language instruction combine theoretically based assumptions empirical findings drawn investigation language courseware hebrew speakers israel list unique features language instruction object language meta language distinction computer written medium language primary spoken medium teaching language skills linguistics computer electronic tool computer cognitive entity simulating speaker show unique characteristics language instruction mother tongue foreign language impose special proposals language courseware proposals observed development language courseware evaluation programs proposals integrate general courseware proposals",0
"Key Wordslexis collocations lexical collocations statistics Xtract language generation machine translation ","xtract an overview","Lexical collocations have particular statistical distributions. We have developed a set of statistical techniques for retrieving and identifying collocations from large textual corpora. The techniques we developed are able to identify collocations of arbitrary length as well as flexible collocations. These techniques have been implemented in a lexicographic tool, Xtract, which is able to automatically acquire collocations with high retrieval performance. Xtract works in three stages. The first stage is based on a statistical technique for identifying word pairs involved in a syntactic relation. The words can appear in the text in any order and can be separated by an arbitrary number of other words. The second stage is based on a technique to extract n-word collocations (or n-grams) in a much simpler way than related methods. These collocations can involve closed class words such as particles and prepositions. A third stage is then applied to the output of stage one and applies parsing techniques to sentences involving a given word pair in order to identify the proper syntactic relation between the two words. A secondary effect of the third stage is to filter out a number of candidate collocations as irrelevant and thus produce higher quality output. In this paper we present an overview of Xtract and we describe several uses for Xtract and the knowledge it retrieves such as language generation and machine translation.","Computers and the Humanities",1992,"No","key wordslexis collocations lexical collocations statistics xtract language generation machine translation xtract overview lexical collocations statistical distributions developed set statistical techniques retrieving identifying collocations large textual corpora techniques developed identify collocations arbitrary length flexible collocations techniques implemented lexicographic tool xtract automatically acquire collocations high retrieval performance xtract works stages stage based statistical technique identifying word pairs involved syntactic relation words text order separated arbitrary number words stage based technique extract word collocations grams simpler related methods collocations involve closed class words particles prepositions stage applied output stage applies parsing techniques sentences involving word pair order identify proper syntactic relation words secondary effect stage filter number candidate collocations irrelevant produce higher quality output paper present overview xtract describe xtract knowledge retrieves language generation machine translation",0
"Key Wordsintelligent computer-assisted instruction intelligent tutoring instructional systems, ITS computer-assisted language instruction natural language understanding stylisties ","an intelligent computer assistant for stylistic instruction","This article describes an intelligent computer-assisted language instruction system that is designed to teach principles of syntactic style to students of English. Unlike conventional style checkers, the system performs a complete syntactic analysis of its input, and takes the student's stylistic intent into account when providing a diagnosis. Named STASEL for Stylistic Treatment At the Sentence Level, the system is specifically developed for the teaching of style, and makes use of artificial intelligence techniques in natural language processing to analyze free-form input sentences interactively.","Computers and the Humanities",1992,"No","key wordsintelligent computer assisted instruction intelligent tutoring instructional systems computer assisted language instruction natural language understanding stylisties intelligent computer assistant stylistic instruction article describes intelligent computer assisted language instruction system designed teach principles syntactic style students english unlike conventional style checkers system performs complete syntactic analysis input takes student stylistic intent account providing diagnosis named stasel stylistic treatment sentence level system specifically developed teaching style makes artificial intelligence techniques natural language processing analyze free form input sentences interactively",0
"Key Wordsanthropology social interactions kinship relations ","automatic rule discovery for field work in anthropology","This paper deals with the problem of discovering rules that govern social interactions and relations in preliteral societies. Two older computer programs are first described which can receive data, possibly incomplete and redundant, representing kinship relations among named individuals. The programs then establish a knowledge base in the form of a directed graph, which the user can query in a variety of ways. Another program, written on the “top” of these (rewritten in LISP), can form concepts of various properties, including kinship relations, of and between the individuals. The concepts are derived from the examples and non-examples of a certain social pattern, such as inheritance, succession, marriage, class (tribe, moiety, clan, etc.) membership, domination-subordination, incest and exogamy. The concepts become hypotheses about the rules, which are corroborated, modified or rejected by further examples and non-examples.","Computers and the Humanities",1992,"No","key wordsanthropology social interactions kinship relations automatic rule discovery field work anthropology paper deals problem discovering rules govern social interactions relations preliteral societies older computer programs receive data possibly incomplete redundant representing kinship relations named individuals programs establish knowledge base form directed graph user query variety ways program written top rewritten lisp form concepts properties including kinship relations individuals concepts derived examples examples social pattern inheritance succession marriage class tribe moiety clan membership domination subordination incest exogamy concepts hypotheses rules corroborated modified rejected examples examples",0
"KeywordsProfessional Development Development System Computational Linguistic Professional Development System ","microsoft basic professional development system",NA,"Computers and the Humanities",1992,"No"," professional development development system computational linguistic professional development system microsoft basic professional development system na",0
"Key Wordslarge text databases sex-role stereotyping gender text searching strategies ","a study of sex role stereotyping in the oxford english dictionary 2e","Using software (PAT and LECTOR) developed for the creation of the electronic Oxford English Dictionary, strategies applicable to other large data bases were developed to analyse systemic sex-role stereotyping. These are applied to the OED database as a whole and to sub-files of gender-related definition or quotation text. The corpus is also studied manually. Software-based strategies including text searches for collocations with gender-specific pronouns and possessive adjectives produce interesting results which are tested against information in previous research. Stereotypes are found most frequently in quotation text, to a lesser degree in definition text.","Computers and the Humanities",1992,"No","key wordslarge text databases sex role stereotyping gender text searching strategies study sex role stereotyping oxford english dictionary e software pat lector developed creation electronic oxford english dictionary strategies applicable large data bases developed analyse systemic sex role stereotyping applied oed database files gender related definition quotation text corpus studied manually software based strategies including text searches collocations gender specific pronouns possessive adjectives produce interesting results tested information previous research stereotypes found frequently quotation text lesser degree definition text",0
"KeywordsComputational Linguistic Common Methodology Humanity Computing ","introduction common methodologies in humanities computing and computational linguistics",NA,"Computers and the Humanities",1992,"No"," computational linguistic common methodology humanity computing introduction common methodologies humanities computing computational linguistics na",0
"Key Wordscontext meaning sense discrimination ambiguity polysemy bilingual ","a method for disambiguating word senses in a large corpus","Word sense disambiguation has been recognized as a major problem in natural language processing research for over forty years. Both quantitive and qualitative methods have been tried, but much of this work has been stymied by difficulties in acquiring appropriate lexical resources. The availability of this testing and training material has enabled us to develop quantitative disambiguation methods that achieve 92% accuracy in discriminating between two very distinct senses of a noun. In the training phase, we collect a number of instances of each sense of the polysemous noun. Then in the testing phase, we are given a new instance of the noun, and are asked to assign the instance to one of the senses. We attempt to answer this question by comparing the context of the unknown instance with contexts of known instances using a Bayesian argument that has been applied successfully in related tasks such as author identification and information retrieval. The proposed method is probably most appropriate for those aspects of sense disambiguation that are closest to the information retrieval task. In particular, the proposed method was designed to disambiguate senses that are usually associated with different topics.","Computers and the Humanities",1992,"No","key wordscontext meaning sense discrimination ambiguity polysemy bilingual method disambiguating word senses large corpus word sense disambiguation recognized major problem natural language processing research forty years quantitive qualitative methods work stymied difficulties acquiring lexical resources availability testing training material enabled develop quantitative disambiguation methods achieve accuracy discriminating distinct senses noun training phase collect number instances sense polysemous noun testing phase instance noun asked assign instance senses attempt answer question comparing context unknown instance contexts instances bayesian argument applied successfully related tasks author identification information retrieval proposed method aspects sense disambiguation closest information retrieval task proposed method designed disambiguate senses topics",0
"Key Wordslinguistic variation genre style register dimension factor analysis cluster analysis historical change English Somali ","the multi dimensional approach to linguistic analyses of genre variation an overview of methodology and findings","The present paper summarizes the major methods and results of the multi-dimensional approach to genre variation. The approach combines the resources of computational tools, large text corpora, and multivariate statistical tools (such as factor analysis and cluster analysis). It has been used to address issues such as the relations among spoken and written genres in English, and the historical development of genres and styles. The approach has also been applied to other languages; in this regard it has been used to address broader theoretical issues, such as the extent to which genre, and style variation are comparable cross-linguistically, and the linguistic consequences of literacy.","Computers and the Humanities",1992,"No","key wordslinguistic variation genre style register dimension factor analysis cluster analysis historical change english somali multi dimensional approach linguistic analyses genre variation overview methodology findings present paper summarizes major methods results multi dimensional approach genre variation approach combines resources computational tools large text corpora multivariate statistical tools factor analysis cluster analysis address issues relations spoken written genres english historical development genres styles approach applied languages regard address broader theoretical issues extent genre style variation comparable cross linguistically linguistic consequences literacy",0
"Key Wordsword frequency distribution lognormal generalized inverse Gauss-Poisson extended generalized Zipf's law vocabulary richness morphological productivity goodness of fit ","statistical models for word frequency distributions a linguistic evaluation","Three models for word frequency distributions, the lognormal law, the generalized inverse Gauss-Poisson law and the extended generalized Zipf's law are compared and evaluated with respect to goodness of fit and rationale. Application of these models to frequency distributions of a text, a corpus and morphological data reveals that no model can lay claim to exclusive validity, while inspection of the extrapolated theoretical vocabulary sizes raises doubts as to whether the urn scheme with independent trials is the correct underlying model for word frequency data. The role of morphology in shaping word frequency distributions is discussed, as well as parallelisms between vocabulary richness in literary studies and morphological productivity in linguistics.","Computers and the Humanities",1992,"No","key wordsword frequency distribution lognormal generalized inverse gauss poisson extended generalized zipf law vocabulary richness morphological productivity goodness fit statistical models word frequency distributions linguistic evaluation models word frequency distributions lognormal law generalized inverse gauss poisson law extended generalized zipf law compared evaluated respect goodness fit rationale application models frequency distributions text corpus morphological data reveals model lay claim exclusive validity inspection extrapolated theoretical vocabulary sizes raises doubts urn scheme independent trials correct underlying model word frequency data role morphology shaping word frequency distributions discussed parallelisms vocabulary richness literary studies morphological productivity linguistics",0
"Key Wordscomputer-assisted discourse analysis content analysis political discourse theme functional grammar ","automated syntactic text description enhancement the thematic structure of discourse utterances","Our work aims at the optimization of existing tools for computer-assisted description and analysis of textual data. More specifically, we have been involved in the thematic description of clauses and clause complexes of Quebec budget speeches from 1934 to 1960. Our main objective is to enhance the work already done in this direction by elaborating the analytic framework through a study of the thematic structure of these discourses. We first set out the general context of our work by briefly explaining the research project on political discourse under the Duplessis Regime in Quebec (1936–60) and giving a brief survey of the parsing strategy applied to the corpus. Second, we present the theoretical background of thematic analysis and the operational model that we are using here. Finally, we try to illustrate the relevance of such methodological work on research data.","Computers and the Humanities",1992,"No","key wordscomputer assisted discourse analysis content analysis political discourse theme functional grammar automated syntactic text description enhancement thematic structure discourse utterances work aims optimization existing tools computer assisted description analysis textual data specifically involved thematic description clauses clause complexes quebec budget speeches main objective enhance work direction elaborating analytic framework study thematic structure discourses set general context work briefly explaining research project political discourse duplessis regime quebec giving survey parsing strategy applied corpus present theoretical background thematic analysis operational model finally illustrate relevance methodological work research data",0
"KeywordsComputational Linguistic ","networking in the humanities lessons from ansaxnet",NA,"Computers and the Humanities",1992,"No"," computational linguistic networking humanities lessons ansaxnet na",0
"Key Wordsproper names lexicon acquisition text understanding news text ","the analysis and acquisition of proper names for the understanding of free text","Proper Names (PNs) present a problem for the automatic processing and understanding of naturally occurring text. Due to their poor coverage in existing lexical resources and the continual appearance of new names, they represent a large body of unknown lexical data. Moreover, the complexity of the constructions in which they can appear and their own internal structure make them difficult to process, even if they are initially known. Yet the successful analysis of names is often crucial to the full understanding of a text. This paper proposes a solution to the problem and describes a natural language processing (NLP) system, FUMES, which makes use of the internal structure of names and the descriptive information that regularly accompanies them to produce lexical and knowledge base entries for unknown PNs. We present some preliminary results showing the viability of this approach for the identification of proper names.","Computers and the Humanities",1992,"No","key wordsproper names lexicon acquisition text understanding news text analysis acquisition proper names understanding free text proper names pns present problem automatic processing understanding naturally occurring text due poor coverage existing lexical resources continual appearance names represent large body unknown lexical data complexity constructions internal structure make difficult process initially successful analysis names crucial full understanding text paper proposes solution problem describes natural language processing nlp system fumes makes internal structure names descriptive information regularly accompanies produce lexical knowledge base entries unknown pns present preliminary results showing viability approach identification proper names",0
"Key Wordsnatural language generation linguistic modelling attribute grammar applied linguistics language learning ","a system for natural language sentence generation","This paper describes a natural language generation system known as VINCI, which accepts as input a formal description of some subset of a natural language, and generates strings in the language. With the help of an attribute grammar formalism, the system can be used to simulate on a computer components of several current linguistic theories. The program, implemented in C, runs under a variety of operating systems, including UNIX, MS-DOS and VM/CMS. In this paper we consider not only the design of the system, but also some of its applications in linguistic modelling and second language acquisition research.","Computers and the Humanities",1992,"No","key wordsnatural language generation linguistic modelling attribute grammar applied linguistics language learning system natural language sentence generation paper describes natural language generation system vinci accepts input formal description subset natural language generates strings language attribute grammar formalism system simulate computer components current linguistic theories program implemented runs variety operating systems including unix ms dos vmcms paper design system applications linguistic modelling language acquisition research",0
"Key Wordsco-occurrence key words library science plus and minus words stylistics statistics ","about the statistical analysis of co occurrence","Various objections are raised against current practice in co-occurrence analysis. The use of Yule's coefficient Y is then advocated.","Computers and the Humanities",1992,"No","key wordsco occurrence key words library science minus words stylistics statistics statistical analysis occurrence objections raised current practice occurrence analysis yule coefficient advocated",0
NA,"technical review",NA,"Computers and the Humanities",1992,"No"," technical review na",0
NA,"the awk programming language",NA,"Computers and the Humanities",1992,"No"," awk programming language na",0
"KeywordsComputational Linguistic ","introductions telecommunications and the scholar",NA,"Computers and the Humanities",1992,"No"," computational linguistic introductions telecommunications scholar na",0
"Key Wordseducation human computer interaction artificial intelligence computing humanities ","beauty and the beast new approaches to teaching computing for humanities students at the university of aberdeen","This paper reports on the history and development of a new undergraduate course teaching computing for humanities students at the University of Aberdeen, and assesses some new teaching approaches developed in the course. It is noted that teaching computing to humanities students appears to be viewed with suspicion by some Computer Science and Humanities Departments. The two camps seem to fear, for different reasons, that issues and practices important to their disciplines will be compromised or watered down. This paper describes an attempt to reverse any such attitudes on the part of staff and students and to take undergraduates considerably beyond mere word processing and computer literacy. Various methods and techniques used in the course are presented and their value assessed. The importance of using a consistent computer interface to helping students form a stable conceptual model of computers is considered. We reflect on the value of teaching more about Human Computer Interaction and Artificial Intelligence than is usual in Humanities Computing courses. A number of lessons are drawn from the course.","Computers and the Humanities",1992,"No","key wordseducation human computer interaction artificial intelligence computing humanities beauty beast approaches teaching computing humanities students university aberdeen paper reports history development undergraduate teaching computing humanities students university aberdeen assesses teaching approaches developed noted teaching computing humanities students appears viewed suspicion computer science humanities departments camps fear reasons issues practices important disciplines compromised watered paper describes attempt reverse attitudes part staff students undergraduates considerably mere word processing computer literacy methods techniques presented assessed importance consistent computer interface helping students form stable conceptual model computers considered reflect teaching human computer interaction artificial intelligence usual humanities computing courses number lessons drawn ",0
"Key Wordscomputational morphology two-level morphology morphological parsing Koskenniemi KIMMO interlinear text Tagalog TEX ","glossing text with the pc kimmo morphological parser","For those studying languages with rich word structures, a morphological parser is a valuable tool. PC-KIMMO is a parser for small computers that is based on Koskenniemi's two-level model of morphology. Of the many practical uses for a morphological parser such as PC-KIMMO, this article describes one: producing automatically glossed interlinear text.","Computers and the Humanities",1992,"No","key wordscomputational morphology level morphology morphological parsing koskenniemi kimmo interlinear text tagalog tex glossing text pc kimmo morphological parser studying languages rich word structures morphological parser valuable tool pc kimmo parser small computers based koskenniemi level model morphology practical morphological parser pc kimmo article describes producing automatically glossed interlinear text",0
"KeywordsComputational Linguistic Large Text ","estimating changes in collocations of key words across a large text a case study of coleridges notebooks",NA,"Computers and the Humanities",1992,"No"," computational linguistic large text estimating collocations key words large text case study coleridges notebooks na",0
"KeywordsComputational Linguistic Modern Data ","the medieval and early modern data bank",NA,"Computers and the Humanities",1992,"No"," computational linguistic modern data medieval early modern data bank na",0
"Key Wordscorpora databases dictionary informatiop retrieval systems lemmatization lexicography lexicology lexicon machine readable OED2 ","electronic lexicography","This paper offers a brief survey of some important developments in the use of computers in making dictionaries and lexicons. Making a dictionary involves collecting the data, sorting and lemmatizing, editing and printing. Five major types of machine-readable dictionaries have developed from these procedures: Machine Readable Lexicons of individual authors, Machine Readable Dictionaries with codes for linguistic information, Machine Dictionaries with selected information, and Lexical Databases with lexical information abstracted from machine-readable dictionaries. The second edition of the QED is a machine-readable dictionary with codes that may provide the basis for a diachronic lexical database.","Computers and the Humanities",1991,"No","key wordscorpora databases dictionary informatiop retrieval systems lemmatization lexicography lexicology lexicon machine readable oed electronic lexicography paper offers survey important developments computers making dictionaries lexicons making dictionary involves collecting data sorting lemmatizing editing printing major types machine readable dictionaries developed procedures machine readable lexicons individual authors machine readable dictionaries codes linguistic information machine dictionaries selected information lexical databases lexical information abstracted machine readable dictionaries edition qed machine readable dictionary codes provide basis diachronic lexical database",0
"Key Wordscomputational stylistics style stylistics statistics literary style ","progress in stylistics theory statistics computers","This paper attempts to assess the progress made in computational stylistics dyring the course of the past twenty-five years. First, we discuss some theoretical notions of style, and then we sketch certain trends that emerge from relevant articles appearing in a variety of publications including conference proceedings and academic journals (other than CHum). The conclusion is that progress has been mixed.","Computers and the Humanities",1991,"No","key wordscomputational stylistics style stylistics statistics literary style progress stylistics theory statistics computers paper attempts assess progress made computational stylistics dyring past twenty years discuss theoretical notions style sketch trends emerge relevant articles appearing variety publications including conference proceedings academic journals chum conclusion progress mixed",0
"KeywordsComputational Linguistic Correct Grammar ","correct grammar",NA,"Computers and the Humanities",1991,"No"," computational linguistic correct grammar correct grammar na",0
"Key Wordshistory historical editions editions survey CDROM editions NLCindex indexing Text Encoding Initiative ","historical editions in the states","A late 1990 survey found that most historical editors in the United States continue to use the computer primarily as a word processing tool to prepare texts and editorial apparatus. Among older projects, a migration from mainframe or mini-computers to PCs has been the norm. New developments in the field include the “Founding Fathers” CD-ROM project, the impending release of Version 2.0 of NLCindex, and a strong interest in the Text Encoding Initiative.","Computers and the Humanities",1991,"No","key wordshistory historical editions editions survey cdrom editions nlcindex indexing text encoding initiative historical editions states late survey found historical editors united states continue computer primarily word processing tool prepare texts editorial apparatus older projects migration mainframe mini computers pcs norm developments field include founding fathers cd rom project impending release version nlcindex strong interest text encoding initiative",0
"Key WordsBach Database musical databases DARMS (Digital Alternate Representation of Musical Scores) ESAC (Essen Associative Code) IML-MIR (Intermediary Musical Language-Music Information Retrieved) IRCAM language models (for musical analysis) MIDI (Musical Instrument Digital Interface) MIPS (Musical Information Processing Standards) musical data musical information RISM SCORE musical analysis music printing ","computing in musicology 196691","While there are many parallels between computing activities in musicology and those in other humanities disciplines, the particular nature of musical material and the ways in which this must be accommodated set many activities apart from those in text-based disciplines. As in other disciplines, early applications were beset by hardware constraints, which placed a premium on expertise and promoted design-intensive projects. Massive musical encoding and bibliographical projects were initiated. Diversification of hardware platforms and languages in the Seventies led to task-specific undertakings, including preliminary work on many of today's programs for music printing and analysis. The rise of personal computers and associated general-purpose software in the Eighties has enabled many scholars to pursue projects individually, particularly with the assistance of database, word processing, and notation software. Current issues facing the field include the need for standards for data interchange, the creation of banks of reusable data, the establishment of qualitative standards for encoded data, and the encouragement of realistic appraisals of what computers can do.","Computers and the Humanities",1991,"No","key wordsbach database musical databases darms digital alternate representation musical scores esac essen associative code iml mir intermediary musical language music information retrieved ircam language models musical analysis midi musical instrument digital interface mips musical information processing standards musical data musical information rism score musical analysis music printing computing musicology parallels computing activities musicology humanities disciplines nature musical material ways accommodated set activities text based disciplines disciplines early applications beset hardware constraints premium expertise promoted design intensive projects massive musical encoding bibliographical projects initiated diversification hardware platforms languages seventies led task specific undertakings including preliminary work today programs music printing analysis rise personal computers general purpose software eighties enabled scholars pursue projects individually assistance database word processing notation software current issues facing field include standards data interchange creation banks reusable data establishment qualitative standards encoded data encouragement realistic appraisals computers ",0
"Key Wordslaw legal reasoning logic in law legal informatics expert systems ","computers and legal reasoning developments in germany","The importance of “reasoning” in law is pointed out. Law and jurisprudence belong to the “reasoning-conscious” disciplines. Accordingly, there is a long tradition of logic in law. The specific methods of professional work in law are to be seen in close connection with legal reasoning. The advent of computers at first did not touch upon legal reasoning (or the professional work in law). At first computers could be used only for general auxiliary functions (e.g., numerical calculations in tax law). Gradually, the use of computers for auxiliary functions in law has become more specific and more sophisticated (e.g., legal information retrieval), touching more closely upon professional legal work. Moreover, renewed interest in AI has also fostered interest in AI in law, especially for legal expert systems. AI techniques can be used in support of legal reasoning. Yet until now legal expert systems have remained in the research and development stage and have hardly succeeded in becoming a profitable tool for the profession. Therefore it is hoped that the two lines of computer support, for auxiliary functions in law and for immediate support of legal reasoning, may unite in the future.","Computers and the Humanities",1991,"No","key wordslaw legal reasoning logic law legal informatics expert systems computers legal reasoning developments germany importance reasoning law pointed law jurisprudence belong reasoning conscious disciplines long tradition logic law specific methods professional work law close connection legal reasoning advent computers touch legal reasoning professional work law computers general auxiliary functions numerical calculations tax law gradually computers auxiliary functions law specific sophisticated legal information retrieval touching closely professional legal work renewed interest ai fostered interest ai law legal expert systems ai techniques support legal reasoning legal expert systems remained research development stage succeeded profitable tool profession hoped lines computer support auxiliary functions law support legal reasoning unite future",0
"Key WordsComputers and the Humanities statistical analysis literature language philosophical essays thematic analysis theory replication retrospective polemic ","statistical analysis of literature a retrospective on computers and the humanities 19661990","This retrospective on statistical analysis of literature in the first twenty-four years of Computers and the Humanities divides the essays under review into four groups: the philosophical, the statistical analyses of language, the statistical analyses of literary texts, and the statistical analyses of themes. It begins with the question: must valid statistical analysis of any literary text be based on a complete linguistic description of the language of the text? It summarizes and evaluates over forty essays, giving details on works discussed, sample sizes used, statistical methods applied, and quotations from the researchers. The essay ends with a polemical summary of what has been done and what the future holds. It emphasizes the importance of extended pre-computational stages of learning about language and discourse analysis; reading previous research, building on and challenging theory; and the use of carefully crafted, small databases to test specific questions.","Computers and the Humanities",1991,"No","key wordscomputers humanities statistical analysis literature language philosophical essays thematic analysis theory replication retrospective polemic statistical analysis literature retrospective computers humanities retrospective statistical analysis literature twenty years computers humanities divides essays review groups philosophical statistical analyses language statistical analyses literary texts statistical analyses themes begins question valid statistical analysis literary text based complete linguistic description language text summarizes evaluates forty essays giving details works discussed sample sizes statistical methods applied quotations researchers essay ends polemical summary future holds emphasizes importance extended pre computational stages learning language discourse analysis reading previous research building challenging theory carefully crafted small databases test specific questions",0
"KeywordsPattern Match Computational Linguistic ","synoname1 the gettys new approach to pattern matching for personal names",NA,"Computers and the Humanities",1991,"No"," pattern match computational linguistic synoname gettys approach pattern matching personal names na",0
"Key Wordsstylometry literary detection Shakespeare Authorship Question Shakespearean canon Elizabethan poets Karhunen-Loeve transform ","a touchstone for the bard","We introduce an authorship identification test, called modal analysis, based on a new statistic derived from the Karhunen-Loeve transform. Application to the poems of the Shakespearean canon and to other contemporary poetry strongly supports the case for disqualification of most major claimants. Results also cast doubt that the recently discovered poems, Shall I Die and Elegy, were written by William Shakespeare, but do suggest that eight unascribed poems of The Passionate Pilgrim may have been his work.","Computers and the Humanities",1991,"No","key wordsstylometry literary detection shakespeare authorship question shakespearean canon elizabethan poets karhunen loeve transform touchstone bard introduce authorship identification test called modal analysis based statistic derived karhunen loeve transform application poems shakespearean canon contemporary poetry strongly supports case disqualification major claimants results cast doubt recently discovered poems die elegy written william shakespeare suggest unascribed poems passionate pilgrim work",0
"Key Wordsstylometry literary detection Thisted-Efron tests Shakespearean canon ","are the thisted efron authorship tests valid","We assess the validity of the Thisted-Efron author-ship tests in two stages. First, we construct simulated texts in accordance with the assumptions implicit in the underlying model and use these to validate the basic computations, to determine their range of applicability, and to evaluate their sensitivity to basic lexical parameters. Second, we experiment with actual texts from the Shakespearean canon and the plays of Christopher Marlowe. The results of the tests are mixed, showing good consistency for the Shakespeare plays (with some discrimination among early, middle and late works) but poor consistency between Shakespeare's poems and plays, or among Marlowe's plays.","Computers and the Humanities",1991,"No","key wordsstylometry literary detection thisted efron tests shakespearean canon thisted efron authorship tests valid assess validity thisted efron author ship tests stages construct simulated texts accordance assumptions implicit underlying model validate basic computations determine range applicability evaluate sensitivity basic lexical parameters experiment actual texts shakespearean canon plays christopher marlowe results tests mixed showing good consistency shakespeare plays discrimination early middle late works poor consistency shakespeare poems plays marlowe plays",0
"Key Wordselectronic text machine-readable text database on-line corpora humanities microcomputer SGML electronic publishing text-analysis tools ","the very pulse of the machine three trends toward improvement in electronic versions of humanities texts","Since April 1989, the Center for Text and Technology at Georgetown University has gathered information on the structure of projects that produce electronic text in the humanities. This report — based on the April, 1991 version of the Georgetown Catalogue and emphasizing its full-text projects in humanities disciplines other than linguistics —surveys the countries in which projects are found, the languages encoded, the disciplines served, and the auspices represented. Then the report explores three trends toward the improvement of electronic texts: increased scope of the new projects, improved quality of the editions used, and greater sophistication in the text-analysis tools added. Included among the notes is a list of titles and contacts for 42 projects cited in the report.","Computers and the Humanities",1991,"No","key wordselectronic text machine readable text database line corpora humanities microcomputer sgml electronic publishing text analysis tools pulse machine trends improvement electronic versions humanities texts april center text technology georgetown university gathered information structure projects produce electronic text humanities report based april version georgetown catalogue emphasizing full text projects humanities disciplines linguistics surveys countries projects found languages encoded disciplines served auspices represented report explores trends improvement electronic texts increased scope projects improved quality editions greater sophistication text analysis tools added included notes list titles contacts projects cited report",0
"Key Wordsdiscourse analysis expert systems LEX project law natural language processing information retrieval ","discourse analysis for a legal expert system","This paper deals with discourse analysis, with specific reference to the Linguistic and Logic Based Legal Expert System, LEX. In the LEX project we concentrated on a few arbitrarily selected court decisions, extracted the case descriptions, and then added the necessary background knowledge to our prototype expert system to analyze the case descriptions and to deduce the answers to some juridical questions. In this paper we present and comment on a typical discourse representation structure for an accident description in the corpus we studied.","Computers and the Humanities",1991,"No","key wordsdiscourse analysis expert systems lex project law natural language processing information retrieval discourse analysis legal expert system paper deals discourse analysis specific reference linguistic logic based legal expert system lex lex project concentrated arbitrarily selected court decisions extracted case descriptions added background knowledge prototype expert system analyze case descriptions deduce answers juridical questions paper present comment typical discourse representation structure accident description corpus studied",0
"Key Wordsworkstation κλεlω data models WORM fuzzy structures lemmatization family reconstitution historical microanalysis ","the historical workstation project","Since 1978 research in the development of software dedicated to the specific problems of historical research has been undertaken at the Max-Planck-Institute für Geschichte in Göttingen. From a background of practical experiences during these years, a concept of what an appropriate ‘workstation” for an historian would be has been derived. It stresses the necessity of three components: (a) software, derived from a detailed analysis of what differentiates information contained in historical sources from such present in current material, (b) databases which are as easily available as printed books and (c) knowledge bases which allow software and data bases to draw upon the information contained in historical reference works. A loose network of European research projects, dedicated to the realization of such a setup, is described.","Computers and the Humanities",1991,"No","key wordsworkstation data models worm fuzzy structures lemmatization family reconstitution historical microanalysis historical workstation project research development software dedicated specific problems historical research undertaken max planck institute geschichte ttingen background practical experiences years concept workstation historian derived stresses necessity components software derived detailed analysis differentiates information contained historical sources present current material databases easily printed books knowledge bases software data bases draw information contained historical reference works loose network european research projects dedicated realization setup ",0
"Key Wordshumanities computing concordances databases literary criticism social history musicology art history lexicography ","humanities computing 25 years later","This paper attempts to provide an overview of the development of humanities computing during the past twenty-five years. Mention is made of the major applications of the computer to humanities disciplines, and of the most important and representative projects across the world.","Computers and the Humanities",1991,"No","key wordshumanities computing concordances databases literary criticism social history musicology art history lexicography humanities computing years paper attempts provide overview development humanities computing past twenty years mention made major applications computer humanities disciplines important representative projects world",0
NA,"the problem of a statistical approach",NA,"Computers and the Humanities",1991,"No"," problem statistical approach na",0
"KeywordsComputational Linguistic Naturalistic Inquiry ","evaluating evolution naturalistic inquiry and the perseus project",NA,"Computers and the Humanities",1991,"No"," computational linguistic naturalistic inquiry evaluating evolution naturalistic inquiry perseus project na",0
"Key WordsIstituto di Linguistica Computazionale CNR, computational linguistics databases linguistic analysis parsing ","summary of the activities of the istituto di linguistica computazionale","This article summarizes the activities of the Istituto di Linguistica Computazionale. We discuss the Italian Multi-functional Lexical Databases; the projects focussing on linguistic analysis and generation; corpora in the MRF, textual databases and linguistic workstations; computer-assisted humanities teaching; and the various cooperative ventures, seminars and conferences offered by the Institute.","Computers and the Humanities",1990,"Yes","key wordsistituto di linguistica computazionale cnr computational linguistics databases linguistic analysis parsing summary activities istituto di linguistica computazionale article summarizes activities istituto di linguistica computazionale discuss italian multi functional lexical databases projects focussing linguistic analysis generation corpora mrf textual databases linguistic workstations computer assisted humanities teaching cooperative ventures seminars conferences offered institute",1
"Key WordsLatin lexicography lemmatization word-lists word-searching ","a project for latin lexicography 1 automatic lemmatization and word list","A cooperative team of researchers from various Italian universities are collaborating on a project for Latin lexicography. This article describes the linguistic work being done on the Latin language. The various problems — and their solutions — are discussed: allographs, homographs, source material and classification methods, word searching, etc.","Computers and the Humanities",1990,"No","key wordslatin lexicography lemmatization word lists word searching project latin lexicography automatic lemmatization word list cooperative team researchers italian universities collaborating project latin lexicography article describes linguistic work latin language problems solutions discussed allographs homographs source material classification methods word searching ",0
"Key WordsLatin lexicology morphology lemmatization databases ","a project for latin lexicography 2 a latin morphological analyzer","This article describes a second aspect of the Project for Latin Lexicography (see previous article). We here concentrate on two aspects of the project. First, we describe the morphological analyzer, which comprises a base dictionary, a table of suffixes, a table of endings and a table of postfixes. Second, we describe the lemmatization module, which operates by reference to a series of grammatical codes or information given for the base, and reference codes.","Computers and the Humanities",1990,"No","key wordslatin lexicology morphology lemmatization databases project latin lexicography latin morphological analyzer article describes aspect project latin lexicography previous article concentrate aspects project describe morphological analyzer comprises base dictionary table suffixes table endings table postfixes describe lemmatization module operates reference series grammatical codes information base reference codes",0
"Key Wordslegal informatics law Italian legal system databases artificial intelligence ","legal informatics research in italy the istituto per la documentazione giuridica of the italian national research council","The IDG was originally founded to carry out research into the collection and processing of documentation relating to Italian legislation, case law and legal authority. The Institute has since concentrated on automated documentation and legal informatics, as well as the application of artificial intelligence to the law. This article describes the many projects undertaken at the Institute.","Computers and the Humanities",1990,"No","key wordslegal informatics law italian legal system databases artificial intelligence legal informatics research italy istituto la documentazione giuridica italian national research council idg originally founded carry research collection processing documentation relating italian legislation case law legal authority institute concentrated automated documentation legal informatics application artificial intelligence law article describes projects undertaken institute",0
"Key Wordspoetry verse limerick meter prosody computer-assisted instruction speech synthesis interactive freshman English literature ","poetry i teaching verse with cai","The teaching of literature through CAI raises problems of both a linguistic and instructional nature; student involvement and creativity in studying literature, and especially poetry, is difficult to build into a computer-based lesson. We have confronted these difficulties in the lessonPoetry I, which introduces undergraduates to basic concepts of poetic verse in a design using screen display, speech synthesis, and verse processing to maximize interactivity and student involvement.","Computers and the Humanities",1990,"No","key wordspoetry verse limerick meter prosody computer assisted instruction speech synthesis interactive freshman english literature poetry teaching verse cai teaching literature cai raises problems linguistic instructional nature student involvement creativity studying literature poetry difficult build computer based lesson confronted difficulties lessonpoetry introduces undergraduates basic concepts poetic verse design screen display speech synthesis verse processing maximize interactivity student involvement",0
NA,"notes and news",NA,"Computers and the Humanities",1990,"No"," notes news na",0
"Key Wordslexicography concordances early Italian poetic language poetry databases text encoding philology dialectology ","a concordance of the early italian poetic language","A concordance of the early Italian poetic language is being compiled at the University of Florence, based on the need to recognize the special nature of the language in earlier times. The corpus consists of some forty-five manuscripts, that is, all that remains of book production from the origins to the end of the thirteenth century. Once the work of entering the single word-tokens is done, a complete concordance results, with accompanying grammatical connotations, in which not only are Tuscan and dialectal words grouped under separate standard headwords, but also homographs are clearly distinguished.","Computers and the Humanities",1990,"No","key wordslexicography concordances early italian poetic language poetry databases text encoding philology dialectology concordance early italian poetic language concordance early italian poetic language compiled university florence based recognize special nature language earlier times corpus consists forty manuscripts remains book production origins end thirteenth century work entering single word tokens complete concordance results accompanying grammatical connotations tuscan dialectal words grouped separate standard headwords homographs distinguished",0
NA,"introduction",NA,"Computers and the Humanities",1990,"No"," introduction na",0
"KeywordsComputational Linguistic ","paperless writing revisited",NA,"Computers and the Humanities",1990,"No"," computational linguistic paperless writing revisited na",0
"Key Wordslexicography philosophy seventeenth century eighteenth century Latin language concordances lexicons Lessico Intellettuale Europe ","philosophical lexicography the lie and the use of the computer","This manuscript is a description of the research activities of the Lessico Intellettuale Europeo. The Centre works on the lexicographical analysis of philosophical and scientific texts, mainly of the seventeenth and eighteenth centuries. An important project is the Philosophical Dictionary of the Seventeenth and Eighteenth Centuries. The LIE series of publications include a number of indices, concordances and lexicons. Another project at the Centre is the Thesaurus Mediae et Recentioris Latinitatis. The Centre also publishes the proceedings of the three-yearly Colloqui internazionali.","Computers and the Humanities",1990,"No","key wordslexicography philosophy seventeenth century eighteenth century latin language concordances lexicons lessico intellettuale europe philosophical lexicography lie computer manuscript description research activities lessico intellettuale europeo centre works lexicographical analysis philosophical scientific texts seventeenth eighteenth centuries important project philosophical dictionary seventeenth eighteenth centuries lie series publications include number indices concordances lexicons project centre thesaurus mediae recentioris latinitatis centre publishes proceedings yearly colloqui internazionali",0
"Key WordsCAI in literature LAN networks pseudonyms gender introvert extrovert creativity minorities reader response collaborative writing and exams ","radical changes in class discussion using networked computers","This study examines the effects of conducting class discussion on a local area network. A real time networking program (INTERCHANGE) was used for class discussion in freshman and senior literature courses and in a graduate humanities computing class. Pseudonyms, collaborative exams and essays, and computer-assisted reading were tested, along with organization of the students by sex and personality type. At the beginning and end of each semester in each class students were asked 50 to 70 multiple choice questions. Their answers revealed that the many advantages of computer assisted class discussion (CACD) clearly outweigh the disadvantages.","Computers and the Humanities",1990,"No","key wordscai literature lan networks pseudonyms gender introvert extrovert creativity minorities reader response collaborative writing exams radical class discussion networked computers study examines effects conducting class discussion local area network real time networking program interchange class discussion freshman senior literature courses graduate humanities computing class pseudonyms collaborative exams essays computer assisted reading tested organization students sex personality type beginning end semester class students asked multiple choice questions answers revealed advantages computer assisted class discussion cacd outweigh disadvantages",0
NA,"courseware reviews",NA,"Computers and the Humanities",1990,"No"," courseware reviews na",0
"Key Wordsstylistic analysis Spanish twentieth-century corpus samples ","stylistic analysis of a corpus of twentieth century spanish narrative","Statistical information on a substantial corpus of representative Spanish texts is needed in order to determine the significance of data about individual authors or texts by means of comparison. This study describes the organization and analysis of a 150,000-word corpus of 30 well-known twentieth-century Spanish authors. Tables show the computational results of analyses involving sentences, segments, quotations, and word length.","Computers and the Humanities",1990,"Yes","key wordsstylistic analysis spanish twentieth century corpus samples stylistic analysis corpus twentieth century spanish narrative statistical information substantial corpus representative spanish texts needed order determine significance data individual authors texts means comparison study describes organization analysis word corpus twentieth century spanish authors tables show computational results analyses involving sentences segments quotations word length",1
"Key Wordsaesthetics sonnets Shakespeare linguistic diversity primary process type-token ratio interaction effects ","lexical choices and aesthetic success a computer content analysis of 154 shakespeare sonnets","A research paradigm is suggested that combines the perspectives of the humanistic scholar and the behavioral scientist: After differentiating the popularity of actual aesthetic products using archival indices and then subjecting these compositions to objective computer content analyses, further statistical treatment may divulge the intrinsic properties responsible for differences in impact. This approach is illustrated by an analysis of the 154 sonnets attributed to William Shakespeare. Each sonnet was partitioned into four consecutive units (three quatrains and a couplet), and then a computer gauged how the number of words, different words, unique words, primary process imagery, and secondary process imagery changed within each sonnet. Taking advantage of a previous objective measure of the relative aesthetic merit of the sonnets, and implementing a statistical search for interaction effects, it was demonstrated that Shakespeare's lexical choices adopt a discernible pattern in the highly popular creations that is not found in the more obscure poems. Perhaps the most fascinating aspect of this pattern shift is the distinct manner in which the poet modifies his vocabulary when composing the concluding couplet in his best sonnets.","Computers and the Humanities",1990,"No","key wordsaesthetics sonnets shakespeare linguistic diversity primary process type token ratio interaction effects lexical choices aesthetic success computer content analysis shakespeare sonnets research paradigm suggested combines perspectives humanistic scholar behavioral scientist differentiating popularity actual aesthetic products archival indices subjecting compositions objective computer content analyses statistical treatment divulge intrinsic properties responsible differences impact approach illustrated analysis sonnets attributed william shakespeare sonnet partitioned consecutive units quatrains couplet computer gauged number words words unique words primary process imagery secondary process imagery changed sonnet taking advantage previous objective measure relative aesthetic merit sonnets implementing statistical search interaction effects demonstrated shakespeare lexical choices adopt discernible pattern highly popular creations found obscure poems fascinating aspect pattern shift distinct manner poet modifies vocabulary composing concluding couplet sonnets",0
"Key Wordsinformatics computer science philology lexicography linguistics ","informatics and new philology","This paper considers the impact of informatics and the new technology on the field of philology. A brief overview of developments in the field is given, and the present bottleneck of computational linguistics are considered. Also discussed are some of the challenges posed by natural language processing, and the ways in which we can rise to the challenge.","Computers and the Humanities",1990,"No","key wordsinformatics computer science philology lexicography linguistics informatics philology paper considers impact informatics technology field philology overview developments field present bottleneck computational linguistics considered discussed challenges posed natural language processing ways rise challenge",0
"Key WordsItalian language literary language lexicography concordances databases lemmatization ","a literary lexicography project for the italian language","CLIPON is an acronym for Concordanze della Lingua Italiana Poetica dell'Otto/Novecento. The aim of the project described here is to produce lexicons and lemmatized concordances of the literary Italian language of the nineteenth and twentieth centuries. The corpus involves groups of mainly poetic works and authors that have a common denominator as regards schools, currents, culture and chronology.","Computers and the Humanities",1990,"Yes","key wordsitalian language literary language lexicography concordances databases lemmatization literary lexicography project italian language clipon acronym concordanze della lingua italiana poetica dellottonovecento aim project produce lexicons lemmatized concordances literary italian language nineteenth twentieth centuries corpus involves groups poetic works authors common denominator schools currents culture chronology",1
"Key WordsCoptic Coptic literature databases text manipulation non-Roman characters ","the corpus dei manoscritti copti letterari","The Corpus dei Manoscritti Copti Letterari is a project whose original aim was to reconstruct the Coptic codices from the White Monastery in Upper Egypt. The project was later expanded to include all Coptic literature. In 1980 a new project was launched to transfer the data into machine-readable form and make the information available, in as generic a format as possible, to scholars throughout the world.","Computers and the Humanities",1990,"Yes","key wordscoptic coptic literature databases text manipulation roman characters corpus dei manoscritti copti letterari corpus dei manoscritti copti letterari project original aim reconstruct coptic codices white monastery upper egypt project expanded include coptic literature project launched transfer data machine readable form make information generic format scholars world",1
"Key WordsLatin Latin grammarians concordances Heinrich Keil ","a concordance to keils latin grammarians","The following is a description of a computerized version of the corpus of Latin grammarians published by Heinrich Keil in Leipzig between 1855 and 1880. The intent was to prepare an instrument which would serve both as a key to Keil's corpus and as the basis for a re-edition of the work itself. We discuss the corpus itself, the ways in which it was encoded, the pre-editing work, and how the material was organized for analysis by computer.","Computers and the Humanities",1990,"No","key wordslatin latin grammarians concordances heinrich keil concordance keils latin grammarians description computerized version corpus latin grammarians published heinrich keil leipzig intent prepare instrument serve key keil corpus basis edition work discuss corpus ways encoded pre editing work material organized analysis computer",0
"Key WordsLatin Greek Justinian Corpus Iuris lexicography databases text archives lemmatization ","justinian lexicography","This paper describes two research projects, both involving Latin and Greek lexicography. They are undertaken at the University of Florence and at the Italian National Research Council respectively. The one involves the creation of a Dictionary of Justinian's constitutions based on the emperor's legislative lexicon formed in the Corpus Iuris and elsewhere. The most demanding aspect of this task has been the creation of the Dictionary of the Novellae. The other project involves the creation of a Lexicon of the Novellae in the Authenticum version.","Computers and the Humanities",1990,"No","key wordslatin greek justinian corpus iuris lexicography databases text archives lemmatization justinian lexicography paper describes research projects involving latin greek lexicography undertaken university florence italian national research council involves creation dictionary justinian constitutions based emperor legislative lexicon formed corpus iuris demanding aspect task creation dictionary novellae project involves creation lexicon novellae authenticum version",0
"Key WordsChinese language sinological research Mandarin Chinese text encoding non-Roman characters lexical distributions lexicology ","lexical distribution in the guomin xiaoxue guoyu a computer assisted analysis of morpholexical elements","The research described in this paper was originally presented as a dissertation at the Università di Venezia. The objective was to establish the kind of language to which Chinese students are exposed during their primary education. The analysis was based on twelve texts of Guomin Xiaoxue Guoyu. This manuscript concentrates mainly on the techniques used to encode Chinese characters.","Computers and the Humanities",1990,"No","key wordschinese language sinological research mandarin chinese text encoding roman characters lexical distributions lexicology lexical distribution guomin xiaoxue guoyu computer assisted analysis morpholexical elements research paper originally presented dissertation universit di venezia objective establish kind language chinese students exposed primary education analysis based twelve texts guomin xiaoxue guoyu manuscript concentrates techniques encode chinese characters",0
NA,"call for papers",NA,"Computers and the Humanities",1990,"No"," call papers na",0
"Key Wordstextual analysis church history Pope John XXIII indexes Italian language ","church history and the computer","Ever since the pioneering work of Roberto Busa, computers have had an important work in humanistic research. The Istituto per le Scienze Religiose has been involved in such research since the 1970s. This article describes the Pope John XXIII project whose aim is to produce a computer-aided index of all the Pope's writings in both Italian and Latin; as well as future projects currently under consideration.","Computers and the Humanities",1990,"No","key wordstextual analysis church history pope john xxiii indexes italian language church history computer pioneering work roberto busa computers important work humanistic research istituto le scienze religiose involved research s article describes pope john xxiii project aim produce computer aided index pope writings italian latin future projects consideration",0
"Key Wordslexicography historical dictionaries Italian language philology relational databases vocabulary ","the italian vocabulary center","The Opera del Vocabolario Italiano was given a mandate in 1964 to create a Historical Dictionary of the Italian Language. The main objective was to provide a tool which would give vital information on the development of the Italian language from its origins to the present day. In 1986 the Center incorporated modern computer technology into the project and this led to a series of decisions which affected the nature and the outcome of the project. This article traces the development of the project, and describes both hardware and software systems used, as well as the nature of the relational database being created and its linguistic applications.","Computers and the Humanities",1990,"Yes","key wordslexicography historical dictionaries italian language philology relational databases vocabulary italian vocabulary center opera del vocabolario italiano mandate create historical dictionary italian language main objective provide tool give vital information development italian language origins present day center incorporated modern computer technology project led series decisions affected nature outcome project article traces development project describes hardware software systems nature relational database created linguistic applications",1
"KeywordsComputational Linguistic Language Curriculum ","the role of computer assisted learning in a proficiency based language curriculum",NA,"Computers and the Humanities",1990,"No"," computational linguistic language curriculum role computer assisted learning proficiency based language curriculum na",0
"Key Wordscomputer writing aids grammar checkers style checkers computers and writing CorrecText Correct Grammar ","a new grammar checker","CorrecText, from Houghton-Mifflin, is a significant advance in grammar checkers, because it uses a full parse of sentences in its analysis. Though limited by the fact that many English sentences are syntactically ambiguous, the program can find many errors in grammar, style, and usage. Still, questions remain about where and how it can be useful. It is not terribly useful on final versions of good prose; the makers assert that it is more useful on unedited prose, where there are many inadvertent errors. An empirical study of much unedited prose would not only verify this assertion, but would help improve the program.","Computers and the Humanities",1990,"No","key wordscomputer writing aids grammar checkers style checkers computers writing correctext correct grammar grammar checker correctext houghton mifflin significant advance grammar checkers full parse sentences analysis limited fact english sentences syntactically ambiguous program find errors grammar style usage questions remain terribly final versions good prose makers assert unedited prose inadvertent errors empirical study unedited prose verify assertion improve program",0
"Key WordsGIRCSE lexicography philology textual analysis lemmatization pre-editing research projects expression signs computing ","the interdisciplinary group for expression signs computing","The following is a description of the facilities, activities and research projects of the GIRCSE at the Università Cattolica del Sacro Cuore. The Group offers courses on computing in the humanities and assists scholars in their computer-based philological research. We discuss the problems of pre-editing of texts, lemmatization and textual analysis programs. We provide a comprehensive list of the Group's independent and collaborative research projects.","Computers and the Humanities",1990,"No","key wordsgircse lexicography philology textual analysis lemmatization pre editing research projects expression signs computing interdisciplinary group expression signs computing description facilities activities research projects gircse universit cattolica del sacro cuore group offers courses computing humanities assists scholars computer based philological research discuss problems pre editing texts lemmatization textual analysis programs provide comprehensive list group independent collaborative research projects",0
"Key Wordsmachine-readable texts text analysis programs Shakespeare concordances indexes stylo-statistics ","the bard in bits electronic editions of shakespeare and programs to analyze them","Machine-readable texts in the humanities are in a period of rapid growth, as are programs for analyzing them, with concomitant problems of finding and choosing the most suitable of each. No one text-base, and no one search program, proves to be wholly suitable for all projects. Three of the currently available electronic editions of Shakespeare are discussed and compared, along with three commercial programs for text analysis on microcomputers.","Computers and the Humanities",1990,"No","key wordsmachine readable texts text analysis programs shakespeare concordances indexes stylo statistics bard bits electronic editions shakespeare programs analyze machine readable texts humanities period rapid growth programs analyzing concomitant problems finding choosing suitable text base search program proves wholly suitable projects electronic editions shakespeare discussed compared commercial programs text analysis microcomputers",0
"Key Wordsuser interfaces databases programming languages natural language processing corpus linguistics syntactic analysis ","the chameleon approach a technique to reach more users","The possible benefits of computing in humanities research are often wasted because of the psychological barriers that computers evoke in non-specialists. This paper examines the underlying causes and suggests some ways of alleviating the problem. One approach in particular, i.e. ease through familiarity, is discussed in more detail. It is illustrated by means of a description of a database system that uses this approach: the Linguistic DataBase, which contains syntactic analysis trees of natural language data.","Computers and the Humanities",1989,"No","key wordsuser interfaces databases programming languages natural language processing corpus linguistics syntactic analysis chameleon approach technique reach users benefits computing humanities research wasted psychological barriers computers evoke specialists paper examines underlying suggests ways alleviating problem approach ease familiarity discussed detail illustrated means description database system approach linguistic database syntactic analysis trees natural language data",0
"Key Wordscomputational stylistics computer analysis of text dominance G. B. Shaw literary criticism literary output rhetoric statistical analysis syntactic features ","from literary output to literary criticism discovering shaws rhetoric","Research on changes in Shaw's rhetoric inMrs. Warren's Profession, Major Barbara, andHeartbreak House led me to a heuristic for gaining literary critical control over computer output. This essay describes the eleven-step process: stepping away from the data, stating first premises, developing a working hypothesis, classifying computer-sorted data, marking implicit literary sub-structures, collecting sub-structural data into tables, applying earlier statistical observations, choosing parts for detailed analysis, designing a visual method for representing the analysis, presenting segment by segment analysis of the selected data, and making larger descriptive generalizations. While describing this heuristic, the essay also reports on the Shaw research.","Computers and the Humanities",1989,"No","key wordscomputational stylistics computer analysis text dominance shaw literary criticism literary output rhetoric statistical analysis syntactic features literary output literary criticism discovering shaws rhetoric research shaw rhetoric inmrs warren profession major barbara andheartbreak house led heuristic gaining literary critical control computer output essay describes eleven step process stepping data stating premises developing working hypothesis classifying computer sorted data marking implicit literary structures collecting structural data tables applying earlier statistical observations choosing parts detailed analysis designing visual method representing analysis presenting segment segment analysis selected data making larger descriptive generalizations describing heuristic essay reports shaw research",0
"Key Wordsborrowing compounding conversion date derivation etymology GOEDEL loanword OED PAT ","report on a new oed project a study of the history of new words in the new oed","The study of the history of new words in theNewOED described in this paper was undertaken in 1986-87, and is based on the material then available. Since then, theNewOED has been finished, and PAT, the inquiry system developed at the University of Waterloo for the investigation of theNewOED data base, has been much altered and improved. Nevertheless, this report should prove useful in indicating the potentiality for analyzing the computerizedNewOED and some of the problems. This project is a study of the ways in which new words are created in English at various periods of time. A chronological dictionary 's created listing words introduced into the language over 50 year increments. These words are then classified by the processes used in forming them to show, in proportional terms, if certain processes are more common at some times than at others.","Computers and the Humanities",1989,"No","key wordsborrowing compounding conversion date derivation etymology goedel loanword oed pat report oed project study history words oed study history words thenewoed paper undertaken based material thenewoed finished pat inquiry system developed university waterloo investigation thenewoed data base altered improved report prove indicating potentiality analyzing computerizednewoed problems project study ways words created english periods time chronological dictionary created listing words introduced language year increments words classified processes forming show proportional terms processes common times ",0
"Key Wordsstyle analysis hoaxes ready-made software historical chronicle vocabulary Spanish language Puerto Rican literature ","exploring conscious imitation of style with ready made software","This article describes some approaches to imitation analysis and the use of ready-made software for this task. Devising computer-assisted techniques for exploring the conscious literary imitation of style is an application of particular relevance to contemporary Hispanic narrative and one that can be handled with a microcomputer and readily accessible software. The article describes some approaches to imitation analysis and the use of ready-made software to assess the effectiveness of stylistic imitation of eighteenth-century historical chronicle in La renuncia del héroe Baltasar (The Renunciation of the Hero Baltasar), by the Puerto Rican novelist Rodríguez Juliá. Even when employing familiar procedures of text analysis with computer, comparing a fictional text with a multiple and diverse corpus of authentic historical documents requires somewhat unique assumptions and hypotheses, since neither authorship, influence, or authenticity are in question.","Computers and the Humanities",1989,"No","key wordsstyle analysis hoaxes ready made software historical chronicle vocabulary spanish language puerto rican literature exploring conscious imitation style ready made software article describes approaches imitation analysis ready made software task devising computer assisted techniques exploring conscious literary imitation style application relevance contemporary hispanic narrative handled microcomputer readily accessible software article describes approaches imitation analysis ready made software assess effectiveness stylistic imitation eighteenth century historical chronicle la renuncia del roe baltasar renunciation hero baltasar puerto rican novelist rodr guez juli employing familiar procedures text analysis computer comparing fictional text multiple diverse corpus authentic historical documents requires unique assumptions hypotheses authorship influence authenticity question",0
"KeywordsComputational Linguistic Semantic Processing Communicative Exercise ","semantic processing for communicative exercises in foreign language learning",NA,"Computers and the Humanities",1989,"No"," computational linguistic semantic processing communicative exercise semantic processing communicative exercises foreign language learning na",0
"KeywordsComputational Linguistic Tutoring System Teaching Writing Intelligent Tutoring System ","intelligent tutoring systems exploring issues in learning and teaching writing",NA,"Computers and the Humanities",1989,"No"," computational linguistic tutoring system teaching writing intelligent tutoring system intelligent tutoring systems exploring issues learning teaching writing na",0
"Key Wordskey word model statistics stylometry vocabulary ","where have all the key words gone","A key word with regard to a sub-corpus is a word of which the frequency in that sub-corpus is significantly higher than expected under the hypothesis that its use and the variable “part of the corpus” are mutually independent. A study in literary statistics almost invariably includes a chapter devoted to key words. However, a strong attack has been recently launched upon the way stylometry has been modelling texts since the classical works of Herdan, Guiraud or Muller. In fact statistical modelling seems as valid in stylistics as in any other field of the humanities and social sciences. What is questionable is the fact that many studies in literary statistics are more satisfied with the easy identification of monsters, i.e. literary phenomena unexplained by wrong models, than with the laborious research of models fitting the textual data well. A short examination of the mentioned controversy and the quantitative analysis of an example provided by Laclos' novelLes Liaisons dangereuses endeavour to support this argument.","Computers and the Humanities",1989,"No","key wordskey word model statistics stylometry vocabulary key words key word regard corpus word frequency corpus significantly higher expected hypothesis variable part corpus mutually independent study literary statistics invariably includes chapter devoted key words strong attack recently launched stylometry modelling texts classical works herdan guiraud muller fact statistical modelling valid stylistics field humanities social sciences questionable fact studies literary statistics satisfied easy identification monsters literary phenomena unexplained wrong models laborious research models fitting textual data short examination mentioned controversy quantitative analysis provided laclos novelles liaisons dangereuses endeavour support argument",0
"Key Wordsballad “Mary Hamilton” phatic language textual variation collocation stylized language epithet concordance verbal echoes ","the way stylized language means pattern matching in the child ballads","This paper suggests ways in which the pattern-matching capability of the computer can be used to further our understanding of stylized ballad language. The study is based upon a computer-aided analysis of the entire 595,000- word corpus of Francis James Child'sThe English and Scottish Popular Ballads (1882–1892), a collection of 305 textual traditions, most of which are represented by a variety of texts. The paper focuses on the “Mary Hamilton” tradition as a means of discussing the function of phatic language in the ballad genre and the significance of textual variation.","Computers and the Humanities",1989,"No","key wordsballad mary hamilton phatic language textual variation collocation stylized language epithet concordance verbal echoes stylized language means pattern matching child ballads paper suggests ways pattern matching capability computer understanding stylized ballad language study based computer aided analysis entire word corpus francis james childsthe english scottish popular ballads collection textual traditions represented variety texts paper focuses mary hamilton tradition means discussing function phatic language ballad genre significance textual variation",0
"Key Wordstree-analysis additive trees tree-topology tree-representation algorithms computational linguistics English syntax English poetry ","unrooted trees revisited topology and poetic data","Scholars in the humanities often have to account exhaustively for the structure of large masses of data. Tree-diagrams implemented by means of suitable computer programs can be of considerable assistance in achieving a cohesive representation of the data. This paper discusses the respective merits of the two main approaches to tree representation and introduces a new method based on the use of unrooted trees. After a detailed examination of the topological properties of such trees, two algorithms are described. The second part of the paper consists in practical applications of the method of tree representation to a corpus of contemporary English poetry. Several sets of data made up of both lexical and grammatical items (adjectives, modals, auxiliaries and personal pronouns) have been submitted to the method. The findings are assessed in terms of their heuristic value in the light of modern linguistic theory and compared with the results obtained by means of more traditional statistical procedures.","Computers and the Humanities",1989,"No","key wordstree analysis additive trees tree topology tree representation algorithms computational linguistics english syntax english poetry unrooted trees revisited topology poetic data scholars humanities account exhaustively structure large masses data tree diagrams implemented means suitable computer programs considerable assistance achieving cohesive representation data paper discusses respective merits main approaches tree representation introduces method based unrooted trees detailed examination topological properties trees algorithms part paper consists practical applications method tree representation corpus contemporary english poetry sets data made lexical grammatical items adjectives modals auxiliaries personal pronouns submitted method findings assessed terms heuristic light modern linguistic theory compared results obtained means traditional statistical procedures",0
"Key WordsSociete de 1789 collocation cahiers de doléances French Revolution political lexicology ","the language of enlightened politics thesocit de 1789 in the french revolution","The Société de 1789 was a political club founded in early 1790 to propagate the ideals of the Revolution and the Enlightenment. A systematic analysis of the language found in the public discourse of the Société using simple quantitative techniques suggests important distinctions in comparison to the language found in a baseline sample, a selection of the General Cahiers de doléances of 1789. It is further argued that these differences represent an Enlightened reforming tradition that carried into the French Revolution.","Computers and the Humanities",1989,"No","key wordssociete de collocation cahiers de dol ances french revolution political lexicology language enlightened politics thesocit de french revolution soci de political club founded early propagate ideals revolution enlightenment systematic analysis language found public discourse soci simple quantitative techniques suggests important distinctions comparison language found baseline sample selection general cahiers de dol ances argued differences represent enlightened reforming tradition carried french revolution",0
"Key Wordsreading Old English information theory entropy redundancy ","an information theoretic approach to the written transmission of old english","Information theory offers a means for analyzing some constraints on the reading and copying process in Old English. Entropy for strings of various lengths offers a baseline measure of the uncertainty involved in transmission of Old English texts, while avoiding the pitfalls of applying models of modern reading to early medieval practice. Analysis of lengthy prose and verse texts in Old English revealed uniformly high values for entropy at all string lengths. High entropies may be the result of the language's irregular orthography, poetic koiné, and several dialects and imply that the language may have been easy to write but difficult to read. The low redundancy of the language which its high entropy values indicate suggests that the reader of Old English played an enhanced role in “decoding” a text and may provide an explanation for the high variability in the transmission of Old English verse.","Computers and the Humanities",1989,"No","key wordsreading english information theory entropy redundancy information theoretic approach written transmission english information theory offers means analyzing constraints reading copying process english entropy strings lengths offers baseline measure uncertainty involved transmission english texts avoiding pitfalls applying models modern reading early medieval practice analysis lengthy prose verse texts english revealed uniformly high values entropy string lengths high entropies result language irregular orthography poetic koin dialects imply language easy write difficult read low redundancy language high entropy values suggests reader english played enhanced role decoding text provide explanation high variability transmission english verse",0
"Key Wordshypertext hypermedia Intermedia educational computing student-directed learning collaborative learning literary theory interdisciplinary graphics ","hypertext in literary education criticism and scholarship","After describing the English course and the particular hypertext system that supports it at Brown University, the essay surveys the materials on Context32, that part of the system devoted to literature courses, and narrates how a student uses the system during a typical session in our electronic laboratory/classroom. Next, it presents evidence of the effects of such information technology on student performance, after which it examines the relation of hypertext to contemporary literary theory, in particular to the ideas of decentering, intertextuality, and anti-hierarchical texts. Finally, it explains the continuing developments of Intermedia.","Computers and the Humanities",1989,"No","key wordshypertext hypermedia intermedia educational computing student directed learning collaborative learning literary theory interdisciplinary graphics hypertext literary education criticism scholarship describing english hypertext system supports brown university essay surveys materials context part system devoted literature courses narrates student system typical session electronic laboratoryclassroom presents evidence effects information technology student performance examines relation hypertext contemporary literary theory ideas decentering intertextuality anti hierarchical texts finally explains continuing developments intermedia",0
"KeywordsComputational Linguistic ","fortiers accusations a reply",NA,"Computers and the Humanities",1989,"No"," computational linguistic fortiers accusations reply na",0
"Key Wordsstyle/stylistics quantitative studies of literature ambiguity/disambiguation/figurative language textuality literary theory relevance parody reader response (theory/research) ","quantitative studies of literature a critique and an outlook","The present paper is a critique of quantitative studies of literature. It is argued that such studies are involved in an act of reification, in which, moreover, fundamental ingredients of the texts, e.g. their (highly important) range of figurative meanings, are eliminated from the analysis. Instead a concentration on lower levels of linguistic organization, such as grammar and lexis, may be observed, in spite of the fact that these are often the least relevant aspects of the text. In doing so, quantitative studies of literature significantly reduce not only the cultural value of texts, but also the generalizability of its own findings. What is needed, therefore, is an awareness and readiness to relate to matters of textuality as an organizing principle underlying the cultural functioning of literary works of art.","Computers and the Humanities",1989,"No","key wordsstylestylistics quantitative studies literature ambiguitydisambiguationfigurative language textuality literary theory relevance parody reader response theoryresearch quantitative studies literature critique outlook present paper critique quantitative studies literature argued studies involved act reification fundamental ingredients texts highly important range figurative meanings eliminated analysis concentration lower levels linguistic organization grammar lexis observed spite fact relevant aspects text quantitative studies literature significantly reduce cultural texts generalizability findings needed awareness readiness relate matters textuality organizing principle underlying cultural functioning literary works art",0
"Key Wordstree model tree topology fitting tree algorithm textual analysis lexical connexion Victor Hugo Emile Zola semantic memory ","using a tree model in textual analysis","The purpose of this paper is to show the utility of the application of a non-ultrametric tree-model to textual data. The first part introduces a basic topological property of the tree and the notion of neighbourhood, which reflects the structure of the tree. The second part emphasizes through illustration examples the adequacy of this model for representing different varieties of textual data.","Computers and the Humanities",1989,"No","key wordstree model tree topology fitting tree algorithm textual analysis lexical connexion victor hugo emile zola semantic memory tree model textual analysis purpose paper show utility application ultrametric tree model textual data part introduces basic topological property tree notion neighbourhood reflects structure tree part emphasizes illustration examples adequacy model representing varieties textual data",0
NA,"software review",NA,"Computers and the Humanities",1989,"No"," software review na",0
"Key WordsRousseau Emile Profession de foi macro/micro-context modal sequence and length function/content words ","lexical and focal preferences in rousseausprofession de foi du vicaire savoyard book iv ofemile","Based on the ARTFL version of theProfession and excerpts fromEmile, high frequency function and content words, as defined by Brunet, are analyzed via Pearson chi square tests. Next, four measures of narrative voice from the same populations are compared using Markovian chains and further chi square tests. In a third analysis the two orders of evidence are juxtaposed. The lexical and narratological preferences of theVicaire and theGouverneur, while not resolving the problematic of chronological composition (Burgelin, 1969), highlight the distinctiveness of each character.","Computers and the Humanities",1989,"No","key wordsrousseau emile profession de foi macromicro context modal sequence length functioncontent words lexical focal preferences rousseausprofession de foi du vicaire savoyard book iv ofemile based artfl version theprofession excerpts fromemile high frequency function content words defined brunet analyzed pearson chi square tests measures narrative voice populations compared markovian chains chi square tests analysis orders evidence juxtaposed lexical narratological preferences thevicaire thegouverneur resolving problematic chronological composition burgelin highlight distinctiveness character",0
NA,"a bibliography of intelligent computer assisted language instruction",NA,"Computers and the Humanities",1989,"No"," bibliography intelligent computer assisted language instruction na",0
NA,"le trsor gnral des langes et parlers franais de linstitut national de la langue franaise inalf",NA,"Computers and the Humanities",1988,"No"," le trsor gnral des langes parlers franais de linstitut national de la langue franaise inalf na",0
"KeywordsComputational Linguistic Literary Analysis ","the stylo statistical method of literary analysis",NA,"Computers and the Humanities",1988,"No"," computational linguistic literary analysis stylo statistical method literary analysis na",0
"Key Wordsliterary computing literary criticism text input statistical methods textual analysis polemics ","literary criticism and literary computing the difficulties of a synthesis","Currently most literary critics reject the use of science and technology to gain information about texts, while most computer text-analysts have become absorbed in science and technology and forgotten they were seeking information about literature. Whether these two trends will continue into the 1990's remains to be seen; that they explain a good deal about the world we work in now can, I think, be demonstrated. This essay looks at the questions of what literary computing could offer to literary critics, why computer users get lost in scientific jargon, what happens when text becomes input and, most importantly, what happens when text becomes output; it closes with a discussion of why the synthesis will be so difficult.","Computers and the Humanities",1988,"No","key wordsliterary computing literary criticism text input statistical methods textual analysis polemics literary criticism literary computing difficulties synthesis literary critics reject science technology gain information texts computer text analysts absorbed science technology forgotten seeking information literature trends continue remains explain good deal world work demonstrated essay questions literary computing offer literary critics computer users lost scientific jargon text input importantly text output closes discussion synthesis difficult",0
"Key Wordsinteractive fiction literariness canonization ostranenie gap strangeness Shklovskij Portal ","determining literariness in interactive fiction","The authors of interactive fiction are beginning to demonstrate a concern for the literariness of their product. Literariness, as defined by Shklovskij and the Russian Formalists, is the quality of “making strange” that which is linguistically familiar, a quality Shklovskij termed ostranenie. By applying the principle of ostranenie, as well as other well-known literary principles, to the most serious interactive fictions, we can determine if this new genre exhibits the features of literariness. A study of Mindwheel, Brimstone, Breakers, A Mind Forever Voyaging, Portal, and Trinity suggest that the literariness of interactive fiction comes out of its concern both for “making strange” what is familiar and for “making familiar” what is strange.","Computers and the Humanities",1988,"No","key wordsinteractive fiction literariness canonization ostranenie gap strangeness shklovskij portal determining literariness interactive fiction authors interactive fiction beginning demonstrate concern literariness product literariness defined shklovskij russian formalists quality making strange linguistically familiar quality shklovskij termed ostranenie applying principle ostranenie literary principles interactive fictions determine genre exhibits features literariness study mindwheel brimstone breakers mind forever voyaging portal trinity suggest literariness interactive fiction concern making strange familiar making familiar strange",0
"Key WordsEuripides — themes Euripides — political meanings Euripides — Suppliant Women old age and youth in Ancient Greece Guiraudian studies Euripides — Heraclidae Euripides — Andromache Euripides — Hecabe Euripides — Heracles Euripides — Alcestis ","a study of words relating to youth and old age in the plays of euripides and its special implications for euripides suppliant women","This study focuses on the imagery of youth and old age in the plays of Euripides, especially the Suppliant Women, considering frequently used words in each play according to a formula developed by Guiraud. The study identifies a motif, the rejuvenation theme, an elaborate interaction between young and old, in the Suppliant Women and in: Alcestis, Heraclidae, Andromache, Hecabe, and Heracles. The difference between the use of neos (young, new) in the Suppliant Women and in the other plays is statistically significant. This word helps Euripides contrast two different kinds of youth: the fearful, rash, and animalistic (Theban); and that which has been properly schooled and led (Athenian). The greatest ground in the Suppliant Women for praising Athens is in her treatment of the young as a politically valuable force.","Computers and the Humanities",1988,"No","key wordseuripides themes euripides political meanings euripides suppliant women age youth ancient greece guiraudian studies euripides heraclidae euripides andromache euripides hecabe euripides heracles euripides alcestis study words relating youth age plays euripides special implications euripides suppliant women study focuses imagery youth age plays euripides suppliant women frequently words play formula developed guiraud study identifies motif rejuvenation theme elaborate interaction young suppliant women alcestis heraclidae andromache hecabe heracles difference neos young suppliant women plays statistically significant word helps euripides contrast kinds youth fearful rash animalistic theban properly schooled led athenian greatest ground suppliant women praising athens treatment young politically valuable force",0
"Key Wordsdictionary lexicography concording lemmatizing editing CD-ROM database ","unseen users unknown systems computer design for a scholars dictionary","The Dictionary of Old English computing systems have provided access since the 1970s to a database of approximately three million running words. These systems, designed for a variety of machines and written in a variety of languages, have until recently been planned with computing center billing algorithms in mind. With personal workstations emphasis has shifted to building more elegant user interfaces and to providing the entire DOE database to editors around the world. While the shift from sequential files to random access files and the provision of extensive development tools have changed some of the design process, error checking and protection of the database against accidental intrusion have remained as central issues.","Computers and the Humanities",1988,"No","key wordsdictionary lexicography concording lemmatizing editing cd rom database unseen users unknown systems computer design scholars dictionary dictionary english computing systems provided access s database approximately million running words systems designed variety machines written variety languages recently planned computing center billing algorithms mind personal workstations emphasis shifted building elegant user interfaces providing entire doe database editors world shift sequential files random access files provision extensive development tools changed design process error checking protection database accidental intrusion remained central issues",0
"Key WordsDynamic programming Indus texts Harappan civilization segmentation ","segmentation of indus texts a dynamic programming approach","The Indus valley civilization, also known as the Harappan civilization flourished from 2600 B.C. to 1750 B.C. in the Indian subcontinent. The writings of that civilization are in a pictorial type of script which has not been deciphered so far.","Computers and the Humanities",1988,"No","key wordsdynamic programming indus texts harappan civilization segmentation segmentation indus texts dynamic programming approach indus valley civilization harappan civilization flourished indian subcontinent writings civilization pictorial type script deciphered ",0
"Key WordsCD-ROM TLG Project Thesaurus Linguae Graecae Isocrates Project IRIS Ibycus Scholarly Computer Perseus Project Septuagint Studies Project Packard Humanities Foundation ","cd rom and scholarly research in the humanities","The conversion of all classical literature for the period of Homer in the 8th century B.C. through the 6th century A.D. into machine-readable format — designated the Thesaurus Linguae Graecae project — was the impetus behind the use of classical literature in a variety of electronic research environments. Initially targeted for mainframe storage and retrieval, the data is now also being published and distributed on CD-ROM for use with microcomputers. Two such projects, the TLG Project at the University of California-Trvine and the Isocrates Project at Brown University's IRIS Center are described as well as other CD-ROM projects for the storage and dissemination of literature in the humanities and classical research. Various CD-ROM systems are also described, including the Ibycus Scholarly Computer.","Computers and the Humanities",1988,"No","key wordscd rom tlg project thesaurus linguae graecae isocrates project iris ibycus scholarly computer perseus project septuagint studies project packard humanities foundation cd rom scholarly research humanities conversion classical literature period homer th century th century machine readable format designated thesaurus linguae graecae project impetus classical literature variety electronic research environments initially targeted mainframe storage retrieval data published distributed cd rom microcomputers projects tlg project university california trvine isocrates project brown university iris center cd rom projects storage dissemination literature humanities classical research cd rom systems including ibycus scholarly computer",0
"KeywordsComputational Linguistic ","style and structure in the middle english poemcleanness",NA,"Computers and the Humanities",1987,"No"," computational linguistic style structure middle english poemcleanness na",0
"KeywordsClassroom Instruction Heuristic Procedure Tutorial Group College Freshman Human Tutor ","using computer technology to teach and evaluate prewriting","The purpose of this research was to determine whether computer-aided instruction may be effectively utilized in stimulating prewriting composition when the CAI is based upon (1) conceptual (cognitive) strategies, (2) “data-driven” guidance (resulting from CAE techniques), and (3) recent findings in tutorial strategies research. If this specifically designed CAI is as good a means of prewriting instruction as personal tutoring and a better means than classroom instruction, then the practical and economical implications may be weighed in a decision to use such techniques. Forty-three college freshmen in three basic writing classes participated in this study. One class was exposed to a CAI medium, the other two either to a human tutor or to classroom instruction. A computer-aided evaluation of previous essays provided focus, and other intellectual processing cues provided information on an expository topic; this “database” was then used to construct a CAI program to encourage “specificity” and “depth of intellectual processing“ in students' prewriting composition. The program also possessed and was designed to provide “conceptual guidance” through the use of five heuristic procedures; thus it contained two key elements that a human tutor would possess in working with a topic—knowledge of the topic, and a means for eliciting that knowledge from the tutee. The second treatment method used consisted of instruction by human tutors, utilizing the same methodology. The control for the study consisted of a classroom instruction group. Results showed the CAI group demonstrating gains in every category of measurement utilized in this study, and its performances was significantly better than both the tutorial group on two of the post-test measures. The CAI group was superior, through not significantly, on post-test performances in every category used in the study except fluency.","Computers and the Humanities",1987,"No"," classroom instruction heuristic procedure tutorial group college freshman human tutor computer technology teach evaluate prewriting purpose research determine computer aided instruction effectively utilized stimulating prewriting composition cai based conceptual cognitive strategies data driven guidance resulting cae techniques recent findings tutorial strategies research specifically designed cai good means prewriting instruction personal tutoring means classroom instruction practical economical implications weighed decision techniques forty college freshmen basic writing classes participated study class exposed cai medium human tutor classroom instruction computer aided evaluation previous essays provided focus intellectual processing cues provided information expository topic database construct cai program encourage specificity depth intellectual processing students prewriting composition program possessed designed provide conceptual guidance heuristic procedures contained key elements human tutor possess working topic knowledge topic means eliciting knowledge tutee treatment method consisted instruction human tutors utilizing methodology control study consisted classroom instruction group results showed cai group demonstrating gains category measurement utilized study performances significantly tutorial group post test measures cai group superior significantly post test performances category study fluency",0
"KeywordsNatural Language Computational Linguistic Language Text Natural Language Text Computer Detection ","computer detection of errors in natural language texts some research on pattern matching",NA,"Computers and the Humanities",1987,"No"," natural language computational linguistic language text natural language text computer detection computer detection errors natural language texts research pattern matching na",0
"KeywordsComputational Linguistic Computational Consideration Expressive Metaphor Literal Analogy ","computational considerations for the processing of explanatory literal analogies and expressive metaphors",NA,"Computers and the Humanities",1987,"No"," computational linguistic computational consideration expressive metaphor literal analogy computational considerations processing explanatory literal analogies expressive metaphors na",0
NA,"book review",NA,"Computers and the Humanities",1986,"No"," book review na",0
NA,"rapport dactivit sur le traitement informatique des textes mdivaux",NA,"Computers and the Humanities",1986,"No"," rapport dactivit sur le traitement informatique des textes mdivaux na",0
NA,"lurl9 etude statistique des textes littraires",NA,"Computers and the Humanities",1986,"No"," lurl etude statistique des textes littraires na",0
"KeywordsPedagogical Approach Computational Linguistic ","a new pedagogical approach to the study of texts with a microcomputer",NA,"Computers and the Humanities",1986,"No"," pedagogical approach computational linguistic pedagogical approach study texts microcomputer na",0
NA,"quelques rsultats dune analyse automatique du discours duplessiste",NA,"Computers and the Humanities",1986,"No"," quelques rsultats dune analyse automatique du discours duplessiste na",0
"KeywordsComputational Linguistic ","contemporary literary lexicology and terminology an inventory",NA,"Computers and the Humanities",1986,"No"," computational linguistic contemporary literary lexicology terminology inventory na",0
"KeywordsCluster Analysis Computational Linguistic ","cluster analysis for the computer assisted statistical analysis of melodies",NA,"Computers and the Humanities",1986,"No"," cluster analysis computational linguistic cluster analysis computer assisted statistical analysis melodies na",0
NA,"construction dun prototype de systme expert dans le domaine historique",NA,"Computers and the Humanities",1986,"No"," construction dun prototype de systme expert dans le domaine historique na",0
NA,"laboratoire danalyse relationnelle des textes prsentation de travaux",NA,"Computers and the Humanities",1986,"No"," laboratoire danalyse relationnelle des textes prsentation de travaux na",0
"KeywordsComputational Linguistic Oxford English Dictionary ","creating the electronic new oxford english dictionary",NA,"Computers and the Humanities",1986,"No"," computational linguistic oxford english dictionary creating electronic oxford english dictionary na",0
NA,"mthode danalyses statistiques informatises des microtoponymes franc comtois",NA,"Computers and the Humanities",1986,"No"," mthode danalyses statistiques informatises des microtoponymes franc comtois na",0
"KeywordsResearch Centre Computational Linguistic Automatic Treatment Classical Archaeology ","the research centre for automatic treatments in classical archaeology",NA,"Computers and the Humanities",1986,"No"," research centre computational linguistic automatic treatment classical archaeology research centre automatic treatments classical archaeology na",0
"KeywordsComputational Linguistic Egypt Literature ","indexes of citations from ancient egypt literature",NA,"Computers and the Humanities",1986,"No"," computational linguistic egypt literature indexes citations ancient egypt literature na",0
"KeywordsInitial Data Natural Language Internal Representation Specific Structuration Specific Information ","limited context semantic translation from a single knowledge base for a natural language and structuring metarules","The article introduces an experimental system which produces multilingual semantic translations from relatively short texts from a given context.","Computers and the Humanities",1986,"No"," initial data natural language internal representation specific structuration specific information limited context semantic translation single knowledge base natural language structuring metarules article introduces experimental system produces multilingual semantic translations short texts context",0
NA,"du texte latin la concordance imprime","The development of the technique of laser printing opens new ways for the writing of concordances to Latin texts, by means of electronic data processing. It is indeed possible to obtain as good a quality of printing as with traditional typography, even for a very important number of characters, which is always the case with concordances. A first step to obtaining perfection in the format of the final document consists in collecting data carefully and programming the software so as to provide scholars with a tool that will really facilitate their researches. Although the cutting out of the “useful context” is a very difficult problem to solve when using computers, as many data (punctuation, capital letters, diacritic signs...) as possible can be collected from the very beginning. These data will then make the text provided for each keyword more comprehensible. The programming must obviously be improved to take all the collected data into account. The reader of a concordance produced in such a way will have at his disposal a text easy to read not only because of its typographic quality, but also because of all its information on reference, punctuation, lemma, manuscript tradition; also he will be able to find in appendices pertinent frequency lists, lists of names and all sorts of lexical, morphological, syntactical information according to the work involved in the data collecting and programming.","Computers and the Humanities",1986,"No"," du texte latin la concordance imprime development technique laser printing opens ways writing concordances latin texts means electronic data processing obtain good quality printing traditional typography important number characters case concordances step obtaining perfection format final document consists collecting data carefully programming software provide scholars tool facilitate researches cutting context difficult problem solve computers data punctuation capital letters diacritic signs collected beginning data make text provided keyword comprehensible programming improved collected data account reader concordance produced disposal text easy read typographic quality information reference punctuation lemma manuscript tradition find appendices pertinent frequency lists lists names sorts lexical morphological syntactical information work involved data collecting programming",0
"KeywordsComputational Linguistic Software Review ","software reviews",NA,"Computers and the Humanities",1986,"No"," computational linguistic software review software reviews na",0
NA,"vers un systme expert pour lanalyse des textes de moyen francais","Concerning Medieval French, text-processing has to handle two major problems: first the under-developed state of linguistic knowledge of scholars; secondly the very large variations in forms and spellings occurring throughout the manuscripts. In order to extract and provide lexicographical and grammatical information from many yet undescribed texts, our research group has decided to devise an expert system which would help identify and parse the text-words and would be linked to an open relational database. The design of our expert system is outlined here: we especially outline the initial knowledge base and the nature of the different sets of rules we will make use of. Some examples describe how the system will operate and the types of advantages that are expected from its use. Finally the paper lists the studies and published work which in the last years have provided the groundwork for this project.","Computers and the Humanities",1986,"No"," vers systme expert pour lanalyse des textes de moyen francais medieval french text processing handle major problems developed state linguistic knowledge scholars large variations forms spellings occurring manuscripts order extract provide lexicographical grammatical information undescribed texts research group decided devise expert system identify parse text words linked open relational database design expert system outlined outline initial knowledge base nature sets rules make examples describe system operate types advantages expected finally paper lists studies published work years provided groundwork project",0
"KeywordsComputational Linguistic Human Language ","human language and computers",NA,"Computers and the Humanities",1985,"No"," computational linguistic human language human language computers na",0
"KeywordsComputational Linguistic ","computational lexicography",NA,"Computers and the Humanities",1985,"No"," computational linguistic computational lexicography na",0
NA,"book reviews",NA,"Computers and the Humanities",1985,"No"," book reviews na",0
"KeywordsArtificial Intelligence Literary Research Design Specification Computational Linguistic ","integrating artificial intelligence into literary research an invitation to discuss design specifications",NA,"Computers and the Humanities",1985,"No"," artificial intelligence literary research design specification computational linguistic integrating artificial intelligence literary research invitation discuss design specifications na",0
"KeywordsComputational Linguistic Sound Effect ","most by numbers judge a poets song measuring sound effects in poetry",NA,"Computers and the Humanities",1985,"No"," computational linguistic sound effect numbers judge poets song measuring sound effects poetry na",0
"KeywordsStatistical Approach Computational Linguistic ","elements of a statistical approach to the question of authorship in music",NA,"Computers and the Humanities",1985,"No"," statistical approach computational linguistic elements statistical approach question authorship music na",0
"KeywordsWord Frequency Computational Linguistic Text Type English Text ","word frequency and text type some observations based on the lob corpus of british english texts",NA,"Computers and the Humanities",1985,"No"," word frequency computational linguistic text type english text word frequency text type observations based lob corpus british english texts na",0
"KeywordsComputer System Computational Linguistic Final Design ","the dictionary of old english and the final design of its computer system",NA,"Computers and the Humanities",1985,"No"," computer system computational linguistic final design dictionary english final design computer system na",0
"KeywordsComputational Linguistic Short Context ","disambiguation by short contexts",NA,"Computers and the Humanities",1985,"No"," computational linguistic short context disambiguation short contexts na",0
"KeywordsComputational Linguistic Computer Study English Poem ","a poetic formula inbeowulf and seven other old english poems a computer study",NA,"Computers and the Humanities",1985,"No"," computational linguistic computer study english poem poetic formula inbeowulf english poems computer study na",0
NA,"quelques rflexions sur le statut epistmologique du texte electronique",NA,"Computers and the Humanities",1985,"No"," quelques rflexions sur le statut epistmologique du texte electronique na",0
"KeywordsHumanity Research Computational Linguistic Round Table House Round ","the grinnell house round table on a center for computer aided humanities research",NA,"Computers and the Humanities",1985,"No"," humanity research computational linguistic round table house round grinnell house round table center computer aided humanities research na",0
"KeywordsInstructional Unit Essay Question Domestic Program American Historical Association Essay Test ","the individualized history survey course and the computer",NA,"Computers and the Humanities",1984,"No"," instructional unit essay question domestic program american historical association essay test individualized history survey computer na",0
"KeywordsComputational Linguistic Machine Translation ","machine translation in the ussr",NA,"Computers and the Humanities",1984,"No"," computational linguistic machine translation machine translation ussr na",0
NA,"review essay",NA,"Computers and the Humanities",1984,"No"," review essay na",0
"KeywordsForeign Language Computational Linguistic ","blueprint for a comprehensive foreign language cai curriculum",NA,"Computers and the Humanities",1984,"No"," foreign language computational linguistic blueprint comprehensive foreign language cai curriculum na",0
"KeywordsArtificial Intelligence Computational Model Computational Linguistic Narrative Theory ","narrative theories as computational models reader oriented theory and artificial intelligence",NA,"Computers and the Humanities",1983,"No"," artificial intelligence computational model computational linguistic narrative theory narrative theories computational models reader oriented theory artificial intelligence na",0
"KeywordsComputational Linguistic Spanish Language ","procedures and progress on the dictionary of the old spanish language",NA,"Computers and the Humanities",1983,"No"," computational linguistic spanish language procedures progress dictionary spanish language na",0
"KeywordsComputational Linguistic Syntactic Analysis ","choice of grammatical word class without global syntactic analysis tagging words in the lob corpus",NA,"Computers and the Humanities",1983,"No"," computational linguistic syntactic analysis choice grammatical word class global syntactic analysis tagging words lob corpus na",0
"KeywordsDiscriminant Function Production Area Classification Algorithm Computational Linguistic Morphological Attribute ","typologie damphores romaines par une methode logique de classification","For classifying wine amphoras used at the end of the Roman Republic and the beginning of the Empire (the so-called Dressel 2–4), we present a typological approach which combines a classification algorithm with the archeological reasoning. At the first step, clusters contain only nuclei based on the different production areas. To assign a corpus of artifacts to them, it is divided for each cluster into a context (artifacts which certainly do not belong to the cluster) and a residue. For each cluster, we built characteristic definitions with logical discriminant function of morphological attributes. Each definition cuts the residue in two classes: one containing the artifacts assigned to the cluster by the definition and the complementary one in the residue. Assignment and choices of cluster definitions and context remain with the archeological expert, who submits those typological constructions to a validation process founded on archeological knowledge. Such an approach focuses on a very common situation in human sciences: the construction of a cognitive typology beginning with a partially clustered set. Clustering must be done with descriptive attributes, without knowing if they can be connected with the wanted cluster.","Computers and the Humanities",1983,"No"," discriminant function production area classification algorithm computational linguistic morphological attribute typologie damphores romaines par une methode logique de classification classifying wine amphoras end roman republic beginning empire called dressel present typological approach combines classification algorithm archeological reasoning step clusters nuclei based production areas assign corpus artifacts divided cluster context artifacts belong cluster residue cluster built characteristic definitions logical discriminant function morphological attributes definition cuts residue classes artifacts assigned cluster definition complementary residue assignment choices cluster definitions context remain archeological expert submits typological constructions validation process founded archeological knowledge approach focuses common situation human sciences construction cognitive typology beginning partially clustered set clustering descriptive attributes knowing connected wanted cluster",0
"KeywordsComputational Linguistic Computer Generation ","computer generation of melodies further proposals",NA,"Computers and the Humanities",1983,"No"," computational linguistic computer generation computer generation melodies proposals na",0
"KeywordsData Base Knowledge Representation Computational Linguistic Linguistic Study ","data bases and knowledge representation for literary and linguistic studies",NA,"Computers and the Humanities",1983,"No"," data base knowledge representation computational linguistic linguistic study data bases knowledge representation literary linguistic studies na",0
"KeywordsComputational Linguistic Literary Attribution ","literary attribution and likelihood ratio tests the case of the middle englishpearl poems",NA,"Computers and the Humanities",1983,"No"," computational linguistic literary attribution literary attribution likelihood ratio tests case middle englishpearl poems na",0
"KeywordsData Management Computational Linguistic Data Management System Linguistic Data ","ldms a linguistic data management system",NA,"Computers and the Humanities",1983,"No"," data management computational linguistic data management system linguistic data ldms linguistic data management system na",0
"KeywordsComputational Linguistic Machine Decision ","automated concordances and word indexes machine decisions and editorial revisions",NA,"Computers and the Humanities",1982,"No"," computational linguistic machine decision automated concordances word indexes machine decisions editorial revisions na",0
"KeywordsComputational Linguistic ","the annals of computing stylistics",NA,"Computers and the Humanities",1982,"No"," computational linguistic annals computing stylistics na",0
NA,"a statistical study of authorship in the corpus lysiacum",NA,"Computers and the Humanities",1982,"No"," statistical study authorship corpus lysiacum na",0
"KeywordsComputational Linguistic ","computers and philosophical lexicography the activities of the lessico intellettuale europeo",NA,"Computers and the Humanities",1982,"No"," computational linguistic computers philosophical lexicography activities lessico intellettuale europeo na",0
"KeywordsComputational Linguistic Automatic Expansion ","automatic expansion of abbreviations an experiment with old icelandic",NA,"Computers and the Humanities",1982,"No"," computational linguistic automatic expansion automatic expansion abbreviations experiment icelandic na",0
"KeywordsArtificial Intelligence Knowledge Representation Computational Linguistic ","artificial intelligence history and knowledge representation",NA,"Computers and the Humanities",1982,"No"," artificial intelligence knowledge representation computational linguistic artificial intelligence history knowledge representation na",0
"KeywordsComputational Linguistic ","troubadours and transposition a computer aided study",NA,"Computers and the Humanities",1982,"No"," computational linguistic troubadours transposition computer aided study na",0
"KeywordsComputational Linguistic Newspaper Text German Newspaper ","lemmatizing german newspaper texts with the aid of an algorithm",NA,"Computers and the Humanities",1981,"No"," computational linguistic newspaper text german newspaper lemmatizing german newspaper texts aid algorithm na",0
"KeywordsComputational Linguistic ","abstracts and brief reports",NA,"Computers and the Humanities",1981,"No"," computational linguistic abstracts reports na",0
"KeywordsComputational Linguistic Word Index ","automated concordances and word indexes the fifties",NA,"Computers and the Humanities",1981,"No"," computational linguistic word index automated concordances word indexes fifties na",0
"KeywordsComputational Linguistic ","letter from ljubljana",NA,"Computers and the Humanities",1981,"No"," computational linguistic letter ljubljana na",0
"KeywordsComputational Linguistic Early Sixty Early Center Word Index ","automated concordances and word indexes the early sixties and the early centers",NA,"Computers and the Humanities",1981,"No"," computational linguistic early sixty early center word index automated concordances word indexes early sixties early centers na",0
"KeywordsComputational Linguistic ","letter from boulder",NA,"Computers and the Humanities",1981,"No"," computational linguistic letter boulder na",0
"KeywordsComputational Linguistic Word Index ","automated concordances and word indexes the process the programs and the products",NA,"Computers and the Humanities",1981,"No"," computational linguistic word index automated concordances word indexes process programs products na",0
"KeywordsComputational Linguistic ","prolegomena to pictorial concordances",NA,"Computers and the Humanities",1981,"No"," computational linguistic prolegomena pictorial concordances na",0
"KeywordsComputational Linguistic Universal Alphabet ","a universal alphabet for experiments in comparative phonology",NA,"Computers and the Humanities",1981,"No"," computational linguistic universal alphabet universal alphabet experiments comparative phonology na",0
"KeywordsComputational Linguistic System Overview Tune Index National Tune ","the national tune index a systems overview",NA,"Computers and the Humanities",1981,"No"," computational linguistic system overview tune index national tune national tune index systems overview na",0
"KeywordsVerse Computational Linguistic German Verse ","phonology and style a computer assisted approach to german verse",NA,"Computers and the Humanities",1981,"No"," verse computational linguistic german verse phonology style computer assisted approach german verse na",0
"KeywordsData Entry Computational Linguistic ","hardware review the kurzweil data entry machine",NA,"Computers and the Humanities",1981,"No"," data entry computational linguistic hardware review kurzweil data entry machine na",0
"KeywordsRetrieval System Computational Linguistic Responsum Project ","computerized full text retrieval systems and research in the humanities the responsa project",NA,"Computers and the Humanities",1980,"No"," retrieval system computational linguistic responsum project computerized full text retrieval systems research humanities responsa project na",0
"KeywordsComputational Linguistic ","the annals of computing the greek testament",NA,"Computers and the Humanities",1980,"No"," computational linguistic annals computing greek testament na",0
"KeywordsComputational Linguistic ","concordances and indices to middle high german",NA,"Computers and the Humanities",1980,"No"," computational linguistic concordances indices middle high german na",0
"KeywordsHistory Data Automatic Processing Computational Linguistic ","the first international conference on automatic processing of art history data and documents a report",NA,"Computers and the Humanities",1980,"No"," history data automatic processing computational linguistic international conference automatic processing art history data documents report na",0
"KeywordsData Base Computational Linguistic Vocabulary Data ","on a vocabulary data base",NA,"Computers and the Humanities",1980,"No"," data base computational linguistic vocabulary data vocabulary data base na",0
"KeywordsComputational Linguistic ","the making of a masterpiece stephan cranes the red badge of courage",NA,"Computers and the Humanities",1980,"No"," computational linguistic making masterpiece stephan cranes red badge courage na",0
"KeywordsPresent State Good Deal Modern Technology Computational Linguistic Scholarly Community ","computer implemented music analysis and the copyright law","Our discussion has shown not only that there is a good deal of uncertainty in the present state of the law concerning the problem that we posed, but also that, even if currently recommended legislation were to make the recently enacted copyright law fully applicable to it, a number of open questions would remain.","Computers and the Humanities",1980,"No"," present state good deal modern technology computational linguistic scholarly community computer implemented music analysis copyright law discussion shown good deal uncertainty present state law problem posed recommended legislation make recently enacted copyright law fully applicable number open questions remain",0
"KeywordsInternal Structure Computer Analysis Computational Linguistic Transition Area Complex Transition ","computer analysis of a dialectal transition belt","The layout of the isogloss routes suggests a complex transition area with transitions between the East and West Midlands as well as between the North and the South. And so it is. But it is essentially a transition-area between the Core North and the Core South, with the frontiers of minor speech-areas super-imposed on that bilateral division.","Computers and the Humanities",1980,"No"," internal structure computer analysis computational linguistic transition area complex transition computer analysis dialectal transition belt layout isogloss routes suggests complex transition area transitions east west midlands north south essentially transition area core north core south frontiers minor speech areas super imposed bilateral division",0
"KeywordsComputational Linguistic Humanity Computing ","the annals of humanities computing the index thomisticus",NA,"Computers and the Humanities",1980,"No"," computational linguistic humanity computing annals humanities computing index thomisticus na",0
"KeywordsComputational Linguistic Chamber Music Nationalistic Fingerprint ","the nationalistic fingerprint in nineteenth century romantic chamber music",NA,"Computers and the Humanities",1979,"No"," computational linguistic chamber music nationalistic fingerprint nationalistic fingerprint nineteenth century romantic chamber music na",0
"KeywordsComputational Linguistic ","letter from oslo",NA,"Computers and the Humanities",1979,"No"," computational linguistic letter oslo na",0
"KeywordsDiscriminant Function Sample Standard Deviation Personal Pronoun Estimate Regression Equation Henry Versus ","pronouns and genre in shakespeares drama","The Tempest","Computers and the Humanities",1979,"No"," discriminant function sample standard deviation personal pronoun estimate regression equation henry versus pronouns genre shakespeares drama tempest",0
NA,"zur formelhaftigkeit in heinrich wittenwilers ring wortwiederholungen und grammatische versmuster",NA,"Computers and the Humanities",1979,"No"," zur formelhaftigkeit heinrich wittenwilers ring wortwiederholungen und grammatische versmuster na",0
"KeywordsComputational Linguistic ","measurement and the study of literature",NA,"Computers and the Humanities",1979,"No"," computational linguistic measurement study literature na",0
NA,"letter from nancy",NA,"Computers and the Humanities",1979,"No"," letter nancy na",0
NA,"annual bibliography for 1977 and supplement to preceding years",NA,"Computers and the Humanities",1979,"No"," annual bibliography supplement preceding years na",0
"KeywordsComputational Linguistic ","letter from cambridge",NA,"Computers and the Humanities",1979,"No"," computational linguistic letter cambridge na",0
"KeywordsAmbiguous Word Basic Word Lexical Ambiguity Punctuation Mark Word Combination ","the contextological dictionary use in programmed language teaching",NA,"Computers and the Humanities",1979,"No"," ambiguous word basic word lexical ambiguity punctuation mark word combination contextological dictionary programmed language teaching na",0
"KeywordsWord List Magnetic Tape Optical Character Recognition Text Processing Text Element ","a text processing system for the preparation of critical editions",NA,"Computers and the Humanities",1979,"No"," word list magnetic tape optical character recognition text processing text element text processing system preparation critical editions na",0
"KeywordsMachine Translation Target Language Lexical Item Indexing Scheme Semantic Component ","a survey of approaches and issues in machine aided translation systems",NA,"Computers and the Humanities",1979,"No"," machine translation target language lexical item indexing scheme semantic component survey approaches issues machine aided translation systems na",0
"KeywordsComputational Linguistic Review Author Software Review ","software review author",NA,"Computers and the Humanities",1978,"No"," computational linguistic review author software review software review author na",0
"KeywordsComputational Linguistic ","the study of chaucers vocabulary",NA,"Computers and the Humanities",1978,"No"," computational linguistic study chaucers vocabulary na",0
"KeywordsComputational Linguistic ","formulaic analysis of the computer accessible corpus of latvian sun songs",NA,"Computers and the Humanities",1978,"No"," computational linguistic formulaic analysis computer accessible corpus latvian sun songs na",0
"KeywordsComputational Linguistic Belgian Initiative ","vox latina belgian initiatives in data processing the intellectual language of europe ad 1971965",NA,"Computers and the Humanities",1978,"No"," computational linguistic belgian initiative vox latina belgian initiatives data processing intellectual language europe ad na",0
"KeywordsComputational Linguistic Numerical Taxonomy ","a numerical taxonomy of merovingian coins",NA,"Computers and the Humanities",1978,"No"," computational linguistic numerical taxonomy numerical taxonomy merovingian coins na",0
"KeywordsTraditional Method Computational Linguistic Central Problem Language Study Linguistic Data ","computers and medieval english lexicography","Dictionaries and related language reference works constitute a rich but under-exploited resource for the history of languages and of language study in the Middle Ages. Unfortunately, the size and complexity of typical medieval dictionaries make editions and analyses by traditional methods prohibitively expensive in time and money. Using as an example the Latin-Middle English dictionaryMedulla grammatice, the paper describes some central problems in the study of medieval English lexicography and the solutions provided by computers, which, with their immense speed, profound memory, and perfect accuracy can help scholars analyze, edit, and promulgate medieval documents and the linguistic data they contain.","Computers and the Humanities",1978,"No"," traditional method computational linguistic central problem language study linguistic data computers medieval english lexicography dictionaries related language reference works constitute rich exploited resource history languages language study middle ages size complexity typical medieval dictionaries make editions analyses traditional methods prohibitively expensive time money latin middle english dictionarymedulla grammatice paper describes central problems study medieval english lexicography solutions provided computers immense speed profound memory perfect accuracy scholars analyze edit promulgate medieval documents linguistic data ",0
"KeywordsComputational Linguistic Spanish Dictionary ","computers and the old spanish dictionary",NA,"Computers and the Humanities",1978,"No"," computational linguistic spanish dictionary computers spanish dictionary na",0
"KeywordsComputational Linguistic English Drama ","records of early english drama and the computer",NA,"Computers and the Humanities",1978,"No"," computational linguistic english drama records early english drama computer na",0
NA,"informatique et histoire mdivale linstitut de recherche et dhistoire de textes","In order to handle its own documentation as well as in order to come to the aid of isolated researchers who have no access to computers, the Institute of Research and of the History of Texts is attempting to put together a certain number of programs which can satisfy the needs of various types of medieval historical documents.","Computers and the Humanities",1978,"No"," informatique histoire mdivale linstitut de recherche dhistoire de textes order handle documentation order aid isolated researchers access computers institute research history texts attempting put number programs satisfy types medieval historical documents",0
"KeywordsComputational Linguistic Project Report ","project reports",NA,"Computers and the Humanities",1978,"No"," computational linguistic project report project reports na",0
"KeywordsComputational Linguistic ","some considerations concerning encoding and concording texts",NA,"Computers and the Humanities",1978,"No"," computational linguistic considerations encoding concording texts na",0
"KeywordsComputer Analysis Computational Linguistic Metrical Pattern ","a computer analysis of metrical patterns inbeowulf",NA,"Computers and the Humanities",1978,"No"," computer analysis computational linguistic metrical pattern computer analysis metrical patterns inbeowulf na",0
NA,"projekt untersuchungen zu altislndischen rechtstexten","This paper gives a brief survey of the Saarbrücken project on Old Icelandic legal texts, sponsored by the German Research Society, within the Special Research Area “Computer linguistics.” The project's main points of interest are (1) producing adequate machine-readable versions and parsed indices of all legal texts in Old Icelandic, (2) graphemic studies of legal manuscripts, and (3) studies of the distribution and valence of the verbs in those texts. A proposal for encoding Old Norse/Old Icelandic demonstrates how texts of different standards (normalized, diplomatic, graphetic) can be encoded as compatibly as possible. The description of a combined normalization-lemmatization process reveals that even little normalization in a diplomatic text will save much manual parsing.","Computers and the Humanities",1978,"Yes"," projekt untersuchungen zu altislndischen rechtstexten paper survey saarbr cken project icelandic legal texts sponsored german research society special research area computer linguistics project main points interest producing adequate machine readable versions parsed indices legal texts icelandic graphemic studies legal manuscripts studies distribution valence verbs texts proposal encoding norse icelandic demonstrates texts standards normalized diplomatic graphetic encoded compatibly description combined normalization lemmatization process reveals normalization diplomatic text save manual parsing",1
"KeywordsComputational Linguistic National Endowment ","funding computer aided research in the division of research grants at the national endowment for the humanities",NA,"Computers and the Humanities",1978,"No"," computational linguistic national endowment funding computer aided research division research grants national endowment humanities na",0
"KeywordsComputational Linguistic ","cai in college english",NA,"Computers and the Humanities",1978,"No"," computational linguistic cai college english na",0
NA,"annual bibliography for 1976 and supplement to preceding years",NA,"Computers and the Humanities",1977,"No"," annual bibliography supplement preceding years na",0
"KeywordsComputational Linguistic ","a survey of computer assisted research in modern german",NA,"Computers and the Humanities",1977,"No"," computational linguistic survey computer assisted research modern german na",0
"KeywordsComputational Linguistic Sound Change ","iberochange a program to simulate systematic sound change in ibero romance",NA,"Computers and the Humanities",1977,"No"," computational linguistic sound change iberochange program simulate systematic sound change ibero romance na",0
"KeywordsComputational Linguistic ","robertson davies the tory mode",NA,"Computers and the Humanities",1977,"No"," computational linguistic robertson davies tory mode na",0
"KeywordsComputational Linguistic Political History ","the new political history progress and prospects",NA,"Computers and the Humanities",1977,"No"," computational linguistic political history political history progress prospects na",0
"KeywordsComputational Linguistic French Corpus ","computational linguistics and statistics in the analysis of the montreal french corpus",NA,"Computers and the Humanities",1977,"No"," computational linguistic french corpus computational linguistics statistics analysis montreal french corpus na",0
"KeywordsComputational Linguistic ","letter to the editor",NA,"Computers and the Humanities",1977,"No"," computational linguistic letter editor na",0
"KeywordsComputational Linguistic ","the berkeley late egyptian dictionary",NA,"Computers and the Humanities",1977,"No"," computational linguistic berkeley late egyptian dictionary na",0
"KeywordsComputational Linguistic ","from co occurrences to concepts",NA,"Computers and the Humanities",1977,"No"," computational linguistic occurrences concepts na",0
"KeywordsComputational Linguistic ","some notions of similarity among lines of text",NA,"Computers and the Humanities",1977,"No"," computational linguistic notions similarity lines text na",0
"KeywordsComputational Linguistic Musical Computing ","problems of representation in musical computing",NA,"Computers and the Humanities",1977,"No"," computational linguistic musical computing problems representation musical computing na",0
"KeywordsComputational Linguistic ","reports",NA,"Computers and the Humanities",1976,"No"," computational linguistic reports na",0
"KeywordsQuantitative Study Computational Linguistic American Revolution ","quantitative studies and the american revolution",NA,"Computers and the Humanities",1976,"No"," quantitative study computational linguistic american revolution quantitative studies american revolution na",0
"KeywordsComputational Linguistic Resulting Ranking Cluster Property Prose Initial Sound ","on the measurement of alliteration in poetry","This work is a preliminary study of methods to quantify alliteration. Ten pieces made up of poetry and prose (literary and non-literary) were used to create test sets. Three forms of each test set were examined: texts transcribed in IPA notational equivalent, in Chomsky and Halle features, and in Fromkin and Rodman features. Tests included the deletion of vowels, the weighting of the initial sounds, and the weighting of types according to their frequency in the population of the set. The various configurations were analyzed using a gap-recurrence method. Rankings were obtained by combining measures both of high frequency and of clustering properties. The resulting rankings compare not unfavorably with an intuitive ranking.","Computers and the Humanities",1976,"No"," computational linguistic resulting ranking cluster property prose initial sound measurement alliteration poetry work preliminary study methods quantify alliteration ten pieces made poetry prose literary literary create test sets forms test set examined texts transcribed ipa notational equivalent chomsky halle features fromkin rodman features tests included deletion vowels weighting initial sounds weighting types frequency population set configurations analyzed gap recurrence method rankings obtained combining measures high frequency clustering properties resulting rankings compare unfavorably intuitive ranking",0
"KeywordsComputational Linguistic ","the use of the computer in the analysis of german folksongs",NA,"Computers and the Humanities",1976,"No"," computational linguistic computer analysis german folksongs na",0
"KeywordsInformation Processing Computational Linguistic Technical Guideline ","information processing in dictionary making some technical guidelines",NA,"Computers and the Humanities",1976,"No"," information processing computational linguistic technical guideline information processing dictionary making technical guidelines na",0
"KeywordsVerse Computational Linguistic High Coefficient Strong Similarity Conceptual Level ","phonological patterning in german verse","While numerous tentative conclusions might be drawn from the data presented here, the following generalizations are consistently supported by the results of the Phonological Frames Program:","Computers and the Humanities",1976,"No"," verse computational linguistic high coefficient strong similarity conceptual level phonological patterning german verse numerous tentative conclusions drawn data presented generalizations consistently supported results phonological frames program",0
NA,"annual bibliography for 1975 and supplement to preceding years",NA,"Computers and the Humanities",1976,"No"," annual bibliography supplement preceding years na",0
"KeywordsComputational Linguistic Future History ","machine readable archives and future history",NA,"Computers and the Humanities",1976,"No"," computational linguistic future history machine readable archives future history na",0
"KeywordsContent Analysis Computational Linguistic ","a system for text and content analysis",NA,"Computers and the Humanities",1976,"No"," content analysis computational linguistic system text content analysis na",0
"KeywordsWord Frequency Computational Linguistic Function Word Function Word Frequency ","the use of function word frequencies as indicators of style",NA,"Computers and the Humanities",1975,"No"," word frequency computational linguistic function word function word frequency function word frequencies indicators style na",0
"KeywordsComputational Linguistic ","tables for comparing the richness and structure of vocabulary in texts of different lengths",NA,"Computers and the Humanities",1975,"No"," computational linguistic tables comparing richness structure vocabulary texts lengths na",0
"KeywordsComputational Linguistic Semantic Description ","a shakespeare dictionary shad some preliminaries for a semantic description",NA,"Computers and the Humanities",1975,"Yes"," computational linguistic semantic description shakespeare dictionary shad preliminaries semantic description na",1
"KeywordsComputational Linguistic Annual Bibliography ","annual bibliography for 1974 and supplement to preceding years",NA,"Computers and the Humanities",1975,"No"," computational linguistic annual bibliography annual bibliography supplement preceding years na",0
"KeywordsComputational Linguistic Chinese Character ","the morphology of chinese characters a survey of models and applications","Preprints for seminar on Input/Output Systems for Japanese and Chinese Characters, Tokyo, 1971. U.S.-Japan Committee on scientific cooperation","Computers and the Humanities",1975,"No"," computational linguistic chinese character morphology chinese characters survey models applications preprints seminar inputoutput systems japanese chinese characters tokyo japan committee scientific cooperation",0
"KeywordsComputational Linguistic Interactive Program Dialect Dictionary ","use of an interactive program in analyzing data for a dialect dictionary",NA,"Computers and the Humanities",1975,"No"," computational linguistic interactive program dialect dictionary interactive program analyzing data dialect dictionary na",0
"KeywordsComputational Linguistic ","culture structure and the new history a critique and an agenda",NA,"Computers and the Humanities",1975,"No"," computational linguistic culture structure history critique agenda na",0
"KeywordsComputational Linguistic ","on automatic speech understanding systems",NA,"Computers and the Humanities",1975,"No"," computational linguistic automatic speech understanding systems na",0
"KeywordsVerbal Behavior Literary Study Dissertation Abstract Undergraduate Curriculum Educational Computing ","annual bibliography for 1973 and supplement to preceding years",NA,"Computers and the Humanities",1974,"No"," verbal behavior literary study dissertation abstract undergraduate curriculum educational computing annual bibliography supplement preceding years na",0
"KeywordsComputational Linguistic Sixteenth Century General Inquirer ","a general inquirer analysis of sixteenth century and contemporary catechisms",NA,"Computers and the Humanities",1974,"No"," computational linguistic sixteenth century general inquirer general inquirer analysis sixteenth century contemporary catechisms na",0
"KeywordsComputational Linguistic ","a survey of computer aided research in early german",NA,"Computers and the Humanities",1974,"No"," computational linguistic survey computer aided research early german na",0
"KeywordsComputational Linguistic ","analysis of corpora of variations",NA,"Computers and the Humanities",1974,"No"," computational linguistic analysis corpora variations na",0
"KeywordsComputational Linguistic ","solar a semantically oriented lexical archive current status and plans",NA,"Computers and the Humanities",1974,"No"," computational linguistic solar semantically oriented lexical archive current status plans na",0
"KeywordsComputational Linguistic Mutual Benefit Recent Conference ","recent conferences in italy","The conferences were very different in nature, each being valuable in its own way. The first brought together many people from all parts of the world making it possible to gain an overall view on the state of the art in the growing field of computational linguistics, and provided a forum for the interchange of ideas, and also enabled those who are new to such research to benefit from communication with those who are acknowledged leaders in the field. The LIE colloquium was more akin to a working group of experts who had been invited to participate in the LIE project to ensure the end product is of the highest possible standard by consultation and planning of a type that can only be achieved under such “interactive” conditions. Communication was two-way to the mutual benefit of all who attended.","Computers and the Humanities",1974,"No"," computational linguistic mutual benefit recent conference recent conferences italy conferences nature valuable brought people parts world making gain view state art growing field computational linguistics provided forum interchange ideas enabled research benefit communication acknowledged leaders field lie colloquium akin working group experts invited participate lie project ensure end product highest standard consultation planning type achieved interactive conditions communication mutual benefit attended",0
"KeywordsComputational Linguistic Oral Poetry ","thought clusters in early greek oral poetry",NA,"Computers and the Humanities",1974,"No"," computational linguistic oral poetry thought clusters early greek oral poetry na",0
"KeywordsComputational Linguistic ","sasanian pahlavi inscriptions a concordance",NA,"Computers and the Humanities",1974,"No"," computational linguistic sasanian pahlavi inscriptions concordance na",0
"KeywordsComputational Linguistic Computer Format Collegiate Dictionary ","a new computer format for websters seventh collegiate dictionary",NA,"Computers and the Humanities",1974,"No"," computational linguistic computer format collegiate dictionary computer format websters seventh collegiate dictionary na",0
"KeywordsComputational Linguistic Phonetic Transcription ","narrow phonetic transcription on the computer taking the phone off the hook",NA,"Computers and the Humanities",1974,"No"," computational linguistic phonetic transcription narrow phonetic transcription computer taking phone hook na",0
"KeywordsComputational Linguistic ","statistical analysis of dialectal boundaries",NA,"Computers and the Humanities",1974,"No"," computational linguistic statistical analysis dialectal boundaries na",0
NA,"concordances syntagmatiques et analyse de surface","This paper describes two procedures for the establishment of syntagmatic concordances through pre-coding and automatical analysis for the nominal syntagm. The first is pre-coding of the text, in this instance the poetry of P. Valéry, in accordance with taxonomic grids enumerating the typical patterns followed by the syntagm to be analyzed. From a strictly mathematical point of view, there are too many possible combinations (several thousand), and the number of typical patterns is therefore reduced as much as possible. In these taxonomic grids the analytical structures are poor. The second is the automatic analysis of simultaneous occurrences to left and right of a pivot term set by pre-coding. All the categories known as the “parts of speech” are precoded. In this process of recognition, the categories comprised by the units of the chain and their syntactical functions are also pre-coded.","Computers and the Humanities",1974,"No"," concordances syntagmatiques analyse de surface paper describes procedures establishment syntagmatic concordances pre coding automatical analysis nominal syntagm pre coding text instance poetry val ry accordance taxonomic grids enumerating typical patterns syntagm analyzed strictly mathematical point view combinations thousand number typical patterns reduced taxonomic grids analytical structures poor automatic analysis simultaneous occurrences left pivot term set pre coding categories parts speech precoded process recognition categories comprised units chain syntactical functions pre coded",0
"KeywordsComputer Programming Computational Linguistic Teaching Language ","flow a teaching language for computer programming in the humanities",NA,"Computers and the Humanities",1974,"No"," computer programming computational linguistic teaching language flow teaching language computer programming humanities na",0
"WORDNET (Book)","reviews","Reviews the book `WordNet: An Electronic Lexical Database,' edited by Christiane Fellbaum.","Computers and the Humanities",1974,"No","wordnet book reviews reviews book wordnet electronic lexical database edited christiane fellbaum",0
"KeywordsComputational Linguistic ","humanities computing in italy",NA,"Computers and the Humanities",1973,"No"," computational linguistic humanities computing italy na",0
"KeywordsComputational Linguistic Quantitative Research French Study ","recent quantitative research in french studies",NA,"Computers and the Humanities",1973,"No"," computational linguistic quantitative research french study recent quantitative research french studies na",0
"KeywordsComputational Linguistic ","the study of english loan words in modern french",NA,"Computers and the Humanities",1973,"No"," computational linguistic study english loan words modern french na",0
"KeywordsLarge Body Significant Part Computer Processing Computational Linguistic Linguistic Theory ","a computer assisted study of the vocabulary of young navajo children","Computer processing made it feasible for us to base our study of the vocabulary of six-year-old Navajo children on as large a corpus as we could collect in the time available. Our difficulties, as so often in computational linguistics, were with matters of linguistic theory rather than of computing. What we ran into was the as-yet unsolved question of the nature of the word in Navajo: how many affixes should be written as part of the verb and how many as separate words; and how does one handle the unbelievably complex morphophonemics in choosing headwords. The computer once again showed its ability, not just as an aid in handling large bodies of data, but as a heuristic device that makes clear to the researcher the limitations of his understanding of the material he is working with.","Computers and the Humanities",1973,"No"," large body significant part computer processing computational linguistic linguistic theory computer assisted study vocabulary young navajo children computer processing made feasible base study vocabulary year navajo children large corpus collect time difficulties computational linguistics matters linguistic theory computing ran unsolved question nature word navajo affixes written part verb separate words handle unbelievably complex morphophonemics choosing headwords computer showed ability aid handling large bodies data heuristic device makes clear researcher limitations understanding material working ",0
"KeywordsRoot System Computational Linguistic ","establishing a german root system by computer",NA,"Computers and the Humanities",1973,"No"," root system computational linguistic establishing german root system computer na",0
"KeywordsPrincipal Component Analysis Computational Linguistic ","an application of principal component analysis to the works of molire",NA,"Computers and the Humanities",1973,"No"," principal component analysis computational linguistic application principal component analysis works molire na",0
"KeywordsComputational Linguistic ","computational linguistics in denmark a report on the third aila congress",NA,"Computers and the Humanities",1973,"No"," computational linguistic computational linguistics denmark report aila congress na",0
"KeywordsHumanity Program Computational Linguistic ","humanities programs available",NA,"Computers and the Humanities",1973,"No"," humanity program computational linguistic humanities programs na",0
"KeywordsAnnual Meeting Computational Linguistic ","the annual meeting of the acl",NA,"Computers and the Humanities",1973,"No"," annual meeting computational linguistic annual meeting acl na",0
"KeywordsComputational Linguistic Medieval Text Automatic Collation ","automatic collation a technique for medieval texts",NA,"Computers and the Humanities",1973,"No"," computational linguistic medieval text automatic collation automatic collation technique medieval texts na",0
"KeywordsComputational Linguistic Annual Bibliography ","annual bibliography for 1972 and supplement to preceding years",NA,"Computers and the Humanities",1973,"No"," computational linguistic annual bibliography annual bibliography supplement preceding years na",0
"KeywordsLiterary Research Computational Linguistic ","symposium on the uses of computers in literary research edinburgh 2730 march 1972",NA,"Computers and the Humanities",1972,"No"," literary research computational linguistic symposium computers literary research edinburgh march na",0
"KeywordsData Analysis Critical Thought Application Software Computational Linguistic Social Data ","wrapping up the package critical thoughts on applications software for social data analysis",NA,"Computers and the Humanities",1972,"No"," data analysis critical thought application software computational linguistic social data wrapping package critical thoughts applications software social data analysis na",0
"KeywordsComputational Linguistic Recent Scholarship Linguistic Study ","recent scholarship in literary and linguistic studies",NA,"Computers and the Humanities",1972,"No"," computational linguistic recent scholarship linguistic study recent scholarship literary linguistic studies na",0
"KeywordsComputational Linguistic ","clio and computers moving into phase ii 19701972",NA,"Computers and the Humanities",1972,"No"," computational linguistic clio computers moving phase ii na",0
NA,"annual bibliography for 1971",NA,"Computers and the Humanities",1972,"No"," annual bibliography na",0
"KeywordsComputational Linguistic ","computerized lemmatization without the use of a dictionary a case study from swedish lexicology",NA,"Computers and the Humanities",1972,"No"," computational linguistic computerized lemmatization dictionary case study swedish lexicology na",0
"KeywordsComputational Linguistic Chinese Dialect Dialect Dictionary ","doc 1971 a chinese dialect dictionary on computer"," Project on Linguistic Analysis, Berkeley.","Computers and the Humanities",1972,"Yes"," computational linguistic chinese dialect dialect dictionary doc chinese dialect dictionary computer project linguistic analysis berkeley",1
"KeywordsComputational Linguistic ","letter to the editor shakespeare concordance",NA,"Computers and the Humanities",1972,"No"," computational linguistic letter editor shakespeare concordance na",0
"KeywordsComputational Linguistic Critical Survey ","the computer in archaeology a critical survey",NA,"Computers and the Humanities",1972,"No"," computational linguistic critical survey computer archaeology critical survey na",0
"KeywordsComputational Linguistic Linguistic Performance ","computational linguistics and the study of linguistic performance",NA,"Computers and the Humanities",1972,"No"," computational linguistic linguistic performance computational linguistics study linguistic performance na",0
"KeywordsComputational Linguistic ","abstracts and brief notices",NA,"Computers and the Humanities",1971,"No"," computational linguistic abstracts notices na",0
"KeywordsComputational Linguistic Computer Study English Poetry ","formulas and syntax in old english poetry a computer study",NA,"Computers and the Humanities",1971,"No"," computational linguistic computer study english poetry formulas syntax english poetry computer study na",0
NA,"annual bibliography for 1970",NA,"Computers and the Humanities",1971,"No"," annual bibliography na",0
"KeywordsComputational Linguistic Text Variant ","using the computer to identify differences among text variants",NA,"Computers and the Humanities",1971,"No"," computational linguistic text variant computer identify differences text variants na",0
"KeywordsComputational Linguistic Verbal Material ","verbal materials in machine readable form",NA,"Computers and the Humanities",1971,"No"," computational linguistic verbal material verbal materials machine readable form na",0
NA,"news and notes",NA,"Computers and the Humanities",1971,"No"," news notes na",0
"KeywordsLiterature Research Computational Linguistic ","current scandinavian computer assisted language and literature research",NA,"Computers and the Humanities",1971,"No"," literature research computational linguistic current scandinavian computer assisted language literature research na",0
"KeywordsComputational Linguistic Progress Report ","the dictionary of old english a progress report",NA,"Computers and the Humanities",1971,"No"," computational linguistic progress report dictionary english progress report na",0
"KeywordsComputational Linguistic Literary Scholarship ","computers and literary scholarship",NA,"Computers and the Humanities",1971,"No"," computational linguistic literary scholarship computers literary scholarship na",0
"KeywordsComputational Linguistic Classical Literature ","computers and classical literature 19701971",NA,"Computers and the Humanities",1971,"No"," computational linguistic classical literature computers classical literature na",0
NA,"etat prsent de lutilisation des ordinateurs pour ltude de la littrature franaise","This article examines only published work directly related to the study of French literature; excluded are automatic translation, production of bibliographies or critical editions, and linguistic studies.","Computers and the Humanities",1971,"No"," etat prsent de lutilisation des ordinateurs pour ltude de la littrature franaise article examines published work directly related study french literature excluded automatic translation production bibliographies critical editions linguistic studies",0
"KeywordsComputational Linguistic ","style precept personality a test case thomas sprat 16351713",NA,"Computers and the Humanities",1971,"No"," computational linguistic style precept personality test case thomas sprat na",0
"KeywordsComputational Linguistic Linguistic Research Preliminary Survey ","a preliminary survey on the use of computers in linguistic research",NA,"Computers and the Humanities",1970,"No"," computational linguistic linguistic research preliminary survey preliminary survey computers linguistic research na",0
"KeywordsComputational Linguistic Annual Bibliography ","annual bibliography for 1969",NA,"Computers and the Humanities",1970,"No"," computational linguistic annual bibliography annual bibliography na",0
"KeywordsQuantitative Approach Computational Linguistic ","words and numbers a quantitative approach to swift and some understrappers",NA,"Computers and the Humanities",1970,"No"," quantitative approach computational linguistic words numbers quantitative approach swift understrappers na",0
"KeywordsComputational Linguistic Computerize Research ","clio and computers a survey of computerized research in history",NA,"Computers and the Humanities",1970,"No"," computational linguistic computerize research clio computers survey computerized research history na",0
"KeywordsPreliminary Report Computational Linguistic Mutual Relationship Great Relevance Good World ","isaiah and the computer a preliminary report","In the eighth century B.C.E. there lived in Jerusalem a prophet of royal descent whose name was Isaiah ben Amoż. His oracles, mostly in poetry, are concerned with the contemporary political scene dominated by the Assyrians, with the fate of the Jewish people in history in general, with the mutual relationship between God and Man, and with the ultimate fate of Mankind in the fullness of time. For Jews, these oracles are perhaps second in importance only to the Pentateuch. For Christians, they are of greatest relevance because no other book influenced Jesus to the same extent. And for humanity they are so much the expression of an idealistic Weltanschauungthat they are quoted wherever educators and statesmen try to imbue their audiences with the vision of, and the hope for, a better world. They are collected in sixty-six chapters, constituting the first and longest book of the “Latter Prophets.”","Computers and the Humanities",1970,"No"," preliminary report computational linguistic mutual relationship great relevance good world isaiah computer preliminary report eighth century lived jerusalem prophet royal descent isaiah ben amo oracles poetry concerned contemporary political scene dominated assyrians fate jewish people history general mutual relationship god man ultimate fate mankind fullness time jews oracles importance pentateuch christians greatest relevance book influenced jesus extent humanity expression idealistic weltanschauungthat quoted educators statesmen imbue audiences vision hope world collected sixty chapters constituting longest book prophets ",0
"KeywordsComputational Linguistic Dialectal Variation ","computer produced mapping of dialectal variation",NA,"Computers and the Humanities",1970,"No"," computational linguistic dialectal variation computer produced mapping dialectal variation na",0
"KeywordsLiterary Research Computational Linguistic Conference Report ","symposium on the uses of the computer in literary research a conference report",NA,"Computers and the Humanities",1970,"No"," literary research computational linguistic conference report symposium computer literary research conference report na",0
"KeywordsComputational Linguistic Concordance Program ","an experimental concordance program",NA,"Computers and the Humanities",1970,"No"," computational linguistic concordance program experimental concordance program na",0
"KeywordsField Data Statistical Summary Contingency Table Formal Method Careful Research ","computer applications in cultural anthropology","This paper covers important developments in the use of computers for quantitative research in cultural anthropology, particularly in areas which (unlike statistics) are uniquely anthropological. These fall into statistical topics and topics in scaling and measurement. By far the largest single usage of computers by cultural anthropologists is for statistical summaries of field data and for simple statistical tests such as thechi-squared for the analysis of field data or for cross-cultural studies. As the discipline develops this situation will remain the same. In fact, the proportion of people who use the computer primarily for contingency tables, frequency counts, and correlation analysis may very well increase, since there are many potential users who would fall in this category and only a few potential users who would perform other operations such as multi-dimensional scaling or simulation. The few other computer techniques that would be relevant to anthropology, and for which the technology already exists, include linear regression, as practiced by economists, and linear programming (also practiced by economists), both of which could be extremely useful in the study of peasant economy. Careful research with such models could dispel some of the controversy which has been hindering the development of economic anthropology for the last fifteen years. The training of anthropologists who can understand the relevance of such models to their work may be far in the future, since the majority of them are still skeptical of most formal methods and of the computers which make them work.","Computers and the Humanities",1970,"No"," field data statistical summary contingency table formal method careful research computer applications cultural anthropology paper covers important developments computers quantitative research cultural anthropology areas unlike statistics uniquely anthropological fall statistical topics topics scaling measurement largest single usage computers cultural anthropologists statistical summaries field data simple statistical tests thechi squared analysis field data cross cultural studies discipline develops situation remain fact proportion people computer primarily contingency tables frequency counts correlation analysis increase potential users fall category potential users perform operations multi dimensional scaling simulation computer techniques relevant anthropology technology exists include linear regression practiced economists linear programming practiced economists extremely study peasant economy careful research models dispel controversy hindering development economic anthropology fifteen years training anthropologists understand relevance models work future majority skeptical formal methods computers make work",0
"KeywordsData Bank Computational Linguistic Common Procedure Usable Program Limited Effort ","the current state of music research and the computer","In our opening remarks we noted the emphasis on the tools of computer-oriented music research prevailing in most writing. It is not likely that this situation will change for some time. It is apparent that there is a wide variety of computer applications being tested in music research and equally apparent that there is only limited collaboration and limited effort at devising common procedures. It is to be hoped that the future will see the acceptance of one music representation as a lingua franca; the development of widely usable programs with a central clearing house for such programs; and the development of data banks of encoded music scores, thematic indices of various repertories, and other information useful to the music researcher.","Computers and the Humanities",1970,"No"," data bank computational linguistic common procedure usable program limited effort current state music research computer opening remarks noted emphasis tools computer oriented music research prevailing writing situation change time apparent wide variety computer applications tested music research equally apparent limited collaboration limited effort devising common procedures hoped future acceptance music representation lingua franca development widely usable programs central clearing house programs development data banks encoded music scores thematic indices repertories information music researcher",0
"KeywordsComputational Linguistic ","recent publications",NA,"Computers and the Humanities",1969,"No"," computational linguistic recent publications na",0
"KeywordsComputational Linguistic ","directory of scholars active",NA,"Computers and the Humanities",1969,"No"," computational linguistic directory scholars active na",0
"KeywordsComputational Linguistic ","computerstylistics seminar a report",NA,"Computers and the Humanities",1969,"No"," computational linguistic computerstylistics seminar report na",0
"KeywordsComputational Linguistic ","numbers and history the dilemma of measurement",NA,"Computers and the Humanities",1969,"No"," computational linguistic numbers history dilemma measurement na",0
"KeywordsComputational Linguistic ","technology and humanistic values",NA,"Computers and the Humanities",1969,"No"," computational linguistic technology humanistic values na",0
"KeywordsComputational Linguistic Progress Report ","the study of old italian at utrecht a progress report",NA,"Computers and the Humanities",1969,"No"," computational linguistic progress report study italian utrecht progress report na",0
"KeywordsComputational Linguistic Philosophical Text ","a computer assisted study of a philosophical text",NA,"Computers and the Humanities",1969,"No"," computational linguistic philosophical text computer assisted study philosophical text na",0
NA,"annual bibliography for 1968",NA,"Computers and the Humanities",1969,"No"," annual bibliography na",0
"KeywordsHumanity Research Computational Linguistic ","computer aided humanities research at the university of wisconsin",NA,"Computers and the Humanities",1969,"No"," humanity research computational linguistic computer aided humanities research university wisconsin na",0
"KeywordsComputational Linguistic Computer Center Compleat Computer ","the compleat computer center",NA,"Computers and the Humanities",1969,"No"," computational linguistic computer center compleat computer compleat computer center na",0
"KeywordsComputational Linguistic ","the new philology an old discipline or a new science",NA,"Computers and the Humanities",1969,"No"," computational linguistic philology discipline science na",0
NA,"annual bibliography for 1967",NA,"Computers and the Humanities",1968,"No"," annual bibliography na",0
"KeywordsComputational Linguistic ","some uses of a grammatical concordance",NA,"Computers and the Humanities",1968,"No"," computational linguistic grammatical concordance na",0
"KeywordsLiterature Research Computational Linguistic Computing Activity Current Computing ","current computing activity in scandinavia related to language and literature research",NA,"Computers and the Humanities",1968,"No"," literature research computational linguistic computing activity current computing current computing activity scandinavia related language literature research na",0
"KeywordsComputational Linguistic ","computers and the classics",NA,"Computers and the Humanities",1968,"No"," computational linguistic computers classics na",0
"KeywordsComputational Linguistic Essay Review Computational Stylistic ","respicefinem and thetantum quantum an essay review of computational stylistics for 19671968",NA,"Computers and the Humanities",1968,"No"," computational linguistic essay review computational stylistic respicefinem thetantum quantum essay review computational stylistics na",0
"KeywordsStatistical Operation Automatic Processing Computational Linguistic Automatic Composition Latin Work ","computers and the classics a supplement","As an addition to the report by McDonough in CHum,II (1967), 37–40, Delatte lists the operations of the Laboratoire d'Analyse statistique des Langues Anciennes at Liège. He describes eight programs for automatic processing, automatic composition and printing of concordances, and various statistical operations. Forty-three Latin works and five Greek, totaling approximately 600,000 words, are catalogued.","Computers and the Humanities",1968,"No"," statistical operation automatic processing computational linguistic automatic composition latin work computers classics supplement addition report mcdonough chumii delatte lists operations laboratoire analyse statistique des langues anciennes li ge describes programs automatic processing automatic composition printing concordances statistical operations forty latin works greek totaling approximately words catalogued",0
"KeywordsComputational Linguistic ","humanities computing activities in italy",NA,"Computers and the Humanities",1968,"No"," computational linguistic humanities computing activities italy na",0
"KeywordsTechnical Development Computational Linguistic Library Science American Library ","automation in american libraries","Despite its concentration on technical developments in library science, this survey is offered here because it is relevant to humanists for two reasons: first, that humanists all use libraries and therefore need to encourage their evolution to higher levels of efficiency, and, second, that the procedures outlined here may be of help to humanists in establishing procedures for their own research.","Computers and the Humanities",1968,"No"," technical development computational linguistic library science american library automation american libraries concentration technical developments library science survey offered relevant humanists reasons humanists libraries encourage evolution higher levels efficiency procedures outlined humanists establishing procedures research",0
NA,"meetings announced",NA,"Computers and the Humanities",1967,"No"," meetings announced na",0
"KeywordsComputational Linguistic Tentative Beginning ","the computer and the historiansome tentative beginnings",NA,"Computers and the Humanities",1967,"No"," computational linguistic tentative beginning computer historiansome tentative beginnings na",0
"KeywordsComputational Linguistic Cultural Measurement ","ethnometrics cultural measurements as a necessary counterbalance to the cost effectiveness revolution some implications of the computers in anthropology conference",NA,"Computers and the Humanities",1967,"No"," computational linguistic cultural measurement ethnometrics cultural measurements counterbalance cost effectiveness revolution implications computers anthropology conference na",0
"KeywordsComputational Linguistic ","literary works in machine readable form",NA,"Computers and the Humanities",1967,"No"," computational linguistic literary works machine readable form na",0
"KeywordsComputer Study Stylistic Analysis Entire Corpus Edited Text Middle High German ","computer study of medieval german poetry a conference report",NA,"Computers and the Humanities",1967,"No"," computer study stylistic analysis entire corpus edited text middle high german computer study medieval german poetry conference report na",0
"KeywordsComputational Linguistic Winged Word ","winged words varieties of computer application to literature",NA,"Computers and the Humanities",1967,"No"," computational linguistic winged word winged words varieties computer application literature na",0
"KeywordsPresent Situation Computational Linguistic ","music and computing the present situation",NA,"Computers and the Humanities",1967,"No"," present situation computational linguistic music computing present situation na",0
NA,"standards for encoding data in a natural language",NA,"Computers and the Humanities",1967,"No"," standards encoding data natural language na",0
"KeywordsComputational Linguistic ","computer applications in archaeology",NA,"Computers and the Humanities",1967,"No"," computational linguistic computer applications archaeology na",0
"KeywordsComputational Linguistic ","art art history and the computer",NA,"Computers and the Humanities",1966,"No"," computational linguistic art art history computer na",0
"KeywordsComputational Linguistic Computer Study ACLS Program ","the acls program for computer studies in the humanities notes on computers and the humanities",NA,"Computers and the Humanities",1966,"No"," computational linguistic computer study acls program acls program computer studies humanities notes computers humanities na",0
