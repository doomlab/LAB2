"KEYWORDS","TITLE","ABSTRACT","JOURNAL","YEAR","code","text","class"
"lexicon semantic network Natural Language Processing ","a semantic network of english the mother of all wordnets","We give a brief outline of the design and contents of the English lexical database WordNet, which serves as a model for similarly conceived wordnets in several European languages. WordNet is a semantic network, in which the meanings of nouns, verbs, adjectives, and adverbs are represented in terms of their links to other (groups of) words via conceptual-semantic and lexical relations. Each part of speech is treated differently reflecting different semantic properties. We briefly discuss polysemy in WordNet, and focus on the case of meaning extensions in the verb lexicon. Finally, we outline the potential uses of WordNet not only for applications in natural language processing, but also for research in stylistic analyses in conjunction with a semantic concordance.","Computers and the Humanities",1998,"No","lexicon semant network natur languag process semant network english mother wordnet give outlin design content english lexic databas wordnet serv model similar conceiv wordnet european languag wordnet semant network mean noun verb adject adverb repres term link group word conceptu semant lexic relat part speech treat differ reflect semant properti briefli discuss polysemi wordnet focus case mean extens verb lexicon final outlin potenti wordnet applic natur languag process research stylist analys conjunct semant concord",0
"Key Wordscorpora databases dictionary informatiop retrieval systems lemmatization lexicography lexicology lexicon machine readable OED2 ","electronic lexicography","This paper offers a brief survey of some important developments in the use of computers in making dictionaries and lexicons. Making a dictionary involves collecting the data, sorting and lemmatizing, editing and printing. Five major types of machine-readable dictionaries have developed from these procedures: Machine Readable Lexicons of individual authors, Machine Readable Dictionaries with codes for linguistic information, Machine Dictionaries with selected information, and Lexical Databases with lexical information abstracted from machine-readable dictionaries. The second edition of the QED is a machine-readable dictionary with codes that may provide the basis for a diachronic lexical database.","Computers and the Humanities",1991,"No","key wordscorpora databas dictionari informatiop retriev system lemmat lexicographi lexicolog lexicon machin readabl o electron lexicographi paper offer survey import develop comput make dictionari lexicon make dictionari involv collect data sort lemmat edit print major type machin readabl dictionari develop procedur machin readabl lexicon individu author machin readabl dictionari code linguist inform machin dictionari select inform lexic databas lexic inform abstract machin readabl dictionari edit qed machin readabl dictionari code provid basi diachron lexic databas",0
"Base Concept ontology building Top Ontology ","the top down strategy for building eurowordnet vocabulary coverage base concepts and top ontology","This paper describes two fundamental aspects in the process of building of the EuroWordNet database. In EuroWordNet we have chosen for a flexible design in which local wordnets are built relatively independently as language-specific structures, which are linked to an Inter-Lingual-Index (ILI). To ensure compatibility between the wordnets, a core set of common concepts has been defined that has to be covered by every language. Furthermore, these concepts have been classified via the ILI in terms of a Top Ontology of 63 fundamental semantic distinctions used in various semantic theories and paradigms. This paper first discusses the process leading to the definition of the set of Base Concepts, and the structure and the rationale of the Top Ontology.","Computers and the Humanities",1998,"No","base concept ontolog build top ontolog top strategi build eurowordnet vocabulari coverag base concept top ontolog paper describ fundament aspect process build eurowordnet databas eurowordnet chosen flexibl design local wordnet built independ languag specif structur link inter lingual index ili ensur compat wordnet core set common concept defin cover languag concept classifi ili term top ontolog fundament semant distinct semant theori paradigm paper discuss process lead definit set base concept structur rational top ontolog",0
"Key WordsIstituto di Linguistica Computazionale CNR, computational linguistics databases linguistic analysis parsing ","summary of the activities of the istituto di linguistica computazionale","This article summarizes the activities of the Istituto di Linguistica Computazionale. We discuss the Italian Multi-functional Lexical Databases; the projects focussing on linguistic analysis and generation; corpora in the MRF, textual databases and linguistic workstations; computer-assisted humanities teaching; and the various cooperative ventures, seminars and conferences offered by the Institute.","Computers and the Humanities",1990,"No","key wordsistituto di linguistica computazional cnr comput linguist databas linguist analysi pars summari activ istituto di linguistica computazional articl summar activ istituto di linguistica computazional discuss italian multi function lexic databas project focuss linguist analysi generat corpora mrf textual databas linguist workstat comput assist human teach cooper ventur seminar confer offer institut",0
"generalised dictionary heterogeneous dictionary databases query language TEI tag definitions ","an architecture and query language for a federation ofheterogeneous dictionary databases","An architecture for federating heterogeneousdictionary databases is described. It proposes acommon description language and query language toprovide for the exchange of information betweendatabases with different organizations, on differentplatforms and in different DBMSs. The common querylanguage has an SQL like structure. The first versionof the description language follows the TEI standardtag definitions for dictionaries with the expectationthat the description language will be expanded in thefuture. A practical implementation of the proposalsusing WWW technology for two multi-lingualdictionaries is described.","Computers and the Humanities",2000,"No","generalis dictionari heterogen dictionari databas queri languag tei tag definit architectur queri languag feder ofheterogen dictionari databas architectur feder heterogeneousdictionari databas propos acommon descript languag queri languag toprovid exchang inform betweendatabas organ differentplatform dbmss common querylanguag sql structur versionof descript languag tei standardtag definit dictionari expectationthat descript languag expand thefutur practic implement proposalsus www technolog multi lingualdictionari",0
"multilingual lexical semantic database ","introduction to eurowordnet","This paper gives a global introduction to the aims and objectives of the EuroWordNet project, and it provides a general framework for the other papers in this volume. EuroWordNet is an EC project that develops a multilingual database with wordnets in several European languages, structured along the same lines as the Princeton WordNet. Each wordnet represents an autonomous structure of language-specific lexicalizations, which are interconnected via an Inter-Lingual-Index. The wordnets are built at different sites from existing resources, starting from a shared level of basic concepts and extended top-down. The results will be publicly available and will be tested in cross-language information retrieval applications.","Computers and the Humanities",1998,"No","multilingu lexic semant databas introduct eurowordnet paper global introduct aim object eurowordnet project general framework paper volum eurowordnet ec project develop multilingu databas wordnet european languag structur line princeton wordnet wordnet repres autonom structur languag specif lexic interconnect inter lingual index wordnet built site exist resourc start share level basic concept extend top result public test cross languag inform retriev applic",0
"corpus search parsing syntactic annotation SGML computational linguistics psycholinguistics ","finding syntactic structure in unparsed corpora the gsearch corpus query system","The Gsearch system allows the selection of sentences by syntacticcriteria from text corpora, even when these corpora contain no priorsyntactic markup. This is achieved by means of a fast chart parser,which takes as input a grammar and a search expression specified by theuser. Gsearch features a modular architecture that can be extendedstraightforwardly to give access to new corpora. The Gsearcharchitecture also allows interfacing with external linguistic resources(such as taggers and lexical databases). Gsearch can be used withgraphical tools for visualizing the results of a query.","Computers and the Humanities",2001,"No","corpus search pars syntact annot sgml comput linguist psycholinguist find syntact structur unpars corpora gsearch corpus queri system gsearch system select sentenc syntacticcriteria text corpora corpora priorsyntact markup achiev mean fast chart parser take input grammar search express theuser gsearch featur modular architectur extendedstraightforward give access corpora gsearcharchitectur interfac extern linguist resourc tagger lexic databas gsearch withgraph tool visual result queri",0
"Key wordsdatabases in the humanities NIAM formalism ","making an information system for the humanities","The Documentation Project is a cooperative project between Faculties of Arts in the Norwegian universities. It aims to produce “the Norwegian universities' databases for language and culture” from the paper-based archives at the participating institutions. The project has been active on a national basis since 1992. This paper describes the methodologies involved and ongoing subprojects.","Computers and the Humanities",1994,"No","key wordsdatabas human niam formal make inform system human document project cooper project faculti art norwegian univers aim produc norwegian univers databas languag cultur paper base archiv particip institut project activ nation basi paper describ methodolog involv ongo subproject",0
NA,"index of key words of volume 35",NA,"Computers and the Humanities",2001,"No","index key word volum na",0
"equivalence relations lexical-semantic relations language-internal relations synset ","the linguistic design of the eurowordnet database","In this paper the linguistic design of the database under construction within the EuroWordNet project is described. This is mainly structured along the same lines as the Princeton WordNet, although some changes have been made to the WordNet overall design due to both theoretical and practical reasons. The most important reasons for such changes are the multilinguality of the EuroWordNet database and the fact that it is intended to be used in Language Engineering applications. Thus, i) some relations have been added to those identified in WordNet; ii) some labels have been identified which can be added to the relations in order to make their implications more explicit and precise; iii) some relations, already present in the WordNet design, have been modified in order to specify their role more clearly.","Computers and the Humanities",1998,"No","equival relat lexic semant relat languag intern relat synset linguist design eurowordnet databas paper linguist design databas construct eurowordnet project structur line princeton wordnet made wordnet design due theoret practic reason import reason multilingu eurowordnet databas fact intend languag engin applic relat ad identifi wordnet ii label identifi ad relat order make implic explicit precis iii relat present wordnet design modifi order role",0
"morphology analyzer parser part of speech proper nouns tokenizer ","extracting an arabic lexicon from arabic newspaper text","We describe how to build a largecomprehensive, integrated Arabic lexicon byautomatic parsing of newspaper text. We havebuilt a parser system to read Arabic newspaperarticles, isolate the tokens from them, findthe part of speech, and the features for eachtoken. To achieve this goal we designed a setof algorithms, we generated several sets ofrules, and we developed a set of techniques,and a set of components to carry out thesetechniques. As each sentence is processed, newwords and features are added to the lexicon, sothat it grows continuously as the system runs.To test the system we have used 100 articles(80,444 words) from the Al-Raya newspaper.The system consists of several modules: thetokenizer module to isolate the tokens, the type findersystem to find the part of speech of eachtoken, the proper noun phrase parser module tomark the proper nouns and to discover someinformation about them and the feature findermodule to find the features of the words.","Computers and the Humanities",2002,"No","morpholog analyz parser part speech proper noun token extract arab lexicon arab newspap text describ build largecomprehens integr arab lexicon byautomat pars newspap text havebuilt parser system read arab newspaperarticl isol token findth part speech featur eachtoken achiev goal design setof algorithm generat set ofrul develop set techniqu set compon carri thesetechniqu sentenc process newword featur ad lexicon sothat grow continu system run test system articl word al raya newspap system consist modul thetoken modul isol token type findersystem find part speech eachtoken proper noun phrase parser modul tomark proper noun discov someinform featur findermodul find featur word",0
"overlapping relations and lexical gaps sense differentiation ","compatibility in interpretation of relations in eurowordnet","This paper describes how the Euro WordNet project established a maximum level of consensus in the interpretation of relations, without loosing the possibility of encoding language-specific lexicalizations. Problematic cases arise due to the fact that each site re-used different resources and because the core vocabulary of the wordnets show complex properties. Many of these cases are discussed with respect to language internal and equivalence relations. Possible solutions are given in the form of additional criteria.","Computers and the Humanities",1998,"No","overlap relat lexic gap sens differenti compat interpret relat eurowordnet paper describ euro wordnet project establish maximum level consensus interpret relat loos possibl encod languag specif lexic problemat case aris due fact site resourc core vocabulari wordnet show complex properti case discuss respect languag intern equival relat solut form addit criteria",0
"evaluation SENSEVAL word sense disambiguation ","framework and results for english senseval","Senseval was the first open, community-based evaluation exercisefor Word Sense Disambiguation programs. It adopted the quantitativeapproach to evaluation developed in MUC and other ARPA evaluationexercises. It took place in 1998. In this paper we describe thestructure, organisation and results of the SENSEVAL exercise forEnglish. We present and defend various design choices for theexercise, describe the data and gold-standard preparation, considerissues of scoring strategies and baselines, and present the resultsfor the 18 participating systems. The exercise identifies thestate-of-the-art for fine-grained word sense disambiguation, wheretraining data is available, as 74–78% correct, with a number ofalgorithms approaching this level of performance. For systems thatdid not assume the availability of training data, performance wasmarkedly lower and also more variable. Human inter-tagger agreementwas high, with the gold standard taggings being around 95%replicable.","Computers and the Humanities",2000,"No","evalu sensev word sens disambigu framework result english sensev sensev open communiti base evalu exercisefor word sens disambigu program adopt quantitativeapproach evalu develop muc arpa evaluationexercis place paper describ thestructur organis result sensev exercis forenglish present defend design choic theexercis describ data gold standard prepar considerissu score strategi baselin present resultsfor particip system exercis identifi thestat art fine grain word sens disambigu wheretrain data correct number ofalgorithm approach level perform system thatdid assum avail train data perform wasmark lower variabl human inter tagger agreementwa high gold standard tag replic",0
"parallel corpora sense disambiguation translation ","cross lingual sense determination can it work","This article reports the results of apreliminary analysis of translation equivalents infour languages from different language families,extracted from an on-line parallel corpus of GeorgeOrwell's Nineteen Eighty-Four. The goal ofthe study is to determine the degree to whichtranslation equivalents for different meanings of apolysemous word in English are lexicalized differentlyacross a variety of languages, and to determinewhether this information can be used to structure orcreate a set of sense distinctions useful in naturallanguage processing applications. A coherenceindex is computed that measures the tendency fordifferent senses of the same English word to belexicalized differently, and from this data aclustering algorithm is used to create sensehierarchies.","Computers and the Humanities",2000,"No","parallel corpora sens disambigu translat cross lingual sens determin work articl report result apreliminari analysi translat equival infour languag languag familiesextract line parallel corpus georgeorwel nineteen eighti goal ofth studi determin degre whichtransl equival mean apolysem word english lexic differentlyacross varieti languag determinewheth inform structur orcreat set sens distinct naturallanguag process applic coherenceindex comput measur tendenc fordiffer sens english word belexic differ data aclust algorithm creat sensehierarchi",0
"aligning wordnets equivalence relations multilingual database ","cross linguistic alignment of wordnets with an inter lingual index","This paper discusses the design of the EuroWordNet database, in which semantic databases like WordNet1.5 for several languages are combined via a so-called inter-lingual-index. In this database, language-independent data is shared whilst language-specific properties are maintained. A special interface has been developed to compare the semantic configurations across languages and to track down differences.","Computers and the Humanities",1998,"No","align wordnet equival relat multilingu databas cross linguist align wordnet inter lingual index paper discuss design eurowordnet databas semant databas wordnet languag combin call inter lingual index databas languag independ data share whilst languag specif properti maintain special interfac develop compar semant configur languag track differ",0
"EuroWordNet OIL methodology ontology semantic relation semantic Web SIMPLE top ontology ","semantic roles as slots in oil ontologies","The purpose of our research is to consider how the paradigms of EuroWordNet and SIMPLE linguistic projects on the one hand and the OIL methodology on the other hand may affect each other. OIL (Ontology Inference Layer) aims at implementing the ``semantic'' Web idea and is based on the notion of ontology, which is also employed in EuroWordNet and SIMPLE. In both latter projects the meanings of words are partially described by means of the finite sets of relations to other meanings of words, whereas in OIL the user is free to define the arbitrary relations of this kind.The relations considered in EuroWordNet and SIMPLE were defined on the basis of a careful observation of the large linguistic area, andt hey aim at reflecting the meaning as precisely as possible, therefore it seems useful to merge them with OIL. Moreover, the valuable feature of OIL is its formal language with precisely defined semantics. All things considered, we suggest how certain EuroWordNet and SIMPLE definitions may be expressed in OIL.","Computers and the Humanities",2004,"No","eurowordnet oil methodolog ontolog semant relat semant web simpl top ontolog semant role slot oil ontolog purpos research paradigm eurowordnet simpl linguist project hand oil methodolog hand affect oil ontolog infer layer aim implement semant web idea base notion ontolog employ eurowordnet simpl project mean word partial mean finit set relat mean word oil user free defin arbitrari relat kind relat consid eurowordnet simpl defin basi care observ larg linguist area andt hey aim reflect mean precis merg oil valuabl featur oil formal languag precis defin semant thing consid suggest eurowordnet simpl definit express oil",0
"ambiguity disambiguation lexicography polysemy word sense ","i dont believe in word senses","Word sense disambiguation assumes word senses. Withinthe lexicography and linguistics literature, they areknown to bevery slippery entities. The first part of the paperlooks at problemswith existing accounts of ‘word sense’ and describesthe various kinds of ways in which a word's meaning candeviate from its coremeaning. An analysis is presented in which wordsenses areabstractions from clusters of corpus citations, inaccordance withcurrent lexicographic practice. The corpus citations,not the wordsenses, are the basic objects in the ontology. Thecorpus citationswill be clustered into senses according to thepurposes of whoever or whatever does the clustering. In theabsence of suchpurposes, word senses do not exist.","Computers and the Humanities",1997,"No","ambigu disambigu lexicographi polysemi word sens dont word sens word sens disambigu assum word sens withinth lexicographi linguist literatur areknown beveri slipperi entiti part paperlook problemswith exist account word sens describesth kind way word mean candevi coremean analysi present wordsens areabstract cluster corpus citat inaccord withcurr lexicograph practic corpus citat wordsens basic object ontolog thecorpus citationswil cluster sens thepurpos cluster theabsenc suchpurpos word sens exist",0
"KeywordsLexical resources WordNet Machine learning ","constructing and utilizing wordnets using statistical methods","Lexical databases following the wordnet paradigm capture information about words, word senses, and their relationships. A large number of existing tools and datasets are based on the original WordNet, so extending the landscape of resources aligned with WordNet leads to great potential for interoperability and to substantial synergies. Wordnets are being compiled for a considerable number of languages, however most have yet to reach a comparable level of coverage. We propose a method for automatically producing such resources for new languages based on WordNet, and analyse the implications of this approach both from a linguistic perspective as well as by considering natural language processing tasks. Our approach takes advantage of the original WordNet in conjunction with translation dictionaries. A small set of training associations is used to learn a statistical model for predicting associations between terms and senses. The associations are represented using a variety of scores that take into account structural properties as well as semantic relatedness and corpus frequency information. Although the resulting wordnets are imperfect in terms of their quality and coverage of language-specific phenomena, we show that they constitute a cheap and suitable alternative for many applications, both for monolingual tasks as well as for cross-lingual interoperability. Apart from analysing the resources directly, we conducted tests on semantic relatedness assessment and cross-lingual text classification with very promising results.","Language Resources and Evaluation",2012,"No","lexic resourc wordnet machin learn construct util wordnet statist method lexic databas wordnet paradigm captur inform word word sens relationship larg number exist tool dataset base origin wordnet extend landscap resourc align wordnet lead great potenti interoper substanti synergi wordnet compil consider number languag reach compar level coverag propos method automat produc resourc languag base wordnet analys implic approach linguist perspect natur languag process task approach take advantag origin wordnet conjunct translat dictionari small set train associ learn statist model predict associ term sens associ repres varieti score account structur properti semant related corpus frequenc inform result wordnet imperfect term qualiti coverag languag specif phenomena show constitut cheap suitabl altern applic monolingu task cross lingual interoper analys resourc direct conduct test semant related assess cross lingual text classif promis result",0
"selectional preferences ","word sense disambiguation using automatically acquired verbal preferences","The selectional preferences of verbal predicates are an importantcomponent of a computational lexicon. They have frequently been citedas being useful for WSD, alongside other sources ofknowledge. We evaluate automatically acquired selectional preferenceson the level playing field provided by SENSEVAL to examine towhat extent they help in WSD.","Computers and the Humanities",2000,"No","select prefer word sens disambigu automat acquir verbal prefer select prefer verbal predic importantcompon comput lexicon frequent citeda wsd alongsid sourc ofknowledg evalu automat acquir select preferenceson level play field provid sensev examin towhat extent wsd",0
"Key WordsLatin lexicography lemmatization word-lists word-searching ","a project for latin lexicography 1 automatic lemmatization and word list","A cooperative team of researchers from various Italian universities are collaborating on a project for Latin lexicography. This article describes the linguistic work being done on the Latin language. The various problems — and their solutions — are discussed: allographs, homographs, source material and classification methods, word searching, etc.","Computers and the Humanities",1990,"No","key wordslatin lexicographi lemmat word list word search project latin lexicographi automat lemmat word list cooper team research italian univers collabor project latin lexicographi articl describ linguist work latin languag problem solut discuss allograph homograph sourc materi classif method word search",0
"KeywordsComputational Linguistic Word Sense ","ginger ii an example driven word sense disambiguator",NA,"Computers and the Humanities",2000,"No","comput linguist word sens ginger ii driven word sens disambigu na",0
"evaluation ambiguity resolution WSD inter-annotator agreement ","tagger evaluation given hierarchical tag sets","We present methods for evaluating human and automatictaggers that extend current practice in three ways. First, we show howto evaluate taggers that assign multiple tags to each test instance,even if they do not assign probabilities. Second, we show how toaccommodate a common property of manually constructed ``gold standards''that are typically used for objective evaluation, namely that there isoften more than one correct answer. Third, we show how to measureperformance when the set of possible tags is tree-structured in an IS-Ahierarchy. To illustrate how our methods can be used to measureinter-annotator agreement, we show how to compute the kappa coefficientover hierarchical tag sets.","Computers and the Humanities",2000,"No","evalu ambigu resolut wsd inter annot agreement tagger evalu hierarch tag set present method evalu human automatictagg extend current practic way show howto evalu tagger assign multipl tag test instanc assign probabl show toaccommod common properti manual construct gold standard typic object evalu isoften correct answer show measureperform set tag tree structur ahierarchi illustr method measureint annot agreement show comput kappa coefficientov hierarch tag set",0
"disambiguation Senseval Bayesian classifier ","a topicallocal classifier for word sense identification","TLC is a supervised training (S) system that uses a Bayesianstatistical model and features of a word's context to identifyword sense. We describe the classifier's operation and how itcan be configured to use only topical context cues, only localcues, or a combination of both. Our results on Senseval'sfinal run are presented along with a comparison to theperformance of the best S system and the average for S systems.We discuss ways to improve TLC by enriching its featureset and by substituting other decision procedures for the Bayesianmodel. Future development of supervised training classifiers willdepend on the availability of tagged training data. TLC canassist in the hand-tagging effort by helping human taggers locateinfrequent senses of polysemous words.","Computers and the Humanities",2000,"No","disambigu sensev bayesian classifi topicalloc classifi word sens identif tlc supervis train system bayesianstatist model featur word context identifyword sens describ classifi oper itcan configur topic context cue localcu combin result sensevalsfin run present comparison theperform system averag system discuss way improv tlc enrich featureset substitut decis procedur bayesianmodel futur develop supervis train classifi willdepend avail tag train data tlc canassist hand tag effort help human tagger locateinfrequ sens polysem word",0
"KeywordsComputational Linguistic Word Sense Word Sense Disambiguation Case Library ","word sense disambiguation with a similarity smoothed case library",NA,"Computers and the Humanities",2000,"No","comput linguist word sens word sens disambigu case librari word sens disambigu similar smooth case librari na",0
"word-sense disambiguation Senseval dictionary software analysis of parsing output ","senseval the cl research experience","The CL Research Senseval system wasthe highest performing system among the ``All-words''systems, with an overall fine-grained score of 61.6percent for precision and 60.5 percent for recall on98 percent of the 8,448 texts on the revisedsubmission (up by almost 6 and 9 percent from thefirst). The results were achieved with an almostcomplete reliance on syntactic behavior, using (1) arobust and fast ATN-style parser producing parse treeswith annotations on nodes, (2) DIMAP dictionarycreation and maintenance software (after conversion ofthe Hector dictionary files) to hold dictionaryentries, and (3) a strategy for analyzing the parsetrees in concert with the dictionary data. Furtherconsiderable improvements are possible in the parser,exploitation of the Hector data (and representation ofdictionary entries), and the analysis strategy, stillwith syntactic and collocational data. The Sensevaldata (the dictionary entries and the corpora) providean excellent testbed for understanding the sources offailures and for evaluating changes in the CL Researchsystem.","Computers and the Humanities",2000,"No","word sens disambigu sensev dictionari softwar analysi pars output sensev cl research experi cl research sensev system wasth highest perform system wordssystem fine grain score percent precis percent recal on percent text revisedsubmiss percent thefirst result achiev almostcomplet relianc syntact behavior arobust fast atn style parser produc pars treeswith annot node dimap dictionarycr mainten softwar convers ofth hector dictionari file hold dictionaryentri strategi analyz parsetre concert dictionari data furtherconsider improv parserexploit hector data represent ofdictionari entri analysi strategi stillwith syntact colloc data sensevaldata dictionari entri corpora providean excel testb understand sourc offailur evalu cl researchsystem",0
"semantic classification trees SENSEVAL word sense disambiguation WSD evaluation ","using semantic classification trees for wsd","This paper describes the evaluation of a WSD method withinSENSEVAL. This method is based on Semantic Classification Trees (SCTs)and short context dependencies between nouns and verbs. The trainingprocedure creates a binary tree for each word to be disambiguated. SCTsare easy to implement and yield some promising results. The integrationof linguistic knowledge could lead to substantial improvement.","Computers and the Humanities",2000,"No","semant classif tree sensev word sens disambigu wsd evalu semant classif tree wsd paper describ evalu wsd method withinsensev method base semant classif tree scts short context depend noun verb trainingprocedur creat binari tree word disambigu sctsare easi implement yield promis result integrationof linguist knowledg lead substanti improv",0
"KeywordsMultilingual wordnets Formal ontology Information system ","challenges for a multilingual wordnet","Wordnets have been created in many languages, revealing both their lexical commonalities and diversity. The next challenge is to make multilingual wordnets fully interoperable. The EuroWordNet experience revealed the shortcomings of an interlingua based on a natural language. Instead, we propose a model based on the division of the lexicon and a language-independent, formal ontology that serves as the hub interlinking the language-specific lexicons. The ontology avoids the idiosyncracies of the lexicon and furthermore allows formal reasoning about the concepts it contains. We address the division of labor between ontology and lexicon. Finally, we illustrate our model in the context of a domain-specific multilingual information system based on a central ontology and interconnected wordnets in seven languages.","Language Resources and Evaluation",2012,"No","multilingu wordnet formal ontolog inform system challeng multilingu wordnet wordnet creat languag reveal lexic common divers challeng make multilingu wordnet fulli interoper eurowordnet experi reveal shortcom interlingua base natur languag propos model base divis lexicon languag independ formal ontolog serv hub interlink languag specif lexicon ontolog avoid idiosyncraci lexicon formal reason concept address divis labor ontolog lexicon final illustr model context domain specif multilingu inform system base central ontolog interconnect wordnet languag",0
"KeywordsAgreement Annotation Conceptual information Evaluation Lexical information Mapping Metaphor Resource creation ","the hamburg metaphor database project issues in resource creation","This paper concerns metaphor resource creation. It provides an account of methods used, problems discovered, and insights gained at the Hamburg Metaphor Database project, intended to inform similar resource creation initiatives, as well as future metaphor processing algorithms. After introducing the project, the theoretical underpinnings that motivate the subdivision of represented information into a conceptual and a lexical level are laid out. The acquisition of metaphor attestations from electronic corpora is explained, and annotation practices as well as database contents are evaluated. The paper concludes with an overview of related projects and an outline of possible future work.","Language Resources and Evaluation",2008,"No","agreement annot conceptu inform evalu lexic inform map metaphor resourc creation hamburg metaphor databas project issu resourc creation paper concern metaphor resourc creation account method problem discov insight gain hamburg metaphor databas project intend inform similar resourc creation initi futur metaphor process algorithm introduc project theoret underpin motiv subdivis repres inform conceptu lexic level laid acquisit metaphor attest electron corpora explain annot practic databas content evalu paper conclud overview relat project outlin futur work",0
"Key wordsencoding TEI dictionaries SGML ","encoding dictionaries","This article describes the major problems in devising a TEI encoding format for dictionaries, which, because of their high degree of structuring and compression of information, are among the most complex text types treated in the TEI. The major problems for this task were (1) the tension between generality of the description, in order to be widely applicable across dictionaries, and descriptive power, that is, the ability to describe with precision the particular structure of any given dictionary; and (2) the need to accommodate different views and uses of the encoded dictionary, for example, as printed object and as a database of information.","Computers and the Humanities",1995,"No","key wordsencod tei dictionari sgml encod dictionari articl describ major problem devis tei encod format dictionari high degre structur compress inform complex text type treat tei major problem task tension general descript order wide applic dictionari descript power abil describ precis structur dictionari accommod view encod dictionari print object databas inform",0
"cross-language text retrieval multilingual lexical resources large-scale ontologies ","applying eurowordnet to cross language text retrieval","We discuss ways in which EuroWordNet (EWN) can be used in multilingual information retrieval activities, focusing on two approaches to Cross-Language Text Retrieval that use the EWN database as a large-scale multilingual semantic resource. The first approach indexes documents and queries in terms of the EuroWordNet Inter-Lingual-Index, thus turning term weighting and query/document matching into language-independent tasks. The second describes how the information in the EWN database could be integrated with a corpus-based technique, thus allowing retrieval of domain-specific terms that may not be present in our multilingual database. Our objective is to show the potential of EuroWordNet as a promising alternative to existing approaches to Cross-Language Text Retrieval.","Computers and the Humanities",1998,"No","cross languag text retriev multilingu lexic resourc larg scale ontolog appli eurowordnet cross languag text retriev discuss way eurowordnet ewn multilingu inform retriev activ focus approach cross languag text retriev ewn databas larg scale multilingu semant resourc approach index document queri term eurowordnet inter lingual index turn term weight querydocu match languag independ task describ inform ewn databas integr corpus base techniqu allow retriev domain specif term present multilingu databas object show potenti eurowordnet promis altern exist approach cross languag text retriev",0
"Bible computational linguistics parallel corpora Corpus Encoding Standard translation lexicons ","the bible as a parallel corpus annotating the book of 2000 tongues","We report on a project to annotate biblical texts in order to create an aligned multilingual Bible corpus for linguistic research, particularly computational linguistics, including automatically creating and evaluating translation lexicons and semantically tagged texts. The output of this project will enable researchers to take advantage of parallel translations across a wider number of languages than previously available, providing, with relatively little effort, a corpus that contains careful translations and reliable alignment at the near-sentence level. We discuss the nature of the text, our annotation process, preliminary and planned uses for the corpus, and relevant aspects of the Corpus Encoding Standard (CES) with respect to this corpus. We also present a quantitative comparison with dictionary and corpus resources for modern-day English, confirming the relevance of this corpus for research on present day language.","Computers and the Humanities",1999,"No","bibl comput linguist parallel corpora corpus encod standard translat lexicon bibl parallel corpus annot book tongu report project annot biblic text order creat align multilingu bibl corpus linguist research comput linguist includ automat creat evalu translat lexicon semant tag text output project enabl research advantag parallel translat wider number languag previous provid effort corpus care translat reliabl align sentenc level discuss natur text annot process preliminari plan corpus relev aspect corpus encod standard ces respect corpus present quantit comparison dictionari corpus resourc modern day english confirm relev corpus research present day languag",0
"KeywordsWordNet Computational lexicography Acquisition of lexical information Computational terminology Linguistic resources Ontologies ","building the galician wordnet methods and applications","This paper presents the different methodologies and resources used to build Galnet, the Galician version of WordNet. It reviews the different extraction processes and the lexicographical and textual sources used to develop this resource, and describes some of its applications in ontology research and terminology processing.","Language Resources and Evaluation",2018,"No","wordnet comput lexicographi acquisit lexic inform comput terminolog linguist resourc ontolog build galician wordnet method applic paper present methodolog resourc build galnet galician version wordnet review extract process lexicograph textual sourc develop resourc describ applic ontolog research terminolog process",0
"Keywordscorpus hypernymy pattern semantic variation terminology thesaurus ","automatic acquisition and expansion of hypernym links","Recent developments in computational terminology call for the design of multiple and complementary tools for the acquisition, the structuring and the exploitation of terminological data. This paper proposes to bridge the gap between term acquisition and thesaurus construction by offering a framework for automatic structuring of multi-word candidate terms with the help of corpus-based links between single-word terms. First, we present a system for corpus-based acquisition of terminological relationships through discursive patterns. This system is built on previous work on automatic extraction of hyponymy links through shallow parsing. Second, we show how hypernym links between single-word terms can be extended to semantic links between multi-word terms through corpus-based extraction of semantic variants. The induced hierarchy is incomplete but provides an automatic generalization of single-word terms relations to multi-word terms that are pervasive in technical thesauri and corpora.","Computers and the Humanities",2004,"No","corpus hypernymi pattern semant variat terminolog thesaurus automat acquisit expans hypernym link recent develop comput terminolog call design multipl complementari tool acquisit structur exploit terminolog data paper propos bridg gap term acquisit thesaurus construct offer framework automat structur multi word candid term corpus base link singl word term present system corpus base acquisit terminolog relationship discurs pattern system built previous work automat extract hyponymi link shallow pars show hypernym link singl word term extend semant link multi word term corpus base extract semant variant induc hierarchi incomplet automat general singl word term relat multi word term pervas technic thesauri corpora",0
"KeywordsNews corpus Sentiment analysis Lexicon Annotated corpus Corpus linguistics Web-crawling Word list AFINN Slovene Machine learning Document classification Monitoring sentiment dynamics ","annotated news corpora and a lexicon for sentiment analysis in slovene","In this study, we introduce Slovene web-crawled news corpora with sentiment annotation on three levels of granularity: sentence, paragraph and document levels. We describe the methodology and tools that were required for their construction. The corpora contain more than 250,000 documents with political, business, economic and financial content from five Slovene media resources on the web. More than 10,000 of them were manually annotated as negative, neutral or positive. All corpora are publicly available under a Creative Commons copyright license. We used the annotated documents to construct a Slovene sentiment lexicon, which is the first of its kind for Slovene, and to assess the sentiment classification approaches used. The constructed corpora were also utilised to monitor within-the-document sentiment dynamics, its changes over time and relations with news topics. We show that sentiment is, on average, more explicit at the beginning of documents, and it loses sharpness towards the end of documents.","Language Resources and Evaluation",2018,"No","news corpus sentiment analysi lexicon annot corpus corpus linguist web crawl word list afinn sloven machin learn document classif monitor sentiment dynam annot news corpora lexicon sentiment analysi sloven studi introduc sloven web crawl news corpora sentiment annot level granular sentenc paragraph document level describ methodolog tool requir construct corpora document polit busi econom financi content sloven media resourc web manual annot negat neutral posit corpora public creativ common copyright licens annot document construct sloven sentiment lexicon kind sloven assess sentiment classif approach construct corpora utilis monitor document sentiment dynam time relat news topic show sentiment averag explicit begin document lose sharp end document",0
"KeywordsLexicon Linguistic resources Part-of-speech Standardization Syntactic description Vietnamese ","a lexicon for vietnamese language processing","Only very recently have Vietnamese researchers begun to be involved in the domain of Natural Language Processing (NLP). As there does not exist any published work in formal linguistics nor any recognizable standard for Vietnamese word definition and word categories, the fundamental tasks for automatic Vietnamese language processing, such as part-of-speech tagging, parsing, etc., are very difficult tasks for computer scientists. The fact that all necessary linguistic resources have to be built from scratch by each research team is a real obstacle to the development of Vietnamese language processing. The aim of our projects is thus to build a common linguistic database that is freely and easily exploitable for the automatic processing of Vietnamese. In this paper, we present our work on creating a Vietnamese lexicon for NLP applications. We emphasize the standardization aspect of the lexicon representation. We especially propose an extensible set of Vietnamese syntactic descriptions that can be used for tagset definition and morphosyntactic analysis. These descriptors are established in such a way as to be a reference set proposal for Vietnamese in the context of ISO subcommittee TC 37/SC 4 (Language Resource Management).","Language Resources and Evaluation",2006,"No","lexicon linguist resourc part speech standard syntact descript vietnames lexicon vietnames languag process recent vietnames research begun involv domain natur languag process nlp exist publish work formal linguist recogniz standard vietnames word definit word categori fundament task automat vietnames languag process part speech tag pars difficult task comput scientist fact linguist resourc built scratch research team real obstacl develop vietnames languag process aim project build common linguist databas freeli easili exploit automat process vietnames paper present work creat vietnames lexicon nlp applic emphas standard aspect lexicon represent propos extens set vietnames syntact descript tagset definit morphosyntact analysi descriptor establish refer set propos vietnames context iso subcommitte tc sc languag resourc manag",0
"Key wordsword frequency distributions lexical conceptual structure lognormality bimodal density estimation ","word frequency distributions and lexical semantics","This paper addresses the relation between meaning, lexical productivity, and frequency of use. Using density estimation as a visualization tool, we show that differences in semantic structure can be reflected in probability density functions estimated for word frequency distributions. We call attention to an example of a bimodal density, and suggest that bimodality arises when distributions of well-entrenched lexical items, which appear to be lognormal, are mixed with distributions of productively created nonce formations.","Computers and the Humanities",1996,"No","key wordsword frequenc distribut lexic conceptu structur lognorm bimod densiti estim word frequenc distribut lexic semant paper address relat mean lexic product frequenc densiti estim visual tool show differ semant structur reflect probabl densiti function estim word frequenc distribut call attent bimod densiti suggest bimod aris distribut entrench lexic item lognorm mix distribut product creat nonc format",0
"computational lexical semantics defining formats formats learning minimal cover pattern matching thesaurus building ","adding new words into a chinese thesaurus","In this paper, we study the problem of adding a large number of new words into a Chinese thesaurus according to their definitions in a Chinese dictionary, while minimizing the effort of hand tagging. To deal with the problem, we first make use of a kind of supervised learning technique to learn a set of defining formats for each class in the thesaurus, which tries to characterize the regularities about the definitions of the words in the class. We then use traditional techniques in Graph theory to derive a minimal subset of the new words to be added into the thesaurus, which meets the following condition: if we add the new words in the subset into the thesaurus by hand, the other new words can be added into the thesaurus automatically by matching their definitions with the defining formats of each class in the thesaurus. The method uses little, if any, language-specific or thesaurus-specific knowledge, and can be applied to the thesauri of other languages.","Computers and the Humanities",1997,"No","comput lexic semant defin format format learn minim cover pattern match thesaurus build ad word chines thesaurus paper studi problem ad larg number word chines thesaurus definit chines dictionari minim effort hand tag deal problem make kind supervis learn techniqu learn set defin format class thesaurus character regular definit word class tradit techniqu graph theori deriv minim subset word ad thesaurus meet condit add word subset thesaurus hand word ad thesaurus automat match definit defin format class thesaurus method languag specif thesaurus specif knowledg appli thesauri languag",0
"KeywordsComputational Linguistic Argument Structure Consistent Criterion Sense Distinction Syntactic Frame ","consistent criteria for sense distinctions","This paper specifically addresses the question of polysemy with respect toverbs, and whether or not the sense distinctions that are made in on-linelexical resources such as WordNet are appropriate for computational lexicons.The use of sets of related syntactic frames and verb classes are examined as ameans of simplifying the task of defining different senses, and the importanceof concrete criteria such as different predicate argument structures, semanticclass constraints and lexical co-occurrences is emphasized.","Computers and the Humanities",2000,"No","comput linguist argument structur consist criterion sens distinct syntact frame consist criteria sens distinct paper specif address question polysemi respect toverb sens distinct made linelex resourc wordnet comput lexicon set relat syntact frame verb class examin amean simplifi task defin sens importanceof concret criteria predic argument structur semanticclass constraint lexic occurr emphas",0
"lexicon on-line dictionary syntactic dictionary ","comlex syntax a large syntactic dictionary for natural language processing","This article is a detailed account of COMLEX Syntax, an on-line syntactic dictionary of English, developed by the Proteus Project at New York University under the auspices of the Linguistics Data Consortium. This lexicon was intended to be used for a variety of tasks in natural language processing by computer and as such has very detailed classes with a large number of syntactic features and complements for the major parts of speech and is, as far as possible, theory neutral. The dictionary was entered by hand with reference to hard copy dictionaries, an on-line concordance and native speakers‘intuition. Thus it is without prior encumbrances and can be used for both pure research and commercial purposes.","Computers and the Humanities",1997,"No","lexicon line dictionari syntact dictionari comlex syntax larg syntact dictionari natur languag process articl detail account comlex syntax line syntact dictionari english develop proteus project york univers auspic linguist data consortium lexicon intend varieti task natur languag process comput detail class larg number syntact featur complement major part speech theori neutral dictionari enter hand refer hard copi dictionari line concord nativ speaker intuit prior encumbr pure research commerci purpos",0
"automated scoring content-based scoring short answer scoring ","c rater automated scoring of short answer questions","C-rater is an automated scoringengine that has been developed to scoreresponses to content-based short answerquestions. It is not simply a stringmatching program – instead it uses predicateargument structure, pronominal reference,morphological analysis and synonyms to assignfull or partial credit to a short answerquestion. C-rater has been used in two studies:National Assessment for Educational Progress(NAEP) and a statewide assessment in Indiana.In both studies, c-rater agreed with humangraders about 84% of the time.","Computers and the Humanities",2003,"No","autom score content base score short answer score rater autom score short answer question rater autom scoringengin develop scorerespons content base short answerquest simpli stringmatch program predicateargu structur pronomin referencemorpholog analysi synonym assignful partial credit short answerquest rater studiesn assess educ progressnaep statewid assess indiana studi rater agre humangrad time",0
"author identification coherence computational linguistics content analysis corpus linguistics idiolect latent semantic analysis literary period sociolect ","semantic variation in idiolect and sociolect corpus linguistic evidence from literary texts","Idiolects are person-dependent similarities in language use. They imply that texts by one author show more similarities in language use than texts between authors. Sociolects, on the other hand, are group-dependent similarities in language use. They imply that texts by a group of authors, for instance in terms of gender or time period, share more similarities within a group than between groups. Although idiolects and sociolects are commonly used terms in the humanities, they have not been investigated a great deal from corpus and computational linguistic points of view. To test several idiolect and sociolect hypotheses a factorial combination was used of time period (Modernism, Realism), gender of author (male, female) and author (Eliot, Dickens, Woolf, Joyce) totaling 16 corresponding literary texts. In a series of corpus linguistic studies using Boolean and vector models, no conclusive evidence was found for the selected idiolect and sociolect hypotheses. In final analyses testing the semantics within each literary text, this lack of evidence was explained by the low homogeneity within a literary text.","Computers and the Humanities",2004,"No","author identif coher comput linguist content analysi corpus linguist idiolect latent semant analysi literari period sociolect semant variat idiolect sociolect corpus linguist evid literari text idiolect person depend similar languag impli text author show similar languag text author sociolect hand group depend similar languag impli text group author instanc term gender time period share similar group group idiolect sociolect common term human investig great deal corpus comput linguist point view test idiolect sociolect hypothes factori combin time period modern realism gender author male femal author eliot dicken woolf joyc total literari text seri corpus linguist studi boolean vector model conclus evid found select idiolect sociolect hypothes final analys test semant literari text lack evid explain low homogen literari text",0
"authorship attribution collaboration federalist papers statistics ","detecting collaborations in text comparing the authors rhetorical language choices in the federalist papers","In author attribution studies function words or lexical measures areoften used to differentiate the authors' textual fingerprints. Thesestudies can be thought of as quantifying the texts, representing thetext with measured variables that stand for specific textual features.The resulting quantifications, while proven useful for statisticallydifferentiating among the texts, bear no resemblance to the understanding a human reader – even an astute one – would develop whilereading the texts. In this paper we present an attribution study that,instead, characterizes the texts according to the representationallanguage choices of the authors, similar to a way we believe close humanreaders come to know a text and distinguish its rhetorical purpose. Fromour automated quantification of The Federalist papers, it isclear why human readers find it impossible to distinguish the authorshipof the disputed papers. Our findings suggest that changes occur in theprocesses of rhetorical invention when undertaken in collaborativesituations. This points to a need to re-evaluate the premise ofautonomous authorship that has informed attribution studies of The Federalist case.","Computers and the Humanities",2004,"No","authorship attribut collabor federalist paper statist detect collabor text compar author rhetor languag choic federalist paper author attribut studi function word lexic measur areoften differenti author textual fingerprint thesestudi thought quantifi text repres thetext measur variabl stand specif textual featur result quantif proven statisticallydifferenti text bear resembl understand human reader astut develop whileread text paper present attribut studi character text representationallanguag choic author similar close humanread text distinguish rhetor purpos fromour autom quantif federalist paper isclear human reader find imposs distinguish authorshipof disput paper find suggest occur theprocess rhetor invent undertaken collaborativesitu point evalu premis ofautonom authorship inform attribut studi federalist case",0
"Key wordsmetaphors semantics netmet analogy ","netmet a program for generating and interpreting metaphors","Metaphors have computable semantics. A program called NETMET both generates metaphors and produces partial literal interpretations of metaphors. NETMET is based on Kittay's semantic field theory of metaphor and Black's interaction theory of metaphor. Input to NETMET consists of a list of literal propositions. NETMET creates metaphors by finding topic and source semantic fields, producing an analogical map from source to topic, then generating utterances in which terms in the source are identified with or predicated of terms in the topic. Given a metaphor, NETMET utilizes if-then rules to generate the implication complex of that metaphor. The literal leaves of the implication complex comprise a partial literal interpretation.","Computers and the Humanities",1994,"No","key wordsmetaphor semant netmet analog netmet program generat interpret metaphor metaphor comput semant program call netmet generat metaphor produc partial liter interpret metaphor netmet base kittay semant field theori metaphor black interact theori metaphor input netmet consist list liter proposit netmet creat metaphor find topic sourc semant field produc analog map sourc topic generat utter term sourc identifi predic term topic metaphor netmet util rule generat implic complex metaphor liter leav implic complex compris partial liter interpret",0
"linguistic resources TEI text encoding header ","silfide a system for open access and distributed delivery of tei encoded documents","This paper presents some aspects of the Silfide server, a system dedicated to the delivery of linguistic resources on the web. After presenting the main issues behind the design of such a system, we focus on the editorial choices related to the use of the Text Encoding Initiative to represent our textual documents. In particular, we focus on the accommodations we have had to carry with regards to the TEI header and address the trade-off between extensive enrichment and genericity of the primary data when one wants to precisely mark-up a given document content. As a whole, we show how essential the TEI has proven to be for a project such as ours both from a practical and conceptual point of view.","Computers and the Humanities",1999,"No","linguist resourc tei text encod header silfid system open access distribut deliveri tei encod document paper present aspect silfid server system dedic deliveri linguist resourc web present main issu design system focus editori choic relat text encod initi repres textual document focus accommod carri tei header address trade extens enrich generic primari data precis mark document content show essenti tei proven project practic conceptu point view",0
"KeywordsSystem Performance Information Retrieval Major Topic Technical Document Computational Linguistic ","japaneseenglish cross language information retrieval exploration of query translation and transliteration","Cross-language information retrieval (CLIR), where queriesand documents are in different languages, has of late become one ofthe major topics within the information retrieval community. Thispaper proposes a Japanese/English CLIR system, where we combine aquery translation and retrieval modules. We currently target theretrieval of technical documents, and therefore the performance of oursystem is highly dependent on the quality of the translation oftechnical terms. However, the technical term translation is stillproblematic in that technical terms are often compound words, and thusnew terms are progressively created by combining existing basewords. In addition, Japanese often represents loanwords based on itsspecial phonogram. Consequently, existing dictionaries find itdifficult to achieve sufficient coverage. To counter the firstproblem, we produce a Japanese/English dictionary for base words, andtranslate compound words on a word-by-word basis. We also use aprobabilistic method to resolve translation ambiguity. For the secondproblem, we use a transliteration method, which corresponds wordsunlisted in the base word dictionary to their phonetic equivalents inthe target language. We evaluate our system using a test collectionfor CLIR, and show that both the compound word translation andtransliteration methods improve the system performance.","Computers and the Humanities",2001,"No","system perform inform retriev major topic technic document comput linguist japaneseenglish cross languag inform retriev explor queri translat transliter cross languag inform retriev clir queriesand document languag late ofth major topic inform retriev communiti thispap propos japaneseenglish clir system combin aqueri translat retriev modul target theretriev technic document perform oursystem high depend qualiti translat oftechn term technic term translat stillproblemat technic term compound word thusnew term progress creat combin exist baseword addit japanes repres loanword base itsspeci phonogram exist dictionari find itdifficult achiev suffici coverag counter firstproblem produc japaneseenglish dictionari base word andtransl compound word word word basi aprobabilist method resolv translat ambigu secondproblem transliter method correspond wordsunlist base word dictionari phonet equival inth target languag evalu system test collectionfor clir show compound word translat andtransliter method improv system perform",0
"Key WordsLatin lexicology morphology lemmatization databases ","a project for latin lexicography 2 a latin morphological analyzer","This article describes a second aspect of the Project for Latin Lexicography (see previous article). We here concentrate on two aspects of the project. First, we describe the morphological analyzer, which comprises a base dictionary, a table of suffixes, a table of endings and a table of postfixes. Second, we describe the lemmatization module, which operates by reference to a series of grammatical codes or information given for the base, and reference codes.","Computers and the Humanities",1990,"No","key wordslatin lexicolog morpholog lemmat databas project latin lexicographi latin morpholog analyz articl describ aspect project latin lexicographi previous articl concentr aspect project describ morpholog analyz compris base dictionari tabl suffix tabl end tabl postfix describ lemmat modul oper refer seri grammat code inform base refer code",0
"Key Wordslegal informatics law Italian legal system databases artificial intelligence ","legal informatics research in italy the istituto per la documentazione giuridica of the italian national research council","The IDG was originally founded to carry out research into the collection and processing of documentation relating to Italian legislation, case law and legal authority. The Institute has since concentrated on automated documentation and legal informatics, as well as the application of artificial intelligence to the law. This article describes the many projects undertaken at the Institute.","Computers and the Humanities",1990,"No","key wordsleg informat law italian legal system databas artifici intellig legal informat research itali istituto la documentazion giuridica italian nation research council idg origin found carri research collect process document relat italian legisl case law legal author institut concentr autom document legal informat applic artifici intellig law articl describ project undertaken institut",0
"KeywordsGeneral Purpose Specific Factor Base Form Related Issue Computational Linguistic ","on the corpus size needed for compiling a comprehensive computational lexicon by automatic lexical acquisition","Comprehensive computational lexicons areessential to practical natural languageprocessing (NLP). To compile such computationallexicons by automatically acquiring lexicalinformation, however, we previously requiresufficiently large corpora. This study aims atpredicting the ideal size of suchautomatic-lexical-acquisition oriented corpora,focusing on six specific factors: (1) specificversus general purpose prediction, (2)variation among corpora, (3) base forms versus inflected forms, (4) open class items,(5) homographs, and (6) unknown words.Another important and related issue withregard to predictability has something to dowith data sparseness. Research using theTOTAL Corpus reveals serious datasparseness in this corpus. This, again, pointstowards the importance and necessity ofreducing data sparseness to a satisfactorylevel for the automatic lexical acquisition andreliable corpus predictions. The functions ofpredicting the number of tokens and lemmas in acorpus are based on the piecewisecurve-fitting algorithm. Unfortunately, thepredicted size of a corpus for automaticlexical acquisition is too astronomicalto compile it by using presently existingcompiling strategies. Therefore, we suggest apractical and efficient alternative method. Weare confident that this study will shed newlight on issues such as corpus predictability,compiling strategies and linguisticcomprehensiveness.","Computers and the Humanities",2002,"No","general purpos specif factor base form relat issu comput linguist corpus size need compil comprehens comput lexicon automat lexic acquisit comprehens comput lexicon areessenti practic natur languageprocess nlp compil computationallexicon automat acquir lexicalinform previous requiresuffici larg corpora studi aim atpredict ideal size suchautomat lexic acquisit orient corporafocus specif factor specificversus general purpos predict variat corpora base form versus inflect form open class item homograph unknown word import relat issu withregard predict dowith data spars research thetot corpus reveal dataspars corpus pointstoward import necess ofreduc data spars satisfactorylevel automat lexic acquisit andreli corpus predict function ofpredict number token lemma acorpus base piecewisecurv fit algorithm thepredict size corpus automaticlex acquisit astronomicalto compil present existingcompil strategi suggest apract effici altern method wear confid studi shed newlight issu corpus predictabilitycompil strategi linguisticcomprehens",0
"KeywordsComputational Linguistic Human Language ","human language and computers",NA,"Computers and the Humanities",1985,"No","comput linguist human languag human languag comput na",0
"KeywordsMachine translation evaluation Error identification and classification Two-way ANOVA Linear regression Ensemble techniques ","factor based evaluation for english to hindi mt outputs","Design and implementation of automatic evaluation methods is an integral part of any scientific research in accelerating the development cycle of the output. This is no less true for automatic machine translation (MT) systems. However, no such global and systematic scheme exists for evaluation of performance of an MT system. The existing evaluation metrics, such as BLEU, METEOR, TER, although used extensively in literature have faced a lot of criticism from users. Moreover, performance of these metrics often varies with the pair of languages under consideration. The above observation is no less pertinent with respect to translations involving languages of the Indian subcontinent. This study aims at developing an evaluation metric for English to Hindi MT outputs. As a part of this process, a set of probable errors have been identified manually as well as automatically. Linear regression has been used for computing weight/penalty for each error, while taking human evaluations into consideration. A sentence score is computed as the weighted sum of the errors. A set of 126 models has been built using different single classifiers and ensemble of classifiers in order to find the most suitable model for allocating appropriate weight/penalty for each error. The outputs of the models have been compared with the state-of-the-art evaluation metrics. The models developed for manually identified errors correlate well with manual evaluation scores, whereas the models for the automatically identified errors have low correlation with the manual scores. This indicates the need for further improvement and development of sophisticated linguistic tools for automatic identification and extraction of errors. Although many automatic machine translation tools are being developed for many different language pairs, there is no such generalized scheme that would lead to designing meaningful metrics for their evaluation. The proposed scheme should help in developing such metrics for different language pairs in the coming days.","Language Resources and Evaluation",2018,"No","machin translat evalu error identif classif anova linear regress ensembl techniqu factor base evalu english hindi mt output design implement automat evalu method integr part scientif research acceler develop cycl output true automat machin translat mt system global systemat scheme exist evalu perform mt system exist evalu metric bleu meteor ter extens literatur face lot critic user perform metric vari pair languag consider observ pertin respect translat involv languag indian subcontin studi aim develop evalu metric english hindi mt output part process set probabl error identifi manual automat linear regress comput weightpenalti error take human evalu consider sentenc score comput weight sum error set model built singl classifi ensembl classifi order find suitabl model alloc weightpenalti error output model compar state art evalu metric model develop manual identifi error correl manual evalu score model automat identifi error low correl manual score improv develop sophist linguist tool automat identif extract error automat machin translat tool develop languag pair general scheme lead design meaning metric evalu propos scheme develop metric languag pair come day",0
"KeywordsWeb as corpus News corpus Web-based tagged Bengali news corpus Named entity Named entity recognition ","a web based bengali news corpus for named entity recognition","The rapid development of language resources and tools using machine learning techniques for less computerized languages requires appropriately tagged corpus. A tagged Bengali news corpus has been developed from the web archive of a widely read Bengali newspaper. A web crawler retrieves the web pages in Hyper Text Markup Language (HTML) format from the news archive. At present, the corpus contains approximately 34 million wordforms. Named Entity Recognition (NER) systems based on pattern based shallow parsing with or without using linguistic knowledge have been developed using a part of this corpus. The NER system that uses linguistic knowledge has performed better yielding highest F-Score values of 75.40%, 72.30%, 71.37%, and 70.13% for person, location, organization, and miscellaneous names, respectively.","Language Resources and Evaluation",2008,"No","web corpus news corpus web base tag bengali news corpus name entiti name entiti recognit web base bengali news corpus name entiti recognit rapid develop languag resourc tool machin learn techniqu computer languag requir appropri tag corpus tag bengali news corpus develop web archiv wide read bengali newspap web crawler retriev web page hyper text markup languag html format news archiv present corpus approxim million wordform name entiti recognit ner system base pattern base shallow pars linguist knowledg develop part corpus ner system linguist knowledg perform yield highest score valu person locat organ miscellan name",0
"KeywordsHLT programme STEVIN HLT policy design Dutch ","joint research coordination and programming for hlt for dutch in the low countries","Since 1999, the Dutch Language Union (NTU) fosters the exchange of plans and policy initiatives amongst government officials of Flanders and the Netherlands on human language technology for Dutch (HLTD). One of the outcomes is the STEVIN R&D programme for HLTD, coordinated by the NTU and funded by the Flemish and Dutch governments. STEVIN is an example of successful joint research programming. Its set-up, highlights and scientific results are presented as well as an outlook to future initiatives.","Language Resources and Evaluation",2013,"No","hlt programm stevin hlt polici design dutch joint research coordin program hlt dutch low countri dutch languag union ntu foster exchang plan polici initi govern offici flander netherland human languag technolog dutch hltd outcom stevin programm hltd coordin ntu fund flemish dutch govern stevin success joint research program set highlight scientif result present outlook futur initi",0
"KeywordsMultilingual lexicon Under-resourced languages Malay Iban ","lexicontx rapid construction of a multilingual lexicon with under resourced languages","Most efforts at automatically creating multilingual lexicons require input lexical resources with rich content (e.g. semantic networks, domain codes, semantic categories) or large corpora. Such material is often unavailable and difficult to construct for under-resourced languages. In some cases, particularly for some ethnic languages, even unannotated corpora are still in the process of collection. We show how multilingual lexicons with under-resourced languages can be constructed using simple bilingual translation lists, which are more readily available. The prototype multilingual lexicon developed comprise six member languages: English, Malay, Chinese, French, Thai and Iban, the last of which is an under-resourced language in Borneo. Quick evaluations showed that 91.2  % of 500 random multilingual entries in the generated lexicon require minimal or no human correction.","Language Resources and Evaluation",2014,"No","multilingu lexicon resourc languag malay iban lexicontx rapid construct multilingu lexicon resourc languag effort automat creat multilingu lexicon requir input lexic resourc rich content semant network domain code semant categori larg corpora materi unavail difficult construct resourc languag case ethnic languag unannot corpora process collect show multilingu lexicon resourc languag construct simpl bilingu translat list readili prototyp multilingu lexicon develop compris member languag english malay chines french thai iban resourc languag borneo quick evalu show random multilingu entri generat lexicon requir minim human correct",0
"KeywordsProper Noun Lexical Knowledge Generative Lexicon Corpus Processing Subcategorization Frame ","branimir boguraev and james pustejovsky corpus processing for lexical acquisition",NA,"Computers and the Humanities",1999,"No","proper noun lexic knowledg generat lexicon corpus process subcategor frame branimir boguraev jame pustejovski corpus process lexic acquisit na",0
"KeywordsInteroperability Standards Language resources ","global interoperability for language resources introduction to the special section","This special section of Language Resources and Evaluation contains a selection of presentations from ICGL that focus on interoperability for lexical and semantic databases and ontologies. These resources in effect constitute the “hub” of semantic interoperability by providing means to link language resources such as corpora to common categories and concepts. As such, interoperability within and among these databases is the necessary next step to enable semantic compatibility for language data.","Language Resources and Evaluation",2012,"No","interoper standard languag resourc global interoper languag resourc introduct special section special section languag resourc evalu select present icgl focus interoper lexic semant databas ontolog resourc effect constitut hub semant interoper provid mean link languag resourc corpora common categori concept interoper databas step enabl semant compat languag data",0
"KeywordsLanguage Resources Named Entities Web 2.0 Standards ","web 20 language resources and standards to automatically build a multilingual named entity lexicon","This paper proposes to advance in the current state-of-the-art of automatic Language Resource (LR) building by taking into consideration three elements: (1) the knowledge available in existing LRs, (2) the vast amount of information available from the collaborative paradigm that has emerged from the Web 2.0 and (3) the use of standards to improve interoperability. We present a case study in which a set of LRs for different languages (WordNet for English and Spanish and Parole-Simple-Clips for Italian) are extended with Named Entities (NE) by exploiting Wikipedia and the aforementioned LRs. The practical result is a multilingual NE lexicon connected to these LRs and to two ontologies: SUMO and SIMPLE. Furthermore, the paper addresses an important problem which affects the Computational Linguistics area in the present, interoperability, by making use of the ISO LMF standard to encode this lexicon. The different steps of the procedure (mapping, disambiguation, extraction, NE identification and postprocessing) are comprehensively explained and evaluated. The resulting resource contains 974,567, 137,583 and 125,806 NEs for English, Spanish and Italian respectively. Finally, in order to check the usefulness of the constructed resource, we apply it into a state-of-the-art Question Answering system and evaluate its impact; the NE lexicon improves the system’s accuracy by 28.1%. Compared to previous approaches to build NE repositories, the current proposal represents a step forward in terms of automation, language independence, amount of NEs acquired and richness of the information represented.","Language Resources and Evaluation",2012,"No","languag resourc name entiti web standard web languag resourc standard automat build multilingu name entiti lexicon paper propos advanc current state art automat languag resourc lr build take consider element knowledg exist lrs vast amount inform collabor paradigm emerg web standard improv interoper present case studi set lrs languag wordnet english spanish parol simpl clip italian extend name entiti ne exploit wikipedia aforement lrs practic result multilingu ne lexicon connect lrs ontolog sumo simpl paper address import problem affect comput linguist area present interoper make iso lmf standard encod lexicon step procedur map disambigu extract ne identif postprocess comprehens explain evalu result resourc nes english spanish italian final order check use construct resourc appli state art question answer system evalu impact ne lexicon improv system accuraci compar previous approach build ne repositori current propos repres step forward term autom languag independ amount nes acquir rich inform repres",0
"KeywordsNatural Language Processing Machine Translation Query Expansion Word Sense Statistical Machine Translation ","asian language processing current state of the art","Asian language processing presents formidable challenges to achieving multilingualism and multiculturalism in our society. One of the first and most obvious challenges is the multitude and diversity of languages: more than 2,000 languages are listed as languages in Asia by Ethnologue (Gordon 2005), representing four major language families: Austronesian, Trans-New Guinea, Indo-European, and Sino-Tibetan. 1The challenge is made more formidable by the fact that as a whole, Asian languages range from the language with most speakers in the world (Mandarin Chinese, close to 900 million native speakers) to the more than 70 nearly extinct languages (e.g. Pazeh in Taiwan, one speaker). As a result, there are vast differences in the level of language processing capability and the number of sharable resources available for individual languages. Major Asian languages such as Mandarin Chinese, Hindi, Japanese, Korean, and Thai have benefited...","Language Resources and Evaluation",2006,"No","natur languag process machin translat queri expans word sens statist machin translat asian languag process current state art asian languag process present formid challeng achiev multilingu multicultur societi obvious challeng multitud divers languag languag list languag asia ethnologu gordon repres major languag famili austronesian tran guinea indo european sino tibetan the challeng made formid fact asian languag rang languag speaker world mandarin chines close million nativ speaker extinct languag pazeh taiwan speaker result vast differ level languag process capabl number sharabl resourc individu languag major asian languag mandarin chines hindi japanes korean thai benefit",0
"KeywordsGradable adjectives Scales Intensity relation WordNet ","large huge or gigantic identifying and encoding intensity relations among adjectives in wordnet","We propose a new semantic relation for gradable adjectives in WordNet, which enriches the present, vague, similar relation with information on the degree or intensity with which different adjectives express a shared attribute. Using lexical-semantic patterns, we mine the Web for evidence of the relative strength of adjectives like “large”, “huge” and “gigantic” with respect to their attribute (“size”). The pairwise orderings we derive allow us to construct scales on which the adjectives are located. To represent the intensity relation among gradable adjectives in WordNet, we combine ordered scales with the current WordNet dumbbells based on the relation between a pair of central adjectives and a group of undifferentiated semantically similar adjectives. A new intensity relation links the adjectives in the dumbbells and their concurrent representation on scales. Besides capturing the semantics of gradable adjectives in a way that is both intuitively clear as well as consistent with corpus data, the introduction of an intensity relation would potentially result in several specific benefits for NLP.","Language Resources and Evaluation",2013,"No","gradabl adject scale intens relat wordnet larg huge gigant identifi encod intens relat adject wordnet propos semant relat gradabl adject wordnet enrich present vagu similar relat inform degre intens adject express share attribut lexic semant pattern mine web evid relat strength adject larg huge gigant respect attribut size pairwis order deriv construct scale adject locat repres intens relat gradabl adject wordnet combin order scale current wordnet dumbbel base relat pair central adject group undifferenti semant similar adject intens relat link adject dumbbel concurr represent scale captur semant gradabl adject intuit clear consist corpus data introduct intens relat potenti result specif benefit nlp",0
"KeywordsLexical semantics Verb classes Semantic resources Semantic features Resource linking ","comparing and combining semantic verb classifications","In this article, we address the task of comparing and combining different semantic verb classifications within one language. We present a methodology for the manual analysis of individual resources on the level of semantic features. The resulting representations can be aligned across resources, and allow a contrastive analysis of these resources. In a case study on the Manner of Motion domain across four German verb classifications, we find that some features are used in all resources, while others reflect individual emphases on specific meaning aspects. We also provide evidence that feature representations can ultimately provide the basis for linking verb classes themselves across resources, which allows us to combine their coverage and descriptive detail.","Language Resources and Evaluation",2008,"No","lexic semant verb class semant resourc semant featur resourc link compar combin semant verb classif articl address task compar combin semant verb classif languag present methodolog manual analysi individu resourc level semant featur result represent align resourc contrast analysi resourc case studi manner motion domain german verb classif find featur resourc reflect individu emphas specif mean aspect provid evid featur represent ultim provid basi link verb class resourc combin coverag descript detail",0
"KeywordsDefinition-to-synset mapping Pre-Qin ancient Chinese Graph-based WSD Global wordnet ","pqac wn constructing a wordnet for pre qin ancient chinese","The Princeton WordNet® (PWN) is a widely used lexical knowledge database for semantic information processing. There are now many wordnets under creation for languages worldwide. In this paper, we endeavor to construct a wordnet for Pre-Qin ancient Chinese (PQAC), called PQAC WordNet (PQAC-WN), to process the semantic information of PQAC. In previous work, most recently constructed wordnets have been established either manually by experts or automatically using resources from which translation pairs between English and the target language can be extracted. The former method, however, is time-consuming, and the latter method, owing to a lack of language resources, cannot be performed on PQAC. As a result, a method based on word definitions in a monolingual dictionary is proposed. Specifically, for each sense, kernel words are first extracted from its definition, and the senses of each kernel word are then determined by graph-based Word Sense Disambiguation. Finally, one optimal sense is chosen from the kernel word senses to guide the mapping between the word sense and PWN synset. In this research, we obtain 66 % PQAC senses that can be shared with English and another 14 % language-specific senses that were added to PQAC-WN as new synsets. Overall, the automatic mapping achieves a precision of over 85 %.","Language Resources and Evaluation",2017,"No","definit synset map pre qin ancient chines graph base wsd global wordnet pqac wn construct wordnet pre qin ancient chines princeton wordnet pwn wide lexic knowledg databas semant inform process wordnet creation languag worldwid paper endeavor construct wordnet pre qin ancient chines pqac call pqac wordnet pqac wn process semant inform pqac previous work recent construct wordnet establish manual expert automat resourc translat pair english target languag extract method time consum method owe lack languag resourc perform pqac result method base word definit monolingu dictionari propos specif sens kernel word extract definit sens kernel word determin graph base word sens disambigu final optim sens chosen kernel word sens guid map word sens pwn synset research obtain pqac sens share english languag specif sens ad pqac wn synset automat map achiev precis",0
"KeywordsText categorization Ontologies Thesauri Unlabeled short texts ","classifying unlabeled short texts using a fuzzy declarative approach","Web 2.0 provides user-friendly tools that allow persons to create and publish content online. User generated content often takes the form of short texts (e.g., blog posts, news feeds, snippets, etc). This has motivated an increasing interest on the analysis of short texts and, specifically, on their categorisation. Text categorisation is the task of classifying documents into a certain number of predefined categories. Traditional text classification techniques are mainly based on word frequency statistical analysis and have been proved inadequate for the classification of short texts where word occurrence is too small. On the other hand, the classic approach to text categorization is based on a learning process that requires a large number of labeled training texts to achieve an accurate performance. However labeled documents might not be available, when unlabeled documents can be easily collected. This paper presents an approach to text categorisation which does not need a pre-classified set of training documents. The proposed method only requires the category names as user input. Each one of these categories is defined by means of an ontology of terms modelled by a set of what we call proximity equations. Hence, our method is not category occurrence frequency based, but highly depends on the definition of that category and how the text fits that definition. Therefore, the proposed approach is an appropriate method for short text classification where the frequency of occurrence of a category is very small or even zero. Another feature of our method is that the classification process is based on the ability of an extension of the standard Prolog language, named  Bousi~Prolog , for flexible matching and knowledge representation. This declarative approach provides a text classifier which is quick and easy to build, and a classification process which is easy for the user to understand. The results of experiments showed that the proposed method achieved a reasonably useful performance.","Language Resources and Evaluation",2013,"No","text categor ontolog thesauri unlabel short text classifi unlabel short text fuzzi declar approach web user friend tool person creat publish content onlin user generat content take form short text blog post news feed snippet motiv increas interest analysi short text specif categoris text categoris task classifi document number predefin categori tradit text classif techniqu base word frequenc statist analysi prove inadequ classif short text word occurr small hand classic approach text categor base learn process requir larg number label train text achiev accur perform label document unlabel document easili collect paper present approach text categoris pre classifi set train document propos method requir categori name user input categori defin mean ontolog term model set call proxim equat method categori occurr frequenc base high depend definit categori text fit definit propos approach method short text classif frequenc occurr categori small featur method classif process base abil extens standard prolog languag name bousiprolog flexibl match knowledg represent declar approach text classifi quick easi build classif process easi user understand result experi show propos method achiev perform",0
"KeywordsNumeral classifier Classifier ontology Semantic representation Ontological relations Natural language processing (NLP) techniques Human language technology (HLT) ","semantic representation of korean numeral classifier and its ontology building for hlt applications","The complexity of Korean numeral classifiers demands semantic as well as computational approaches that employ natural language processing (NLP) techniques. The classifier is a universal linguistic device, having the two functions of quantifying and classifying nouns in noun phrase constructions. Many linguistic studies have focused on the fact that numeral classifiers afford decisive clues to categorizing nouns. However, few studies have dealt with the semantic categorization of classifiers and their semantic relations to the nouns they quantify and categorize in building ontologies. In this article, we propose the semantic recategorization of the Korean numeral classifiers in the context of classifier ontology based on large corpora and KorLex Noun 1.5 (Korean wordnet; Korean Lexical Semantic Network), considering its high applicability in the NLP domain. In particular, the classifier can be effectively used to predict the semantic characteristics of nouns and to process them appropriately in NLP. The major challenge is to make such semantic classification and the attendant NLP techniques efficient. Accordingly, a Korean numeral classifier ontology (KorLexClas 1.0), including semantic hierarchies and relations to nouns, was constructed.","Language Resources and Evaluation",2008,"No","numer classifi classifi ontolog semant represent ontolog relat natur languag process nlp techniqu human languag technolog hlt semant represent korean numer classifi ontolog build hlt applic complex korean numer classifi demand semant comput approach employ natur languag process nlp techniqu classifi univers linguist devic function quantifi classifi noun noun phrase construct linguist studi focus fact numer classifi afford decis clue categor noun studi dealt semant categor classifi semant relat noun quantifi categor build ontolog articl propos semant recategor korean numer classifi context classifi ontolog base larg corpora korlex noun korean wordnet korean lexic semant network high applic nlp domain classifi effect predict semant characterist noun process appropri nlp major challeng make semant classif attend nlp techniqu effici korean numer classifi ontolog korlexcla includ semant hierarchi relat noun construct",0
"KeywordsSearch engines Web-as-corpus Basque NLP Morphological query expansion Language-filtering words ","morphological query expansion and language filtering words for improving basque web retrieval","The experience of a user of major search engines or other web information retrieval services looking for information in the Basque language is far from satisfactory: they only return pages with exact matches but no inflections (necessary for an agglutinative language like Basque), many results in other languages (no search engine gives the option to restrict its results to Basque), etc. This paper proposes using morphological query expansion and language-filtering words in combination with the APIs of search engines as a very cost-effective solution to build appropriate web search services for Basque. The implementation details of the methodology (choosing the most appropriate language-filtering words, the number of them, the most frequent inflections for the morphological query expansion, etc.) have been specified by corpora-based studies. The improvements produced have been measured in terms of precision and recall both over corpora and real web searches. Morphological query expansion can improve recall up to 47 % and language-filtering words can raise precision from 15 % to around 90 %, although with a loss in recall of about 30–35 %. The proposed methodology has already been successfully used in the Basque search service Elebila (http://www.elebila.eu) and the web-as-corpus tool CorpEus (http://www.corpeus.org), and the approach could be applied to other morphologically rich or under-resourced languages as well.","Language Resources and Evaluation",2013,"No","search engin web corpus basqu nlp morpholog queri expans languag filter word morpholog queri expans languag filter word improv basqu web retriev experi user major search engin web inform retriev servic inform basqu languag satisfactori return page exact match inflect agglutin languag basqu result languag search engin option restrict result basqu paper propos morpholog queri expans languag filter word combin api search engin cost effect solut build web search servic basqu implement detail methodolog choos languag filter word number frequent inflect morpholog queri expans corpora base studi improv produc measur term precis recal corpora real web search morpholog queri expans improv recal languag filter word rais precis loss recal propos methodolog success basqu search servic elebila httpwwwelebilaeu web corpus tool corpeus httpwwwcorpeusorg approach appli morpholog rich resourc languag",0
"KeywordsNatural language processing Name-based Text Categorization Semantic similarity ","text categorization from category name in an industry motivated scenario","In this work we suggest a novel Text Categorization (TC) scenario, motivated by an ad-hoc industrial need to assign documents to a set of predefined categories, while labeled training data for the categories is not available. The scenario is applicable in many industrial settings and is interesting from the academic perspective. We present a new dataset geared for the main characteristics of the scenario, and utilize it to investigate the name-based TC approach, which uses the category names as its only input and does not require training data. We evaluate and analyze the performance of state-of-the-art methods for this dataset to identify the shortcomings of these methods for our scenario, and suggest ways for overcoming these shortcomings. We utilize statistical correlation measured over a target corpus for improving the state-of-the-art, and offer a different classification scheme based on the characteristics of the setting. We evaluate our improvements and adaptations and show superior performance of our suggested method.","Language Resources and Evaluation",2015,"No","natur languag process base text categor semant similar text categor categori industri motiv scenario work suggest text categor tc scenario motiv ad hoc industri assign document set predefin categori label train data categori scenario applic industri set interest academ perspect present dataset gear main characterist scenario util investig base tc approach categori name input requir train data evalu analyz perform state art method dataset identifi shortcom method scenario suggest way overcom shortcom util statist correl measur target corpus improv state art offer classif scheme base characterist set evalu improv adapt show superior perform suggest method",0
"KeywordsLexical resources Lexical semantics Common sense knowledge Vector representation Concept similarity NLP ","cover a linguistic resource combining common sense and lexicographic information","Lexical resources are fundamental to tackle many tasks that are central to present and prospective research in Text Mining, Information Retrieval, and connected to Natural Language Processing. In this article we introduce COVER, a novel lexical resource, along with COVERAGE, the algorithm devised to build it. In order to describe concepts, COVER proposes a compact vectorial representation that combines the lexicographic precision characterizing BabelNet and the rich common-sense knowledge featuring ConceptNet. We propose COVER as a reliable and mature resource, that has been employed in as diverse tasks as conceptual categorization, keywords extraction, and conceptual similarity. The experimental assessment is performed on the last task: we report and discuss the obtained results, pointing out future improvements. We conclude that COVER can be directly exploited to build applications, and coupled with existing resources, as well.","Language Resources and Evaluation",2018,"No","lexic resourc lexic semant common sens knowledg vector represent concept similar nlp cover linguist resourc combin common sens lexicograph inform lexic resourc fundament tackl task central present prospect research text mine inform retriev connect natur languag process articl introduc cover lexic resourc coverag algorithm devis build order describ concept cover propos compact vectori represent combin lexicograph precis character babelnet rich common sens knowledg featur conceptnet propos cover reliabl matur resourc employ divers task conceptu categor keyword extract conceptu similar experiment assess perform task report discuss obtain result point futur improv conclud cover direct exploit build applic coupl exist resourc",0
"KeywordsMachine translation Crowd evaluation Pair-wise comparison English Basque ","ebaluatoia crowd evaluation for englishbasque machine translation","This work explores the feasibility of a crowd-based pair-wise comparison evaluation to get feedback on machine translation progress for under-resourced languages. Specifically, we propose a task based on simple work units to compare the outputs of five English-to-Basque systems, which we implement in a web application. In our design, we put forward two key aspects that we believe community collaboration initiatives should consider in order to attract and maintain participants, that is, providing both a community challenge and a personal challenge. We describe how these aspects can comply with a strict methodology to ensure research validity. In particular, we consider the evaluation set size and the characteristics of the test sentences, the number of evaluators per comparison pair, and a mechanism to identify dishonest participation (or participants with insufficient linguistic knowledge). We also describe our dissemination effort, which targeted both general users and interest groups. Over 500 people participated actively in the Ebaluatoia campaign and we were able to collect over 35,000 evaluations in a short period of 10 days. From the results, we complete the ranking of the systems under evaluation and establish whether the difference in quality between the systems is significant.","Language Resources and Evaluation",2017,"No","machin translat crowd evalu pair wise comparison english basqu ebaluatoia crowd evalu englishbasqu machin translat work explor feasibl crowd base pair wise comparison evalu feedback machin translat progress resourc languag specif propos task base simpl work unit compar output english basqu system implement web applic design put forward key aspect communiti collabor initi order attract maintain particip provid communiti challeng person challeng describ aspect compli strict methodolog ensur research valid evalu set size characterist test sentenc number evalu comparison pair mechan identifi dishonest particip particip insuffici linguist knowledg describ dissemin effort target general user interest group peopl particip activ ebaluatoia campaign collect evalu short period day result complet rank system evalu establish differ qualiti system signific",0
"KeywordsCorpus annotation Cross-linguistic comparison Metaphor Metaphor density Metaphor identification Register variation ","towards a metaphor annotated corpus of mandarin chinese","
Building on the success of the VU Amsterdam Metaphor Corpus, which comprises English texts annotated with metaphor following the Metaphor Identification Procedure Vrjie Universiteit (MIPVU; Steen et al. in Cogn Linguist 21(4):765–796, 2010a; Steen et al. in A method for linguistic metaphor identification: from MIP to MIPVU. John Benjamins, Amsterdam/Philadelphia, 2010b), this study has three aims: (1) to adapt and evaluate the transferability and reliability of MIPVU for Mandarin Chinese; (2) to construct a corpus of Chinese texts annotated for metaphor using the adapted procedure; and (3) to examine the distribution of metaphor-related words across Chinese texts in three different written registers: academic discourse, fiction, and news. The results of our inter-annotator reliability test show that MIPVU can be reliably applied to linguistic metaphor identification in Chinese texts. Our metaphor-annotated corpus consists of texts randomly sampled from the Lancaster Corpus of Mandarin Chinese, totaling 30,012 words (about 10,000 for each register). Data analysis reveals that approximately one out of every nine lexical units in our Chinese corpus is related to metaphor, that there is considerable variation in metaphor density across different registers and lexical categories, and that metaphor density is significantly lower in Chinese than in English texts. Our assessment of the replicability of MIPVU for Mandarin Chinese adds to the groundbreaking methodological contribution that Steen et al. (2010a, b) has made to metaphor research. The metaphor-annotated corpus of Mandarin Chinese contributes a valuable language resource for Chinese metaphor researchers, and our analysis of the distribution of metaphor-related words in the corpus offers useful new insights into the extent and use of metaphor in Chinese discourse.","Language Resources and Evaluation",2017,"No","corpus annot cross linguist comparison metaphor metaphor densiti metaphor identif regist variat metaphor annot corpus mandarin chines build success vu amsterdam metaphor corpus compris english text annot metaphor metaphor identif procedur vrjie universiteit mipvu steen al cogn linguist a steen al method linguist metaphor identif mip mipvu john benjamin amsterdamphiladelphia b studi aim adapt evalu transfer reliabl mipvu mandarin chines construct corpus chines text annot metaphor adapt procedur examin distribut metaphor relat word chines text written regist academ discours fiction news result inter annot reliabl test show mipvu reliabl appli linguist metaphor identif chines text metaphor annot corpus consist text random sampl lancast corpus mandarin chines total word regist data analysi reveal approxim lexic unit chines corpus relat metaphor consider variat metaphor densiti regist lexic categori metaphor densiti signific lower chines english text assess replic mipvu mandarin chines add groundbreak methodolog contribut steen al a made metaphor research metaphor annot corpus mandarin chines contribut valuabl languag resourc chines metaphor research analysi distribut metaphor relat word corpus offer insight extent metaphor chines discours",0
"KeywordsLexica Terminology Semantic Web Linked data Ontologies ","interchanging lexical resources on the semantic web","Lexica and terminology databases play a vital role in many NLP applications, but currently most such resources are published in application-specific formats, or with custom access interfaces, leading to the problem that much of this data is in “data silos” and hence difficult to access. The Semantic Web and in particular the Linked Data initiative provide effective solutions to this problem, as well as possibilities for data reuse by inter-lexicon linking, and incorporation of data categories by dereferencable URIs. The Semantic Web focuses on the use of ontologies to describe semantics on the Web, but currently there is no standard for providing complex lexical information for such ontologies and for describing the relationship between the lexicon and the ontology. We present our model, lemon, which aims to address these gaps while building on existing work, in particular the Lexical Markup Framework, the ISOcat Data Category Registry, SKOS (Simple Knowledge Organization System) and the LexInfo and LIR ontology-lexicon models.","Language Resources and Evaluation",2012,"No","lexica terminolog semant web link data ontolog interchang lexic resourc semant web lexica terminolog databas play vital role nlp applic resourc publish applic specif format custom access interfac lead problem data data silo difficult access semant web link data initi provid effect solut problem possibl data reus inter lexicon link incorpor data categori dereferenc uri semant web focus ontolog describ semant web standard provid complex lexic inform ontolog describ relationship lexicon ontolog present model lemon aim address gap build exist work lexic markup framework isocat data categori registri skos simpl knowledg organ system lexinfo lir ontolog lexicon model",0
"KeywordsCorpus linguistics Lexicography Computational linguistics Natural language processing Dictionaries Irish Gaelic Hiberno-English Language technology ","efficient corpus development for lexicography building the new corpus for ireland","In a 12-month project we have developed a new, register-diverse, 55-million-word bilingual corpus—the New Corpus for Ireland (NCI)—to support the creation of a new English-to-Irish dictionary. The paper describes the strategies we employed, and the solutions to problems encountered. We believe we have a good model for corpus creation for lexicography, and others may find it useful as a blueprint. The corpus has two parts, one Irish, the other Hiberno-English (English as spoken in Ireland). We describe its design, collection and encoding.","Language Resources and Evaluation",2006,"No","corpus linguist lexicographi comput linguist natur languag process dictionari irish gaelic hiberno english languag technolog effici corpus develop lexicographi build corpus ireland month project develop regist divers million word bilingu corpus corpus ireland nci support creation english irish dictionari paper describ strategi employ solut problem encount good model corpus creation lexicographi find blueprint corpus part irish hiberno english english spoken ireland describ design collect encod",0
"KeywordsLanguage resources Hebrew Corpora Lexicon Morphological processing WordNet ","language resources for hebrew","We describe a suite of standards, resources and tools for computational encoding and processing of Modern Hebrew texts. These include an array of XML schemas for representing linguistic resources; a variety of text corpora, raw, automatically processed and manually annotated; lexical databases, including a broad-coverage monolingual lexicon, a bilingual dictionary and a WordNet; and morphological processors which can analyze, generate and disambiguate Hebrew word forms. The resources are developed under centralized supervision, so that they are compatible with each other. They are freely available and many of them have already been used for several applications, both academic and industrial.","Language Resources and Evaluation",2008,"No","languag resourc hebrew corpora lexicon morpholog process wordnet languag resourc hebrew describ suit standard resourc tool comput encod process modern hebrew text includ array xml schema repres linguist resourc varieti text corpora raw automat process manual annot lexic databas includ broad coverag monolingu lexicon bilingu dictionari wordnet morpholog processor analyz generat disambigu hebrew word form resourc develop central supervis compat freeli applic academ industri",0
"Key Wordspoetry verse limerick meter prosody computer-assisted instruction speech synthesis interactive freshman English literature ","poetry i teaching verse with cai","The teaching of literature through CAI raises problems of both a linguistic and instructional nature; student involvement and creativity in studying literature, and especially poetry, is difficult to build into a computer-based lesson. We have confronted these difficulties in the lessonPoetry I, which introduces undergraduates to basic concepts of poetic verse in a design using screen display, speech synthesis, and verse processing to maximize interactivity and student involvement.","Computers and the Humanities",1990,"No","key wordspoetri vers limerick meter prosodi comput assist instruct speech synthesi interact freshman english literatur poetri teach vers cai teach literatur cai rais problem linguist instruct natur student involv creativ studi literatur poetri difficult build comput base lesson confront difficulti lessonpoetri introduc undergradu basic concept poetic vers design screen display speech synthesi vers process maxim interact student involv",0
NA,"notes and news",NA,"Computers and the Humanities",1990,"No","note news na",0
"KeywordsComputational Linguistic ","computational lexicography",NA,"Computers and the Humanities",1985,"No","comput linguist comput lexicographi na",0
"Key wordsderivational morphology lexical transformation lexicon extension word-formation ","new words from old a formalism for word formation","Many languages make use of word-formation devices to allow speakers or writers to create new words when the existing vocabulary proves inadequate. In this paper we consider how these devices can be expressed formally, allowing them to be used in word- and sentence-generation, for dictionary expansion, and the like. The paper begins with some typical word-formation rules drawn mostly from French. Attention is drawn to some features of these rules which must be captured in any formal representation. The formal representation of a basic lexical transformation is presented in some detail, along with a number of examples. A computer implementation of the transformation system is described, together with a range of applications. A discussion of static and dynamic generation leads to the concept of an inverted transformation.","Computers and the Humanities",1995,"No","key wordsderiv morpholog lexic transform lexicon extens word format word formal word format languag make word format devic speaker writer creat word exist vocabulari prove inadequ paper devic express formal allow word sentenc generat dictionari expans paper begin typic word format rule drawn french attent drawn featur rule captur formal represent formal represent basic lexic transform present detail number exampl comput implement transform system rang applic discuss static dynam generat lead concept invert transform",0
"Key Wordsdictionary lexicography word senses polysemy homonymy corpus ","dictionary word sense distinctions an enquiry into their nature","The word senses in a published dictionary are a valuable resource for natural language processing and textual criticism alike. In order that they can be further exploited, their nature must be better understood. Lexicographers have always had to decide where to say a word has one sense, where two. The two studies described here look into their grounds for making distinctions. The first develops a classification scheme to describe the commonly occurring distinction types. The second examines the task of matching the usages of a word from a corpus with the senses a dictionary provides. Finally, a view of the ontological status of dictionary word senses is presented.","Computers and the Humanities",1992,"No","key wordsdictionari lexicographi word sens polysemi homonymi corpus dictionari word sens distinct enquiri natur word sens publish dictionari valuabl resourc natur languag process textual critic alik order exploit natur understood lexicograph decid word sens studi ground make distinct develop classif scheme describ common occur distinct type examin task match usag word corpus sens dictionari final view ontolog status dictionari word sens present",0
"KeywordsWord Sense Induction Word Sense Disambiguation Lexical Semantics ","evaluating word sense induction and disambiguation methods","Word Sense Induction (WSI) is the task of identifying the different uses (senses) of a target word in a given text in an unsupervised manner, i.e. without relying on any external resources such as dictionaries or sense-tagged data. This paper presents a thorough description of the SemEval-2010 WSI task and a new evaluation setting for sense induction methods. Our contributions are two-fold: firstly, we provide a detailed analysis of the Semeval-2010 WSI task evaluation results and identify the shortcomings of current evaluation measures. Secondly, we present a new evaluation setting by assessing participating systems’ performance according to the skewness of target words’ distribution of senses showing that there are methods able to perform well above the Most Frequent Sense (MFS) baseline in highly skewed distributions.","Language Resources and Evaluation",2013,"No","word sens induct word sens disambigu lexic semant evalu word sens induct disambigu method word sens induct wsi task identifi sens target word text unsupervis manner reli extern resourc dictionari sens tag data paper present descript semev wsi task evalu set sens induct method contribut fold first provid detail analysi semev wsi task evalu result identifi shortcom current evalu measur present evalu set assess particip system perform skew target word distribut sens show method perform frequent sens mfs baselin high skew distribut",0
"KeywordsComplex predicates Wordnet Ontology Noun incorporation Compound verbs Verb hierarchy ","complex predicates in indian languages and wordnets","Wordnets, which are repositories of lexical semantic knowledge containing semantically linked synsets and lexically linked words, are indispensable for work on computational linguistics and natural language processing. While building wordnets for Hindi and Marathi, two major Indo-European languages, we observed that the verb hierarchy in the Princeton Wordnet was rather shallow. We set to constructing a verb knowledge base for Hindi, which arranges the Hindi verbs in a hierarchy of is-a (hypernymy) relation. We realized that there are unique Indian language phenomena that bear upon the lexicalization vs. syntactically derived choice. One such example is the occurrence of conjunct and compound verbs (called Complex Predicates) which are found in all Indian languages. This paper presents our experience in the construction of lexical knowledge bases for Indian languages with special attention to Hindi. The question of storing versus deriving complex predicates has been dealt with linguistically and computationally. We have constructed empirical tests to decide if a combination of two words, the second of which is a verb, is a complex predicate or not. Such tests provide a principled way of deciding the status of complex predicates in Indian language wordnets.","Language Resources and Evaluation",2006,"No","complex predic wordnet ontolog noun incorpor compound verb verb hierarchi complex predic indian languag wordnet wordnet repositori lexic semant knowledg semant link synset lexic link word indispens work comput linguist natur languag process build wordnet hindi marathi major indo european languag observ verb hierarchi princeton wordnet shallow set construct verb knowledg base hindi arrang hindi verb hierarchi hypernymi relat realiz uniqu indian languag phenomena bear lexic syntact deriv choic occurr conjunct compound verb call complex predic found indian languag paper present experi construct lexic knowledg base indian languag special attent hindi question store versus deriv complex predic dealt linguist comput construct empir test decid combin word verb complex predic test provid principl decid status complex predic indian languag wordnet",0
"KeywordsWordnet Bilingual lexicon Quality assessment Knowledge representation Word-sense disambiguation ","is it possible to create a very large wordnet in 100 days an evaluation","Wordnets are large-scale lexical databases of related words and concepts, useful for language-aware software applications. They have recently been built for many languages by using various approaches. The Finnish wordnet, FinnWordNet (FiWN), was created by translating the more than 200,000 word senses in the English Princeton WordNet (PWN) 3.0 in 100 days. To ensure quality, they were translated by professional translators. The direct translation approach was based on the assumption that most synsets in PWN represent language-independent real-world concepts. Thus also the semantic relations between synsets were assumed mostly language-independent, so the structure of PWN could be reused as well. This approach allowed the creation of an extensive Finnish wordnet directly aligned with PWN and also provided us with a translation relation and thus a bilingual wordnet usable as a dictionary. In this paper, we address several concerns raised with regard to our approach, many of them for the first time. We evaluate the craftsmanship of the translators by checking the spelling and translation quality, the viability of the approach by assessing the synonym quality both on the lexeme and concept level, as well as the usefulness of the resulting lexical resource both for humans and in a language-technological task. We discovered no new problems compared with those already known in PWN. As a whole, the paper contributes to the scientific discourse on what it takes to create a very large wordnet. As a side-effect of the evaluation, we extended FiWN to contain 208,645 word senses in 120,449 synsets, effectively making version 2.0 of FiWN currently the largest wordnet in the world by these statistics.","Language Resources and Evaluation",2014,"No","wordnet bilingu lexicon qualiti assess knowledg represent word sens disambigu creat larg wordnet day evalu wordnet larg scale lexic databas relat word concept languag awar softwar applic recent built languag approach finnish wordnet finnwordnet fiwn creat translat word sens english princeton wordnet pwn day ensur qualiti translat profession translat direct translat approach base assumpt synset pwn repres languag independ real world concept semant relat synset assum languag independ structur pwn reus approach allow creation extens finnish wordnet direct align pwn provid translat relat bilingu wordnet usabl dictionari paper address concern rais regard approach time evalu craftsmanship translat check spell translat qualiti viabil approach assess synonym qualiti lexem concept level use result lexic resourc human languag technolog task discov problem compar pwn paper contribut scientif discours take creat larg wordnet side effect evalu extend fiwn word sens synset effect make version fiwn largest wordnet world statist",0
"KeywordsOnline resources Text corpora Sublexical variables Psycholinguistics Greek Syllabification Bigrams ","iplr an online resource for greek word level and sublexical information","We present a new online psycholinguistic resource for Greek based on analyses of written corpora combined with text processing technologies developed at the Institute for Language & Speech Processing (ILSP), Greece. The “ILSP PsychoLinguistic Resource” (IPLR) is a freely accessible service via a dedicated web page, at http://speech.ilsp.gr/iplr. IPLR provides analyses of user-submitted letter strings (words and nonwords) as well as frequency tables for important units and conditions such as syllables, bigrams, and neighbors, calculated over two word lists based on printed text corpora and their phonetic transcription. Online tools allow retrieval of words matching user-specified orthographic or phonetic patterns. All results and processing code (in the Python programming language) are freely available for noncommercial educational or research use.","Language Resources and Evaluation",2012,"No","onlin resourc text corpora sublex variabl psycholinguist greek syllabif bigram iplr onlin resourc greek word level sublex inform present onlin psycholinguist resourc greek base analys written corpora combin text process technolog develop institut languag speech process ilsp greec ilsp psycholinguist resourc iplr freeli access servic dedic web page httpspeechilspgriplr iplr analys user submit letter string word nonword frequenc tabl import unit condit syllabl bigram neighbor calcul word list base print text corpora phonet transcript onlin tool retriev word match user orthograph phonet pattern result process code python program languag freeli noncommerci educ research",0
"KeywordsParallel treebank Parallel corpus Machine translation Syntax-based machine translation Constituent alignment Tree alignment Resource development ","large aligned treebanks for syntax based machine translation","We present a collection of parallel treebanks that have been automatically aligned on both the terminal and the non-terminal constituent level for use in syntax-based machine translation. 
We describe how they were constructed and applied to a syntax- and example-based machine translation system called Parse and Corpus-Based Machine Translation (PaCo-MT). For the language pair Dutch to English, we present non-terminal alignment evaluation scores for a variety of tree alignment approaches. Finally, based on the parallel treebanks created by these approaches, we evaluate the MT system itself and compare the scores with those of Moses, a current state-of-the-art statistical MT system, when trained on the same data.","Language Resources and Evaluation",2017,"No","parallel treebank parallel corpus machin translat syntax base machin translat constitu align tree align resourc develop larg align treebank syntax base machin translat present collect parallel treebank automat align termin termin constitu level syntax base machin translat describ construct appli syntax base machin translat system call pars corpus base machin translat paco mt languag pair dutch english present termin align evalu score varieti tree align approach final base parallel treebank creat approach evalu mt system compar score mose current state art statist mt system train data",0
"KeywordsVerbNet Multilingual NLP Levin verb classes Lexical-semantic classification ","investigating the cross lingual translatability of verbnet style classification","VerbNet—the most extensive online verb lexicon currently available for English—has proved useful in supporting a variety of NLP tasks. However, its exploitation in multilingual NLP has been limited by the fact that such classifications are available for few languages only. Since manual development of VerbNet is a major undertaking, researchers have recently translated VerbNet classes from English to other languages. However, no systematic investigation has been conducted into the applicability and accuracy of such a translation approach across different, typologically diverse languages. Our study is aimed at filling this gap. We develop a systematic method for translation of VerbNet classes from English to other languages which we first apply to Polish and subsequently to Croatian, Mandarin, Japanese, Italian, and Finnish. Our results on Polish demonstrate high translatability with all the classes (96% of English member verbs successfully translated into Polish) and strong inter-annotator agreement, revealing a promising degree of overlap in the resultant classifications. The results on other languages are equally promising. This demonstrates that VerbNet classes have strong cross-lingual potential and the proposed method could be applied to obtain gold standards for automatic verb classification in different languages. We make our annotation guidelines and the six language-specific verb classifications available with this paper.","Language Resources and Evaluation",2018,"No","verbnet multilingu nlp levin verb class lexic semant classif investig cross lingual translat verbnet style classif verbnet extens onlin verb lexicon english prove support varieti nlp task exploit multilingu nlp limit fact classif languag manual develop verbnet major undertak research recent translat verbnet class english languag systemat investig conduct applic accuraci translat approach typolog divers languag studi aim fill gap develop systemat method translat verbnet class english languag appli polish subsequ croatian mandarin japanes italian finnish result polish demonstr high translat class english member verb success translat polish strong inter annot agreement reveal promis degre overlap result classif result languag equal promis demonstr verbnet class strong cross lingual potenti propos method appli obtain gold standard automat verb classif languag make annot guidelin languag specif verb classif paper",0
"KeywordsNominalization Argument structure Semantic corpus annotation Heuristic rules ","annotating the argument structure of deverbal nominalizations in spanish","Over recent years, there has been a growing interest in the computational treatment of nominalized Noun Phrases due to the rich semantic information they contain. These Noun Phrases can be understood as verbal paraphrases and, just like them, they can also denote argument and thematic-role relations. This paper presents the methodology followed to annotate the argument structure of deverbal nominalizations in the Spanish AnCora-Es corpus. We focus on the automated annotation process that is mostly based on the semantic information specified in a verbal lexicon but also on the syntactic and semantic information annotated in the corpus. The heuristic rules that make use of this information rely on linguistic assumptions that are also evaluated as we evaluate the reliability of the automated process. The automated annotation was manually checked in order to ensure the accuracy of the final resource. We demonstrate its feasibility (77% F-measure) and show that it facilitates corpus annotation, which is always a time-consuming and costly process. The result is the enrichment of the AnCora-Es corpus with the argument structure and thematic roles of deverbal nominalizations. It is the first Spanish corpus with this kind of information that is freely available.","Language Resources and Evaluation",2012,"No","nomin argument structur semant corpus annot heurist rule annot argument structur deverb nomin spanish recent year grow interest comput treatment nomin noun phrase due rich semant inform noun phrase understood verbal paraphras denot argument themat role relat paper present methodolog annot argument structur deverb nomin spanish ancora es corpus focus autom annot process base semant inform verbal lexicon syntact semant inform annot corpus heurist rule make inform reli linguist assumpt evalu evalu reliabl autom process autom annot manual check order ensur accuraci final resourc demonstr feasibl measur show facilit corpus annot time consum cost process result enrich ancora es corpus argument structur themat role deverb nomin spanish corpus kind inform freeli",0
"KeywordsText mining Information extraction Multilinguality Saving effort Rule-based Machine learning Cross-lingual projection Methods Algorithms Sentiment analysis Summarisation Quotation recognition String similarity calculation Media monitoring ","a survey of methods to ease the development of highly multilingual text mining applications","Multilingual text processing is useful because the information content found in different languages is complementary, both regarding facts and opinions. While Information Extraction and other text mining software can, in principle, be developed for many languages, most text analysis tools have only been applied to small sets of languages because the development effort per language is large. Self-training tools obviously alleviate the problem, but even the effort of providing training data and of manually tuning the results is usually considerable. In this paper, we gather insights by various multilingual system developers on how to minimise the effort of developing natural language processing applications for many languages. We also explain the main guidelines underlying our own effort to develop complex text mining software for tens of languages. While these guidelines—most of all: extreme simplicity—can be very restrictive and limiting, we believe to have shown the feasibility of the approach through the development of the Europe Media Monitor (EMM) family of applications (http://emm.newsbrief.eu/overview.html). EMM is a set of complex media monitoring tools that process and analyse up to 100,000 online news articles per day in between twenty and fifty languages. We will also touch upon the kind of language resources that would make it easier for all to develop highly multilingual text mining applications. We will argue that—to achieve this—the most needed resources would be freely available, simple, parallel and uniform multilingual dictionaries, corpora and software tools.","Language Resources and Evaluation",2012,"No","text mine inform extract multilingu save effort rule base machin learn cross lingual project method algorithm sentiment analysi summaris quotat recognit string similar calcul media monitor survey method eas develop high multilingu text mine applic multilingu text process inform content found languag complementari fact opinion inform extract text mine softwar principl develop languag text analysi tool appli small set languag develop effort languag larg train tool allevi problem effort provid train data manual tune result consider paper gather insight multilingu system develop minimis effort develop natur languag process applic languag explain main guidelin under effort develop complex text mine softwar ten languag guidelin extrem simplic restrict limit shown feasibl approach develop europ media monitor emm famili applic httpemmnewsbriefeuoverviewhtml emm set complex media monitor tool process analys onlin news articl day twenti fifti languag touch kind languag resourc make easier develop high multilingu text mine applic argu achiev need resourc freeli simpl parallel uniform multilingu dictionari corpora softwar tool",0
"KeywordsFrameNet Frame semantics Lexical semantics interoperability WordNet Lexicon Corpus Semantic role Thematic role Lexical resource Crowdsourcing ","framenet current collaborations and future goals","This paper will focus on recent and near-term future developments at FrameNet (FN) and the interoperability issues they raise. We begin by discussing the current state of the Berkeley FN database including major changes in the data format for the latest data release. We then briefly review two recent local projects, ""Rapid Vanguarding”, which has created a new interface for the frame and lexical unit definition process based on the Word Sketch Engine of Kilgarriff et al. (2004), and “Beyond the Core”, which has developed tools for annotating constructions, and created a sample “construction” of especially “interesting” constructions which are neither simply lexical nor easy for the standard parsers to parse. We also cover two current collaborations, FN’s part in the development of the manually annotated subcorpus of the American National Corpus, and a pilot study on aligning WordNet and FrameNet, to exploit the complementary strengths of these quite different resources. We discuss FN-related research on Spanish, Japanese, German (SALSA), Chinese and other languages, and the language-independence of frames, along with interesting FN-related work by others, and a sketch of a large group of image-schematic frames which are now being added to FN. We close with some ideas about how FrameNet can be opened up, to allow broader participation in the development process without losing precision and coherence, including a small-scale study on acquiring data for FN using Amazon’s Mechanical Turk crowd-sourcing system.","Language Resources and Evaluation",2012,"No","framenet frame semant lexic semant interoper wordnet lexicon corpus semant role themat role lexic resourc crowdsourc framenet current collabor futur goal paper focus recent term futur develop framenet fn interoper issu rais begin discuss current state berkeley fn databas includ major data format latest data releas briefli review recent local project rapid vanguard creat interfac frame lexic unit definit process base word sketch engin kilgarriff al core develop tool annot construct creat sampl construct interest construct simpli lexic easi standard parser pars cover current collabor fn part develop manual annot subcorpus american nation corpus pilot studi align wordnet framenet exploit complementari strength resourc discuss fn relat research spanish japanes german salsa chines languag languag independ frame interest fn relat work sketch larg group imag schemat frame ad fn close idea framenet open broader particip develop process lose precis coher includ small scale studi acquir data fn amazon mechan turk crowd sourc system",0
"Key Wordscomputational stylistics style stylistics statistics literary style ","progress in stylistics theory statistics computers","This paper attempts to assess the progress made in computational stylistics dyring the course of the past twenty-five years. First, we discuss some theoretical notions of style, and then we sketch certain trends that emerge from relevant articles appearing in a variety of publications including conference proceedings and academic journals (other than CHum). The conclusion is that progress has been mixed.","Computers and the Humanities",1991,"No","key wordscomput stylist style stylist statist literari style progress stylist theori statist comput paper attempt assess progress made comput stylist dyre past twenti year discuss theoret notion style sketch trend emerg relev articl appear varieti public includ confer proceed academ journal chum conclus progress mix",0
"word sense disambiguation semantics, grammar knowledge representation ","senses and texts","This paper addresses the question of whether it is possible tosense-tag systematically, and on a large scale, and how we shouldassess progress so far. That is to say, how to attach each occurrenceof a word in a text to one and only one sense in a dictionary – aparticular dictionary of course, and that is part of the problem. Thepaper does not propose a solution to the question, though we havereported empirical findings elsewhere (Cowie et al., 1992;Wilks et al., 1996; Wilks and Stevenson, 1997), and intend to continue andrefine that work. The point of this paper is to examine two well-knowncontributions critically: The first (Kilgarriff, 1993), which is widelytaken to show that the task, as defined, cannot be carried outsystematically by humans and, secondly (Yarowsky, 1995), which claimsstrikingly good results at doing exactly that.","Computers and the Humanities",1997,"No","word sens disambigu semant grammar knowledg represent sens text paper address question tosens tag systemat larg scale shouldassess progress attach occurrenceof word text sens dictionari aparticular dictionari part problem thepap propos solut question havereport empir find cowi al wilk al wilk stevenson intend continu andrefin work point paper examin knowncontribut critic kilgarriff widelytaken show task defin carri outsystemat human yarowski claimsstrik good result",0
"KeywordsComputational Linguistic Machine Decision ","automated concordances and word indexes machine decisions and editorial revisions",NA,"Computers and the Humanities",1982,"No","comput linguist machin decis autom concord word index machin decis editori revis na",0
"KeywordsComputational Linguistic Important Approach Common Word Newspaper Corpus Authorship Attribution ","computer based authorship attribution without lexical measures","The most important approaches to computer-assistedauthorship attribution are exclusively based onlexical measures that either represent the vocabularyrichness of the author or simply comprise frequenciesof occurrence of common words. In this paper wepresent a fully-automated approach to theidentification of the authorship of unrestricted textthat excludes any lexical measure. Instead we adapt aset of style markers to the analysis of the textperformed by an already existing natural languageprocessing tool using three stylometric levels, i.e.,token-level, phrase-level, and analysis-levelmeasures. The latter represent the way in which thetext has been analyzed. The presented experiments ona Modern Greek newspaper corpus show that the proposedset of style markers is able to distinguish reliablythe authors of a randomly-chosen group and performsbetter than a lexically-based approach. However, thecombination of these two approaches provides the mostaccurate solution (i.e., 87% accuracy). Moreover, wedescribe experiments on various sizes of the trainingdata as well as tests dealing with the significance ofthe proposed set of style markers.","Computers and the Humanities",2001,"No","comput linguist import approach common word newspap corpus authorship attribut comput base authorship attribut lexic measur import approach comput assistedauthorship attribut exclus base onlex measur repres vocabularyrich author simpli compris frequenciesof occurr common word paper wepres fulli autom approach theidentif authorship unrestrict textthat exclud lexic measur adapt aset style marker analysi textperform exist natur languageprocess tool stylometr level token level phrase level analysi levelmeasur repres thetext analyz present experi ona modern greek newspap corpus show proposedset style marker distinguish reliablyth author random chosen group performsbett lexic base approach thecombin approach mostaccur solut accuraci wedescrib experi size trainingdata test deal signific ofth propos set style marker",0
"Key Wordsstylometry authorship vocabulary model multivariate ","authorship attribution","This paper considers the problem of quantifying literary style and looks at several variables which may be used as stylistic “fingerprints” of a writer. A review of work done on the statistical analysis of “change over time” in literary style is then presented, followed by a look at a specific application area, the authorship of Biblical texts.","Computers and the Humanities",1994,"No","key wordsstylometri authorship vocabulari model multivari authorship attribut paper consid problem quantifi literari style variabl stylist fingerprint writer review work statist analysi chang time literari style present specif applic area authorship biblic text",0
"KeywordsVerbal Behavior Literary Study Dissertation Abstract Undergraduate Curriculum Educational Computing ","annual bibliography for 1973 and supplement to preceding years",NA,"Computers and the Humanities",1974,"No","verbal behavior literari studi dissert abstract undergradu curriculum educ comput annual bibliographi supplement preced year na",0
"Key Wordsuser interfaces databases programming languages natural language processing corpus linguistics syntactic analysis ","the chameleon approach a technique to reach more users","The possible benefits of computing in humanities research are often wasted because of the psychological barriers that computers evoke in non-specialists. This paper examines the underlying causes and suggests some ways of alleviating the problem. One approach in particular, i.e. ease through familiarity, is discussed in more detail. It is illustrated by means of a description of a database system that uses this approach: the Linguistic DataBase, which contains syntactic analysis trees of natural language data.","Computers and the Humanities",1989,"No","key wordsus interfac databas program languag natur languag process corpus linguist syntact analysi chameleon approach techniqu reach user benefit comput human research wast psycholog barrier comput evok specialist paper examin under suggest way allevi problem approach eas familiar discuss detail illustr mean descript databas system approach linguist databas syntact analysi tree natur languag data",0
"KeywordsAfrican language resources Pragmatics Corpus search infrastructure ","information structure in african languages corpora and tools","In this paper, we describe tools and resources for the study of African languages developed at the Collaborative Research Centre 632 “Information Structure”. These include deeply annotated data collections of 25 sub-Saharan languages that are described together with their annotation scheme, as well as the corpus tool ANNIS, which provides unified access to a broad variety of annotations created with a range of different tools. With the application of ANNIS to several African data collections, we illustrate its suitability for the purpose of language documentation, distributed access, and the creation of data archives.","Language Resources and Evaluation",2011,"No","african languag resourc pragmat corpus search infrastructur inform structur african languag corpora tool paper describ tool resourc studi african languag develop collabor research centr inform structur includ deepli annot data collect saharan languag annot scheme corpus tool anni unifi access broad varieti annot creat rang tool applic anni african data collect illustr suitabl purpos languag document distribut access creation data archiv",0
"KeywordsMetadata Infrastructure CLARIN ","creating testing clarin metadata components","The CLARIN Metadata Infrastructure (CMDI) that is being developed in Common Language Resources and Technology Infrastructure (CLARIN) is a computer-supported framework that combines a flexible component approach with the explicit declaration of semantics. The goal of the Dutch CLARIN project “Creating & Testing CLARIN Metadata Components” was to create metadata components and profiles for a wide variety of existing resources housed at two data centres according to the CMDI specifications. In doing so the principles of the framework were tested. The results of the project are of benefit to other CLARIN-projects that are expected to adhere to the CMDI framework and its accompanying tools.","Language Resources and Evaluation",2013,"No","metadata infrastructur clarin creat test clarin metadata compon clarin metadata infrastructur cmdi develop common languag resourc technolog infrastructur clarin comput support framework combin flexibl compon approach explicit declar semant goal dutch clarin project creat test clarin metadata compon creat metadata compon profil wide varieti exist resourc hous data centr cmdi specif principl framework test result project benefit clarin project expect adher cmdi framework accompani tool",0
"KeywordsCollaborative annotation Arabic Treebank Quran Corpus ","supervised collaboration for syntactic annotation of quranic arabic","The Quranic Arabic Corpus (http://corpus.quran.com) is a collaboratively constructed linguistic resource initiated at the University of Leeds, with multiple layers of annotation including part-of-speech tagging, morphological segmentation (Dukes and Habash 2010) and syntactic analysis using dependency grammar (Dukes and Buckwalter 2010). The motivation behind this work is to produce a resource that enables further analysis of the Quran, the 1,400 year-old central religious text of Islam. This project contrasts with other Arabic treebanks by providing a deep linguistic model based on the historical traditional grammar known as i′rāb (إعراب). By adapting this well-known canon of Quranic grammar into a familiar tagset, it is possible to encourage online annotation by Arabic linguists and Quranic experts. This article presents a new approach to linguistic annotation of an Arabic corpus: online supervised collaboration using a multi-stage approach. The different stages include automatic rule-based tagging, initial manual verification, and online supervised collaborative proofreading. A popular website attracting thousands of visitors per day, the Quranic Arabic Corpus has approximately 100 unpaid volunteer annotators each suggesting corrections to existing linguistic tagging. To ensure a high-quality resource, a small number of expert annotators are promoted to a supervisory role, allowing them to review or veto suggestions made by other collaborators. The Quran also benefits from a large body of existing historical grammatical analysis, which may be leveraged during this review. In this paper we evaluate and report on the effectiveness of the chosen annotation methodology. We also discuss the unique challenges of annotating Quranic Arabic online and describe the custom linguistic software used to aid collaborative annotation.","Language Resources and Evaluation",2013,"No","collabor annot arab treebank quran corpus supervis collabor syntact annot quran arab quran arab corpus httpcorpusquran collabor construct linguist resourc initi univers leed multipl layer annot includ part speech tag morpholog segment duke habash syntact analysi depend grammar duke buckwalt motiv work produc resourc enabl analysi quran year central religi text islam project contrast arab treebank provid deep linguist model base histor tradit grammar adapt canon quran grammar familiar tagset encourag onlin annot arab linguist quran expert articl present approach linguist annot arab corpus onlin supervis collabor multi stage approach stage includ automat rule base tag initi manual verif onlin supervis collabor proofread popular websit attract thousand visitor day quran arab corpus approxim unpaid volunt annot suggest correct exist linguist tag ensur high qualiti resourc small number expert annot promot supervisori role allow review veto suggest made collabor quran benefit larg bodi exist histor grammat analysi leverag review paper evalu report effect chosen annot methodolog discuss uniqu challeng annot quran arab onlin describ custom linguist softwar aid collabor annot",0
"Key wordslanguage corpora corpus linguistics representativeness of corpora structure of corpora uses of corpora text encoding ","icame quo vadis reflections on the use of computer corpora in linguistics","The focus of the paper is on the use of computer corpora in language research. The historical background is touched on, with special reference to work within the International Computer Archive of Modern English (ICAME). Developments in the use of corpora are surveyed. Issues taken up include the representativeness and structure of corpora. Special attention is paid to pitfalls in the use of corpora. Corpus compilers must provide adequate documentation on the texts. Corpus users must know the corpus in order to evaluate whether it is appropriate for their research problem and in order to evaluate the results of their studies.","Computers and the Humanities",1994,"No","key wordslanguag corpora corpus linguist repres corpora structur corpora corpora text encod icam quo vadi reflect comput corpora linguist focus paper comput corpora languag research histor background touch special refer work intern comput archiv modern english icam develop corpora survey issu includ repres structur corpora special attent paid pitfal corpora corpus compil provid adequ document text corpus user corpus order evalu research problem order evalu result studi",0
"KeywordsChinese PropBank Frameset Frame Alternation Semantic roles ","a chinese semantic lexicon of senses and roles","We describe a Chinese lexical semantic resource that consists of 11,765 predicates (mostly verbs and their nominalizations) analyzed with coarse-grained senses and semantic roles. We show that distinguishing senses at a coarse-grained level is a necessary part of specifying the semantic roles and describe our strategies for sense determination for purposes of predicate-argument structure specification. The semantic roles are postulated to account for syntactic variations, the different ways in which the semantic roles of a predicate are realized. The immediate purpose for this lexical semantic resource is to support the annotation of the Chinese PropBank, but we believe it can also serve as stepping stone for higher-level semantic generalizations.","Language Resources and Evaluation",2006,"No","chines propbank frameset frame altern semant role chines semant lexicon sens role describ chines lexic semant resourc consist predic verb nomin analyz coars grain sens semant role show distinguish sens coars grain level part semant role describ strategi sens determin purpos predic argument structur specif semant role postul account syntact variat way semant role predic realiz purpos lexic semant resourc support annot chines propbank serv step stone higher level semant general",0
NA,"book reviews",NA,"Computers and the Humanities",1985,"No","book review na",0
NA,"le trsor gnral des langes et parlers franais de linstitut national de la langue franaise inalf",NA,"Computers and the Humanities",1988,"No","le trsor gnral des lang parler franai de linstitut nation de la langu franais inalf na",0
"Keywordssyntactic annotation XML format corpus corpora Treebank Tiger XML ","tiger2 serialising the iso synaf syntactic object model","
This paper introduces <tiger2/>, an XML format developed to serialise the object model defined by the ISO Syntactic Annotation Framework SynAF. Based on widespread best practices we adapt a popular XML format for syntactic annotation, TigerXML, with additional features to support a variety of syntactic phenomena including constituent and dependency structures, binding, and different node types such as compounds or empty elements. We also define interfaces to other formats and standards including the Morpho-syntactic Annotation Framework MAF and the ISOCat Data Category Registry. Finally a case study of the German Treebank TueBa-D/Z is presented, showcasing the handling of constituent structures, topological fields and coreference annotation in tandem.","Language Resources and Evaluation",2015,"No","syntact annot xml format corpus corpora treebank tiger xml tiger serialis iso synaf syntact object model paper introduc tiger xml format develop serialis object model defin iso syntact annot framework synaf base widespread practic adapt popular xml format syntact annot tigerxml addit featur support varieti syntact phenomena includ constitu depend structur bind node type compound empti element defin interfac format standard includ morpho syntact annot framework maf isocat data categori registri final case studi german treebank tueba present showcas handl constitu structur topolog field corefer annot tandem",0
"KeywordsHistorical corpus Corpus annotation Morphological analysis PoS tagging Middle Hungarian Old Hungarian Corpus query tool ","creation of an annotated corpus of old and middle hungarian court records and private correspondence","
The paper introduces a novel annotated corpus of Old and Middle Hungarian (16–18 century), the texts of which were selected in order to approximate the vernacular of the given historical periods as closely as possible. The corpus consists of testimonies of witnesses in trials and samples of private correspondence. The texts are not only analyzed morphologically, but each file contains metadata that would also facilitate sociolinguistic research. The texts were segmented into clauses, manually normalized and morphosyntactically annotated using an annotation system consisting of the PurePos PoS tagger and the Hungarian morphological analyzer HuMor originally developed for Modern Hungarian but adapted to analyze Old and Middle Hungarian morphological constructions. The automatically disambiguated morphological annotation was manually checked and corrected using an easy-to-use web-based manual disambiguation interface. The normalization process and the manual validation of the annotation required extensive teamwork and provided continuous feedback for the refinement of the computational morphology and iterative retraining of the statistical models of the tagger. The paper discusses some of the typical problems that occurred during the normalization procedure and their tentative solutions. Besides, we also describe the automatic annotation tools, the process of semi-automatic disambiguation, and the query interface, a special function of which also makes correction of the annotation possible. Displaying the original, the normalized and the parsed versions of the selected texts, the beta version of the first fully normalized and annotated historical corpus of Hungarian is freely accessible at the address http://tmk.nytud.hu/.","Language Resources and Evaluation",2018,"No","histor corpus corpus annot morpholog analysi pos tag middl hungarian hungarian corpus queri tool creation annot corpus middl hungarian court record privat correspond paper introduc annot corpus middl hungarian centuri text select order approxim vernacular histor period close corpus consist testimoni wit trial sampl privat correspond text analyz morpholog file metadata facilit sociolinguist research text segment claus manual normal morphosyntact annot annot system consist purepo pos tagger hungarian morpholog analyz humor origin develop modern hungarian adapt analyz middl hungarian morpholog construct automat disambigu morpholog annot manual check correct easi web base manual disambigu interfac normal process manual valid annot requir extens teamwork provid continu feedback refin comput morpholog iter retrain statist model tagger paper discuss typic problem occur normal procedur tentat solut describ automat annot tool process semi automat disambigu queri interfac special function make correct annot display origin normal pars version select text beta version fulli normal annot histor corpus hungarian freeli access address httptmknytudhu",0
"KeywordsContemporary Persian Corpus EAGLES-based tagset Ezafe construction Homographs ","lessons from building a persian written corpus peykare","This paper addresses some of the issues learned during the course of building a written language resource, called ‘Peykare’, for the contemporary Persian. After defining five linguistic varieties and 24 different registers based on these linguistic varieties, we collected texts for Peykare to do a linguistic analysis, including cross-register differences. For tokenization of Persian, we propose a descriptive generalization to normalize orthographic variations existing in texts. To annotate Peykare, we use EAGLES guidelines which result to have a hierarchy in the part-of-speech tags. To this aim, we apply a semi-automatic approach for the annotation methodology. In the paper, we also give a special attention to the Ezafe construction and homographs which are important in Persian text analyses.","Language Resources and Evaluation",2011,"No","contemporari persian corpus eagl base tagset ezaf construct homograph lesson build persian written corpus peykar paper address issu learn build written languag resourc call peykar contemporari persian defin linguist varieti regist base linguist varieti collect text peykar linguist analysi includ cross regist differ token persian propos descript general normal orthograph variat exist text annot peykar eagl guidelin result hierarchi part speech tag aim appli semi automat approach annot methodolog paper give special attent ezaf construct homograph import persian text analys",0
"Key wordsSGML textual criticism electronic text editions text encoding TEI ","encoding textual criticism","This paper chronicles the work of the TEI textual criticism working groups through several phases, documenting how and why the design goals were shaped by the requirements of several distinct user communities and by the nature of the textual evidence itself. Encoding schemes for the representation of physical details of textual witnesses were unified with encoding schemes for critical editing practices when it was observed that the two phenomena were inextricably layered and linked within real texts. Rationale is offered for the development teams' adherence to exceedingly general design principles: (a) the requirement that the encoding notations be neutral in text-theoretic terms; (b) the need to accommodate dramatically different text-transmission phenomena and research goals within diverse text-critical arenas; (c) the need for commensurability of the text-critical markup with encoding notations used in closely related text-analytic research. The paper also assesses the results of the effort in terms of the encoding scheme's adequacy for several scholarly purposes: suggestions are made concerning the need for programmatic testing, for refinement, and for extension of the encoding model to support a broader range of text-transmission phenomena and research objectives.","Computers and the Humanities",1995,"No","key wordssgml textual critic electron text edit text encod tei encod textual critic paper chronicl work tei textual critic work group phase document design goal shape requir distinct user communiti natur textual evid encod scheme represent physic detail textual wit unifi encod scheme critic edit practic observ phenomena inextric layer link real text rational offer develop team adher exceed general design principl requir encod notat neutral text theoret term accommod dramat text transmiss phenomena research goal divers text critic arena commensur text critic markup encod notat close relat text analyt research paper assess result effort term encod scheme adequaci scholar purpos suggest made programmat test refin extens encod model support broader rang text transmiss phenomena research object",0
"Key Wordsdatabase querying information retrieval linguistic databases french interrogatives ","a database for linguists intelligent querying and increase of data","What do database (DB) querying and information retrieval imply for linguistics? What are data for the linguist? How can one envisage efficient access to data? I propose that DB querying in language sciences be designed linguistically and directly determined by linguistic data. Linguists from various backgrounds could then use a consensual query tool. An implemented data-directed DB querying system, which was developed for research on French interrogative structures, is presented in detail.","Computers and the Humanities",1994,"No","key wordsdatabas queri inform retriev linguist databas french interrog databas linguist intellig queri increas data databas db queri inform retriev impli linguist data linguist envisag effici access data propos db queri languag scienc design linguist direct determin linguist data linguist background consensu queri tool implement data direct db queri system develop research french interrog structur present detail",0
"commercial text expert systems French theory industrial text literary criticism literary text SATOR structuralism WinBrill ","industrial text and french neo structuralism","Parallel to, and to some degree inreaction to French poststructuralisttheorization (as championed by Derrida,Foucault, and Lacan, among others) is a Frenchneo-structuralism built directly on theachievements of structuralism using electronicmeans. This paper examines some exemplaryapproaches to text analysis in thisneo-structuralist vein: SATOR's topoidictionary, the WinBrill POS tagger andFrançois Rastier's interpretativesemantics. I consider how a computer-assisted``Wissenschaft'' accumulation of expertisecomplements the neo-structuralist approach.Ultimately, electronic critical studies will bedefined by their strategic position at theintersection of the two chief technologiesshaping our society: the new informationprocessing technology of computers and therepresentational techniques that haveaccumulated for centuries in texts.Understanding how these two informationmanagement paradigms complement each other is akey issue for the humanities, for computerscience, and vital to industry, even beyond thenarrow realm of the language industries. Thedirection of critical studies, a small planetlong orbiting in only rarefied academiccircles, will be radically altered by the sheersize of the economic stakes implied by a newkind of text, the industrial text, thetechnological heart of an information society.","Computers and the Humanities",2002,"No","commerci text expert system french theori industri text literari critic literari text sator structur winbril industri text french neo structur parallel degre inreact french poststructuralisttheor champion derridafoucault lacan frenchneo structur built direct theachiev structur electronicmean paper examin exemplaryapproach text analysi thisneo structuralist vein sator topoidictionari winbril pos tagger andfran oi rastier interpretativesemant comput assistedwissenschaft accumul expertisecompl neo structuralist approachultim electron critic studi bedefin strateg posit theintersect chief technologiesshap societi informationprocess technolog comput therepresent techniqu haveaccumul centuri textsunderstand informationmanag paradigm complement akey issu human computersci vital industri thenarrow realm languag industri thedirect critic studi small planetlong orbit rarefi academiccircl radic alter sheersiz econom stake impli newkind text industri text thetechnolog heart inform societi",0
"KeywordsArtificial Intelligence Literary Research Design Specification Computational Linguistic ","integrating artificial intelligence into literary research an invitation to discuss design specifications",NA,"Computers and the Humanities",1985,"No","artifici intellig literari research design specif comput linguist integr artifici intellig literari research invit discuss design specif na",0
"KeywordsObject Model Computational Linguistic Database Schema Architectural Processing Architectural Form ","using architectural forms to map tei data into an object oriented database","This paper develops a solution to the problem of importing existing TEI data into an existing object-oriented database schema without changing the TEI data or the database schema. The solution is based on architectural processing. Two meta-DTDs are used, one to define the architectural forms for the object model and another to map the existing SGML data onto those forms. A full example using a critical text in TEI markup is developed.","Computers and the Humanities",1999,"No","object model comput linguist databas schema architectur process architectur form architectur form map tei data object orient databas paper develop solut problem import exist tei data exist object orient databas schema chang tei data databas schema solut base architectur process meta dtds defin architectur form object model map exist sgml data form full critic text tei markup develop",0
"KeywordsInterlinear glossed text (IGT) Annotation Storage format ","xigt extensible interlinear glossed text for natural language processing","This paper presents Xigt, an extensible storage format for interlinear glossed text (IGT). We review design desiderata for such a format based on our own use cases as well as general best practices, and then explore existing representations of IGT through the lens of those desiderata. We give an overview of the data model and XML serialization of Xigt, and then describe its application to the use case of representing a large, noisy, heterogeneous set of IGT.","Language Resources and Evaluation",2015,"No","interlinear gloss text igt annot storag format xigt extens interlinear gloss text natur languag process paper present xigt extens storag format interlinear gloss text igt review design desiderata format base case general practic explor exist represent igt len desiderata give overview data model xml serial xigt describ applic case repres larg noisi heterogen set igt",0
"Key wordsidiolect genderlect stylistics women's language essay Mexican literature Spanish feminist criticism ","a computer assisted investigation of gender related idiolect in octavio paz and rosario castellanos","This study reports on computer-aided investigation of salient differences in the essay idiolects of the Mexican writers Octavio Paz and Rosario Castellanos and suggests that some of them may be linked to gender. It describes use of ready-made software and computational strategies requiring no tagging and minimal ocular scan. It suggests some parameters that can be searched and in most cases quantified to explore characteristics posited by linguistic and literary scholars, taking into consideration the particular language and culture of the authors.","Computers and the Humanities",1992,"No","key wordsidiolect genderlect stylist women languag essay mexican literatur spanish feminist critic comput assist investig gender relat idiolect octavio paz rosario castellano studi report comput aid investig salient differ essay idiolect mexican writer octavio paz rosario castellano suggest link gender describ readi made softwar comput strategi requir tag minim ocular scan suggest paramet search case quantifi explor characterist posit linguist literari scholar take consider languag cultur author",0
"KeywordsComputational Linguistic Linguistic Research Preliminary Survey ","a preliminary survey on the use of computers in linguistic research",NA,"Computers and the Humanities",1970,"No","comput linguist linguist research preliminari survey preliminari survey comput linguist research na",0
"KeywordsComputational Linguistic Word Meaning Meaning Exist ","do word meanings exist",NA,"Computers and the Humanities",2000,"No","comput linguist word mean mean exist word mean exist na",0
"KeywordsComputational Linguistic ","humanities computing in italy",NA,"Computers and the Humanities",1973,"No","comput linguist human comput itali na",0
"Key wordsCentury of Prose Corpus COPC historical corpora literary corpora tagging text markup ","the century of prose corpus a half million word historical data base","The Century of Prose Corpus is a historical corpus of British English of the period 1680–1780. It has been designed to provide a resource for students of the language of that era. The COPC is diachronic and may be considered a unit in what will eventually become a series of corpora providing access to the whole of the English language from the oldest specimens to the present. This article describes and explains the various features of the COPC.","Computers and the Humanities",1995,"No","key wordscenturi prose corpus copc histor corpora literari corpora tag text markup centuri prose corpus half million word histor data base centuri prose corpus histor corpus british english period design provid resourc student languag era copc diachron consid unit eventu seri corpora provid access english languag oldest specimen present articl describ explain featur copc",0
"KeywordsComputational Linguistic Chamber Music Nationalistic Fingerprint ","the nationalistic fingerprint in nineteenth century romantic chamber music",NA,"Computers and the Humanities",1979,"No","comput linguist chamber music nationalist fingerprint nationalist fingerprint nineteenth centuri romant chamber music na",0
"character coding standards document editing keyboard input multilingual text textual data interchange ","mtscript a multi lingual text editor","This paper describes the multilingual text editor MtScript developed in the framework of the MULTEXT project.MtScript enables the use of many differentwriting systems in the same document (Latin, Arabic,Cyrillic, Hebrew, Chinese, Japanese, etc.). Editingfunctions enable the insertion or deletion of textzones even if they have opposite writing directions.In addition, the languages in the text can be marked,customized keyboard input rules can be associated witheach language and different character coding systems(one or two bytes) can be combined. MtScript isbased on a portable environment (Tcl/Tk). MtScript.1.1version has been developed underUnix/X-Windows (Solaris, Linux systems) and otherversions are planned to be ported to the Windows andMacintosh environments. The current 1.1 versionpresents several limits that will be fixed in futureversions, such as the justification of bi-directionaltexts, printing support, and text import/exportsupport. Future versions will use SGML and TEI norms,which offer ways of encoding multilingual texts andare to a large extent meant for interchange.","Computers and the Humanities",1997,"No","charact code standard document edit keyboard input multilingu text textual data interchang mtscript multi lingual text editor paper describ multilingu text editor mtscript develop framework multext projectmtscript enabl differentwrit system document latin arabiccyril hebrew chines japanes editingfunct enabl insert delet textzon opposit write direct addit languag text markedcustom keyboard input rule witheach languag charact code system byte combin mtscript isbas portabl environ tcltk mtscriptvers develop underunix window solari linux system othervers plan port window andmacintosh environ current versionpres limit fix futurevers justif bi directionaltext print support text importexportsupport futur version sgml tei norm offer way encod multilingu text andar larg extent meant interchang",0
NA,"meetings announced",NA,"Computers and the Humanities",1967,"No","meet announc na",0
"KeywordsEvocation Free association WordNet relations ","evocation analyzing and propagating a semantic link based on free word association","Studies of lexical–semantic relations aim to understand the mechanism of semantic memory and the organization of the mental lexicon. However, standard paradigmatic relations such as “hypernym” and “hyponym” cannot capture connections among concepts from different parts of speech. WordNet, which organizes synsets (i.e., synonym sets) using these lexical–semantic relations, is rather sparse in its connectivity. According to WordNet statistics, the average number of outgoing/incoming arcs for the hypernym/hyponym relation per synset is 1.33. Evocation, defined as how much a concept (expressed by one or more words) brings to mind another, is proposed as a new directed and weighted measure for the semantic relatedness among concepts. Commonly applied semantic relations and relatedness measures do not seem to be fully compatible with data that reflect evocations among concepts. They are compatible but evocation captures MORE. This work aims to provide a reliable and extendable dataset of concepts evoked by, and evoking, other concepts to enrich WordNet, the existing semantic network. We propose the use of disambiguated free word association data (first responses to verbal stimuli) to infer and collect evocation ratings. WordNet aims to represent the organization of mental lexicon, and free word association which has been used by psycholinguists to explore semantic organization can contribute to the understanding. This work was carried out in two phases. In the first phase, it was confirmed that existing free word association norms can be converted into evocation data computationally. In the second phase, a two-stage association-annotation procedure of collecting evocation data from human judgment was compared to the state-of-the-art method, showing that introducing free association can greatly improve the quality of the evocation data generated. Evocation can be incorporated into WordNet as directed links with scales, and benefits various natural language processing applications.","Language Resources and Evaluation",2013,"No","evoc free associ wordnet relat evoc analyz propag semant link base free word associ studi lexic semant relat aim understand mechan semant memori organ mental lexicon standard paradigmat relat hypernym hyponym captur connect concept part speech wordnet organ synset synonym set lexic semant relat spars connect wordnet statist averag number outgoingincom arc hypernymhyponym relat synset evoc defin concept express word bring mind propos direct weight measur semant related concept common appli semant relat related measur fulli compat data reflect evoc concept compat evoc captur work aim provid reliabl extend dataset concept evok evok concept enrich wordnet exist semant network propos disambigu free word associ data respons verbal stimuli infer collect evoc rate wordnet aim repres organ mental lexicon free word associ psycholinguist explor semant organ contribut understand work carri phase phase confirm exist free word associ norm convert evoc data comput phase stage associ annot procedur collect evoc data human judgment compar state art method show introduc free associ great improv qualiti evoc data generat evoc incorpor wordnet direct link scale benefit natur languag process applic",0
NA,"book review",NA,"Computers and the Humanities",1986,"No","book review na",0
"KeywordsComputational Linguistic ","abstracts and brief notices",NA,"Computers and the Humanities",1971,"No","comput linguist abstract notic na",0
"Key Wordsquantitative studies textual analysis ARTFL Gobineau themes Stendhal André Gide literary criticism text encoding quantitative methods reception studies Riffaterre ","an argument for single author and similar studies using quantitative methods is there safety in numbers","Lack of a critical mass of scholars involved with the computer-assisted analysis of texts (CAAT), coupled with insufficient communication among various sectors of the literary and linguistic disciplines, has led to a skewed notion of computing humanists' work among their colleagues. This paper highlights the gap through examples of misunderstood humanist needs and achievements drawn from both recent media reports and humanities conferences. It suggests that networking and less modesty in manuscript submission can be at least partial solutions. The author cites some of his own published work and work-in-progress on Stendhal and Gobineau in refuting Mark Olsen's thesis that the dominance of single- or dual-author studies must be the cause of CAAT's “failure” to make significant inroads in mainstream literary journals. The author builds a case for the use of both diachronic and synchronic lexico-statistical data in carrying out such studies successfully. He recommends a new “Synthetic Criticism” where relevant quantitative methods would not be absent.","Computers and the Humanities",1993,"No","key wordsquantit studi textual analysi artfl gobineau theme stendhal andr gide literari critic text encod quantit method recept studi riffaterr argument singl author similar studi quantit method safeti number lack critic mass scholar involv comput assist analysi text caat coupl insuffici communic sector literari linguist disciplin led skew notion comput humanist work colleagu paper highlight gap exampl misunderstood humanist achiev drawn recent media report human confer suggest network modesti manuscript submiss partial solut author cite publish work work progress stendhal gobineau refut mark olsen thesi domin singl dual author studi caat failur make signific inroad mainstream literari journal author build case diachron synchron lexico statist data carri studi success recommend synthet critic relev quantit method absent",0
"KeywordsComputational Linguistic Literary Analysis ","the stylo statistical method of literary analysis",NA,"Computers and the Humanities",1988,"No","comput linguist literari analysi stylo statist method literari analysi na",0
"KeywordsControlled natural language Corpus linguistics Requirements Technical writing Textual genre ","towards the creation of a cnl adapted to requirements writing by combining writing recommendations and spontaneous regularities example in a space project","The Quality Department of the French National Space Agency (CNES, Centre National d’Études Spatiales) wishes to design a writing guide based on the real and regular writing of requirements. As a first step in this project, the present article proposes a linguistic analysis of requirements written in French by CNES engineers. One of our goals is to determine to what extent they conform to several rules laid down in two existing Controlled Natural Languages (CNLs), namely the Simplified Technical English developed by the AeroSpace and Defense Industries Association of Europe and the Guide for Writing Requirements proposed by the International Council on Systems Engineering. Indeed, although CNES engineers are not obliged to follow any controlled language in their writing of requirements, we believe that language regularities are likely to emerge from this task, mainly due to the writers’ experience. We are seeking to identify these regularities in order to use them as a basis for a new CNL for the writing of requirements. The issue is approached using natural language processing tools to identify sentences that do not comply with the rules or contain specific linguistic phenomena. We further review these sentences to understand why the recommendations cannot (or should not) always be applied when specifying large-scale projects.","Language Resources and Evaluation",2017,"No","control natur languag corpus linguist requir technic write textual genr creation cnl adapt requir write combin write recommend spontan regular space project qualiti depart french nation space agenc cnes centr nation tude spatial wish design write guid base real regular write requir step project present articl propos linguist analysi requir written french cnes engin goal determin extent conform rule laid exist control natur languag cnls simplifi technic english develop aerospac defens industri associ europ guid write requir propos intern council system engin cnes engin oblig follow control languag write requir languag regular emerg task due writer experi seek identifi regular order basi cnl write requir issu approach natur languag process tool identifi sentenc compli rule specif linguist phenomena review sentenc understand recommend appli larg scale project",0
NA,"infornttica y humanidades",NA,"Computers and the Humanities",1996,"No","infornttica humanidad na",0
"KeywordsComputational Linguistic ","recent publications",NA,"Computers and the Humanities",1969,"No","comput linguist recent public na",0
"Key Wordslexicography concordances early Italian poetic language poetry databases text encoding philology dialectology ","a concordance of the early italian poetic language","A concordance of the early Italian poetic language is being compiled at the University of Florence, based on the need to recognize the special nature of the language in earlier times. The corpus consists of some forty-five manuscripts, that is, all that remains of book production from the origins to the end of the thirteenth century. Once the work of entering the single word-tokens is done, a complete concordance results, with accompanying grammatical connotations, in which not only are Tuscan and dialectal words grouped under separate standard headwords, but also homographs are clearly distinguished.","Computers and the Humanities",1990,"No","key wordslexicographi concord earli italian poetic languag poetri databas text encod philolog dialectolog concord earli italian poetic languag concord earli italian poetic languag compil univers florenc base recogn special natur languag earlier time corpus consist forti manuscript remain book product origin end thirteenth centuri work enter singl word token complet concord result accompani grammat connot tuscan dialect word group separ standard headword homograph distinguish",0
"KeywordsComputational Linguistic Annual Bibliography ","annual bibliography for 1969",NA,"Computers and the Humanities",1970,"No","comput linguist annual bibliographi annual bibliographi na",0
"KeywordsComputational Linguistic ","the annals of computing stylistics",NA,"Computers and the Humanities",1982,"No","comput linguist annal comput stylist na",0
"KeywordsComputational Linguistic Sixteenth Century General Inquirer ","a general inquirer analysis of sixteenth century and contemporary catechisms",NA,"Computers and the Humanities",1974,"No","comput linguist sixteenth centuri general inquir general inquir analysi sixteenth centuri contemporari catech na",0
"KeywordsComputational Linguistic Tentative Beginning ","the computer and the historiansome tentative beginnings",NA,"Computers and the Humanities",1967,"No","comput linguist tentat begin comput historiansom tentat begin na",0
"KeywordsQuantitative Approach Computational Linguistic ","words and numbers a quantitative approach to swift and some understrappers",NA,"Computers and the Humanities",1970,"No","quantit approach comput linguist word number quantit approach swift understrapp na",0
"KeywordsComputational Linguistic ","directory of scholars active",NA,"Computers and the Humanities",1969,"No","comput linguist directori scholar activ na",0
"Key wordsauthor attribution content analysis discriminant analysis lexical statistics neural networks The Federalist ","on the utility of content analysis in author attributionthe federalist","In studies of author attribution, measurement of differential use of function words is the most common procedure, though lexical statistics are often used. Content analysis has seldom been employed. We compare the success of lexical statistics, content analysis, and function words in classifying the 12 disputedFederalist papers. Of course, Mosteller and Wallace (1964) have presented overwhelming evidence that all 12 were by James Madison rather than by Alexander Hamilton. Our purpose is not to challenge these attributions but rather to useThe Federalist as a test case. We found lexical statistics to be of no use in classifying the disputed papers. Using both classical canonical discriminant analysis and a neural-network approach, content analytic measures — the Harvard III Psychosociological Dictionary and semantic differential indices — were found to be successful at attributing most of the disputed papers to Madison. However, a function-word approach is more successful. We argue that content analysis can be useful in cases where the function-word approach does not yield compelling conclusions and, perhaps, in preliminary screening in cases where there are a large number of possible authors.","Computers and the Humanities",1995,"No","key wordsauthor attribut content analysi discrimin analysi lexic statist neural network federalist util content analysi author attributionth federalist studi author attribut measur differenti function word common procedur lexic statist content analysi seldom employ compar success lexic statist content analysi function word classifi disputedfederalist paper mostel wallac present overwhelm evid jame madison alexand hamilton purpos challeng attribut useth federalist test case found lexic statist classifi disput paper classic canon discrimin analysi neural network approach content analyt measur harvard iii psychosociolog dictionari semant differenti indic found success attribut disput paper madison function word approach success argu content analysi case function word approach yield compel conclus preliminari screen case larg number author",0
"KeywordsCompositionality Computational semantics Distributional semantics models ","sick through the semeval glasses lesson learned from the evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment","This paper is an extended description of SemEval-2014 Task 1, the task on the evaluation of Compositional Distributional Semantics Models on full sentences. Systems participating in the task were presented with pairs of sentences and were evaluated on their ability to predict human judgments on (1) semantic relatedness and (2) entailment.
 Training and testing data were subsets of the SICK (Sentences Involving Compositional Knowledge) data set. SICK was developed with the aim of providing a proper benchmark to evaluate compositional semantic systems, though task participation was open to systems based on any approach. Taking advantage of the SemEval experience, in this paper we analyze the SICK data set, in order to evaluate the extent to which it meets its design goal and to shed light on the linguistic phenomena that are still challenging for state-of-the-art computational semantic systems.
 Qualitative and quantitative error analyses show that many systems are quite sensitive to changes in the proportion of sentence pair types, and degrade in the presence of additional lexico-syntactic complexities which do not affect human judgements. More compositional systems seem to perform better when the task proportions are changed, but the effect needs further confirmation.
","Language Resources and Evaluation",2016,"No","composit comput semant distribut semant model sick semev glass lesson learn evalu composit distribut semant model full sentenc semant related textual entail paper extend descript semev task task evalu composit distribut semant model full sentenc system particip task present pair sentenc evalu abil predict human judgment semant related entail train test data subset sick sentenc involv composit knowledg data set sick develop aim provid proper benchmark evalu composit semant system task particip open system base approach take advantag semev experi paper analyz sick data set order evalu extent meet design goal shed light linguist phenomena challeng state art comput semant system qualit quantit error analys show system sensit proport sentenc pair type degrad presenc addit lexico syntact complex affect human judgement composit system perform task proport chang effect confirm",0
"Key wordsstatistical analysis stylistic analysis style Raymond Chandler ","the not so simple art of imitation pastiche literary style and raymond chandler","This analysis extends the tools of statistical analysis to the challenging task of distinguishing between genuine works by an author, the preeminent American writer of mysteries, Raymond Chandler, and deliberate attempts by others to mimic the author's style. Rendering the task all the more challenging, the analysis focuses exclusively on the main elements of Chandler's style rather than on his minor but telling stylistic idiosyncrasies. Statistical analysis establishes that indicators of these stylistic elements can successfully detect the pastiches.","Computers and the Humanities",1996,"No","key wordsstatist analysi stylist analysi style raymond chandler simpl art imit pastich literari style raymond chandler analysi extend tool statist analysi challeng task distinguish genuin work author preemin american writer mysteri raymond chandler deliber attempt mimic author style render task challeng analysi focus exclus main element chandler style minor tell stylist idiosyncrasi statist analysi establish indic stylist element success detect pastich",0
"KeywordsComputational Linguistic ","computerstylistics seminar a report",NA,"Computers and the Humanities",1969,"No","comput linguist computerstylist seminar report na",0
NA,"introduction",NA,"Computers and the Humanities",1990,"No","introduct na",0
"KeywordsComputational Linguistic ","letter from oslo",NA,"Computers and the Humanities",1979,"No","comput linguist letter oslo na",0
"KeywordsLiterary Research Computational Linguistic ","symposium on the uses of computers in literary research edinburgh 2730 march 1972",NA,"Computers and the Humanities",1972,"No","literari research comput linguist symposium comput literari research edinburgh march na",0
"KeywordsComputational Linguistic Correct Grammar ","correct grammar",NA,"Computers and the Humanities",1991,"No","comput linguist correct grammar correct grammar na",0
"KeywordsComputational Linguistic ","art art history and the computer",NA,"Computers and the Humanities",1966,"No","comput linguist art art histori comput na",0
"KeywordsComputational Linguistic Review Author Software Review ","software review author",NA,"Computers and the Humanities",1978,"No","comput linguist review author softwar review softwar review author na",0
NA,"e francesconi s montemagni w peters d tiscornia semantic processing of legal texts where the language of law meets the law of language lecture notes in computer science lecture notes in artificial intelligence vol 6036","The volume Semantic Processing of Legal Texts contains a total of thirteen papers that share the common theme of processing legal documents. One undisputable merit of the book is that of being the first collection to focus specifically on computational linguistic aspects of this task. Otherwise the papers in the collection represent a variety of topics as distinct as ontology engineering, multi-label classification, and translation quality assurance. They deal with theoretical foundations as well as commercial applications, and the authors’ affiliations range from universities to industry. The book is based on selected papers presented at the first workshop on Semantic Processing of Legal Texts (held at LREC 2008 in Marrakech) but comprises further, invited contributions.","Language Resources and Evaluation",2012,"No","francesconi montemagni peter tiscornia semant process legal text languag law meet law languag lectur note comput scienc lectur note artifici intellig vol volum semant process legal text total thirteen paper share common theme process legal document undisput merit book collect focus specif comput linguist aspect task paper collect repres varieti topic distinct ontolog engin multi label classif translat qualiti assur deal theoret foundat commerci applic author affili rang univers industri book base select paper present workshop semant process legal text held lrec marrakech compris invit contribut",0
"Key Wordslinguistics language instruction CALL evaluation ","developing and evaluating language courseware","The paper sets out twenty proposals for the development and evaluation of Computer Assisted Language Learning (CALL) programs. These proposals emerge from special characteristics of language instruction and of the use of computers to assist in language instruction. We combine theoretically-based assumptions with empirical findings drawn from investigation of language courseware for Hebrew speakers in Israel. We first list four unique features of language instruction: (1) the object-language-meta-language distinction; (2) computer as written medium vs. language as primary spoken medium; (3) teaching of second language skills vs. linguistics; (4) the computer as an electronic tool vs. the computer as a cognitive entity simulating the speaker. We then show how these unique characteristics of language instruction (mother-tongue and foreign language) impose special proposals on language courseware. These proposals should be observed in the development of language courseware and in the evaluation of such programs. Clearly, these proposals integrate with general courseware proposals.","Computers and the Humanities",1992,"No","key wordslinguist languag instruct call evalu develop evalu languag coursewar paper set twenti propos develop evalu comput assist languag learn call program propos emerg special characterist languag instruct comput assist languag instruct combin theoret base assumpt empir find drawn investig languag coursewar hebrew speaker israel list uniqu featur languag instruct object languag meta languag distinct comput written medium languag primari spoken medium teach languag skill linguist comput electron tool comput cognit entiti simul speaker show uniqu characterist languag instruct mother tongu foreign languag impos special propos languag coursewar propos observ develop languag coursewar evalu program propos integr general coursewar propos",0
"Key Wordsliterary computing literary criticism text input statistical methods textual analysis polemics ","literary criticism and literary computing the difficulties of a synthesis","Currently most literary critics reject the use of science and technology to gain information about texts, while most computer text-analysts have become absorbed in science and technology and forgotten they were seeking information about literature. Whether these two trends will continue into the 1990's remains to be seen; that they explain a good deal about the world we work in now can, I think, be demonstrated. This essay looks at the questions of what literary computing could offer to literary critics, why computer users get lost in scientific jargon, what happens when text becomes input and, most importantly, what happens when text becomes output; it closes with a discussion of why the synthesis will be so difficult.","Computers and the Humanities",1988,"No","key wordsliterari comput literari critic text input statist method textual analysi polem literari critic literari comput difficulti synthesi literari critic reject scienc technolog gain inform text comput text analyst absorb scienc technolog forgotten seek inform literatur trend continu remain explain good deal world work demonstr essay question literari comput offer literari critic comput user lost scientif jargon text input import text output close discuss synthesi difficult",0
"Key wordscritique empirical research literary computing reader response criticism textual databases women as readers women writers ","empirical literary research on women and readers","Both traditional and computerized scholars face problems when they attempt empirical research on women writers and women readers using currently available computational tools. This essay discusses some factors that have inhibited empirical research; it develops its examples from work in progress on 18th century English poetry and on reader responses. A number of large linguistic and text databases are almost useless for research on women writers because works by women are either not included or represented by easily accessible, rather than editorially clean, texts. Traditional and contemporary reader response studies are also insufficiently empirical for reasons of sexual bias or flaws in research design.","Computers and the Humanities",1994,"No","key wordscritiqu empir research literari comput reader respons critic textual databas women reader women writer empir literari research women reader tradit computer scholar face problem attempt empir research women writer women reader comput tool essay discuss factor inhibit empir research develop exampl work progress th centuri english poetri reader respons number larg linguist text databas useless research women writer work women includ repres easili access editori clean text tradit contemporari reader respons studi insuffici empir reason sexual bias flaw research design",0
"Key Wordslexis collocations lexical collocations statistics Xtract language generation machine translation ","xtract an overview","Lexical collocations have particular statistical distributions. We have developed a set of statistical techniques for retrieving and identifying collocations from large textual corpora. The techniques we developed are able to identify collocations of arbitrary length as well as flexible collocations. These techniques have been implemented in a lexicographic tool, Xtract, which is able to automatically acquire collocations with high retrieval performance. Xtract works in three stages. The first stage is based on a statistical technique for identifying word pairs involved in a syntactic relation. The words can appear in the text in any order and can be separated by an arbitrary number of other words. The second stage is based on a technique to extract n-word collocations (or n-grams) in a much simpler way than related methods. These collocations can involve closed class words such as particles and prepositions. A third stage is then applied to the output of stage one and applies parsing techniques to sentences involving a given word pair in order to identify the proper syntactic relation between the two words. A secondary effect of the third stage is to filter out a number of candidate collocations as irrelevant and thus produce higher quality output. In this paper we present an overview of Xtract and we describe several uses for Xtract and the knowledge it retrieves such as language generation and machine translation.","Computers and the Humanities",1992,"No","key wordslexi colloc lexic colloc statist xtract languag generat machin translat xtract overview lexic colloc statist distribut develop set statist techniqu retriev identifi colloc larg textual corpora techniqu develop identifi colloc arbitrari length flexibl colloc techniqu implement lexicograph tool xtract automat acquir colloc high retriev perform xtract work stage stage base statist techniqu identifi word pair involv syntact relat word text order separ arbitrari number word stage base techniqu extract word colloc gram simpler relat method colloc involv close class word particl preposit stage appli output stage appli pars techniqu sentenc involv word pair order identifi proper syntact relat word secondari effect stage filter number candid colloc irrelev produc higher qualiti output paper present overview xtract describ xtract knowledg retriev languag generat machin translat",0
"KeywordsComputational Linguistic ","reports",NA,"Computers and the Humanities",1976,"No","comput linguist report na",0
"Key Wordsintelligent computer-assisted instruction intelligent tutoring instructional systems, ITS computer-assisted language instruction natural language understanding stylisties ","an intelligent computer assistant for stylistic instruction","This article describes an intelligent computer-assisted language instruction system that is designed to teach principles of syntactic style to students of English. Unlike conventional style checkers, the system performs a complete syntactic analysis of its input, and takes the student's stylistic intent into account when providing a diagnosis. Named STASEL for Stylistic Treatment At the Sentence Level, the system is specifically developed for the teaching of style, and makes use of artificial intelligence techniques in natural language processing to analyze free-form input sentences interactively.","Computers and the Humanities",1992,"No","key wordsintellig comput assist instruct intellig tutor instruct system comput assist languag instruct natur languag understand stylisti intellig comput assist stylist instruct articl describ intellig comput assist languag instruct system design teach principl syntact style student english unlik convent style checker system perform complet syntact analysi input take student stylist intent account provid diagnosi name stasel stylist treatment sentenc level system specif develop teach style make artifici intellig techniqu natur languag process analyz free form input sentenc interact",0
"Key Wordstext intertextuality discourse interdiscursivity literary theory chaos theory conceptual convergence ","towards the implementation of text and discourse theory in computer assisted textual analysis","Humanities computing (HC) has failed to integrate into its practices many of the key theoretical elements of contemporary text and discourse theory. This has in turn contributed to the marginalization of HC in research and teaching. Outdated theoretical models must be abandoned in order to develop a critical discourse based on the insights of HC. HC projects remain far too attached to micro-analyses and have not developed the theoretical and methodological tools necessary to undertake systemic macro-analyses on the level of discourse. Given that texts are a mixture of determinate and dynamic systems, recent developments in chaos theory may be of help in modelling the interrelationship of these elements at discourse level.","Computers and the Humanities",1993,"No","key wordstext intertextu discours interdiscurs literari theori chao theori conceptu converg implement text discours theori comput assist textual analysi human comput hc fail integr practic key theoret element contemporari text discours theori turn contribut margin hc research teach outdat theoret model abandon order develop critic discours base insight hc hc project remain attach micro analys develop theoret methodolog tool undertak system macro analys level discours text mixtur determin dynam system recent develop chao theori model interrelationship element discours level",0
"Key wordsCommon Sense content analysis forcefulness simplicity stylistic analysis Thomas Paine ","the common style of common sense","The extraordinary impact of Thomas Paine's Common Sense has often been attributed to its style — to the simplicity and forcefulness with which Paine expressed ideas that many others before him had expressed. Comparative analysis of Common Sense and other pre-Revolutionary pamphlets suggests that Common Sense was indeed stylistically unique; no other pamphleteer came close to matching Paine's combination of simplicity and forcefulness.","Computers and the Humanities",1996,"No","key wordscommon sens content analysi forc simplic stylist analysi thoma pain common style common sens extraordinari impact thoma pain common sens attribut style simplic forc pain express idea express compar analysi common sens pre revolutionari pamphlet suggest common sens stylist uniqu pamphlet close match pain combin simplic forc",0
"KeywordsPattern Recognition Relative Weight Successful Implementation Computational Linguistic Error Criterion ","perceptual issues in music pattern recognition complexity of rhythm and key finding","We consider several perceptual issues in the context of machine recognition ofmusic patterns. It is argued that a successful implementation of a musicrecognition system must incorporate perceptual information and error criteria.We discuss several measures of rhythm complexity which are used fordetermining relative weights of pitch and rhythm errors. Then, a new methodfor determining a localized tonal context is proposed. This method is based onempirically derived key distances. The generated key assignments are then usedto construct the perceptual pitch error criterion which is based on noterelatedness ratings obtained from experiments with human listeners.","Computers and the Humanities",2001,"No","pattern recognit relat weight success implement comput linguist error criterion perceptu issu music pattern recognit complex rhythm key find perceptu issu context machin recognit ofmus pattern argu success implement musicrecognit system incorpor perceptu inform error criteria discuss measur rhythm complex fordetermin relat weight pitch rhythm error methodfor determin local tonal context propos method base onempir deriv key distanc generat key assign usedto construct perceptu pitch error criterion base noterelated rate obtain experi human listen",0
"Keywordsinternational cooperation language resources language technology language technology evaluation ","developing language technologies with the support of language resources and evaluation programs","The role of language resources and language technology evaluation is now recognized as being crucial for the development of written and spoken language processing systems. Given the increasing challenge of multilingualism in Europe, the development of language technologies requires a more internationally distributed effort. This paper first describes several recent and on-going activities in France aimed at the development of language resources and evaluation. We then outline a new project intended to enhance collaboration, cooperation, and resource sharing among the international language processing research community.","Language Resources and Evaluation",2005,"No","intern cooper languag resourc languag technolog languag technolog evalu develop languag technolog support languag resourc evalu program role languag resourc languag technolog evalu recogn crucial develop written spoken languag process system increas challeng multilingu europ develop languag technolog requir intern distribut effort paper describ recent activ franc aim develop languag resourc evalu outlin project intend enhanc collabor cooper resourc share intern languag process research communiti",0
"Key Wordshistory historical editions editions survey CDROM editions NLCindex indexing Text Encoding Initiative ","historical editions in the states","A late 1990 survey found that most historical editors in the United States continue to use the computer primarily as a word processing tool to prepare texts and editorial apparatus. Among older projects, a migration from mainframe or mini-computers to PCs has been the norm. New developments in the field include the “Founding Fathers” CD-ROM project, the impending release of Version 2.0 of NLCindex, and a strong interest in the Text Encoding Initiative.","Computers and the Humanities",1991,"No","key wordshistori histor edit edit survey cdrom edit nlcindex index text encod initi histor edit state late survey found histor editor unit state continu comput primarili word process tool prepar text editori apparatus older project migrat mainfram mini comput pcs norm develop field includ found father cd rom project impend releas version nlcindex strong interest text encod initi",0
"Key wordsBible English Gospels Greek Hebrew information retrieval latent semantic indexing singular value decomposition ","using latent semantic indexing for multilanguage information retrieval","In this paper, a method for indexing cross-language databases for conceptual query matching is presented. Two languages (Greek and English) are combined by appending a small portion of documents from one language to the identical documents in the other language. The proposed merging strategy duplicates less than 7% of the entire database (made up of different translations of the Gospels). Previous strategies duplicated up to 34% of the initial database in order to perform the merger. The proposed method retrieves a larger number of relevant documents for both languages with higher cosine rankings when Latent Semantic Indexing (LSI) is employed. Using the proposed merge strategies, LSI is shown to be effective in retrieving documents from either language (Greek or English) without requiring any translation of a user's query. An effective Bible search product needs to allow the use of natural language for searching (queries). LSI enables the user to form queries with using natural expressions in the user's own native language. The merging strategy proposed in this study enables LSI to retrieve relevant documents effectively using a minimum of the database in a foreign language.","Computers and the Humanities",1995,"No","key wordsbibl english gospel greek hebrew inform retriev latent semant index singular decomposit latent semant index multilanguag inform retriev paper method index cross languag databas conceptu queri match present languag greek english combin append small portion document languag ident document languag propos merg strategi duplic entir databas made translat gospel previous strategi duplic initi databas order perform merger propos method retriev larger number relev document languag higher cosin rank latent semant index lsi employ propos merg strategi lsi shown effect retriev document languag greek english requir translat user queri effect bibl search product natur languag search queri lsi enabl user form queri natur express user nativ languag merg strategi propos studi enabl lsi retriev relev document effect minimum databas foreign languag",0
"KeywordsComputational Linguistic ","paperless writing revisited",NA,"Computers and the Humanities",1990,"No","comput linguist paperless write revisit na",0
"Key Wordssyntax parsing statistical stylistics "," eyeball an interactive system for producing stylistic descriptions and comparisons","We have revised the mainframe EYEBALL, written in the early 1970s and used by several researchers during that decade, to run on microcomputers. This program parses English language texts and provides a statistical description of many linguistic features which are of interest to those involved in stylistics. In order to produce an analytical approach which goes beyond the simple reporting of linguistic data, we have selected a small number of features which characterize noticeable elements of the language used and we have written programs to facilitate statistical comparisons among texts. While EYEBALL is not automatic, the interactive parsing routines are elegant enough that the users spend most of their time confirming accurate guesses, rather than having to determine the structure of each phrase and clause. We hope that the program will be used by humanities scholars who have not previously been inclined to wrestle with the barriers of mainframe computing in the past.","Computers and the Humanities",1994,"No","key wordssyntax pars statist stylist eyebal interact system produc stylist descript comparison revis mainfram eyebal written earli s research decad run microcomput program pars english languag text statist descript linguist featur interest involv stylist order produc analyt approach simpl report linguist data select small number featur character notic element languag written program facilit statist comparison text eyebal automat interact pars routin eleg user spend time confirm accur guess determin structur phrase claus hope program human scholar previous inclin wrestl barrier mainfram comput past",0
"KeywordsInstructional Unit Essay Question Domestic Program American Historical Association Essay Test ","the individualized history survey course and the computer",NA,"Computers and the Humanities",1984,"No","instruct unit essay question domest program american histor associ essay test individu histori survey comput na",0
"KeywordsData Analysis Critical Thought Application Software Computational Linguistic Social Data ","wrapping up the package critical thoughts on applications software for social data analysis",NA,"Computers and the Humanities",1972,"No","data analysi critic thought applic softwar comput linguist social data wrap packag critic thought applic softwar social data analysi na",0
"Key WordsBach Database musical databases DARMS (Digital Alternate Representation of Musical Scores) ESAC (Essen Associative Code) IML-MIR (Intermediary Musical Language-Music Information Retrieved) IRCAM language models (for musical analysis) MIDI (Musical Instrument Digital Interface) MIPS (Musical Information Processing Standards) musical data musical information RISM SCORE musical analysis music printing ","computing in musicology 196691","While there are many parallels between computing activities in musicology and those in other humanities disciplines, the particular nature of musical material and the ways in which this must be accommodated set many activities apart from those in text-based disciplines. As in other disciplines, early applications were beset by hardware constraints, which placed a premium on expertise and promoted design-intensive projects. Massive musical encoding and bibliographical projects were initiated. Diversification of hardware platforms and languages in the Seventies led to task-specific undertakings, including preliminary work on many of today's programs for music printing and analysis. The rise of personal computers and associated general-purpose software in the Eighties has enabled many scholars to pursue projects individually, particularly with the assistance of database, word processing, and notation software. Current issues facing the field include the need for standards for data interchange, the creation of banks of reusable data, the establishment of qualitative standards for encoded data, and the encouragement of realistic appraisals of what computers can do.","Computers and the Humanities",1991,"No","key wordsbach databas music databas darm digit altern represent music score esac essen associ code iml mir intermediari music languag music inform retriev ircam languag model music analysi midi music instrument digit interfac mip music inform process standard music data music inform rism score music analysi music print comput musicolog parallel comput activ musicolog human disciplin natur music materi way accommod set activ text base disciplin disciplin earli applic beset hardwar constraint premium expertis promot design intens project massiv music encod bibliograph project initi diversif hardwar platform languag seventi led task specif undertak includ preliminari work today program music print analysi rise person comput general purpos softwar eighti enabl scholar pursu project individu assist databas word process notat softwar current issu face field includ standard data interchang creation bank reusabl data establish qualit standard encod data encourag realist apprais comput",0
"Key Wordslexicography philosophy seventeenth century eighteenth century Latin language concordances lexicons Lessico Intellettuale Europe ","philosophical lexicography the lie and the use of the computer","This manuscript is a description of the research activities of the Lessico Intellettuale Europeo. The Centre works on the lexicographical analysis of philosophical and scientific texts, mainly of the seventeenth and eighteenth centuries. An important project is the Philosophical Dictionary of the Seventeenth and Eighteenth Centuries. The LIE series of publications include a number of indices, concordances and lexicons. Another project at the Centre is the Thesaurus Mediae et Recentioris Latinitatis. The Centre also publishes the proceedings of the three-yearly Colloqui internazionali.","Computers and the Humanities",1990,"No","key wordslexicographi philosophi seventeenth centuri eighteenth centuri latin languag concord lexicon lessico intellettual europ philosoph lexicographi lie comput manuscript descript research activ lessico intellettual europeo centr work lexicograph analysi philosoph scientif text seventeenth eighteenth centuri import project philosoph dictionari seventeenth eighteenth centuri lie seri public includ number indic concord lexicon project centr thesaurus media recentiori latinitati centr publish proceed year colloqui internazionali",0
"SGML women's writing document type definition (DTD) design content tagging ","sgml and the orland project descriptive markup for an electronic ghistory of womens writing","This paper describes the novel ways in which the Orlando Project, based at the Universities of Alberta and Guelph, is using SGML to create an integrated electronic history of British women's writing in English. Unlike most other SGML-based humanities computing projects which are tagging existing texts, we are researching and writing new material, including biographies, items of historical significance, and many kinds of literary and historical interpretation, all of which incorporates sophisticated SGML encoding for content as well as structure. We have created three DTDs, for biographies, for writing-related activities and publications, and for social, political and other events. A major factor influencing the design of the DTDs was the requirement to be able to merge and restructure the entire text base in many ways in order to retrieve and index it and to reflect multiple views and interpretations. In addition a stable and well-documented system for tagging was deemed essential for a team which involves almost twenty people, including eight graduate students, in two locations.","Computers and the Humanities",1997,"No","sgml women write document type definit dtd design content tag sgml orland project descript markup electron ghistori women write paper describ way orlando project base univers alberta guelph sgml creat integr electron histori british women write english unlik sgml base human comput project tag exist text research write materi includ biographi item histor signific kind literari histor interpret incorpor sophist sgml encod content structur creat dtds biographi write relat activ public social polit event major factor influenc design dtds requir merg restructur entir text base way order retriev index reflect multipl view interpret addit stabl document system tag deem essenti team involv twenti peopl includ graduat student locat",0
"Key Wordsanthropology social interactions kinship relations ","automatic rule discovery for field work in anthropology","This paper deals with the problem of discovering rules that govern social interactions and relations in preliteral societies. Two older computer programs are first described which can receive data, possibly incomplete and redundant, representing kinship relations among named individuals. The programs then establish a knowledge base in the form of a directed graph, which the user can query in a variety of ways. Another program, written on the “top” of these (rewritten in LISP), can form concepts of various properties, including kinship relations, of and between the individuals. The concepts are derived from the examples and non-examples of a certain social pattern, such as inheritance, succession, marriage, class (tribe, moiety, clan, etc.) membership, domination-subordination, incest and exogamy. The concepts become hypotheses about the rules, which are corroborated, modified or rejected by further examples and non-examples.","Computers and the Humanities",1992,"No","key wordsanthropolog social interact kinship relat automat rule discoveri field work anthropolog paper deal problem discov rule govern social interact relat preliter societi older comput program receiv data possibl incomplet redund repres kinship relat name individu program establish knowledg base form direct graph user queri varieti way program written top rewritten lisp form concept properti includ kinship relat individu concept deriv exampl exampl social pattern inherit success marriag class tribe moieti clan membership domin subordin incest exogami concept hypothes rule corrobor modifi reject exampl exampl",0
NA,"annual bibliography for 1967",NA,"Computers and the Humanities",1968,"No","annual bibliographi na",0
"Key WordsMusical database quality control melodic progression Gregorian Chant German folksong ","on the accuracy of musical data with examples from gregorian chant and german folksong","Attention is drawn to the need for controlling (during encoding) and checking (after encoding) the quality or accuracy of musical data. Some large databases of melodies are now becoming available, and methods of control and checking are presented which are specially suited to these. Two applications are discussed in detail: to Gregorian Chant and to German folksong. An effective method in tonal and modal music is found to be the investigation of melodic progressions which remain unusual even after amalgamation by transposition to a central register.","Computers and the Humanities",1993,"No","key wordsmus databas qualiti control melod progress gregorian chant german folksong accuraci music data exampl gregorian chant german folksong attent drawn control encod check encod qualiti accuraci music data larg databas melodi method control check present special suit applic discuss detail gregorian chant german folksong effect method tonal modal music found investig melod progress remain unusu amalgam transposit central regist",0
"Key Wordslaw legal reasoning logic in law legal informatics expert systems ","computers and legal reasoning developments in germany","The importance of “reasoning” in law is pointed out. Law and jurisprudence belong to the “reasoning-conscious” disciplines. Accordingly, there is a long tradition of logic in law. The specific methods of professional work in law are to be seen in close connection with legal reasoning. The advent of computers at first did not touch upon legal reasoning (or the professional work in law). At first computers could be used only for general auxiliary functions (e.g., numerical calculations in tax law). Gradually, the use of computers for auxiliary functions in law has become more specific and more sophisticated (e.g., legal information retrieval), touching more closely upon professional legal work. Moreover, renewed interest in AI has also fostered interest in AI in law, especially for legal expert systems. AI techniques can be used in support of legal reasoning. Yet until now legal expert systems have remained in the research and development stage and have hardly succeeded in becoming a profitable tool for the profession. Therefore it is hoped that the two lines of computer support, for auxiliary functions in law and for immediate support of legal reasoning, may unite in the future.","Computers and the Humanities",1991,"No","key wordslaw legal reason logic law legal informat expert system comput legal reason develop germani import reason law point law jurisprud belong reason conscious disciplin long tradit logic law specif method profession work law close connect legal reason advent comput touch legal reason profession work law comput general auxiliari function numer calcul tax law gradual comput auxiliari function law specif sophist legal inform retriev touch close profession legal work renew interest ai foster interest ai law legal expert system ai techniqu support legal reason legal expert system remain research develop stage succeed profit tool profess hope line comput support auxiliari function law support legal reason unit futur",0
"KeywordsComputational Linguistic Recent Scholarship Linguistic Study ","recent scholarship in literary and linguistic studies",NA,"Computers and the Humanities",1972,"No","comput linguist recent scholarship linguist studi recent scholarship literari linguist studi na",0
"KeywordsComputational Linguistic ","numbers and history the dilemma of measurement",NA,"Computers and the Humanities",1969,"No","comput linguist number histori dilemma measur na",0
"Key wordsdatabases thematics Gide faits divers ","andr gides collection offaits divers","André Gide's collection offaits divers spans a period of fifty years and includes more than 650 documents. A database analysis offers an efficient means of handling such extensive material. Our goal is to trace statistically this author's varied interests as they evolved throughout his career.","Computers and the Humanities",1994,"No","key wordsdatabas themat gide fait diver andr gide collect offait diver andr gide collect offait diver span period fifti year includ document databas analysi offer effici mean handl extens materi goal trace statist author vari interest evolv career",0
"KeywordsDiscriminant Function Sample Standard Deviation Personal Pronoun Estimate Regression Equation Henry Versus ","pronouns and genre in shakespeares drama","The Tempest","Computers and the Humanities",1979,"No","discrimin function sampl standard deviat person pronoun estim regress equat henri versus pronoun genr shakespear drama tempest",0
"KeywordsComputational Linguistic Computerize Research ","clio and computers a survey of computerized research in history",NA,"Computers and the Humanities",1970,"No","comput linguist computer research clio comput survey computer research histori na",0
"Key WordsCAI in literature LAN networks pseudonyms gender introvert extrovert creativity minorities reader response collaborative writing and exams ","radical changes in class discussion using networked computers","This study examines the effects of conducting class discussion on a local area network. A real time networking program (INTERCHANGE) was used for class discussion in freshman and senior literature courses and in a graduate humanities computing class. Pseudonyms, collaborative exams and essays, and computer-assisted reading were tested, along with organization of the students by sex and personality type. At the beginning and end of each semester in each class students were asked 50 to 70 multiple choice questions. Their answers revealed that the many advantages of computer assisted class discussion (CACD) clearly outweigh the disadvantages.","Computers and the Humanities",1990,"No","key wordscai literatur lan network pseudonym gender introvert extrovert creativ minor reader respons collabor write exam radic class discuss network comput studi examin effect conduct class discuss local area network real time network program interchang class discuss freshman senior literatur cours graduat human comput class pseudonym collabor exam essay comput assist read test organ student sex person type begin end semest class student ask multipl choic question answer reveal advantag comput assist class discuss cacd outweigh disadvantag",0
"KeywordsComputational Linguistic ","a survey of computer aided research in early german",NA,"Computers and the Humanities",1974,"No","comput linguist survey comput aid research earli german na",0
"KeywordsComputational Linguistic ","clio and computers moving into phase ii 19701972",NA,"Computers and the Humanities",1972,"No","comput linguist clio comput move phase ii na",0
"Key wordsinterpretive theory meaning computer-assisted research stylistics style cognitive theory private language feminist language theory ","have it your way and mine the theory of styles","Olsen is right to note what can be done with a good theory and the right machine. His particular theory, however, is not transferable to literary studies. If we need a new model, I would suggest that cognitive science can provide a few interesting ones. I have begun to do some work based on David Marr's VISION, in which he hypothesizes two levels of processing within the visual module. My speculation has been on the parallel existence of distinguishable levels of conceptual or language organization which would correspond to the viewer and object centered perspectives Marr describes for vision. I propose to explore the possibility that we may find here the model for the existence of stylistic individualism within overarching historical stylistic generalizations, and even more, that this may be what feminists are searching for when they try to resist being coopted by the masculine language of objectivity.","Computers and the Humanities",1993,"No","key wordsinterpret theori mean comput assist research stylist style cognit theori privat languag feminist languag theori mine theori style olsen note good theori machin theori transfer literari studi model suggest cognit scienc provid interest begun work base david marr vision hypothes level process visual modul specul parallel exist distinguish level conceptu languag organ correspond viewer object center perspect marr describ vision propos explor possibl find model exist stylist individu overarch histor stylist general feminist search resist coopt masculin languag object",0
"Key Wordscomputational stylistics computer analysis of text dominance G. B. Shaw literary criticism literary output rhetoric statistical analysis syntactic features ","from literary output to literary criticism discovering shaws rhetoric","Research on changes in Shaw's rhetoric inMrs. Warren's Profession, Major Barbara, andHeartbreak House led me to a heuristic for gaining literary critical control over computer output. This essay describes the eleven-step process: stepping away from the data, stating first premises, developing a working hypothesis, classifying computer-sorted data, marking implicit literary sub-structures, collecting sub-structural data into tables, applying earlier statistical observations, choosing parts for detailed analysis, designing a visual method for representing the analysis, presenting segment by segment analysis of the selected data, and making larger descriptive generalizations. While describing this heuristic, the essay also reports on the Shaw research.","Computers and the Humanities",1989,"No","key wordscomput stylist comput analysi text domin shaw literari critic literari output rhetor statist analysi syntact featur literari output literari critic discov shaw rhetor research shaw rhetor inmr warren profess major barbara andheartbreak hous led heurist gain literari critic control comput output essay describ eleven step process step data state premis develop work hypothesi classifi comput sort data mark implicit literari structur collect structur data tabl appli earlier statist observ choos part detail analysi design visual method repres analysi present segment segment analysi select data make larger descript general describ heurist essay report shaw research",0
"KeywordsComputational Linguistic Quantitative Research French Study ","recent quantitative research in french studies",NA,"Computers and the Humanities",1973,"No","comput linguist quantit research french studi recent quantit research french studi na",0
NA,"annual bibliography for 1976 and supplement to preceding years",NA,"Computers and the Humanities",1977,"No","annual bibliographi supplement preced year na",0
"Key Wordsborrowing compounding conversion date derivation etymology GOEDEL loanword OED PAT ","report on a new oed project a study of the history of new words in the new oed","The study of the history of new words in theNewOED described in this paper was undertaken in 1986-87, and is based on the material then available. Since then, theNewOED has been finished, and PAT, the inquiry system developed at the University of Waterloo for the investigation of theNewOED data base, has been much altered and improved. Nevertheless, this report should prove useful in indicating the potentiality for analyzing the computerizedNewOED and some of the problems. This project is a study of the ways in which new words are created in English at various periods of time. A chronological dictionary 's created listing words introduced into the language over 50 year increments. These words are then classified by the processes used in forming them to show, in proportional terms, if certain processes are more common at some times than at others.","Computers and the Humanities",1989,"No","key wordsborrow compound convers date deriv etymolog goedel loanword o pat report o project studi histori word o studi histori word thenewo paper undertaken base materi thenewo finish pat inquiri system develop univers waterloo investig thenewo data base alter improv report prove indic potenti analyz computerizednewo problem project studi way word creat english period time chronolog dictionari creat list word introduc languag year increment word classifi process form show proport term process common time",0
"clustering semantic acquisition noun phrase extraction ","elementary dependency trees for identifying corpus specific semantic classes","Elementary dependency relationships between words within parse trees produced by robust analyzers on a corpus help automate the discovery of semantic classes relevant for the underlying domain. We introduce two methods for extracting elementary syntactic dependencies from normalized parse trees. The groupings which are obtained help identify coarse-grain semantic categories and isolate lexical idiosyncrasies belonging to a specific sublanguage. A comparison shows a satisfactory overlapping with an existing nomenclature for medical language processing. This symbolic approach is efficient on medium size corpora which resist to statistical clustering methods but seems more appropriate for specialized texts.","Computers and the Humanities",1999,"No","cluster semant acquisit noun phrase extract elementari depend tree identifi corpus specif semant class elementari depend relationship word pars tree produc robust analyz corpus autom discoveri semant class relev under domain introduc method extract elementari syntact depend normal pars tree group obtain identifi coars grain semant categori isol lexic idiosyncrasi belong specif sublanguag comparison show satisfactori overlap exist nomenclatur medic languag process symbol approach effici medium size corpora resist statist cluster method special text",0
"KeywordsArtificial Intelligence Computational Model Computational Linguistic Narrative Theory ","narrative theories as computational models reader oriented theory and artificial intelligence",NA,"Computers and the Humanities",1983,"No","artifici intellig comput model comput linguist narrat theori narrat theori comput model reader orient theori artifici intellig na",0
"KeywordsComputational Linguistic ","dialogue and interpretation at the interface of man and machine reflections on textuality and a proposal for an experiment in machine reading",NA,"Computers and the Humanities",2002,"No","comput linguist dialogu interpret interfac man machin reflect textual propos experi machin read na",0
"KeywordsComputational Linguistic ","technology and humanistic values",NA,"Computers and the Humanities",1969,"No","comput linguist technolog humanist valu na",0
"KeywordsComputational Linguistic Spanish Language ","procedures and progress on the dictionary of the old spanish language",NA,"Computers and the Humanities",1983,"No","comput linguist spanish languag procedur progress dictionari spanish languag na",0
"KeywordsQuantitative Study Computational Linguistic American Revolution ","quantitative studies and the american revolution",NA,"Computers and the Humanities",1976,"No","quantit studi comput linguist american revolut quantit studi american revolut na",0
"AEDI AI-Strata indexing modeling scholarly use of audio-visual documents standardization ","managing full indexed audiovisual documents a new perspective for the humanities","The digitization of library documents and archives increasingly extends to audiovisual (AV) document repositories. As a consequence, new computer-aided techniques are being devised, providing opportunities for new uses of AV documents. As scholars work mainly by reading, annotating, reusing, and producing documents they are directly concerned by these changes. The first part of this article describes AV document use in the humanities, as well as the current and future influence computers might have on evolving practices. After establishing that “full-indexing” (indexing of the content for random access to any segment of an AV document) is a necessary condition if scholars are to develop new practices in using AV material, we will focus on the specific problems raised by AV indexing as opposed to text indexing, followed by a discussion of related AV indexing projects as well as standardization issues. The third part will propose a representation model for the description of AV material (AI-Strata) and an exchange format of AV annotations (AEDI), based on a free segmentation approach. An example of annotation is also provided. The last part is devoted to a discussion regarding potential long-term influences of digital AV indexing techniques on scholarly uses of AV documents.","Computers and the Humanities",1999,"No","aedi ai strata index model scholar audio visual document standard manag full index audiovisu document perspect human digit librari document archiv increas extend audiovisu av document repositori consequ comput aid techniqu devis provid opportun av document scholar work read annot reus produc document direct concern part articl describ av document human current futur influenc comput evolv practic establish full index index content random access segment av document condit scholar develop practic av materi focus specif problem rais av index oppos text index discuss relat av index project standard issu part propos represent model descript av materi ai strata exchang format av annot aedi base free segment approach annot provid part devot discuss potenti long term influenc digit av index techniqu scholar av document",0
"KeywordsWord Frequency Computational Linguistic Function Word Function Word Frequency ","the use of function word frequencies as indicators of style",NA,"Computers and the Humanities",1975,"No","word frequenc comput linguist function word function word frequenc function word frequenc indic style na",0
NA,"courseware reviews",NA,"Computers and the Humanities",1990,"No","coursewar review na",0
"α-cover collocations convergence correlation interrupted bigram randomness ","automatic extraction of collocations from korean text","In this paper, we propose a statistical method to automaticallyextract collocations from Korean POS-tagged corpus. Since a large portion of language is represented by collocation patterns, the collocational knowledge provides a valuable resource for NLP applications. One difficulty of collocation extraction is that Korean has a partially free word order, which also appears in collocations. In this work, we exploit four statistics, ‘frequency’,‘randomness’, ‘convergence’, and ‘correlation' in order to take into account the flexible word order of Korean collocations. We separate meaningful bigrams using an evaluation function based on the four statistics and extend the bigrams to n-gram collocations using a fuzzy relation. Experiments show that this method works well for Korean collocations.","Computers and the Humanities",2001,"No","cover colloc converg correl interrupt bigram random automat extract colloc korean text paper propos statist method automaticallyextract colloc korean pos tag corpus larg portion languag repres colloc pattern colloc knowledg valuabl resourc nlp applic difficulti colloc extract korean partial free word order appear colloc work exploit statist frequenc random converg correl order account flexibl word order korean colloc separ meaning bigram evalu function base statist extend bigram gram colloc fuzzi relat experi show method work korean colloc",0
"KeywordsComputational Linguistic ","a survey of computer assisted research in modern german",NA,"Computers and the Humanities",1977,"No","comput linguist survey comput assist research modern german na",0
"KeywordsComputational Linguistic Computer Study English Poetry ","formulas and syntax in old english poetry a computer study",NA,"Computers and the Humanities",1971,"No","comput linguist comput studi english poetri formula syntax english poetri comput studi na",0
"database data mining data warehouse Defters historical analysis serial documents ","data mining and serial documents","This paper is concerned with the investigation of the relevance and suitability of the data mining approach to serial documents. Conceptually the paper is divided into three parts. The first part presents the salient features of data mining and its symbiotic relationship to data warehousing. In the second part of the paper, historical serial documents are introduced, and the Ottoman Tax Registers (Defters) are taken as a case study. Their conformance to the data mining approach is established in terms of structure, analysis and results. A high-level conceptual model for the Defters is also presented. The final part concludes with a brief consideration of the implication of data mining for historical research.","Computers and the Humanities",2001,"No","databas data mine data warehous defter histor analysi serial document data mine serial document paper concern investig relev suitabl data mine approach serial document conceptu paper divid part part present salient featur data mine symbiot relationship data wareh part paper histor serial document introduc ottoman tax regist defter case studi conform data mine approach establish term structur analysi result high level conceptu model defter present final part conclud consider implic data mine histor research",0
"KeywordsProfessional Development Development System Computational Linguistic Professional Development System ","microsoft basic professional development system",NA,"Computers and the Humanities",1992,"No","profession develop develop system comput linguist profession develop system microsoft basic profession develop system na",0
"Key Wordslarge text databases sex-role stereotyping gender text searching strategies ","a study of sex role stereotyping in the oxford english dictionary 2e","Using software (PAT and LECTOR) developed for the creation of the electronic Oxford English Dictionary, strategies applicable to other large data bases were developed to analyse systemic sex-role stereotyping. These are applied to the OED database as a whole and to sub-files of gender-related definition or quotation text. The corpus is also studied manually. Software-based strategies including text searches for collocations with gender-specific pronouns and possessive adjectives produce interesting results which are tested against information in previous research. Stereotypes are found most frequently in quotation text, to a lesser degree in definition text.","Computers and the Humanities",1992,"No","key wordslarg text databas sex role stereotyp gender text search strategi studi sex role stereotyp oxford english dictionari e softwar pat lector develop creation electron oxford english dictionari strategi applic larg data base develop analys system sex role stereotyp appli o databas file gender relat definit quotat text corpus studi manual softwar base strategi includ text search colloc gender specif pronoun possess adject produc interest result test inform previous research stereotyp found frequent quotat text lesser degre definit text",0
"semantic mark up corpus-based rule development for lexical acquisition SGML ","marking up in tatoe and exporting to sgml","This paper presents a method for developing limited-context grammar rules in order to mark up text automatically, by attaching specific text segments to a small number of well-defined and application-determined semantic categories. The Text Analysis Tool with Object Encoding (TATOE) was used in order to support the iterative process of developing a set of rules as well as for constructing and managing the lexical resources. The work reported here is part of a real-world application scenario: the automatic semantic mark up of German news messages, as provided by a German press agency, according to the SGML-based standard News Industry Text Format (NITF) to facilitate their further exchange. The implemented export mechanism of the semantic mark up into NITF is also described in the paper. ","Computers and the Humanities",1997,"No","semant mark corpus base rule develop lexic acquisit sgml mark tato export sgml paper present method develop limit context grammar rule order mark text automat attach specif text segment small number defin applic determin semant categori text analysi tool object encod tato order support iter process develop set rule construct manag lexic resourc work report part real world applic scenario automat semant mark german news messag provid german press agenc sgml base standard news industri text format nitf facilit exchang implement export mechan semant mark nitf paper",0
"Key Wordsstylistic analysis Spanish twentieth-century corpus samples ","stylistic analysis of a corpus of twentieth century spanish narrative","Statistical information on a substantial corpus of representative Spanish texts is needed in order to determine the significance of data about individual authors or texts by means of comparison. This study describes the organization and analysis of a 150,000-word corpus of 30 well-known twentieth-century Spanish authors. Tables show the computational results of analyses involving sentences, segments, quotations, and word length.","Computers and the Humanities",1990,"No","key wordsstylist analysi spanish twentieth centuri corpus sampl stylist analysi corpus twentieth centuri spanish narrat statist inform substanti corpus repres spanish text need order determin signific data individu author text mean comparison studi describ organ analysi word corpus twentieth centuri spanish author tabl show comput result analys involv sentenc segment quotat word length",0
"KeywordsTemporal information Temporal tagger Named entity recognition Named entity normalization TIMEX2 TIMEX3 ","multilingual and cross domain temporal tagging","Extraction and normalization of temporal expressions from documents are important steps towards deep text understanding and a prerequisite for many NLP tasks such as information extraction, question answering, and document summarization. There are different ways to express (the same) temporal information in documents. However, after identifying temporal expressions, they can be normalized according to some standard format. This allows the usage of temporal information in a term- and language-independent way. In this paper, we describe the challenges of temporal tagging in different domains, give an overview of existing annotated corpora, and survey existing approaches for temporal tagging. Finally, we present our publicly available temporal tagger HeidelTime, which is easily extensible to further languages due to its strict separation of source code and language resources like patterns and rules. We present a broad evaluation on multiple languages and domains on existing corpora as well as on a newly created corpus for a language/domain combination for which no annotated corpus has been available so far.","Language Resources and Evaluation",2013,"No","tempor inform tempor tagger name entiti recognit name entiti normal timex timex multilingu cross domain tempor tag extract normal tempor express document import step deep text understand prerequisit nlp task inform extract question answer document summar way express tempor inform document identifi tempor express normal standard format usag tempor inform term languag independ paper describ challeng tempor tag domain give overview exist annot corpora survey exist approach tempor tag final present public tempor tagger heideltim easili extens languag due strict separ sourc code languag resourc pattern rule present broad evalu multipl languag domain exist corpora newli creat corpus languagedomain combin annot corpus",0
"KeywordsCorpus linguistics German historical corpus Multi-layer architecture Multiple tokenizations Normalizations Open source ","ridges herbology designing a diachronic multi layer corpus","This paper introduces a multi-layer corpus architecture with multiple tokenizations using the open source historical, diachronic corpus of German called Register in Diachronic German Science. The corpus contains herbal texts printed between the fifteenth and nineteenth centuries and is concerned with the development of a German scientific register, independent of Latin. We will discuss difficulties of transcribing, normalizing and annotating historical texts and will thereby argue for the advantages of multiple layers and multiple tokenizations. A virtually infinite number of annotations can be added to the corpus, without the need for deciding between or discarding interpretations. Thus, this flexible architecture enables multiple normalizations and types of annotation and is open to a wide range of research questions in the humanities. We provide case studies concerning the exploitation of our different normalizations as well as structural, register-specific and linguistic annotations. The corpus architecture allows for its reuse as a resource for corpus-based research approaches.","Language Resources and Evaluation",2017,"No","corpus linguist german histor corpus multi layer architectur multipl token normal open sourc ridg herbolog design diachron multi layer corpus paper introduc multi layer corpus architectur multipl token open sourc histor diachron corpus german call regist diachron german scienc corpus herbal text print fifteenth nineteenth centuri concern develop german scientif regist independ latin discuss difficulti transcrib normal annot histor text argu advantag multipl layer multipl token virtual infinit number annot ad corpus decid discard interpret flexibl architectur enabl multipl normal type annot open wide rang research question human provid case studi exploit normal structur regist specif linguist annot corpus architectur reus resourc corpus base research approach",0
"KeywordsComputational Linguistic Common Methodology Humanity Computing ","introduction common methodologies in humanities computing and computational linguistics",NA,"Computers and the Humanities",1992,"No","comput linguist common methodolog human comput introduct common methodolog human comput comput linguist na",0
"KeywordsHistorical language resources Slovene language Text Encoding Initiative Non-standard language normalisation ","the imp historical slovene language resources","The paper describes the combined results of several projects which constitute a basic language resource infrastructure for printed historical Slovene. The IMP language resources consist of a digital library, an annotated corpus and a lexicon, which are interlinked and uniformly encoded following the Text Encoding Initiative Guidelines. The library holds about 650 units (mostly complete books) consisting of facsimiles with 45,000 pages as well as hand-corrected and structured transcriptions. The hand-annotated corpus has 300,000 tokens, where each word is tagged with its modernised word form, lemma, part-of-speech and, in cases of archaic words, its nearest contemporary equivalents. This information was extracted into the lexicon, which also covers an extended target-annotated corpus, resulting in 20,000 lemmas (of these 4,000 archaic) with 50,000 modern word forms and 70,000 attested forms. We have also developed a program to modernise, tag and lemmatise historical Slovene, and annotated the digital library with it, producing an automatically annotated corpus of 15 million words. To serve the humanities, the digital library and lexicon are available for reading and browsing on the web and the corpora via a concordancer. For language technology research and development the resources are available in source TEI XML under the Creative Commons Attribution licence. The paper presents the IMP resources, available from http://nl.ijs.si/imp/, the process of their compilation, encoding and dissemination, and concludes with directions for future research.","Language Resources and Evaluation",2015,"No","histor languag resourc sloven languag text encod initi standard languag normalis imp histor sloven languag resourc paper describ combin result project constitut basic languag resourc infrastructur print histor sloven imp languag resourc consist digit librari annot corpus lexicon interlink uniform encod text encod initi guidelin librari hold unit complet book consist facsimil page hand correct structur transcript hand annot corpus token word tag modernis word form lemma part speech case archaic word nearest contemporari equival inform extract lexicon cover extend target annot corpus result lemma archaic modern word form attest form develop program modernis tag lemmatis histor sloven annot digit librari produc automat annot corpus million word serv human digit librari lexicon read brows web corpora concordanc languag technolog research develop resourc sourc tei xml creativ common attribut licenc paper present imp resourc httpnlijssiimp process compil encod dissemin conclud direct futur research",0
"Ben Jonson idiolects Principal Components Analysis ","contrast and change in the idiolects of ben jonson characters","The paper presents the results of a series of Principal Components Analyses of the frequencies of very common words in the dialogue of characters in plays by Ben Jonson. The first Principal Component in the data, the most important axis of differentiation, proves in each case to be a spectrum from elaborate, authoritative pronouncements to a dialogue style of reaction and interchange. Reference to other quantitative studies, literary and otherwise, suggests that a version of this axis may often be among the most important in stylistic difference generally. In Jonson it has a chronological aspect -- there is a shift over his career from one end to the other -- and there is often significant change within the idiolects of his characters as well. Successive segments of Volpone and Mosca's parts (they are protagonist and antagonist of Volpone, perhaps Jonson's best-known comedy) change markedly along this axis, beginning far apart but coming by the end of the play to resemble each other very closely on this measure.","Computers and the Humanities",1999,"No","ben jonson idiolect princip compon analysi contrast chang idiolect ben jonson charact paper present result seri princip compon analys frequenc common word dialogu charact play ben jonson princip compon data import axi differenti prove case spectrum elabor authorit pronounc dialogu style reaction interchang refer quantit studi literari suggest version axi import stylist differ general jonson chronolog aspect shift career end signific chang idiolect charact success segment volpon mosca part protagonist antagonist volpon jonson comedi chang mark axi begin come end play resembl close measur",0
"Key wordspiano music MIDI computer music etudes piano technique Liszt ","virtuoso pianism from the qwerty keyboard the electronic realization of liszts scores","For a decade or so, Liszt thrilled and astounded audiences at a time when virtuosity (often as an end in itself) was the norm and the piano had rapidly evolved into a form recognisable as a close relative of the instrument we know today. During this period Liszt frequently performed hisGrandes Etudes (1838), which he had developed from his boyhoodEtude en 12 exercices (1826) and which he later revised and technically simplified asEtudes d'Exécution transcendante (1851). Although Liszt's own performances cannot be recreated, procedures for generating electronic realizations, which contain nuances of balance and tempo, are described. All three versions of the eighth of Liszt's set of 12 studies are used for illustration. Contrary to received opinion, it is argued that the 1838 version is more satisfying than the 1851 revision and that this is due to its formal structure.","Computers and the Humanities",1995,"No","key wordspiano music midi comput music etud piano techniqu liszt virtuoso pianism qwerti keyboard electron realiz liszt score decad liszt thrill astound audienc time virtuos end norm piano rapid evolv form recognis close relat instrument today period liszt frequent perform hisgrand etud develop boyhoodetud en exercic revis technic simplifi asetud cution transcendant liszt perform recreat procedur generat electron realiz nuanc balanc tempo version eighth liszt set studi illustr contrari receiv opinion argu version satisfi revis due formal structur",0
"KeywordsComputational Linguistic Sound Effect ","most by numbers judge a poets song measuring sound effects in poetry",NA,"Computers and the Humanities",1985,"No","comput linguist sound effect number judg poet song measur sound effect poetri na",0
"KeywordsComputational Linguistic Resulting Ranking Cluster Property Prose Initial Sound ","on the measurement of alliteration in poetry","This work is a preliminary study of methods to quantify alliteration. Ten pieces made up of poetry and prose (literary and non-literary) were used to create test sets. Three forms of each test set were examined: texts transcribed in IPA notational equivalent, in Chomsky and Halle features, and in Fromkin and Rodman features. Tests included the deletion of vowels, the weighting of the initial sounds, and the weighting of types according to their frequency in the population of the set. The various configurations were analyzed using a gap-recurrence method. Rankings were obtained by combining measures both of high frequency and of clustering properties. The resulting rankings compare not unfavorably with an intuitive ranking.","Computers and the Humanities",1976,"No","comput linguist result rank cluster properti prose initi sound measur alliter poetri work preliminari studi method quantifi alliter ten piec made poetri prose literari literari creat test set form test set examin text transcrib ipa notat equival chomski hall featur fromkin rodman featur test includ delet vowel weight initi sound weight type frequenc popul set configur analyz gap recurr method rank obtain combin measur high frequenc cluster properti result rank compar unfavor intuit rank",0
"KeywordsPreliminary Report Computational Linguistic Mutual Relationship Great Relevance Good World ","isaiah and the computer a preliminary report","In the eighth century B.C.E. there lived in Jerusalem a prophet of royal descent whose name was Isaiah ben Amoż. His oracles, mostly in poetry, are concerned with the contemporary political scene dominated by the Assyrians, with the fate of the Jewish people in history in general, with the mutual relationship between God and Man, and with the ultimate fate of Mankind in the fullness of time. For Jews, these oracles are perhaps second in importance only to the Pentateuch. For Christians, they are of greatest relevance because no other book influenced Jesus to the same extent. And for humanity they are so much the expression of an idealistic Weltanschauungthat they are quoted wherever educators and statesmen try to imbue their audiences with the vision of, and the hope for, a better world. They are collected in sixty-six chapters, constituting the first and longest book of the “Latter Prophets.”","Computers and the Humanities",1970,"No","preliminari report comput linguist mutual relationship great relev good world isaiah comput preliminari report eighth centuri live jerusalem prophet royal descent isaiah ben amo oracl poetri concern contemporari polit scene domin assyrian fate jewish peopl histori general mutual relationship god man ultim fate mankind full time jew oracl import pentateuch christian greatest relev book influenc jesus extent human express idealist weltanschauungthat quot educ statesmen imbu audienc vision hope world collect sixti chapter constitut longest book prophet",0
NA,"editorial computers in humanities teaching and research",NA,"Computers and the Humanities",2000,"No","editori comput human teach research na",0
"KeywordsComputational Linguistic Sound Change ","iberochange a program to simulate systematic sound change in ibero romance",NA,"Computers and the Humanities",1977,"No","comput linguist sound chang iberochang program simul systemat sound chang ibero romanc na",0
"Key Wordsdictionary changing language literary criticism PAT system OED computer research formalist deviation statistics ","literary texts and the state of the language the role of the computer","We should follow Mark Olsen's lead and think with maximum ambition of the role of the computer in supporting literary research of the highest order. Thus the computer enables us to answer one of the great questions of literary criticism: how does a given writer contribute to the changing language? We can now chart the influence of given writers by correlating their words and phrasing with computerized dictionaries so as to produce profiles and histories of the way words have entered the language.","Computers and the Humanities",1993,"No","key wordsdictionari chang languag literari critic pat system o comput research formalist deviat statist literari text state languag role comput follow mark olsen lead maximum ambit role comput support literari research highest order comput enabl answer great question literari critic writer contribut chang languag chart influenc writer correl word phrase computer dictionari produc profil histori word enter languag",0
"KeywordsComputational Linguistic ","the study of chaucers vocabulary",NA,"Computers and the Humanities",1978,"No","comput linguist studi chaucer vocabulari na",0
"KeywordsLexical normalization Twitter Social media Corpus Evaluation ","tweetnorm a benchmark for lexical normalization of spanish tweets","
The language used in social media is often characterized by the abundance of informal and non-standard writing. The normalization of this non-standard language can be crucial to facilitate the subsequent textual processing and to consequently help boost the performance of natural language processing tools applied to social media text. In this paper we present a benchmark for lexical normalization of social media posts, specifically for tweets in Spanish language. We describe the tweet normalization challenge we organized recently, analyze the performance achieved by the different systems submitted to the challenge, and delve into the characteristics of systems to identify the features that were useful. The organization of this challenge has led to the production of a benchmark for lexical normalization of social media, including an evaluation framework, as well as an annotated corpus of Spanish tweets—TweetNorm_es—, which we make publicly available. The creation of this benchmark and the evaluation has brought to light the types of words that submitted systems did best with, and posits the main shortcomings to be addressed in future work.","Language Resources and Evaluation",2015,"No","lexic normal twitter social media corpus evalu tweetnorm benchmark lexic normal spanish tweet languag social media character abund inform standard write normal standard languag crucial facilit subsequ textual process boost perform natur languag process tool appli social media text paper present benchmark lexic normal social media post specif tweet spanish languag describ tweet normal challeng organ recent analyz perform achiev system submit challeng delv characterist system identifi featur organ challeng led product benchmark lexic normal social media includ evalu framework annot corpus spanish tweet tweetnorm make public creation benchmark evalu brought light type word submit system posit main shortcom address futur work",0
"KeywordsEmotional ECA synthesis Expressivity features Facial features extraction Gesture analysis Virtual agents ","virtual agent multimodal mimicry of humans","This work is about multimodal and expressive synthesis on virtual agents, based on the analysis of actions performed by human users. As input we consider the image sequence of the recorded human behavior. Computer vision and image processing techniques are incorporated in order to detect cues needed for expressivity features extraction. The multimodality of the approach lies in the fact that both facial and gestural aspects of the user’s behavior are analyzed and processed. The mimicry consists of perception, interpretation, planning and animation of the expressions shown by the human, resulting not in an exact duplicate rather than an expressive model of the user’s original behavior.","Language Resources and Evaluation",2007,"No","emot eca synthesi express featur facial featur extract gestur analysi virtual agent virtual agent multimod mimicri human work multimod express synthesi virtual agent base analysi action perform human user input imag sequenc record human behavior comput vision imag process techniqu incorpor order detect cue need express featur extract multimod approach lie fact facial gestur aspect user behavior analyz process mimicri consist percept interpret plan anim express shown human result exact duplic express model user origin behavior",0
"KeywordsELRA Anthology Language resources Language processing systems evaluation Text analytics Social networks ISLRN Bibliometrics Scientometrics ","rediscovering 152years of discoveries in language resources and evaluation","This paper analyzes the content of the proceedings of the Language Resources and Evaluation Conference (LREC) over the past 17 years (1998–2014), with the goal of gaining a picture of the LREC community and the topics that are most relevant to the field. We follow the methodology used in similar studies, including the survey of the IEEE ICASSP conference proceedings from 1976 to 1990, the survey of the Association of Computational Linguistics conference proceedings over 50 years, and the survey of the proceedings of the conferences contained in the ISCA Archive over 25 years (1987–2012). We expand on results originally presented at LREC 2014, but include the proceedings of LREC 2014 itself in the study together with an analysis of various citation graphs. We show the evolution over time of the number of papers and authors, including their distribution by gender and affiliation, as well as collaborations and citation patterns among authors and papers, funding sources for reported research, and plagiarism and reuse in LREC papers; results for LREC are compared with similar results for major conferences in related fields. We also consider the evolution of research topics over time and identify the authors who introduced key terms. Finally, we propose and apply a measure of a researcher’s notability and provide the results for LREC authors. The study uses NLP methods that have been published in the corpus considered in the study. In addition to providing a revealing characterization of the LRE community, the study also demonstrates the need for establishing a system for unique identification of authors, papers and other sources to facilitate this type of analysis.","Language Resources and Evaluation",2016,"No","elra antholog languag resourc languag process system evalu text analyt social network islrn bibliometr scientometr rediscov year discoveri languag resourc evalu paper analyz content proceed languag resourc evalu confer lrec past year goal gain pictur lrec communiti topic relev field follow methodolog similar studi includ survey ieee icassp confer proceed survey associ comput linguist confer proceed year survey proceed confer contain isca archiv year expand result origin present lrec includ proceed lrec studi analysi citat graph show evolut time number paper author includ distribut gender affili collabor citat pattern author paper fund sourc report research plagiar reus lrec paper result lrec compar similar result major confer relat field evolut research topic time identifi author introduc key term final propos appli measur research notabl provid result lrec author studi nlp method publish corpus consid studi addit provid reveal character lre communiti studi demonstr establish system uniqu identif author paper sourc facilit type analysi",0
"KeywordsPlagiarism Plagiarism detection Corpus creation Language resources ","developing a corpus of plagiarised short answers","Plagiarism is widely acknowledged to be a significant and increasing problem for higher education institutions (McCabe 2005; Judge 2008). A wide range of solutions, including several commercial systems, have been proposed to assist the educator in the task of identifying plagiarised work, or even to detect them automatically. Direct comparison of these systems is made difficult by the problems in obtaining genuine examples of plagiarised student work. We describe our initial experiences with constructing a corpus consisting of answers to short questions in which plagiarism has been simulated. This corpus is designed to represent types of plagiarism that are not included in existing corpora and will be a useful addition to the set of resources available for the evaluation of plagiarism detection systems.","Language Resources and Evaluation",2011,"No","plagiar plagiar detect corpus creation languag resourc develop corpus plagiaris short answer plagiar wide acknowledg signific increas problem higher educ institut mccabe judg wide rang solut includ commerci system propos assist educ task identifi plagiaris work detect automat direct comparison system made difficult problem obtain genuin exampl plagiaris student work describ initi experi construct corpus consist answer short question plagiar simul corpus design repres type plagiar includ exist corpora addit set resourc evalu plagiar detect system",0
"KeywordsUser generated content Slovene language Corpora Manually annotated datasets Text normalisation ","the janes project language resources and tools for slovene user generated content","The paper presents the results of the Janes project, which aimed to develop language resources and tools for Slovene user generated content. The paper first describes the 200 million word Janes corpus, containing tweets, forum posts, news comments, user and talk pages from Wikipedia, and blogs and blog comments, where each text is accompanied by rich metadata. The developed processing tools for Slovene user generated content are presented next, which include a tokeniser, word-normaliser, part-of-speech tagger and lemmatiser, and a named entity recogniser. A set of manually annotated datasets was also produced, both for tool training as well as for linguistic research.
 The developed resources and tools are made publicly available under Creative Commons licences in the repository of the CLARIN.SI research infrastructure and on GitHub, while the corpora are also available through the CLARIN.SI concordancers.
","Language Resources and Evaluation",2018,"No","user generat content sloven languag corpora manual annot dataset text normalis jane project languag resourc tool sloven user generat content paper present result jane project aim develop languag resourc tool sloven user generat content paper describ million word jane corpus tweet forum post news comment user talk page wikipedia blog blog comment text accompani rich metadata develop process tool sloven user generat content present includ tokenis word normalis part speech tagger lemmatis name entiti recognis set manual annot dataset produc tool train linguist research develop resourc tool made public creativ common licenc repositori clarinsi research infrastructur github corpora clarinsi concordanc",0
"Key Wordsstyle analysis hoaxes ready-made software historical chronicle vocabulary Spanish language Puerto Rican literature ","exploring conscious imitation of style with ready made software","This article describes some approaches to imitation analysis and the use of ready-made software for this task. Devising computer-assisted techniques for exploring the conscious literary imitation of style is an application of particular relevance to contemporary Hispanic narrative and one that can be handled with a microcomputer and readily accessible software. The article describes some approaches to imitation analysis and the use of ready-made software to assess the effectiveness of stylistic imitation of eighteenth-century historical chronicle in La renuncia del héroe Baltasar (The Renunciation of the Hero Baltasar), by the Puerto Rican novelist Rodríguez Juliá. Even when employing familiar procedures of text analysis with computer, comparing a fictional text with a multiple and diverse corpus of authentic historical documents requires somewhat unique assumptions and hypotheses, since neither authorship, influence, or authenticity are in question.","Computers and the Humanities",1989,"No","key wordsstyl analysi hoax readi made softwar histor chronicl vocabulari spanish languag puerto rican literatur explor conscious imit style readi made softwar articl describ approach imit analysi readi made softwar task devis comput assist techniqu explor conscious literari imit style applic relev contemporari hispan narrat handl microcomput readili access softwar articl describ approach imit analysi readi made softwar assess effect stylist imit eighteenth centuri histor chronicl la renuncia del roe baltasar renunci hero baltasar puerto rican novelist rodr guez juli employ familiar procedur text analysi comput compar fiction text multipl divers corpus authent histor document requir uniqu assumpt hypothes authorship influenc authent question",0
NA,"zur formelhaftigkeit in heinrich wittenwilers ring wortwiederholungen und grammatische versmuster",NA,"Computers and the Humanities",1979,"No","zur formelhaftigkeit heinrich wittenwil ring wortwiederholungen und grammatisch versmust na",0
"Key wordsneural networks stylometric analysis Shakespeare Fletcher discrimination classification ","shakespeare vs fletcher a stylometric analysis by radial basis functions","In this paper we show, for the first time, how Radial Basis Function (RBF) network techniques can be used to explore questions surrounding authorship of historic documents. The paper illustrates the technical and practical aspects of RBF's, using data extracted from works written in the early 17th century by William Shakespeare and his contemporary John Fletcher. We also present benchmark comparisons with other standard techniques for contrast and comparison.","Computers and the Humanities",1995,"No","key wordsneur network stylometr analysi shakespear fletcher discrimin classif shakespear fletcher stylometr analysi radial basi function paper show time radial basi function rbf network techniqu explor question surround authorship histor document paper illustr technic practic aspect rbf data extract work written earli th centuri william shakespear contemporari john fletcher present benchmark comparison standard techniqu contrast comparison",0
"Key WordsComputers and the Humanities statistical analysis literature language philosophical essays thematic analysis theory replication retrospective polemic ","statistical analysis of literature a retrospective on computers and the humanities 19661990","This retrospective on statistical analysis of literature in the first twenty-four years of Computers and the Humanities divides the essays under review into four groups: the philosophical, the statistical analyses of language, the statistical analyses of literary texts, and the statistical analyses of themes. It begins with the question: must valid statistical analysis of any literary text be based on a complete linguistic description of the language of the text? It summarizes and evaluates over forty essays, giving details on works discussed, sample sizes used, statistical methods applied, and quotations from the researchers. The essay ends with a polemical summary of what has been done and what the future holds. It emphasizes the importance of extended pre-computational stages of learning about language and discourse analysis; reading previous research, building on and challenging theory; and the use of carefully crafted, small databases to test specific questions.","Computers and the Humanities",1991,"No","key wordscomput human statist analysi literatur languag philosoph essay themat analysi theori replic retrospect polem statist analysi literatur retrospect comput human retrospect statist analysi literatur twenti year comput human divid essay review group philosoph statist analys languag statist analys literari text statist analys theme begin question valid statist analysi literari text base complet linguist descript languag text summar evalu forti essay give detail work discuss sampl size statist method appli quotat research essay end polem summari futur hold emphas import extend pre comput stage learn languag discours analysi read previous research build challeng theori care craft small databas test specif question",0
"KeywordsStatistical Approach Computational Linguistic ","elements of a statistical approach to the question of authorship in music",NA,"Computers and the Humanities",1985,"No","statist approach comput linguist element statist approach question authorship music na",0
"Key Wordscognition semantics riddles fuzzy sets language systems ideation ","the surface of language and humanities computing","Recent articles have noted that humanities computing techniques and methodologies remain marginal to mainstream literary scholarship. Mark Olsen's paper discusses this phenomenon and argues for large scale analyses of text databases that would incorporate a shift in theoretical orientation to include greater stress on intertextuality and sign theory. Part of Olsen's argument revolves on the need to move away from the syntactic and overt grammatical elements of textual language to more subtle semantics and meaning systems. While provocative and important, Olsen's stance remains rooted in literary theoretical constructs. Another level of language, the cognitive, offers equally interesting challenges for humanities computing, though the paradigms for this type of computer-based exploration are derived from disciplines traditionally removed from the humanities. The riddle, a nearly universal genre, offers a window onto some of the cognitive processes involved in deep level language function. By analyzing the riddling process, different methods of computational modelling can be inferred, suggesting new avenues for computing in the humanities.","Computers and the Humanities",1993,"No","key wordscognit semant riddl fuzzi set languag system ideat surfac languag human comput recent articl note human comput techniqu methodolog remain margin mainstream literari scholarship mark olsen paper discuss phenomenon argu larg scale analys text databas incorpor shift theoret orient includ greater stress intertextu sign theori part olsen argument revolv move syntact overt grammat element textual languag subtl semant mean system provoc import olsen stanc remain root literari theoret construct level languag cognit offer equal interest challeng human comput paradigm type comput base explor deriv disciplin tradit remov human riddl univers genr offer window cognit process involv deep level languag function analyz riddl process method comput model infer suggest avenu comput human",0
"CD-Rom development crossing subject boundaries didactic considerations language learning technologies multi-media ","the potential of multi media for foreign language learning a critical evaluation","Multi-Media does not, just by itself, guarantee accelerated learning and enhanced motivation unless there is a clear pedagogical progression and learning strategy. The authors describe and analyze the didactic dimensions to be considered when designing a multi-media tool, based on their own experience as software authors and language trainers.","Computers and the Humanities",1997,"No","cd rom develop cross subject boundari didact consider languag learn technolog multi media potenti multi media foreign languag learn critic evalu multi media guarante acceler learn enhanc motiv clear pedagog progress learn strategi author describ analyz didact dimens consid design multi media tool base experi softwar author languag trainer",0
"KeywordsComputational Linguistic ","robertson davies the tory mode",NA,"Computers and the Humanities",1977,"No","comput linguist robertson davi tori mode na",0
"Key WordsTACT literary analysis text retrieval imagery symbolism structural patterns prosody teaching text analysis ","the computer in literary analysis usingtact with students","TACT, a freeware program from the University of Toronto's Centre for Computing in the Humanities, is a highly sophisticated tool for text retrieval; although written for experienced critics and researchers, it can teach undergraduate students to read literature in new, fresh ways. Without requiring that the user become either a programmer, linguist, mathematician, or statistician,TACT introduces the literature student to the computer as a research tool. Studies of imagery and symbolism, of structural patterns, and of prosody can result from the student's careful tagging of a literary text and can yield significant insights into the work of literature. Students who use the computer as such a tool learn to read literary texts more closely and to think more clearly about literary problems.","Computers and the Humanities",1994,"No","key wordstact literari analysi text retriev imageri symbol structur pattern prosodi teach text analysi comput literari analysi usingtact student tact freewar program univers toronto centr comput human high sophist tool text retriev written experienc critic research teach undergradu student read literatur fresh way requir user programm linguist mathematician statisticiantact introduc literatur student comput research tool studi imageri symbol structur pattern prosodi result student care tag literari text yield signific insight work literatur student comput tool learn read literari text close literari problem",0
"KeywordsComputational Linguistic Semantic Processing Communicative Exercise ","semantic processing for communicative exercises in foreign language learning",NA,"Computers and the Humanities",1989,"No","comput linguist semant process communic exercis semant process communic exercis foreign languag learn na",0
"Key WordsDuras narrative focalization stylostatistics text topography ","toward a narra topography a pilot study applied to marguerite duras novelmoderato cantabile","Although lexical frequencies are familiar measures of stylistic and thematic analysis, only recently have some stylostatisticians been tempted to investigate the relationship between the frequency and topography of repeated lexical items. In the present paper the authors have turned to the study of the four focal types of discursive narratology, using Marguerite Duras'Moderato Cantabile. Their intent is to uncover aspects of narratological performance which further elucidate the communicative strategies in the story. Part 1 summarizes the problematic between frequency and topography. It describes how a topographical index can be computed for any repeated item and how a Global Topography Index (GTI) can summarize the major topographical characteristics of any text sequence. Part 2 presents a four-cell typology of narrational mode: a segmentation of the verbal chain into narrating and narrated speech acts, with each text sequence tagged according to its discursive function: overt sender intervention for story coherence or comment on the focal level of a narrating present; representation of discrete or unlocalized events on the focal level of a mimeticized past. In Part 3 the focal encodings are displayed in numerical and graphic form, first according to the eight surface chapter divisions and then according to twenty-six subsets of approximately equal length. The fluctuations of the topography indices are reviewed, with particular attention being paid to the manifestation of cluster effects. Although sender interventions predominate, the relativized behavior of each focal type contributes to a climactic unraveling of the intrigue in the final chapters. In conclusion, the authors stress the dichotomy between the calm surface of the chapters and the agitated tensions of the twenty-six subsets.","Computers and the Humanities",1993,"No","key wordsdura narrat focal stylostatist text topographi narra topographi pilot studi appli marguerit dura novelmoderato cantabil lexic frequenc familiar measur stylist themat analysi recent stylostatistician tempt investig relationship frequenc topographi repeat lexic item present paper author turn studi focal type discurs narratolog marguerit durasmoderato cantabil intent uncov aspect narratolog perform elucid communic strategi stori part summar problemat frequenc topographi describ topograph index comput repeat item global topographi index gti summar major topograph characterist text sequenc part present cell typolog narrat mode segment verbal chain narrat narrat speech act text sequenc tag discurs function overt sender intervent stori coher comment focal level narrat present represent discret unloc event focal level mimetic past part focal encod display numer graphic form surfac chapter divis twenti subset approxim equal length fluctuat topographi indic review attent paid manifest cluster effect sender intervent predomin relativ behavior focal type contribut climact unravel intrigu final chapter conclus author stress dichotomi calm surfac chapter agit tension twenti subset",0
"KeywordsComputational Linguistic ","some uses of a grammatical concordance",NA,"Computers and the Humanities",1968,"No","comput linguist grammat concord na",0
"KeywordsComputational Linguistic ","the use of the computer in the analysis of german folksongs",NA,"Computers and the Humanities",1976,"No","comput linguist comput analysi german folksong na",0
"Key Wordsaesthetics sonnets Shakespeare linguistic diversity primary process type-token ratio interaction effects ","lexical choices and aesthetic success a computer content analysis of 154 shakespeare sonnets","A research paradigm is suggested that combines the perspectives of the humanistic scholar and the behavioral scientist: After differentiating the popularity of actual aesthetic products using archival indices and then subjecting these compositions to objective computer content analyses, further statistical treatment may divulge the intrinsic properties responsible for differences in impact. This approach is illustrated by an analysis of the 154 sonnets attributed to William Shakespeare. Each sonnet was partitioned into four consecutive units (three quatrains and a couplet), and then a computer gauged how the number of words, different words, unique words, primary process imagery, and secondary process imagery changed within each sonnet. Taking advantage of a previous objective measure of the relative aesthetic merit of the sonnets, and implementing a statistical search for interaction effects, it was demonstrated that Shakespeare's lexical choices adopt a discernible pattern in the highly popular creations that is not found in the more obscure poems. Perhaps the most fascinating aspect of this pattern shift is the distinct manner in which the poet modifies his vocabulary when composing the concluding couplet in his best sonnets.","Computers and the Humanities",1990,"No","key wordsaesthet sonnet shakespear linguist divers primari process type token ratio interact effect lexic choic aesthet success comput content analysi shakespear sonnet research paradigm suggest combin perspect humanist scholar behavior scientist differenti popular actual aesthet product archiv indic subject composit object comput content analys statist treatment divulg intrins properti respons differ impact approach illustr analysi sonnet attribut william shakespear sonnet partit consecut unit quatrain couplet comput gaug number word word uniqu word primari process imageri secondari process imageri chang sonnet take advantag previous object measur relat aesthet merit sonnet implement statist search interact effect demonstr shakespear lexic choic adopt discern pattern high popular creation found obscur poem fascin aspect pattern shift distinct manner poet modifi vocabulari compos conclud couplet sonnet",0
"Key wordsdiscourse information natural language syntax natural language semantics punctuation ","current approaches to punctuation in computational linguistics","Some recent studies in computational linguistics have aimed to take advantage of various cues presented by punctuation marks. This short survey is intended to summarise these research efforts and additionally, to outline a current perspective for the usage and functions of punctuation marks. We conclude by presenting an information-based framework for punctuation, influenced by treatments of several related phenomena in computational linguistics.","Computers and the Humanities",1996,"No","key wordsdiscours inform natur languag syntax natur languag semant punctuat current approach punctuat comput linguist recent studi comput linguist aim advantag cue present punctuat mark short survey intend summaris research effort addit outlin current perspect usag function punctuat mark conclud present inform base framework punctuat influenc treatment relat phenomena comput linguist",0
"KeywordsComputational Linguistic Machine Translation ","machine translation in the ussr",NA,"Computers and the Humanities",1984,"No","comput linguist machin translat machin translat ussr na",0
"KeywordsPattern Match Computational Linguistic ","synoname1 the gettys new approach to pattern matching for personal names",NA,"Computers and the Humanities",1991,"No","pattern match comput linguist synonam getti approach pattern match person name na",0
"KeywordsComputational Linguistic Political History ","the new political history progress and prospects",NA,"Computers and the Humanities",1977,"No","comput linguist polit histori polit histori progress prospect na",0
"Canada Canadian digitization microreproductions research ","attitudes of the canadian research community toward creating and accessing digitized facsimile collections of historical documents","A study commissioned by the Canadian Institute for Historical Microreproductions produced some interesting secondary findings about the attitudes of the Canadian research community towards digitized facsimile collections. In written responses to a questionnaire designed primarily to elicit advice about the subject content and focus of future projects, and in structured follow-up interviews, many respondents demonstrated a marked ambivalence towards the concept of digitized collections. Furthermore, if faced with a choice between fully searchable text and digitized facsimile images with traditional points of access (subject, author, title, etc.), there appears to be a preference for the latter means of access.","Computers and the Humanities",1999,"No","canada canadian digit microreproduct research attitud canadian research communiti creat access digit facsimil collect histor document studi commiss canadian institut histor microreproduct produc interest secondari find attitud canadian research communiti digit facsimil collect written respons questionnair design primarili elicit advic subject content focus futur project structur follow interview respond demonstr mark ambival concept digit collect face choic fulli searchabl text digit facsimil imag tradit point access subject author titl appear prefer mean access",0
"Key Wordsinformatics computer science philology lexicography linguistics ","informatics and new philology","This paper considers the impact of informatics and the new technology on the field of philology. A brief overview of developments in the field is given, and the present bottleneck of computational linguistics are considered. Also discussed are some of the challenges posed by natural language processing, and the ways in which we can rise to the challenge.","Computers and the Humanities",1990,"No","key wordsinformat comput scienc philolog lexicographi linguist informat philolog paper consid impact informat technolog field philolog overview develop field present bottleneck comput linguist consid discuss challeng pose natur languag process way rise challeng",0
"content-based retrieval image databases image indexing image retrieval ","access to pictorial material a review of current research and future prospects","Rapid expansion in the digitization of image and image collections has vastly increased the numbers of images available to scholars and researchers through electronic means. This research review will familiarize the reader with current research applicable to the development of image retrieval systems and provides additional material for exploring the topic further, both in print and online. The discussion will cover several broad areas, among them classification and indexing systems used for describing image collections and research initiatives into image access focusing on image attributes, users, queries, tasks, and cognitive aspects of searching. Prospects for the future of image access, including an outline of future research initiatives, are discussed. Further research in each of these areas will provide basic data which will inform and enrich image access system design and will hopefully provide a richer, more flexible, and satisfactory environment for searching for and discovering images. Harnessing the true power of the digital image environment will only be possible when image retrieval systems are coherently designed from principles derived from the fullest range of applicable disciplines, rather than from isolated or fragmented perspectives.","Computers and the Humanities",1999,"No","content base retriev imag databas imag index imag retriev access pictori materi review current research futur prospect rapid expans digit imag imag collect vast increas number imag scholar research electron mean research review familiar reader current research applic develop imag retriev system addit materi explor topic print onlin discuss cover broad area classif index system describ imag collect research initi imag access focus imag attribut user queri task cognit aspect search prospect futur imag access includ outlin futur research initi discuss research area provid basic data inform enrich imag access system design provid richer flexibl satisfactori environ search discov imag har true power digit imag environ imag retriev system coher design principl deriv fullest rang applic disciplin isol fragment perspect",0
NA,"annual bibliography for 1970",NA,"Computers and the Humanities",1971,"No","annual bibliographi na",0
"Key WordsItalian language literary language lexicography concordances databases lemmatization ","a literary lexicography project for the italian language","CLIPON is an acronym for Concordanze della Lingua Italiana Poetica dell'Otto/Novecento. The aim of the project described here is to produce lexicons and lemmatized concordances of the literary Italian language of the nineteenth and twentieth centuries. The corpus involves groups of mainly poetic works and authors that have a common denominator as regards schools, currents, culture and chronology.","Computers and the Humanities",1990,"No","key wordsitalian languag literari languag lexicographi concord databas lemmat literari lexicographi project italian languag clipon acronym concordanz della lingua italiana poetica dellottonovecento aim project produc lexicon lemmat concord literari italian languag nineteenth twentieth centuri corpus involv group poetic work author common denomin school current cultur chronolog",0
"alignment copyright greedy string tiling journalism n-grams plagiarism text rewriting ","on the ownership of text","The paper explores the notions of text ownership and its partial inverse, plagiarism, and asks how close or different they are from a procedural point of view that might seek to establish either of these properties. The emphasis is on procedures rather than on the conventional subject division of authorship studies, plagiarism detection etc. We use, as a particular example, our research on the notion of computational detection of text rewriting, in the benign sense of a standard journalist's adaptation of the Press Association newsfeed. The conclusion is that, whatever may be the case in copyright law, procedural detection and establishment of the ownership is a complex and vexed matter. Behind the paper is an unspoken appeal to return to an earlier historical phase, one where texts were normally rewritten and rewritten again and the ownership of text by an individual was a less clear matter than in historically recent times.","Computers and the Humanities",2004,"No","align copyright greedi string tile journal gram plagiar text rewrit ownership text paper explor notion text ownership partial invers plagiar ask close procedur point view seek establish properti emphasi procedur convent subject divis authorship studi plagiar detect research notion comput detect text rewrit benign sens standard journalist adapt press associ newsfe conclus case copyright law procedur detect establish ownership complex vex matter paper unspoken appeal return earlier histor phase text rewritten rewritten ownership text individu clear matter histor recent time",0
"KeywordsComputational Linguistic ","the study of english loan words in modern french",NA,"Computers and the Humanities",1973,"No","comput linguist studi english loan word modern french na",0
"KeywordsComputational Linguistic Tutoring System Teaching Writing Intelligent Tutoring System ","intelligent tutoring systems exploring issues in learning and teaching writing",NA,"Computers and the Humanities",1989,"No","comput linguist tutor system teach write intellig tutor system intellig tutor system explor issu learn teach write na",0
"CMC conferencing discourse ethics Habermas pedagogy Rawls ","wag the dog online conferencing and teaching","Web-accessible conferencing softwareand ``conversational ethics'' drawn from Habermas andRawls have successfully brought together on-lineparticipants separated by geography and viewpoint, andoccasionally resulted in consensus regarding otherwisedivisive issues such as abortion. The author describessuccesses, limitations, and costs of incorporatingthese technologies and discourse ethics in a religiousstudies class. Results are striking, but thepedagogical benefits involve technical risks and highlabor and time costs. This experience, coupled withrecent research, suggests that electronic pedagogies,like other teaching strategies, work for some, but notall students: this argues that we take up electronicteaching as one approach among many.","Computers and the Humanities",2000,"No","cmc conferenc discours ethic haberma pedagogi rawl wag dog onlin conferenc teach web access conferenc softwareand convers ethic drawn haberma andrawl success brought lineparticip separ geographi viewpoint andoccasion result consensus otherwisedivis issu abort author describessuccess limit cost incorporatingthes technolog discours ethic religiousstudi class result strike thepedagog benefit involv technic risk highlabor time cost experi coupl withrec research suggest electron pedagogi teach strategi work notal student argu electronicteach approach",0
"Key Wordsinteractive fiction literariness canonization ostranenie gap strangeness Shklovskij Portal ","determining literariness in interactive fiction","The authors of interactive fiction are beginning to demonstrate a concern for the literariness of their product. Literariness, as defined by Shklovskij and the Russian Formalists, is the quality of “making strange” that which is linguistically familiar, a quality Shklovskij termed ostranenie. By applying the principle of ostranenie, as well as other well-known literary principles, to the most serious interactive fictions, we can determine if this new genre exhibits the features of literariness. A study of Mindwheel, Brimstone, Breakers, A Mind Forever Voyaging, Portal, and Trinity suggest that the literariness of interactive fiction comes out of its concern both for “making strange” what is familiar and for “making familiar” what is strange.","Computers and the Humanities",1988,"No","key wordsinteract fiction literari canon ostraneni gap strang shklovskij portal determin literari interact fiction author interact fiction begin demonstr concern literari product literari defin shklovskij russian formalist qualiti make strang linguist familiar qualiti shklovskij term ostraneni appli principl ostraneni literari principl interact fiction determin genr exhibit featur literari studi mindwheel brimston breaker mind forev voyag portal triniti suggest literari interact fiction concern make strang familiar make familiar strang",0
"Key WordsEuripides — themes Euripides — political meanings Euripides — Suppliant Women old age and youth in Ancient Greece Guiraudian studies Euripides — Heraclidae Euripides — Andromache Euripides — Hecabe Euripides — Heracles Euripides — Alcestis ","a study of words relating to youth and old age in the plays of euripides and its special implications for euripides suppliant women","This study focuses on the imagery of youth and old age in the plays of Euripides, especially the Suppliant Women, considering frequently used words in each play according to a formula developed by Guiraud. The study identifies a motif, the rejuvenation theme, an elaborate interaction between young and old, in the Suppliant Women and in: Alcestis, Heraclidae, Andromache, Hecabe, and Heracles. The difference between the use of neos (young, new) in the Suppliant Women and in the other plays is statistically significant. This word helps Euripides contrast two different kinds of youth: the fearful, rash, and animalistic (Theban); and that which has been properly schooled and led (Athenian). The greatest ground in the Suppliant Women for praising Athens is in her treatment of the young as a politically valuable force.","Computers and the Humanities",1988,"No","key wordseuripid theme euripid polit mean euripid suppliant women age youth ancient greec guiraudian studi euripid heraclida euripid andromach euripid hecab euripid heracl euripid alcesti studi word relat youth age play euripid special implic euripid suppliant women studi focus imageri youth age play euripid suppliant women frequent word play formula develop guiraud studi identifi motif rejuven theme elabor interact young suppliant women alcesti heraclida andromach hecab heracl differ neo young suppliant women play statist signific word help euripid contrast kind youth fear rash animalist theban proper school led athenian greatest ground suppliant women prais athen treatment young polit valuabl forc",0
"KeywordsComputational Linguistic ","tables for comparing the richness and structure of vocabulary in texts of different lengths",NA,"Computers and the Humanities",1975,"No","comput linguist tabl compar rich structur vocabulari text length na",0
NA,"annual bibliography for 1971",NA,"Computers and the Humanities",1972,"No","annual bibliographi na",0
"Key Wordsstylometry literary detection Shakespeare Authorship Question Shakespearean canon Elizabethan poets Karhunen-Loeve transform ","a touchstone for the bard","We introduce an authorship identification test, called modal analysis, based on a new statistic derived from the Karhunen-Loeve transform. Application to the poems of the Shakespearean canon and to other contemporary poetry strongly supports the case for disqualification of most major claimants. Results also cast doubt that the recently discovered poems, Shall I Die and Elegy, were written by William Shakespeare, but do suggest that eight unascribed poems of The Passionate Pilgrim may have been his work.","Computers and the Humanities",1991,"No","key wordsstylometri literari detect shakespear authorship question shakespearean canon elizabethan poet karhunen loev transform touchston bard introduc authorship identif test call modal analysi base statist deriv karhunen loev transform applic poem shakespearean canon contemporari poetri strong support case disqualif major claimant result cast doubt recent discov poem die elegi written william shakespear suggest unascrib poem passion pilgrim work",0
"KeywordsComputational Linguistic ","measurement and the study of literature",NA,"Computers and the Humanities",1979,"No","comput linguist measur studi literatur na",0
"KeywordsComputational Linguistic Cultural Measurement ","ethnometrics cultural measurements as a necessary counterbalance to the cost effectiveness revolution some implications of the computers in anthropology conference",NA,"Computers and the Humanities",1967,"No","comput linguist cultur measur ethnometr cultur measur counterbal cost effect revolut implic comput anthropolog confer na",0
NA,"understanding hyper media required readings",NA,"Computers and the Humanities",1995,"No","understand hyper media requir read na",0
"KeywordsEvents Annotation Meta-knowledge Subjectivity Modality Speculation ","enriching news events with meta knowledge information","Given the vast amounts of data available in digitised textual form, it is important to provide mechanisms that allow users to extract nuggets of relevant information from the ever growing volumes of potentially important documents. Text mining techniques can help, through their ability to automatically extract relevant event descriptions, which link entities with situations described in the text. However, correct and complete interpretation of these event descriptions is not possible without considering additional contextual information often present within the surrounding text. This information, which we refer to as meta-knowledge, can include (but is not restricted to) the modality, subjectivity, source, polarity and specificity of the event. We have developed a meta-knowledge annotation scheme specifically tailored for news events, which includes six aspects of event interpretation. We have applied this annotation scheme to the ACE 2005 corpus, which contains 599 documents from various written and spoken news sources. We have also identified and annotated the words and phrases evoking the different types of meta-knowledge. Evaluation of the annotated corpus shows high levels of inter-annotator agreement for five meta-knowledge attributes, and moderate level of agreement for the sixth attribute. Detailed analysis of the annotated corpus has revealed further insights into the expression mechanisms of different types of meta-knowledge, their relative frequencies and mutual correlations.","Language Resources and Evaluation",2017,"No","event annot meta knowledg subject modal specul enrich news event meta knowledg inform vast amount data digitis textual form import provid mechan user extract nugget relev inform grow volum potenti import document text mine techniqu abil automat extract relev event descript link entiti situat text correct complet interpret event descript addit contextu inform present surround text inform refer meta knowledg includ restrict modal subject sourc polar specif event develop meta knowledg annot scheme specif tailor news event includ aspect event interpret appli annot scheme ace corpus document written spoken news sourc identifi annot word phrase evok type meta knowledg evalu annot corpus show high level inter annot agreement meta knowledg attribut moder level agreement sixth attribut detail analysi annot corpus reveal insight express mechan type meta knowledg relat frequenc mutual correl",0
"KeywordsCommunicative embodied feedback Contact Perception Understanding Emotions Multimodal Embodied communication ","the analysis of embodied communicative feedback in multimodal corpora a prerequisite for behavior simulation","Communicative feedback refers to unobtrusive (usually short) vocal or bodily expressions whereby a recipient of information can inform a contributor of information about whether he/she is able and willing to communicate, perceive the information, and understand the information. This paper provides a theory for embodied communicative feedback, describing the different dimensions and features involved. It also provides a corpus analysis part, describing a first data coding and analysis method geared to find the features postulated by the theory. The corpus analysis part describes different methods and statistical procedures and discusses their applicability and the possible insights gained with these methods.","Language Resources and Evaluation",2007,"No","communic embodi feedback contact percept understand emot multimod embodi communic analysi embodi communic feedback multimod corpora prerequisit behavior simul communic feedback refer unobtrus short vocal bodili express recipi inform inform contributor inform communic perceiv inform understand inform paper theori embodi communic feedback describ dimens featur involv corpus analysi part describ data code analysi method gear find featur postul theori corpus analysi part describ method statist procedur discuss applic insight gain method",0
"KeywordsWord meaning Lexicography Lexical universe Amour French Corneille ","how to measure the meanings of words amour in corneilles work","We present a new method to describe the contextual meaning of a key word in a corpus. The vocabulary of the sentences containing this word is compared to that of the entire corpus in order to highlight the words which are significantly overutilized in the neighbourhood of this key word (they are associated in the author’s mind) and the ones which are significantly underutilized (they are mutually exclusive). This method provides an interesting tool for lexicography and literary studies as is shown by applying it to the word amour (love) in the work of Pierre Corneille, the most famous French playwright of the 17th century.","Language Resources and Evaluation",2005,"No","word mean lexicographi lexic univers amour french corneill measur mean word amour corneill work present method describ contextu mean key word corpus vocabulari sentenc word compar entir corpus order highlight word signific overutil neighbourhood key word author mind signific underutil mutual exclus method interest tool lexicographi literari studi shown appli word amour love work pierr corneill famous french playwright th centuri",0
"KeywordsConstructions Subcategorization frames Argument structure Topicalization Detopicalization Pronominal constructions ","constructions at argument structure level in the sensem corpora","
In this paper we present the annotation scheme of constructions at the argument-structure level in the Spanish and Catalan Corpora SenSem. Constructions are accounted for as form-meaning pairs following the theoretical underpinning of Construction Grammar. Regarding meaning, we propose a hierarchy of constructions taking into account, at the highest level, the prominence of the logical subject in the sentence. Thus, we differentiate between topicalized and detopicalized sentences, which is an innovative proposal to solve some terminological issues related to pronominal constructions in Spanish. We further develop this classification taking into account the semantic relation of the logical subject with the verb and its coindexation, if any, with other participants. As regards form, the basic features we consider are syntagmatic categories and syntactic functions. Furthermore, we annotate the form the verb requires, that is, if it requires a pronoun in order to convey a particular meaning. Other relevant contributions are the annotation of some linguistic phenomena not taken into account in other similar resources, such as reciprocal, dative or impersonal constructions. Finally, we present the frequencies of all these constructions in Spanish.","Language Resources and Evaluation",2015,"No","construct subcategor frame argument structur topic detopic pronomin construct construct argument structur level sensem corpora paper present annot scheme construct argument structur level spanish catalan corpora sensem construct account form mean pair theoret underpin construct grammar mean propos hierarchi construct take account highest level promin logic subject sentenc differenti topic detopic sentenc innov propos solv terminolog issu relat pronomin construct spanish develop classif take account semant relat logic subject verb coindex particip form basic featur syntagmat categori syntact function annot form verb requir requir pronoun order convey mean relev contribut annot linguist phenomena account similar resourc reciproc dativ imperson construct final present frequenc construct spanish",0
"KeywordsAppraisal Corpus annotation Inter-annotator agreement Opinion Subjectivity Systemic Functional Linguistics ","annotating expressions of appraisal in english","In the context of Systemic Functional Linguistics, Appraisal is a theory describing the types of language utilised in communicating emotion and opinion. Robust automatic analyses of Appraisal could contribute in a number of ways to computational sentiment analysis by: distinguishing various types of evaluation, for example affect, ethics or aesthetics; discriminating between an author’s opinions and the opinions of authors referenced by the author and determining the strength of evaluations. This paper reviews the typology described by Appraisal, presents a methodology for annotating Appraisal, and the use of this to annotate a corpus of book reviews. It discusses an inter-annotator agreement study, and considers instances of systematic disagreement that indicate areas in which Appraisal may be refined or clarified. Although the annotation task is difficult, there are many instances where the annotators agree; these are used to create a gold-standard corpus for future experimentation with Appraisal.","Language Resources and Evaluation",2012,"No","apprais corpus annot inter annot agreement opinion subject system function linguist annot express apprais english context system function linguist apprais theori describ type languag utilis communic emot opinion robust automat analys apprais contribut number way comput sentiment analysi distinguish type evalu affect ethic aesthet discrimin author opinion opinion author referenc author determin strength evalu paper review typolog apprais present methodolog annot apprais annot corpus book review discuss inter annot agreement studi consid instanc systemat disagr area apprais refin clarifi annot task difficult instanc annot agre creat gold standard corpus futur experiment apprais",0
"KeywordsLexical resources INESS NorGramBank Treebanking LFG Language research infrastructure Automatic syntactic analysis ","the enrichment of lexical resources through incremental parsebanking","Automatic syntactic analysis of a corpus requires detailed lexical and morphological information that cannot always be harvested from traditional dictionaries. Therefore the development of a treebank presents an opportunity to simultaneously enrich the lexicon. In building NorGramBank, we use an incremental parsebanking approach, in which a corpus is parsed and disambiguated, and after improvements to the grammar and the lexicon, reparsed. In this context we have implemented a text preprocessing interface where annotators can enter unknown words or missing lexical information either before parsing or during disambiguation. The information added to the lexicon in this way may be of great interest both to lexicographers and to other language technology efforts.","Language Resources and Evaluation",2016,"No","lexic resourc iness norgrambank treebank lfg languag research infrastructur automat syntact analysi enrich lexic resourc increment parsebank automat syntact analysi corpus requir detail lexic morpholog inform harvest tradit dictionari develop treebank present opportun simultan enrich lexicon build norgrambank increment parsebank approach corpus pars disambigu improv grammar lexicon repars context implement text preprocess interfac annot enter unknown word miss lexic inform pars disambigu inform ad lexicon great interest lexicograph languag technolog effort",0
"KeywordsMultimodal interaction Video corpus Head-mounted eye-tracking Multifocal approach ","insight interaction a multimodal and multifocal dialogue corpus","Research on the multimodal aspects of interactional language use requires high-quality multimodal resources. In contrast to the vast amount of available written language corpora and collections of transcribed spoken language, truly multimodal corpora including visual as well as auditory data are scarce. In this paper, we first discuss a few notable exceptions that do provide high-quality and multiple-angle video recordings of face-to-face conversations. We then present a new multimodal corpus design that adds two dimensions to the existing resources. First, the recording set-up was designed in such a way as to have a full view of the dialogue partners’ gestural behaviour, including hand gestures, facial expressions and body posture. Second, by recording the participant perspective and behaviour during conversation, using head-mounted scene cameras and eye-trackers, we obtained a 3D landscape of the conversation, with detailed production information (scene camera and sound) and indices of cognitive processing (eye movements for gaze analysis) for both participants. In its current form, the resulting InSight Interaction Corpus consists of 15 recorded face-to-face interactions of 20 min each, of which five have been transcribed and annotated for a range of linguistic and gestural features, using the ELAN multimodal annotation tool.","Language Resources and Evaluation",2015,"No","multimod interact video corpus head mount eye track multifoc approach insight interact multimod multifoc dialogu corpus research multimod aspect interact languag requir high qualiti multimod resourc contrast vast amount written languag corpora collect transcrib spoken languag multimod corpora includ visual auditori data scarc paper discuss notabl except provid high qualiti multipl angl video record face face convers present multimod corpus design add dimens exist resourc record set design full view dialogu partner gestur behaviour includ hand gestur facial express bodi postur record particip perspect behaviour convers head mount scene camera eye tracker obtain d landscap convers detail product inform scene camera sound indic cognit process eye movement gaze analysi particip current form result insight interact corpus consist record face face interact min transcrib annot rang linguist gestur featur elan multimod annot tool",0
"KeywordsFacial Expression Annotation Scheme Virtual Character Multimodal Interface Virtual Agent ","introduction to the special issue on multimodal corpora for modeling human multimodal behavior","There is an increasing interest in multimodal communication as suggested by several national and international projects (ISLE, HUMAINE, SIMILAR, CHIL, AMI, CALO, VACE, CALLAS), the attention devoted to the topic by well-known institutions and organizations (the National Institute of Standards and Technology, the Linguistic Data Consortium), and the success of conferences related to multimodal communication (ICMI, IVA, Gesture, Measuring Behavior, Nordic Symposium on Multimodal Communication, LREC Workshops on Multimodal Corpora).","Language Resources and Evaluation",2008,"No","facial express annot scheme virtual charact multimod interfac virtual agent introduct special issu multimod corpora model human multimod behavior increas interest multimod communic suggest nation intern project isl humain similar chil ami calo vace calla attent devot topic institut organ nation institut standard technolog linguist data consortium success confer relat multimod communic icmi iva gestur measur behavior nordic symposium multimod communic lrec workshop multimod corpora",0
"KeywordsMono-lingual text reuse Urdu news corpus Urdu text reuse detection Corpus generation ","counter corpus of urdu news text reuse","Text reuse is the act of borrowing text from existing documents to create new texts. Freely available and easily accessible large online repositories are not only making reuse of text more common in society but also harder to detect. A major hindrance in the development and evaluation of existing/new mono-lingual text reuse detection methods, especially for South Asian languages, is the unavailability of standardized benchmark corpora. Amongst other things, a gold standard corpus enables researchers to directly compare existing state-of-the-art methods. In our study, we address this gap by developing a benchmark corpus for one of the widely spoken but under resourced languages i.e. Urdu. The COrpus of Urdu News TExt Reuse (COUNTER) corpus contains 1200 documents with real examples of text reuse from the field of journalism. It has been manually annotated at document level with three levels of reuse: wholly derived, partially derived and non derived. We also apply a number of similarity estimation methods on our corpus to show how it can be used for the development, evaluation and comparison of text reuse detection systems for the Urdu language. The corpus is a vital resource for the development and evaluation of text reuse detection systems in general and specifically for Urdu language.","Language Resources and Evaluation",2017,"No","mono lingual text reus urdu news corpus urdu text reus detect corpus generat counter corpus urdu news text reus text reus act borrow text exist document creat text freeli easili access larg onlin repositori make reus text common societi harder detect major hindranc develop evalu exist mono lingual text reus detect method south asian languag unavail standard benchmark corpora thing gold standard corpus enabl research direct compar exist state art method studi address gap develop benchmark corpus wide spoken resourc languag urdu corpus urdu news text reus counter corpus document real exampl text reus field journal manual annot document level level reus wholli deriv partial deriv deriv appli number similar estim method corpus show develop evalu comparison text reus detect system urdu languag corpus vital resourc develop evalu text reus detect system general specif urdu languag",0
"KeywordsComputational grammar GPSG Persian language ","a computational grammar for persian based on gpsg","In this paper, we present our attempts to design and implement a large-coverage computational grammar for the Persian language based on the Generalized Phrase Structured Grammar (GPSG) model. This grammatical model was developed for continuous speech recognition (CSR) applications, but is suitable for other applications that need the syntactic analysis of Persian. In this work, we investigate various syntactic structures relevant to the modern Persian language, and then describe these structures according to a phrase structure model. Noun (N), Verb (V), Adjective (ADJ), Adverb (ADV), and Preposition (P) are considered basic syntactic categories, and X-bar theory is used to define Noun phrases, Verb phrases, Adjective phrases, Adverbial phrases, and Prepositional phrases. However, we have to extend Noun phrase levels in X-bar theory to four levels due to certain complexities in the structure of Noun phrases in the Persian language. A set of 120 grammatical rules for describing different phrase structures of Persian is extracted, and a few instances of the rules are presented in this paper. These rules cover the major syntactic structures of the modern Persian language. For evaluation, the obtained grammatical model is utilized in a bottom-up chart parser for parsing 100 Persian sentences. Our grammatical model can take 89 sentences into account. Incorporating this grammar in a Persian CSR system leads to a 31% reduction in word error rate.","Language Resources and Evaluation",2011,"No","comput grammar gpsg persian languag comput grammar persian base gpsg paper present attempt design implement larg coverag comput grammar persian languag base general phrase structur grammar gpsg model grammat model develop continu speech recognit csr applic suitabl applic syntact analysi persian work investig syntact structur relev modern persian languag describ structur phrase structur model noun verb adject adj adverb adv preposit consid basic syntact categori bar theori defin noun phrase verb phrase adject phrase adverbi phrase preposit phrase extend noun phrase level bar theori level due complex structur noun phrase persian languag set grammat rule describ phrase structur persian extract instanc rule present paper rule cover major syntact structur modern persian languag evalu obtain grammat model util bottom chart parser pars persian sentenc grammat model sentenc account incorpor grammar persian csr system lead reduct word error rate",0
"KeywordsNLP Grammatical error detection systems Evaluation Annotation Crowdsourcing ","bucking the trend improved evaluation and annotation practices for esl error detection systems","The last decade has seen an explosion in the number of people learning English as a second language (ESL). In China alone, it is estimated to be over 300 million (Yang in Engl Today 22, 2006). Even in predominantly English-speaking countries, the proportion of non-native speakers can be very substantial. For example, the US National Center for Educational Statistics reported that nearly 10 % of the students in the US public school population speak a language other than English and have limited English proficiency (National Center for Educational Statistics (NCES) in Public school student counts, staff, and graduate counts by state: school year 2000–2001, 2002). As a result, the last few years have seen a rapid increase in the development of NLP tools to detect and correct grammatical errors so that appropriate feedback can be given to ESL writers, a large and growing segment of the world’s population. As a byproduct of this surge in interest, there have been many NLP research papers on the topic, a Synthesis Series book (Leacock et al. in Automated grammatical error detection for language learners. Synthesis lectures on human language technologies. Morgan Claypool, Waterloo 2010), a recurring workshop (Tetreault et al. in Proceedings of the NAACL workshop on innovative use of NLP for building educational applications (BEA), 2012), and a shared task competition (Dale et al. in Proceedings of the seventh workshop on building educational applications using NLP (BEA), pp 54–62, 2012; Dale and Kilgarriff in Proceedings of the European workshop on natural language generation (ENLG), pp 242–249, 2011). Despite this growing body of work, several issues affecting the annotation for and evaluation of ESL error detection systems have received little attention. In this paper, we describe these issues in detail and present our research on alleviating their effects.","Language Resources and Evaluation",2014,"No","nlp grammat error detect system evalu annot crowdsourc buck trend improv evalu annot practic esl error detect system decad explos number peopl learn english languag esl china estim million yang engl today predomin english speak countri proport nativ speaker substanti nation center educ statist report student public school popul speak languag english limit english profici nation center educ statist nces public school student count staff graduat count state school year result year rapid increas develop nlp tool detect correct grammat error feedback esl writer larg grow segment world popul byproduct surg interest nlp research paper topic synthesi seri book leacock al autom grammat error detect languag learner synthesi lectur human languag technolog morgan claypool waterloo recur workshop tetreault al proceed naacl workshop innov nlp build educ applic bea share task competit dale al proceed seventh workshop build educ applic nlp bea pp dale kilgarriff proceed european workshop natur languag generat enlg pp grow bodi work issu affect annot evalu esl error detect system receiv attent paper describ issu detail present research allevi effect",0
"KeywordsStatistical Machine Translation Language Pair Word Alignment Source Sentence Source Word ","reordering space design in statistical machine translation","In Statistical Machine Translation (SMT), the constraints on word reorderings have a great impact on the set of potential translations that is explored during search. Notwithstanding computational issues, the reordering space of a SMT system needs to be designed with great care: if a larger search space is likely to yield better translations, it may also lead to more decoding errors, because of the added ambiguity and the interaction with the pruning strategy. In this paper, we study the reordering search space, using a state-of-the art translation system, where all reorderings are represented in a permutation lattice prior to decoding. This allows us to directly explore and compare different reordering schemes and oracle settings. We also study in detail a rule-based preordering system, varying the length and number of rules, the tagset used, as well as contrasting with purely combinatorial subsets of permutations. We carry out experiments on three language pairs in both directions: English-French, a close language pair; English-German and English-Czech, two much more challenging pairs. We show that even though it might be desirable to design better reordering spaces, model and search errors seem to be the most important issues. Therefore, improvements of the reordering space should come along with improvements of the associated models to be really effective.","Language Resources and Evaluation",2016,"No","statist machin translat languag pair word align sourc sentenc sourc word reorder space design statist machin translat statist machin translat smt constraint word reorder great impact set potenti translat explor search notwithstand comput issu reorder space smt system design great care larger search space yield translat lead decod error ad ambigu interact prune strategi paper studi reorder search space state art translat system reorder repres permut lattic prior decod direct explor compar reorder scheme oracl set studi detail rule base preorder system vari length number rule tagset contrast pure combinatori subset permut carri experi languag pair direct english french close languag pair english german english czech challeng pair show desir design reorder space model search error import issu improv reorder space improv model effect",0
"Keywordsontology terminology text mining thesaurus ","thesaurus or logical ontology which one do we need for text mining","Ontologies are recognised as important tools, not only for effective and efficient information sharing, but also for information extraction and text mining. In the biomedical domain, the need for a common ontology for information sharing has long been recognised, and several ontologies are now widely used. However, there is confusion among researchers concerning the type of ontology that is needed for text mining , and how it can be used for effective knowledge management, sharing, and integration in biomedicine. We argue that there are several different ways to define an ontology and that, while the logical view is popular for some applications, it may be neither possible nor necessary for text mining. We propose a text-centered approach for knowledge sharing, as an alternative to formal ontologies. We argue that a thesaurus (i.e. an organised collection of terms enriched with relations) is more useful for text mining applications than formal ontologies.","Language Resources and Evaluation",2005,"No","ontolog terminolog text mine thesaurus thesaurus logic ontolog text mine ontolog recognis import tool effect effici inform share inform extract text mine biomed domain common ontolog inform share long recognis ontolog wide confus research type ontolog need text mine effect knowledg manag share integr biomedicin argu way defin ontolog logic view popular applic text mine propos text center approach knowledg share altern formal ontolog argu thesaurus organis collect term enrich relat text mine applic formal ontolog",0
"KeywordsPrecision grammar Grammar engineering Grammar diagnostics Deep parsing Parse mining ","gdelta a missing link in the grammar engineering toolchain","The development of precision grammars is an inherently resource-intensive process; their complexity means that changes made to one area of a grammar often introduce unexpected flow-on effects elsewhere in the grammar which may only be discovered after some time has been invested in updating numerous test suite items. In this paper, we present the browser-based gDelta tool, which aims to provide grammar engineers with more immediate feedback on the impact of changes made to a grammar by comparing parser output from two different grammar versions. We describe an attribute weighting algorithm for highlighting components of the grammar that have been strongly impacted by a modification to the grammar, as well as a technique for clustering test suite items whose parsability has changed, in order to locate related groups of effects. These two techniques are used to present the grammar engineer with different views on the grammar to inform them of different aspects of change in a data-driven manner.","Language Resources and Evaluation",2015,"No","precis grammar grammar engin grammar diagnost deep pars pars mine gdelta miss link grammar engin toolchain develop precis grammar inher resourc intens process complex mean made area grammar introduc unexpect flow effect grammar discov time invest updat numer test suit item paper present browser base gdelta tool aim provid grammar engin feedback impact made grammar compar parser output grammar version describ attribut weight algorithm highlight compon grammar strong impact modif grammar techniqu cluster test suit item parsabl chang order locat relat group effect techniqu present grammar engin view grammar inform aspect chang data driven manner",0
"KeywordsEmbodied conversational agents Prosody Dialogue acts ","towards the generation of dialogue acts in socio affective ecas a corpus based prosodic analysis","We present a corpus-based prosodic analysis with the aim of uncovering the relationship between dialogue acts, personality and prosody in view to providing guidelines for the ECA Greta’s text-to-speech system. The corpus used is the SEMAINE corpus, featuring four different personalities, further annotated for dialogue acts and prosodic features. In order to show the importance of the choice of dialogue act taxonomy, two different taxonomies were used, the first corresponding to Searle’s taxonomy of speech acts and the second, inspired by Bunt’s DIT++, including a division of directive acts into finer categories. Our results show that finer-grained distinctions are important when choosing a taxonomy. We also show with some preliminary results that the prosodic correlates of dialogue acts are not always as cited in the literature and prove more complex and variable. By studying the realisation of different directive acts, we also observe differences in the communicative strategies of the ECA depending on personality, in view to providing input to a speech system.","Language Resources and Evaluation",2016,"No","embodi convers agent prosodi dialogu act generat dialogu act socio affect eca corpus base prosod analysi present corpus base prosod analysi aim uncov relationship dialogu act person prosodi view provid guidelin eca greta text speech system corpus semain corpus featur person annot dialogu act prosod featur order show import choic dialogu act taxonomi taxonomi searl taxonomi speech act inspir bunt dit includ divis direct act finer categori result show finer grain distinct import choos taxonomi show preliminari result prosod correl dialogu act cite literatur prove complex variabl studi realis direct act observ differ communic strategi eca depend person view provid input speech system",0
"KeywordsVerbal irony Social media Automatic irony detection Machine learning ","exploring the fine grained analysis and automatic detection of irony on twitter","To push the state of the art in text mining applications, research in natural language processing has increasingly been investigating automatic irony detection, but manually annotated irony corpora are scarce. We present the construction of a manually annotated irony corpus based on a fine-grained annotation scheme that allows for identification of different types of irony. We conduct a series of binary classification experiments for automatic irony recognition using a support vector machine (SVM)  that exploits a varied feature set and compare this method to a deep learning approach that is based on an LSTM network and (pre-trained) word embeddings. Evaluation on a held-out corpus shows that the SVM model outperforms the neural network approach and benefits from combining lexical, semantic and syntactic information sources. A qualitative analysis of the classification output reveals that the classifier performance may be further enhanced by integrating implicit sentiment information and context- and user-based features.","Language Resources and Evaluation",2018,"No","verbal ironi social media automat ironi detect machin learn explor fine grain analysi automat detect ironi twitter push state art text mine applic research natur languag process increas investig automat ironi detect manual annot ironi corpora scarc present construct manual annot ironi corpus base fine grain annot scheme identif type ironi conduct seri binari classif experi automat ironi recognit support vector machin svm exploit vari featur set compar method deep learn approach base lstm network pre train word embed evalu held corpus show svm model outperform neural network approach benefit combin lexic semant syntact inform sourc qualit analysi classif output reveal classifi perform enhanc integr implicit sentiment inform context user base featur",0
"KeywordsWord sense annotation Multilabel learning Inter-annotator reliability ","multiplicity and word sense evaluating and learning from multiply labeled word sense annotations","Supervised machine learning methods to model word sense often rely on human labelers to provide a single, ground truth label for each word in its context. We examine issues in establishing ground truth word sense labels using a fine-grained sense inventory from WordNet. Our data consist of a sentence corpus of 1,000 sentences: 100 for each of ten moderately polysemous words. Each word was given multiple sense labels—or a multilabel—from trained and untrained annotators. The multilabels give a nuanced representation of the degree of agreement on instances. A suite of assessment metrics is used to analyze the sets of multilabels, such as comparisons of sense distributions across annotators. Our assessment indicates that the general annotation procedure is reliable, but that words differ regarding how reliably annotators can assign WordNet sense labels, independent of the number of senses. We also investigate the performance of an unsupervised machine learning method to infer ground truth labels from various combinations of labels from the trained and untrained annotators. We find tentative support for the hypothesis that performance depends on the quality of the set of multilabels, independent of the number of labelers or their training.","Language Resources and Evaluation",2012,"No","word sens annot multilabel learn inter annot reliabl multipl word sens evalu learn multipli label word sens annot supervis machin learn method model word sens reli human label provid singl ground truth label word context examin issu establish ground truth word sens label fine grain sens inventori wordnet data consist sentenc corpus sentenc ten moder polysem word word multipl sens label multilabel train untrain annot multilabel give nuanc represent degre agreement instanc suit assess metric analyz set multilabel comparison sens distribut annot assess general annot procedur reliabl word differ reliabl annot assign wordnet sens label independ number sens investig perform unsupervis machin learn method infer ground truth label combin label train untrain annot find tentat support hypothesi perform depend qualiti set multilabel independ number label train",0
"KeywordsMultiword expressions Document retrieval Endogenous resources ","an efficient any language approach for the integration of phrases in document retrieval","In this paper, we address the problem of the exploitation of text phrases in a multilingual context. We propose a technique to benefit from multi-word units in adhoc document retrieval, whatever the language of the document collection. We present principles to optimize the performance improvement obtained through this approach. The work is validated through retrieval experiments conducted on Chinese, Japanese, Korean and English.","Language Resources and Evaluation",2010,"No","multiword express document retriev endogen resourc effici languag approach integr phrase document retriev paper address problem exploit text phrase multilingu context propos techniqu benefit multi word unit adhoc document retriev languag document collect present principl optim perform improv obtain approach work valid retriev experi conduct chines japanes korean english",0
"KeywordsLinguistic annotation Multi-layer annotation Conflicting tokenizations Tokenization alignment Corpus linguistics ","by all these lovely tokens merging conflicting tokenizations","Given the contemporary trend to modular NLP architectures and multiple annotation frameworks, the existence of concurrent tokenizations of the same text represents a pervasive problem in everyday’s NLP practice and poses a non-trivial theoretical problem to the integration of linguistic annotations and their interpretability in general. This paper describes a solution for integrating different tokenizations using a standoff XML format, and discusses the consequences from a corpus-linguistic perspective.","Language Resources and Evaluation",2012,"No","linguist annot multi layer annot conflict token token align corpus linguist love token merg conflict token contemporari trend modular nlp architectur multipl annot framework exist concurr token text repres pervas problem everyday nlp practic pose trivial theoret problem integr linguist annot interpret general paper describ solut integr token standoff xml format discuss consequ corpus linguist perspect",0
"KeywordsAdaptation Evaluation ","adaptation of an automotive dialogue system to users expertise and evaluation of the system","Spoken dialogue systems (SDSs) can be used to operate devices, e.g. in the automotive environment. People using these systems usually have different levels of experience. However, most systems do not take this into account. In this paper, we present a method to build a dialogue system in an automotive environment that automatically adapts to the user’s experience with the system. We implemented the adaptation in a prototype and carried out exhaustive tests. Our usability tests show that adaptation increases both user performance and user satisfaction. We describe the tests that were performed, and the methods used to assess the test results. One of these methods is a modification of PARADISE, a framework for evaluating the performance of SDSs [Walker MA, Litman DJ, Kamm CA, Abella A (Comput Speech Lang 12(3):317–347, 1998)]. We discuss its drawbacks for the evaluation of SDSs like ours, the modifications we have carried out, and the test results.","Language Resources and Evaluation",2006,"No","adapt evalu adapt automot dialogu system user expertis evalu system spoken dialogu system sdss oper devic automot environ peopl system level experi system account paper present method build dialogu system automot environ automat adapt user experi system implement adapt prototyp carri exhaust test usabl test show adapt increas user perform user satisfact describ test perform method assess test result method modif paradis framework evalu perform sdss walker ma litman dj kamm ca abella comput speech lang discuss drawback evalu sdss modif carri test result",0
NA,"a statistical study of authorship in the corpus lysiacum",NA,"Computers and the Humanities",1982,"No","statist studi authorship corpus lysiacum na",0
"KeywordsAnnotated corpora Meetings Discourse annotation ","unleashing the killer corpus experiences in creating the multi everything ami meeting corpus","The AMI Meeting Corpus contains 100 h of meetings captured using many synchronized recording devices, and is designed to support work in speech and video processing, language engineering, corpus linguistics, and organizational psychology. It has been transcribed orthographically, with annotated subsets for everything from named entities, dialogue acts, and summaries to simple gaze and head movement. In this written version of an LREC conference keynote address, I describe the data and how it was created. If this is “killer” data, that presupposes a platform that it will “sell”; in this case, that is the NITE XML Toolkit, which allows a distributed set of users to create, store, browse, and search annotations for the same base data that are both time-aligned against signal and related to each other structurally.","Language Resources and Evaluation",2007,"No","annot corpora meet discours annot unleash killer corpus experi creat multi ami meet corpus ami meet corpus meet captur synchron record devic design support work speech video process languag engin corpus linguist organiz psycholog transcrib orthograph annot subset name entiti dialogu act summari simpl gaze head movement written version lrec confer keynot address describ data creat killer data presuppos platform sell case nite xml toolkit distribut set user creat store brows search annot base data time align signal relat structur",0
"KeywordsComputational Linguistic Syntactic Analysis ","choice of grammatical word class without global syntactic analysis tagging words in the lob corpus",NA,"Computers and the Humanities",1983,"No","comput linguist syntact analysi choic grammat word class global syntact analysi tag word lob corpus na",0
"Key wordsbibliographic records electronic texts electronic title page large corpora standard generalized markup language SGML text encoding initiative TEI ","practical considerations in the use of tei headers in a large corpus","Many aspects of the guidelines of the Text Encoding Initiative (TEI) are applicable to corpora and text collections, and to the texts that these contain. As the first large corpus developed using mark-up conforming to the guidelines, the British National Corpus (BNC) is a test-bed for many TEI-developed mechanisms. This is particularly true in the case of the TEI header, which has three intended applications — to describe a corpus, to describe an individual text, and as a free-standing bibliographic record — all of them used by the BNC. This paper describes the application of the TEI header to the BNC. It is intended that this information should, through a description of experience on a practical project, serve as a guide for those wishing to use TEI headers in the documentation and management of other corpora and collections of texts.","Computers and the Humanities",1995,"No","key wordsbibliograph record electron text electron titl page larg corpora standard general markup languag sgml text encod initi tei practic consider tei header larg corpus aspect guidelin text encod initi tei applic corpora text collect text larg corpus develop mark conform guidelin british nation corpus bnc test bed tei develop mechan true case tei header intend applic describ corpus describ individu text free stand bibliograph record bnc paper describ applic tei header bnc intend inform descript experi practic project serv guid wish tei header document manag corpora collect text",0
"computer implementation Estonian language engineering morphology text corpora ","an estonian morphological analyser and the impact of a corpus on its development","The paper describes a morphological analyser forEstonian and how using a text corpus influenced theprocess of creating it and the resulting programitself. The influence is not limited to the lexicononly, but is also noticeable in the resulting algorithm andimplementation too. When work on the analyser began,there were no computational treatment of Estonianderivatives and compounds. After some cycles ofdevelopment and testing on the corpus, we came up withan acceptable algorithm for their treatment. Both themorphological analyser and the speller based on ithave been successfully marketed.","Computers and the Humanities",1997,"No","comput implement estonian languag engin morpholog text corpora estonian morpholog analys impact corpus develop paper describ morpholog analys forestonian text corpus influenc theprocess creat result programitself influenc limit lexiconon notic result algorithm andimplement work analys began comput treatment estonianderiv compound cycl ofdevelop test corpus withan accept algorithm treatment themorpholog analys speller base ithav success market",0
"Key Wordscontext meaning sense discrimination ambiguity polysemy bilingual ","a method for disambiguating word senses in a large corpus","Word sense disambiguation has been recognized as a major problem in natural language processing research for over forty years. Both quantitive and qualitative methods have been tried, but much of this work has been stymied by difficulties in acquiring appropriate lexical resources. The availability of this testing and training material has enabled us to develop quantitative disambiguation methods that achieve 92% accuracy in discriminating between two very distinct senses of a noun. In the training phase, we collect a number of instances of each sense of the polysemous noun. Then in the testing phase, we are given a new instance of the noun, and are asked to assign the instance to one of the senses. We attempt to answer this question by comparing the context of the unknown instance with contexts of known instances using a Bayesian argument that has been applied successfully in related tasks such as author identification and information retrieval. The proposed method is probably most appropriate for those aspects of sense disambiguation that are closest to the information retrieval task. In particular, the proposed method was designed to disambiguate senses that are usually associated with different topics.","Computers and the Humanities",1992,"No","key wordscontext mean sens discrimin ambigu polysemi bilingu method disambigu word sens larg corpus word sens disambigu recogn major problem natur languag process research forti year quantit qualit method work stymi difficulti acquir lexic resourc avail test train materi enabl develop quantit disambigu method achiev accuraci discrimin distinct sens noun train phase collect number instanc sens polysem noun test phase instanc noun ask assign instanc sens attempt answer question compar context unknown instanc context instanc bayesian argument appli success relat task author identif inform retriev propos method aspect sens disambigu closest inform retriev task propos method design disambigu sens topic",0
"KeywordsComputational Linguistic ","formulaic analysis of the computer accessible corpus of latvian sun songs",NA,"Computers and the Humanities",1978,"No","comput linguist formula analysi comput access corpus latvian sun song na",0
"collocation concordance lines language independent software lexical statistics ","language independent statistical software for corpus exploration","In this report two programs for statistical analysis of concordance lines are described. The programs have been developed for analyzing he lexical context of a given word. It is shown how different parameter settings influence the outcome of collocational analysis, and how the concept of collocation can be extended to allow the extraction of lines typical for a word from a set of concordance lines. Even though all the examples are for English, the software is completely language independent and only requires minimal linguistic resources.","Computers and the Humanities",1997,"No","colloc concord line languag independ softwar lexic statist languag independ statist softwar corpus explor report program statist analysi concord line program develop analyz lexic context word shown paramet set influenc outcom colloc analysi concept colloc extend extract line typic word set concord line exampl english softwar complet languag independ requir minim linguist resourc",0
"Key WordsCoptic Coptic literature databases text manipulation non-Roman characters ","the corpus dei manoscritti copti letterari","The Corpus dei Manoscritti Copti Letterari is a project whose original aim was to reconstruct the Coptic codices from the White Monastery in Upper Egypt. The project was later expanded to include all Coptic literature. In 1980 a new project was launched to transfer the data into machine-readable form and make the information available, in as generic a format as possible, to scholars throughout the world.","Computers and the Humanities",1990,"No","key wordscopt coptic literatur databas text manipul roman charact corpus dei manoscritti copti letterari corpus dei manoscritti copti letterari project origin aim reconstruct coptic codic white monasteri upper egypt project expand includ coptic literatur project launch transfer data machin readabl form make inform generic format scholar world",0
"KeywordsComputational Linguistic Questionnaire Data Rural Dialect ","the were subjunctive in british rural dialects marrying corpus and questionnaire data",NA,"Computers and the Humanities",2003,"No","comput linguist questionnair data rural dialect subjunct british rural dialect marri corpus questionnair data na",0
"KeywordsComputational Linguistic French Corpus ","computational linguistics and statistics in the analysis of the montreal french corpus",NA,"Computers and the Humanities",1977,"No","comput linguist french corpus comput linguist statist analysi montreal french corpus na",0
"KeywordsWord Frequency Computational Linguistic Text Type English Text ","word frequency and text type some observations based on the lob corpus of british english texts",NA,"Computers and the Humanities",1985,"No","word frequenc comput linguist text type english text word frequenc text type observ base lob corpus british english text na",0
"KeywordsLearner translation corpus Multiple translation corpus LTC-UPF Error-annotation English–Catalan translation Translator training ","the upf learner translation corpus as a resource for translator training","The learner translation corpus developed at the School of Translation and Interpreting of Pompeu Fabra University in Barcelona is a web-searchable resource created for pedagogical and research purposes. It comprises a multiple translation corpus (English–Catalan) featuring automatic linguistic annotation and manual error annotation, complemented with an interface for monolingual or bilingual querying of the data. The corpus can be used to identify common errors in the students’ work and to analyse their patterns of language use. It provides easy access to error samples and to multiple versions of the same source text sequence to be used as learning materials in various courses in the translator-training university curriculum.","Language Resources and Evaluation",2014,"No","learner translat corpus multipl translat corpus ltc upf error annot english catalan translat translat train upf learner translat corpus resourc translat train learner translat corpus develop school translat interpret pompeu fabra univers barcelona web searchabl resourc creat pedagog research purpos compris multipl translat corpus english catalan featur automat linguist annot manual error annot complement interfac monolingu bilingu queri data corpus identifi common error student work analys pattern languag easi access error sampl multipl version sourc text sequenc learn materi cours translat train univers curriculum",0
NA,"marianne hund nadja nesselhauf and carolin biewer corpus linguistics and the web","In 2003, the special issue of “Computational linguistics” (September, 29, 3) dedicated to the Web as corpus and edited by Adam Kilgarriff and Gregory Grefenstette was a landmark event for a promising field of study. Today, this book makes for a fine update, even if it is more limited in scope than its predecessor and less recent in its content than its date of publication would lead to believe. The articles included are in fact partially “based on papers presented at the symposium Corpus linguistics—Perspectives for the Future held (…) in Heidelberg in October 2004” (p. 4). However, the editors state that some of the articles were commissioned later, and many of the texts have in fact been brought up to date to take recent developments into account.","Language Resources and Evaluation",2010,"No","mariann hund nadja nesselhauf carolin biewer corpus linguist web special issu comput linguist septemb dedic web corpus edit adam kilgarriff gregori grefenstett landmark event promis field studi today book make fine updat limit scope predecessor recent content date public lead articl includ fact partial base paper present symposium corpus linguist perspect futur held heidelberg octob editor state articl commiss text fact brought date recent develop account",0
"KeywordsSpoken language Discourse Recordings Transcription conventions Web concordancer ","compilation transcription and usage of a reference speech corpus the case of the slovene corpus gos","In recent years, building reference speech corpora was an important part of the activities which provided the necessary linguistic infrastructure in many European countries, for languages with many speakers (e.g., French, German, Spanish, Italian) as well as for those with smaller numbers of speakers (e.g., Swedish, Dutch, Czech, Slovak). This paper describes the process of the creation of a reference speech corpus and its distribution to potential users, as it was done in the case of the Slovene corpus GOS. The corpus structure and fieldwork experiences with recording, labelling system, and two levels of transcription (pronunciation-based and standardized) are described, as well as the main characteristics of the corpus interface (web concordancer) and the availability of the original corpus files.","Language Resources and Evaluation",2013,"No","spoken languag discours record transcript convent web concordanc compil transcript usag refer speech corpus case sloven corpus gos recent year build refer speech corpora import part activ provid linguist infrastructur european countri languag speaker french german spanish italian smaller number speaker swedish dutch czech slovak paper describ process creation refer speech corpus distribut potenti user case sloven corpus gos corpus structur fieldwork experi record label system level transcript pronunci base standard main characterist corpus interfac web concordanc avail origin corpus file",0
"KeywordsArabic corpora Corpus evaluation Corpus design Corpus compilation Arabic language resources Natural language processing (NLP) ","a 700m arabic corpus kacst arabic corpus design and construction","Compared with English, Arabic is a poorly-resourced language within the field of corpus linguistics. A lack of sufficient data and research has negatively affected Arabic corpus-based researchers and natural language processing practitioners. Although a number of Arabic corpora have been developed in recent years, the overall situation has improved little. The aim of this paper is twofold. First, it reviews 14 Arabic corpora categorized by their designated purpose, target language, mode of text, size, text date, location, text type/medium, text domain, representativeness, and balance. The review also describes the availability of the reviewed corpora, the presence of tokenization, lemmatization and tagging, and whether there are any tools available to search and explore them. Second, it introduces the King Abdulaziz City for Science and Technology (KACST) Arabic corpus, which was designed and created to overcome the limitations of existing Arabic corpora. The KACST Arabic corpus is a large and diverse Arabic corpus with clearly defined design criteria. It is carefully sampled, and its contents are classified based on time, region, medium, domain, and topic, and it can be searched and explored using these classifications. The KACST Arabic corpus comprises more than 700 million words from the pre-Islamic era to the present day (a period covering more than 1,500 years), collected from 10 diverse mediums. Each text has been further classified more specifically into domains and topics. The KACST Arabic corpus is freely available to explore on the Internet (http://www.kacstac.org.sa) using a variety of tools.","Language Resources and Evaluation",2015,"No","arab corpora corpus evalu corpus design corpus compil arab languag resourc natur languag process nlp m arab corpus kacst arab corpus design construct compar english arab poor resourc languag field corpus linguist lack suffici data research negat affect arab corpus base research natur languag process practition number arab corpora develop recent year situat improv aim paper twofold review arab corpora categor design purpos target languag mode text size text date locat text typemedium text domain repres balanc review describ avail review corpora presenc token lemmat tag tool search explor introduc king abdulaziz citi scienc technolog kacst arab corpus design creat overcom limit exist arab corpora kacst arab corpus larg divers arab corpus defin design criteria care sampl content classifi base time region medium domain topic search explor classif kacst arab corpus compris million word pre islam era present day period cover year collect divers medium text classifi specif domain topic kacst arab corpus freeli explor internet httpwwwkacstacorgsa varieti tool",0
"KeywordsSpeech corpus Alcohol detection Intoxication Speaker features and forensic phonetics ","alcohol language corpus the first public corpus of alcoholized german speech","The Alcohol Language Corpus (ALC) is the first publicly available speech corpus comprising intoxicated and sober speech of 162 female and male German speakers. Recordings are done in the automotive environment to allow for the development of automatic alcohol detection and to ensure a consistent acoustic environment for the alcoholized and the sober recording. The recorded speech covers a variety of contents and speech styles. Breath and blood alcohol concentration measurements are provided for all speakers. A transcription according to SpeechDat/Verbmobil standards and disfluency tagging as well as an automatic phonetic segmentation are part of the corpus. An Emu version of ALC allows easy access to basic speech parameters as well as the us of R for statistical analysis of selected parts of ALC. ALC is available without restriction for scientific or commercial use at the Bavarian Archive for Speech Signals.","Language Resources and Evaluation",2012,"No","speech corpus alcohol detect intox speaker featur forens phonet alcohol languag corpus public corpus alcohol german speech alcohol languag corpus alc public speech corpus compris intox sober speech femal male german speaker record automot environ develop automat alcohol detect ensur consist acoust environ alcohol sober record record speech cover varieti content speech style breath blood alcohol concentr measur provid speaker transcript speechdatverbmobil standard disfluenc tag automat phonet segment part corpus emu version alc easi access basic speech paramet statist analysi select part alc alc restrict scientif commerci bavarian archiv speech signal",0
"KeywordsParallel corpus Swahili English Machine translation Projection of annotation African language technology ","exploring the sawa corpus collection and deployment of a parallel corpus englishswahili","Research in machine translation and corpus annotation has greatly benefited from the increasing availability of word-aligned parallel corpora. This paper presents ongoing research on the development and application of the sawa corpus, a two-million-word parallel corpus English—Swahili. We describe the data collection phase and zero in on the difficulties of finding appropriate and easily accessible data for this language pair. In the data annotation phase, the corpus was semi-automatically sentence and word-aligned and morphosyntactic information was added to both the English and Swahili portion of the corpus. The annotated parallel corpus allows us to investigate two possible uses. We describe experiments with the projection of part-of-speech tagging annotation from English onto Swahili, as well as the development of a basic statistical machine translation system for this language pair, using the parallel corpus and a consolidated database of existing English—Swahili translation dictionaries. We particularly focus on the difficulties of translating English into the morphologically more complex Bantu language of Swahili.","Language Resources and Evaluation",2011,"No","parallel corpus swahili english machin translat project annot african languag technolog explor sawa corpus collect deploy parallel corpus englishswahili research machin translat corpus annot great benefit increas avail word align parallel corpora paper present ongo research develop applic sawa corpus million word parallel corpus english swahili describ data collect phase difficulti find easili access data languag pair data annot phase corpus semi automat sentenc word align morphosyntact inform ad english swahili portion corpus annot parallel corpus investig describ experi project part speech tag annot english swahili develop basic statist machin translat system languag pair parallel corpus consolid databas exist english swahili translat dictionari focus difficulti translat english morpholog complex bantu languag swahili",0
"KeywordsRST Signalling Corpus RST Discourse Treebank Coherence relations Rhetorical Structure Theory Signals Discourse markers ","rst signalling corpus a corpus of signals of coherence relations","We present the RST Signalling Corpus (Das et al. in RST signalling corpus, LDC2015T10. https://catalog.ldc.upenn.edu/LDC2015T10, 2015), a corpus annotated for signals of coherence relations. The corpus is developed over the RST Discourse Treebank (Carlson et al. in RST Discourse Treebank, LDC2002T07. https://catalog.ldc.upenn.edu/LDC2002T07, 2002) which is annotated for coherence relations. In the RST Signalling Corpus, these relations are further annotated with signalling information. The corpus includes annotation not only for discourse markers which are considered to be the most typical (or sometimes the only type of) signals in discourse, but also for a wide array of other signals such as reference, lexical, semantic, syntactic, graphical and genre features as potential indicators of coherence relations. We describe the research underlying the development of the corpus and the annotation process, and provide details of the corpus. We also present the results of an inter-annotator agreement study, illustrating the validity and reproducibility of the annotation. The corpus is available through the Linguistic Data Consortium, and can be used to investigate the psycholinguistic mechanisms behind the interpretation of relations through signalling, and also to develop discourse-specific computational systems such as discourse parsing applications.","Language Resources and Evaluation",2018,"No","rst signal corpus rst discours treebank coher relat rhetor structur theori signal discours marker rst signal corpus corpus signal coher relat present rst signal corpus das al rst signal corpus ldct httpscatalogldcupennldct corpus annot signal coher relat corpus develop rst discours treebank carlson al rst discours treebank ldct httpscatalogldcupennldct annot coher relat rst signal corpus relat annot signal inform corpus includ annot discours marker consid typic type signal discours wide array signal refer lexic semant syntact graphic genr featur potenti indic coher relat describ research under develop corpus annot process provid detail corpus present result inter annot agreement studi illustr valid reproduc annot corpus linguist data consortium investig psycholinguist mechan interpret relat signal develop discours specif comput system discours pars applic",0
"KeywordsSpoken dialogue corpora Spoken dialogue systems Cognitive ageing Annotation Information states Speech acts User simulations Speech recognition ","the match corpus a corpus of older and younger users interactions with spoken dialogue systems","We present the MATCH corpus, a unique data set of 447 dialogues in which 26 older and 24 younger adults interact with nine different spoken dialogue systems. The systems varied in the number of options presented and the confirmation strategy used. The corpus also contains information about the users’ cognitive abilities and detailed usability assessments of each dialogue system. The corpus, which was collected using a Wizard-of-Oz methodology, has been fully transcribed and annotated with dialogue acts and “Information State Update” (ISU) representations of dialogue context. Dialogue act and ISU annotations were performed semi-automatically. In addition to describing the corpus collection and annotation, we present a quantitative analysis of the interaction behaviour of older and younger users and discuss further applications of the corpus. We expect that the corpus will provide a key resource for modelling older people’s interaction with spoken dialogue systems.","Language Resources and Evaluation",2010,"No","spoken dialogu corpora spoken dialogu system cognit age annot inform state speech act user simul speech recognit match corpus corpus older younger user interact spoken dialogu system present match corpus uniqu data set dialogu older younger adult interact spoken dialogu system system vari number option present confirm strategi corpus inform user cognit abil detail usabl assess dialogu system corpus collect wizard oz methodolog fulli transcrib annot dialogu act inform state updat isu represent dialogu context dialogu act isu annot perform semi automat addit describ corpus collect annot present quantit analysi interact behaviour older younger user discuss applic corpus expect corpus provid key resourc model older peopl interact spoken dialogu system",0
"KeywordsSMS corpus Corpus creation English Chinese Crowdsourcing Mechanical turk Zhubajie ","creating a live public short message service corpus the nus sms corpus","Short Message Service (SMS) messages are short messages sent from one person to another from their mobile phones. They represent a means of personal communication that is an important communicative artifact in our current digital era. As most existing studies have used private access to SMS corpora, comparative studies using the same raw SMS data have not been possible up to now. We describe our efforts to collect a public SMS corpus to address this problem. We use a battery of methodologies to collect the corpus, paying particular attention to privacy issues to address contributors’ concerns. Our live project collects new SMS message submissions, checks their quality, and adds valid messages. We release the resultant corpus as XML and as SQL dumps, along with monthly corpus statistics. We opportunistically collect as much metadata about the messages and their senders as possible, so as to enable different types of analyses. To date, we have collected more than 71,000 messages, focusing on English and Mandarin Chinese.","Language Resources and Evaluation",2013,"No","sms corpus corpus creation english chines crowdsourc mechan turk zhubaji creat live public short messag servic corpus nus sms corpus short messag servic sms messag short messag person mobil phone repres mean person communic import communic artifact current digit era exist studi privat access sms corpora compar studi raw sms data describ effort collect public sms corpus address problem batteri methodolog collect corpus pay attent privaci issu address contributor concern live project collect sms messag submiss check qualiti add valid messag releas result corpus xml sql dump month corpus statist opportunist collect metadata messag sender enabl type analys date collect messag focus english mandarin chines",0
"KeywordsSemantic annotations Personal health information Inter-annotator agreement Clinical narrative ","a french clinical corpus with comprehensive semantic annotations development of the medical entity and relation limsi annotated text corpus merlot","Quality annotated resources are essential for Natural Language Processing. The objective of this work is to present a corpus of clinical narratives in French annotated for linguistic, semantic and structural information, aimed at clinical information extraction. Six annotators contributed to the corpus annotation, using a comprehensive annotation scheme covering 21 entities, 11 attributes and 37 relations. All annotators trained on a small, common portion of the corpus before proceeding independently. An automatic tool was used to produce entity and attribute pre-annotations. About a tenth of the corpus was doubly annotated and annotation differences were resolved in consensus meetings. To ensure annotation consistency throughout the corpus, we devised harmonization tools to automatically identify annotation differences to be addressed to improve the overall corpus quality. The annotation project spanned over 24 months and resulted in a corpus comprising 500 documents (148,476 tokens) annotated with 44,740 entities and 26,478 relations. The average inter-annotator agreement is 0.793 F-measure for entities and 0.789 for relations. The performance of the pre-annotation tool for entities reached 0.814 F-measure when sufficient training data was available. The performance of our entity pre-annotation tool shows the value of the corpus to build and evaluate information extraction methods. In addition, we introduced harmonization methods that further improved the quality of annotations in the corpus.","Language Resources and Evaluation",2018,"No","semant annot person health inform inter annot agreement clinic narrat french clinic corpus comprehens semant annot develop medic entiti relat limsi annot text corpus merlot qualiti annot resourc essenti natur languag process object work present corpus clinic narrat french annot linguist semant structur inform aim clinic inform extract annot contribut corpus annot comprehens annot scheme cover entiti attribut relat annot train small common portion corpus proceed independ automat tool produc entiti attribut pre annot tenth corpus doubli annot annot differ resolv consensus meet ensur annot consist corpus devis harmon tool automat identifi annot differ address improv corpus qualiti annot project span month result corpus compris document token annot entiti relat averag inter annot agreement measur entiti relat perform pre annot tool entiti reach measur suffici train data perform entiti pre annot tool show corpus build evalu inform extract method addit introduc harmon method improv qualiti annot corpus",0
"KeywordsLearner corpus Textual revision Feedback English as a second language Multi-layer corpus annotation Corpus search and visualization ","cityu corpus of essay drafts of english language learners a corpus of textual revision in second language writing","
Learner corpora consist of texts produced by non-native speakers. In addition to these texts, some learner corpora also contain error annotations, which can reveal common errors made by language learners, and provide training material for automatic error correction. We present a novel type of error-annotated learner corpus containing sequences of revised essay drafts written by non-native speakers of English. Sentences in these drafts are annotated with comments by language tutors, and are aligned to sentences in subsequent drafts. We describe the compilation process of our corpus, present its encoding in TEI XML, and report agreement levels on the error annotations. Further, we demonstrate the potential of the corpus to facilitate research on textual revision in L2 writing, by conducting a case study on verb tenses using ANNIS, a corpus search and visualization platform.","Language Resources and Evaluation",2015,"No","learner corpus textual revis feedback english languag multi layer corpus annot corpus search visual cityu corpus essay draft english languag learner corpus textual revis languag write learner corpora consist text produc nativ speaker addit text learner corpora error annot reveal common error made languag learner provid train materi automat error correct present type error annot learner corpus sequenc revis essay draft written nativ speaker english sentenc draft annot comment languag tutor align sentenc subsequ draft describ compil process corpus present encod tei xml report agreement level error annot demonstr potenti corpus facilit research textual revis l write conduct case studi verb tens anni corpus search visual platform",0
"KeywordsDiachronic corpus Historical Spanish Linguistic annotation TEI ","an open diachronic corpus of historical spanish","The impact-es diachronic corpus of historical Spanish compiles over one hundred books—containing approximately 8 million words—in addition to a complementary lexicon which links more than 10,000 lemmas with attestations of the different variants found in the documents. This textual corpus and the accompanying lexicon have been released under an open license (Creative Commons by-nc-sa) in order to permit their intensive exploitation in linguistic research. Approximately 7 % of the words in the corpus (a selection aimed at enhancing the coverage of the most frequent word forms) have been annotated with their lemma, part of speech, and modern equivalent. This paper describes the annotation criteria followed and the standards, based on the Text Encoding Initiative recommendations, used to represent the texts in digital form.","Language Resources and Evaluation",2013,"No","diachron corpus histor spanish linguist annot tei open diachron corpus histor spanish impact es diachron corpus histor spanish compil hundr book approxim million word addit complementari lexicon link lemma attest variant found document textual corpus accompani lexicon releas open licens creativ common nc sa order permit intens exploit linguist research approxim word corpus select aim enhanc coverag frequent word form annot lemma part speech modern equival paper describ annot criteria standard base text encod initi recommend repres text digit form",0
"KeywordsAutomatic speech recognition Slavic languages Polish Speech corpus Text to speech ","agh corpus of polish speech","A corpus of Polish speech, which has been collected for the purpose of automatic speech recognition (ASR) and text-to-speech (TTS) systems applications, is presented. The corpus consists of several groups of recordings: read sentences, spoken commands, a phonetically balanced TTS training corpus, telephonic speech and others. In summary duration of recordings is above 25 h. Number of unique speakers amounts to 166. The majority of them being in an age group of 20–35 and one third of them being female.
 Analysis of unique word occurrence frequency in relation to larger text resources has been concluded. From them, most commonly appearing words have been found and presented. The corpus was used as training data for the ASR system. Results of cross-validation training and testing the SARMATA ASR system using our corpus have shown that phrase recognition rate is 91.9 %. The corpus was additionally evaluated in comparative test against the CORPORA corpus, which had shown major increase in phrase recognition rate in favour of our corpus.","Language Resources and Evaluation",2016,"No","automat speech recognit slavic languag polish speech corpus text speech agh corpus polish speech corpus polish speech collect purpos automat speech recognit asr text speech tts system applic present corpus consist group record read sentenc spoken command phonet balanc tts train corpus telephon speech summari durat record number uniqu speaker amount major age group femal analysi uniqu word occurr frequenc relat larger text resourc conclud common appear word found present corpus train data asr system result cross valid train test sarmata asr system corpus shown phrase recognit rate corpus addit evalu compar test corpora corpus shown major increas phrase recognit rate favour corpus",0
"KeywordsCorpus linguistics Multilingual Annotation People name disambiguation ","mc4weps a multilingual corpus for web people search disambiguation","This article introduces the MC4WEPS corpus, a new resource for evaluating Web people search disambiguation tasks, and describes its design, collection and annotation process, the agreement between the different annotators, and finally introduces a baseline evaluation. This corpus is built by compiling multilingual search engines results where the queries are person names. Proper noun disambiguation is an open problem in natural language ambiguity resolution and, specifically, resolving the ambiguity of person names in Web search results is still a challenging problem. However, state-of-the-art approaches have been evaluated only with monolingual web page collections. The MC4WEPS corpus aims to provide the research community with a reference corpus for the task of disambiguating search engine results where the query is a person name shared by homonymous individuals. The features of this new corpus stand out from existing corpora for the same task, namely multilingualism and inclusion of social networking websites. These characteristics make it more representative of a real search scenario, especially for evaluating person name disambiguation in a multilingual context. The article also includes detailed information about the format and the availability of the corpus.","Language Resources and Evaluation",2017,"No","corpus linguist multilingu annot peopl disambigu mcwep multilingu corpus web peopl search disambigu articl introduc mcwep corpus resourc evalu web peopl search disambigu task describ design collect annot process agreement annot final introduc baselin evalu corpus built compil multilingu search engin result queri person name proper noun disambigu open problem natur languag ambigu resolut specif resolv ambigu person name web search result challeng problem state art approach evalu monolingu web page collect mcwep corpus aim provid research communiti refer corpus task disambigu search engin result queri person share homonym individu featur corpus stand exist corpora task multilingu inclus social network websit characterist make repres real search scenario evalu person disambigu multilingu context articl includ detail inform format avail corpus",0
"KeywordsLearner corpus Error annotation Czech ","building a learner corpus","The need for data about the acquisition of Czech by non-native learners prompted the compilation of the first learner corpus of Czech. After introducing its basic design and parameters, including a multi-tier manual annotation scheme and error taxonomy, we focus on the more technical aspects: the transcription of hand-written source texts, process of annotation, and options for exploiting the result, together with tools used for these tasks and decisions behind the choices. To support or even substitute manual annotation we assign some error tags automatically and use automatic annotation tools (tagger, spell checker).","Language Resources and Evaluation",2014,"No","learner corpus error annot czech build learner corpus data acquisit czech nativ learner prompt compil learner corpus czech introduc basic design paramet includ multi tier manual annot scheme error taxonomi focus technic aspect transcript hand written sourc text process annot option exploit result tool task decis choic support substitut manual annot assign error tag automat automat annot tool tagger spell checker",0
"KeywordsCorpus Evidence based medicine Annotation Crowdsourcing Text summarization ","a corpus for research in text processing for evidence based medicine","Evidence based medicine (EBM) urges the medical doctor to incorporate the latest available clinical evidence at point of care. A major stumbling block in the practice of EBM is the difficulty to keep up to date with the clinical advances. In this paper we describe a corpus designed for the development and testing of text processing tools for EBM, in particular for tasks related to the extraction and summarisation of answers and corresponding evidence related to a clinical query. The corpus is based on material from the Clinical Inquiries section of The Journal of Family Practice. It was gathered and annotated by a combination of automated information extraction, crowdsourcing tasks, and manual annotation. It has been used for the original summarisation task for which it was designed, as well as for other related tasks such as the appraisal of clinical evidence and the clustering of the results. The corpus is available at SourceForge (http://sourceforge.net/projects/ebmsumcorpus/).","Language Resources and Evaluation",2016,"No","corpus evid base medicin annot crowdsourc text summar corpus research text process evid base medicin evid base medicin ebm urg medic doctor incorpor latest clinic evid point care major stumbl block practic ebm difficulti date clinic advanc paper describ corpus design develop test text process tool ebm task relat extract summaris answer evid relat clinic queri corpus base materi clinic inquiri section journal famili practic gather annot combin autom inform extract crowdsourc task manual annot origin summaris task design relat task apprais clinic evid cluster result corpus sourceforg httpsourceforgenetprojectsebmsumcorpus",0
"KeywordsLexicon PropBank/VerbNet Semantic roles Predicate labelling Valence ","how the corpus based basque verb index lexicon was built","This article describes the method used to build the Basque Verb Index (BVI), a corpus-based lexicon. The BVI is the result of semiautomatic annotation of the EPEC corpus with verb predicate information, following the PropBank-VerbNet model. The method presented is the product of a deep study of the syntactic–semantic behaviour of verbs in EPEC-RolSem (the EPEC corpus tagged with verb predicate information). During the process of annotating EPEC-RolSem, we have identified and stored in the BVI lexicon the different role-patterns associated with all verbs appearing in the corpus. In addition, each entry in the BVI is linked to the corresponding verb entry in well-known resources such as PropBank, VerbNet, WordNet and FrameNet. We have also implemented a tool called e-ROLda to facilitate the process of looking up verb patterns in the BVI and examples in EPEC-RolSem as a basis for future studies.","Language Resources and Evaluation",2018,"No","lexicon propbankverbnet semant role predic label valenc corpus base basqu verb index lexicon built articl describ method build basqu verb index bvi corpus base lexicon bvi result semiautomat annot epec corpus verb predic inform propbank verbnet model method present product deep studi syntact semant behaviour verb epec rolsem epec corpus tag verb predic inform process annot epec rolsem identifi store bvi lexicon role pattern verb appear corpus addit entri bvi link verb entri resourc propbank verbnet wordnet framenet implement tool call rolda facilit process verb pattern bvi exampl epec rolsem basi futur studi",0
"KeywordsMulti-modal corpus Referring expressions Collaborative task Japanese ","rex j japanese referring expression corpus of situated dialogs","Identifying objects in conversation is a fundamental human capability necessary to achieve efficient collaboration on any real world task. Hence the deepening of our understanding of human referential behaviour is indispensable for the creation of systems that collaborate with humans in a meaningful way. We present the construction of REX-J, a multi-modal Japanese corpus of referring expressions in situated dialogs, based on the collaborative task of solving the Tangram puzzle. This corpus contains 24 dialogs with over 4 h of recordings and over 1,400 referring expressions. We outline the characteristics of the collected data and point out the important differences from previous corpora. The corpus records extra-linguistic information during the interaction (e.g. the position of pieces, the actions on the pieces) in synchronization with the participants’ utterances. This in turn allows us to discuss the importance of creating a unified model of linguistic and extra-linguistic information from a new perspective. Demonstrating the potential uses of this corpus, we present the analysis of a specific type of referring expression (“action-mentioning expression”) as well as the results of research into the generation of demonstrative pronouns. Furthermore, we discuss some perspectives on potential uses of this corpus as well as our planned future work, underlining how it is a valuable addition to the existing databases in the community for the study and modeling of referring expressions in situated dialog.","Language Resources and Evaluation",2012,"No","multi modal corpus refer express collabor task japanes rex japanes refer express corpus situat dialog identifi object convers fundament human capabl achiev effici collabor real world task deepen understand human referenti behaviour indispens creation system collabor human meaning present construct rex multi modal japanes corpus refer express situat dialog base collabor task solv tangram puzzl corpus dialog record refer express outlin characterist collect data point import differ previous corpora corpus record extra linguist inform interact posit piec action piec synchron particip utter turn discuss import creat unifi model linguist extra linguist inform perspect demonstr potenti corpus present analysi specif type refer express action mention express result research generat demonstr pronoun discuss perspect potenti corpus plan futur work underlin valuabl addit exist databas communiti studi model refer express situat dialog",0
"KeywordsProsodic corpus Radio news corpus Dialogue corpus Spanish corpus Catalan corpus ","glissando a corpus for multidisciplinary prosodic studies in spanish and catalan","Literature review on prosody reveals the lack of corpora for prosodic studies in Catalan and Spanish. In this paper, we present a corpus intended to fill this gap. The corpus comprises two distinct data-sets, a news subcorpus and a dialogue subcorpus, the latter containing either conversational or task-oriented speech. More than 25 h were recorded by twenty eight speakers per language. Among these speakers, eight were professional (four radio news broadcasters and four advertising actors). The entire material presented here has been transcribed, aligned with the acoustic signal and prosodically annotated. Two major objectives have guided the design of this project: (i) to offer a wide coverage of representative real-life communicative situations which allow for the characterization of prosody in these two languages; and (ii) to conduct research studies which enable us to contrast the speakers different speaking styles and discursive practices. All material contained in the corpus is provided under a Creative Commons Attribution 3.0 Unported License.","Language Resources and Evaluation",2013,"No","prosod corpus radio news corpus dialogu corpus spanish corpus catalan corpus glissando corpus multidisciplinari prosod studi spanish catalan literatur review prosodi reveal lack corpora prosod studi catalan spanish paper present corpus intend fill gap corpus compris distinct data set news subcorpus dialogu subcorpus convers task orient speech record twenti speaker languag speaker profession radio news broadcast advertis actor entir materi present transcrib align acoust signal prosod annot major object guid design project offer wide coverag repres real life communic situat character prosodi languag ii conduct research studi enabl contrast speaker speak style discurs practic materi contain corpus provid creativ common attribut unport licens",0
"KeywordsData-driven generation Embodied conversational agents Evaluation of generated output Multimodal corpora ","corpus based generation of head and eyebrow motion for an embodied conversational agent","Humans are known to use a wide range of non-verbal behaviour while speaking. Generating naturalistic embodied speech for an artificial agent is therefore an application where techniques that draw directly on recorded human motions can be helpful. We present a system that uses corpus-based selection strategies to specify the head and eyebrow motion of an animated talking head. We first describe how a domain-specific corpus of facial displays was recorded and annotated, and outline the regularities that were found in the data. We then present two different methods of selecting motions for the talking head based on the corpus data: one that chooses the majority option in all cases, and one that makes a weighted choice among all of the options. We compare these methods to each other in two ways: through cross-validation against the corpus, and by asking human judges to rate the output. The results of the two evaluation studies differ: the cross-validation study favoured the majority strategy, while the human judges preferred schedules generated using weighted choice. The judges in the second study also showed a preference for the original corpus data over the output of either of the generation strategies.","Language Resources and Evaluation",2007,"No","data driven generat embodi convers agent evalu generat output multimod corpora corpus base generat head eyebrow motion embodi convers agent human wide rang verbal behaviour speak generat naturalist embodi speech artifici agent applic techniqu draw direct record human motion help present system corpus base select strategi head eyebrow motion anim talk head describ domain specif corpus facial display record annot outlin regular found data present method select motion talk head base corpus data choos major option case make weight choic option compar method way cross valid corpus human judg rate output result evalu studi differ cross valid studi favour major strategi human judg prefer schedul generat weight choic judg studi show prefer origin corpus data output generat strategi",0
"KeywordsText corpora Corpus annotation Emotional ontology Emotional categories Emotional dimensions ","emotales creating a corpus of folk tales with emotional annotations","Emotions are inherent to any human activity, including human–computer interactions, and that is the reason why recognizing emotions expressed in natural language is becoming a key feature for the design of more natural user interfaces. In order to obtain useful corpora for this purpose, the manual classification of texts according to their emotional content has been the technique most commonly used by the research community. The use of corpora is widespread in Natural Language Processing, and the existing corpora annotated with emotions support the development, training and evaluation of systems using this type of data. In this paper we present the development of an annotated corpus oriented to the narrative domain, called EmoTales, which uses two different approaches to represent emotional states: emotional categories and emotional dimensions. The corpus consists of a collection of 1,389 English sentences from 18 different folk tales, annotated by 36 different people. Our model of the corpus development process includes a post-processing stage performed after the annotation of the corpus, in which a reference value for each sentence was chosen by taking into account the tags assigned by annotators and some general knowledge about emotions, which is codified in an ontology. The whole process is presented in detail, and revels significant results regarding the corpus such as inter-annotator agreement, while discussing topics such as how human annotators deal with emotional content when performing their work, and presenting some ideas for the application of this corpus that may inspire the research community to develop new ways to annotate corpora using a large set of emotional tags.","Language Resources and Evaluation",2012,"No","text corpora corpus annot emot ontolog emot categori emot dimens emotal creat corpus folk tale emot annot emot inher human activ includ human comput interact reason recogn emot express natur languag key featur design natur user interfac order obtain corpora purpos manual classif text emot content techniqu common research communiti corpora widespread natur languag process exist corpora annot emot support develop train evalu system type data paper present develop annot corpus orient narrat domain call emotal approach repres emot state emot categori emot dimens corpus consist collect english sentenc folk tale annot peopl model corpus develop process includ post process stage perform annot corpus refer sentenc chosen take account tag assign annot general knowledg emot codifi ontolog process present detail revel signific result corpus inter annot agreement discuss topic human annot deal emot content perform work present idea applic corpus inspir research communiti develop way annot corpora larg set emot tag",0
"KeywordsMultilayer corpora Classroom annotation Coreference Information structure Treebank Parsing ","the gum corpus creating multilayer resources in the classroom","This paper presents the methodology, design principles and detailed evaluation of a new freely available multilayer corpus, collected and edited via classroom annotation using collaborative software. After briefly discussing corpus design for open, extensible corpora, five classroom annotation projects are presented, covering structural markup in TEI XML, multiple part of speech tagging, constituent and dependency parsing, information structural and coreference annotation, and Rhetorical Structure Theory analysis. Layers are inspected for annotation quality and together they coalesce to form a richly annotated corpus that can be used to study the interactions between different levels of linguistic description. The evaluation gives an indication of the expected quality of a corpus created by students with relatively little training. A multifactorial example study on lexical NP coreference likelihood is also presented, which illustrates some applications of the corpus. The results of this project show that high quality, richly annotated resources can be created effectively as part of a linguistics curriculum, opening new possibilities not just for research, but also for corpora in linguistics pedagogy.","Language Resources and Evaluation",2017,"No","multilay corpora classroom annot corefer inform structur treebank pars gum corpus creat multilay resourc classroom paper present methodolog design principl detail evalu freeli multilay corpus collect edit classroom annot collabor softwar briefli discuss corpus design open extens corpora classroom annot project present cover structur markup tei xml multipl part speech tag constitu depend pars inform structur corefer annot rhetor structur theori analysi layer inspect annot qualiti coalesc form rich annot corpus studi interact level linguist descript evalu indic expect qualiti corpus creat student train multifactori studi lexic np corefer likelihood present illustr applic corpus result project show high qualiti rich annot resourc creat effect part linguist curriculum open possibl research corpora linguist pedagogi",0
"KeywordsParaphrasing Paraphrase typology Corpus annotation Inter-annotator agreement ","corpus annotation with paraphrase types new annotation scheme and inter annotator agreement measures","Paraphrase corpora annotated with the types of paraphrases they contain constitute an essential resource for the understanding of the phenomenon of paraphrasing and the improvement of paraphrase-related systems in natural language processing. In this article, a new annotation scheme for paraphrase-type annotation is set out, together with newly created measures for the computation of inter-annotator agreement. Three corpora different in nature and in two languages have been annotated using this infrastructure. The annotation results and the inter-annotator agreement scores for these corpora are proof of the adequacy and robustness of our proposal.","Language Resources and Evaluation",2015,"No","paraphras paraphras typolog corpus annot inter annot agreement corpus annot paraphras type annot scheme inter annot agreement measur paraphras corpora annot type paraphras constitut essenti resourc understand phenomenon paraphras improv paraphras relat system natur languag process articl annot scheme paraphras type annot set newli creat measur comput inter annot agreement corpora natur languag annot infrastructur annot result inter annot agreement score corpora proof adequaci robust propos",0
"KeywordsText simplification Monolingual parallel corpora Annotation scheme Basque ","the corpus of basque simplified texts cbst","In this paper we present the corpus of Basque simplified texts. This corpus compiles 227 original sentences of science popularisation domain and two simplified versions of each sentence. The simplified versions have been created following different approaches: the structural, by a court translator who considers easy-to-read guidelines and the intuitive, by a teacher based on her experience. The aim of this corpus is to make a comparative analysis of simplified text. To that end, we also present the annotation scheme we have created to annotate the corpus. The annotation scheme is divided into eight macro-operations: delete, merge, split, transformation, insert, reordering, no operation and other. These macro-operations can be classified into different operations. We also relate our work and results to other languages. This corpus will be used to corroborate the decisions taken and to improve the design of the automatic text simplification system for Basque.
","Language Resources and Evaluation",2018,"No","text simplif monolingu parallel corpora annot scheme basqu corpus basqu simplifi text cbst paper present corpus basqu simplifi text corpus compil origin sentenc scienc popularis domain simplifi version sentenc simplifi version creat approach structur court translat consid easi read guidelin intuit teacher base experi aim corpus make compar analysi simplifi text end present annot scheme creat annot corpus annot scheme divid macro oper delet merg split transform insert reorder oper macro oper classifi oper relat work result languag corpus corrobor decis improv design automat text simplif system basqu",0
"KeywordsPart-of-speech tagging Maximum entropy models Morphosyntactic lexicon French Language resource development ","coupling an annotated corpus and a lexicon for state of the art pos tagging","This paper investigates how to best couple hand-annotated data with information extracted from an external lexical resource to improve part-of-speech tagging performance. Focusing mostly on French tagging, we introduce a maximum entropy Markov model-based tagging system that is enriched with information extracted from a morphological resource. This system gives a 97.75 % accuracy on the French Treebank, an error reduction of 25 % (38 % on unknown words) over the same tagger without lexical information. We perform a series of experiments that help understanding how this lexical information helps improving tagging accuracy. We also conduct experiments on datasets and lexicons of varying sizes in order to assess the best trade-off between annotating data versus developing a lexicon. We find that the use of a lexicon improves the quality of the tagger at any stage of development of either resource, and that for fixed performance levels the availability of the full lexicon consistently reduces the need for supervised data by at least one half.","Language Resources and Evaluation",2012,"No","part speech tag maximum entropi model morphosyntact lexicon french languag resourc develop coupl annot corpus lexicon state art pos tag paper investig coupl hand annot data inform extract extern lexic resourc improv part speech tag perform focus french tag introduc maximum entropi markov model base tag system enrich inform extract morpholog resourc system accuraci french treebank error reduct unknown word tagger lexic inform perform seri experi understand lexic inform help improv tag accuraci conduct experi dataset lexicon vari size order assess trade annot data versus develop lexicon find lexicon improv qualiti tagger stage develop resourc fix perform level avail full lexicon consist reduc supervis data half",0
"KeywordsFairy tale corpus Annotation scheme Inter-annotator agreement Direct quotations Prosody Intonation stylization Text-to-speech Expressivity ","the gv lex corpus of tales in french","
A corpus of French tales is presented. Its two parts, a text corpus and a speech corpus, were designed for studying the relationships between the textual structures of tales and speech prosody, with the targeted application of an expressive text-to-speech synthesis system embedded in a humanoid robot. The 89-tale text corpus, and the 12-tale speech corpus were annotated using a common tale description framework. Lexical level annotations include extended definitions of enumerations, time, place and person named entities, as well as part of speech tags. Supra-lexical level annotations include the segmentation of tales into a sequence of episodes, the localization and attribution of direct quotations, together with tale protagonists co-references. Annotation distributions and inter-annotator agreement were analyzed. The largest coverage and strongest agreement were observed for person named entities, characters’ direct quotations, and their associated coreference chains. Speech corpus annotations were extended to allow the analysis of the relations between tale linguistic information and prosodic properties observed in associated speech. Word and phoneme boundaries were inferred through semi-automatic procedures, resulting in linguistic annotations aligned with the speech signal. Intonation stylization models were used to ease the visual and statistical analysis of tale’s prosody. Additional meta-information is provided with the speech corpus, allowing describing tale characters according to their gender, age, size, valence and kind. The corpora described in this article are publicly available through the European Language Resources Association catalog.","Language Resources and Evaluation",2015,"No","fairi tale corpus annot scheme inter annot agreement direct quotat prosodi inton styliz text speech express gv lex corpus tale french corpus french tale present part text corpus speech corpus design studi relationship textual structur tale speech prosodi target applic express text speech synthesi system embed humanoid robot tale text corpus tale speech corpus annot common tale descript framework lexic level annot includ extend definit enumer time place person name entiti part speech tag supra lexic level annot includ segment tale sequenc episod local attribut direct quotat tale protagonist refer annot distribut inter annot agreement analyz largest coverag strongest agreement observ person name entiti charact direct quotat corefer chain speech corpus annot extend analysi relat tale linguist inform prosod properti observ speech word phonem boundari infer semi automat procedur result linguist annot align speech signal inton styliz model eas visual statist analysi tale prosodi addit meta inform provid speech corpus allow describ tale charact gender age size valenc kind corpora articl public european languag resourc associ catalog",0
"KeywordsParallel corpus Multilingual corpus Comparative corpus linguistics ","a massively parallel corpus the bible in 100 languages","We describe the creation of a massively parallel corpus based on 100 translations of the Bible. We discuss some of the difficulties in acquiring and processing the raw material as well as the potential of the Bible as a corpus for natural language processing. Finally we present a statistical analysis of the corpora collected and a detailed comparison between the English translation and other English corpora.","Language Resources and Evaluation",2015,"No","parallel corpus multilingu corpus compar corpus linguist massiv parallel corpus bibl languag describ creation massiv parallel corpus base translat bibl discuss difficulti acquir process raw materi potenti bibl corpus natur languag process final present statist analysi corpora collect detail comparison english translat english corpora",0
"KeywordsHistorical Arabic corpus Corpus tools Natural language processing Arabic word usage over time Semantic change ","exploring and exploiting a historical corpus for arabic","

This paper presents a historical Arabic corpus named HAC. At this early embryonic stage of the project, we report about the design, the architecture and some of the experiments which we have conducted on HAC. The corpus, and accordingly the search results, will be represented using a primary XML exchange format. This will serve as an intermediate exchange tool within the project and will allow the user to process the results offline using some external tools. HAC is made up of Classical Arabic texts that cover 1600 years of language use; the Quranic text, Modern Standard Arabic texts, as well as a variety of monolingual Arabic dictionaries. The development of this historical corpus assists linguists and Arabic language learners to effectively explore, understand, and discover interesting knowledge hidden in millions of instances of language use. We used techniques from the field of natural language processing to process the data and a graph-based representation for the corpus. We provided researchers with an export facility to render further linguistic analysis possible.","Language Resources and Evaluation",2016,"No","histor arab corpus corpus tool natur languag process arab word usag time semant chang explor exploit histor corpus arab paper present histor arab corpus name hac earli embryon stage project report design architectur experi conduct hac corpus search result repres primari xml exchang format serv intermedi exchang tool project user process result offlin extern tool hac made classic arab text cover year languag quran text modern standard arab text varieti monolingu arab dictionari develop histor corpus assist linguist arab languag learner effect explor understand discov interest knowledg hidden million instanc languag techniqu field natur languag process process data graph base represent corpus provid research export facil render linguist analysi",0
"KeywordsCorpus annotation Annotation guidelines Clinical text Chunking Named entities ","annotating patient clinical records with syntactic chunks and named entities the harvey corpus","The free text notes typed by physicians during patient consultations contain valuable information for the study of disease and treatment. These notes are difficult to process by existing natural language analysis tools since they are highly telegraphic (omitting many words), and contain many spelling mistakes, inconsistencies in punctuation, and non-standard word order. To support information extraction and classification tasks over such text, we describe a de-identified corpus of free text notes, a shallow syntactic and named entity annotation scheme for this kind of text, and an approach to training domain specialists with no linguistic background to annotate the text. Finally, we present a statistical chunking system for such clinical text with a stable learning rate and good accuracy, indicating that the manual annotation is consistent and that the annotation scheme is tractable for machine learning.","Language Resources and Evaluation",2016,"No","corpus annot annot guidelin clinic text chunk name entiti annot patient clinic record syntact chunk name entiti harvey corpus free text note type physician patient consult valuabl inform studi diseas treatment note difficult process exist natur languag analysi tool high telegraph omit word spell mistak inconsist punctuat standard word order support inform extract classif task text describ de identifi corpus free text note shallow syntact name entiti annot scheme kind text approach train domain specialist linguist background annot text final present statist chunk system clinic text stabl learn rate good accuraci indic manual annot consist annot scheme tractabl machin learn",0
"KeywordsLinguistic annotation Language resources Discourse  Prosody Semantics Spoken dialogue ","the nxt format switchboard corpus a rich resource for investigating the syntax semantics pragmatics and prosody of dialogue","This paper describes a recently completed common resource for the study of spoken discourse, the NXT-format Switchboard Corpus. Switchboard is a long-standing corpus of telephone conversations (Godfrey et al. in SWITCHBOARD: Telephone speech corpus for research and development. In Proceedings of ICASSP-92, pp. 517–520, 1992). We have brought together transcriptions with existing annotations for syntax, disfluency, speech acts, animacy, information status, coreference, and prosody; along with substantial new annotations of focus/contrast, more prosody, syllables and phones. The combined corpus uses the format of the NITE XML Toolkit, which allows these annotations to be browsed and searched as a coherent set (Carletta et al. in Lang Resour Eval J 39(4):313–334, 2005). The resulting corpus is a rich resource for the investigation of the linguistic features of dialogue and how they interact. As well as describing the corpus itself, we discuss our approach to overcoming issues involved in such a data integration project, relevant to both users of the corpus and others in the language resource community undertaking similar projects.","Language Resources and Evaluation",2010,"No","linguist annot languag resourc discours prosodi semant spoken dialogu nxt format switchboard corpus rich resourc investig syntax semant pragmat prosodi dialogu paper describ recent complet common resourc studi spoken discours nxt format switchboard corpus switchboard long stand corpus telephon convers godfrey al switchboard telephon speech corpus research develop proceed icassp pp brought transcript exist annot syntax disfluenc speech act animaci inform status corefer prosodi substanti annot focuscontrast prosodi syllabl phone combin corpus format nite xml toolkit annot brows search coher set carletta al lang resour eval result corpus rich resourc investig linguist featur dialogu interact describ corpus discuss approach overcom issu involv data integr project relev user corpus languag resourc communiti undertak similar project",0
"KeywordsCorpora Language learning Vocabulary Frequency Frequency lists ","corpus based vocabulary lists for language learners for nine languages","We present the KELLY project and its work on developing monolingual and bilingual word lists for language learning, using corpus methods, for nine languages and thirty-six language pairs. We describe the method and discuss the many challenges encountered. We have loaded the data into an online database to make it accessible for anyone to explore and we present our own first explorations of it. The focus of the paper is thus twofold, covering pedagogical and methodological aspects of the lists’ construction, and linguistic aspects of the by-product of the project, the KELLY database.","Language Resources and Evaluation",2014,"No","corpora languag learn vocabulari frequenc frequenc list corpus base vocabulari list languag learner languag present kelli project work develop monolingu bilingu word list languag learn corpus method languag thirti languag pair describ method discuss challeng encount load data onlin databas make access explor present explor focus paper twofold cover pedagog methodolog aspect list construct linguist aspect product project kelli databas",0
"KeywordsCHILDES Hebrew Transcription of spoken language Morphological analysis Morphological disambiguation ","the hebrew childes corpus transcription and morphological analysis","We present a corpus of transcribed spoken Hebrew that reflects spoken interactions between children and adults. The corpus is an integral part of the CHILDES database, which distributes similar corpora for over 25 languages. We introduce a dedicated transcription scheme for the spoken Hebrew data that is sensitive to both the phonology and the standard orthography of the language. We also introduce a morphological analyzer that was specifically developed for this corpus. The analyzer adequately covers the entire corpus, producing detailed correct analyses for all tokens. Evaluation on a new corpus reveals high coverage as well. Finally, we describe a morphological disambiguation module that selects the correct analysis of each token in context. The result is a high-quality morphologically-annotated CHILDES corpus of Hebrew, along with a set of tools that can be applied to new corpora.","Language Resources and Evaluation",2013,"No","child hebrew transcript spoken languag morpholog analysi morpholog disambigu hebrew child corpus transcript morpholog analysi present corpus transcrib spoken hebrew reflect spoken interact children adult corpus integr part child databas distribut similar corpora languag introduc dedic transcript scheme spoken hebrew data sensit phonolog standard orthographi languag introduc morpholog analyz specif develop corpus analyz adequ cover entir corpus produc detail correct analys token evalu corpus reveal high coverag final describ morpholog disambigu modul select correct analysi token context result high qualiti morpholog annot child corpus hebrew set tool appli corpora",0
"KeywordsBehaviour analysis Small groups Meetings Multimodality ","a multimodal annotated corpus of consensus decision making meetings","In this paper we present an annotated audio–video corpus of multi-party meetings. The multimodal corpus provides for each subject involved in the experimental sessions six annotation dimensions referring to group dynamics; speech activity and body activity. The corpus is based on 11 audio and video recorded sessions which took place in a lab setting appropriately equipped with cameras and microphones. Our main concern in collecting this multimodal corpus was to explore the possibility of providing feedback services to facilitate group processes and to enhance self awareness among small groups engaged in meetings. We therefore introduce a coding scheme for annotating relevant functional roles that appear in a small group interaction. We also discuss the reliability of the coding scheme and we present the first results for automatic classification.","Language Resources and Evaluation",2007,"No","behaviour analysi small group meet multimod multimod annot corpus consensus decis make meet paper present annot audio video corpus multi parti meet multimod corpus subject involv experiment session annot dimens refer group dynam speech activ bodi activ corpus base audio video record session place lab set appropri equip camera microphon main concern collect multimod corpus explor possibl provid feedback servic facilit group process enhanc awar small group engag meet introduc code scheme annot relev function role small group interact discuss reliabl code scheme present result automat classif",0
"KeywordsComputational political sciences Computational social science Language technology Natural language processing Parliamentary proceedings ","the talk of norway a richly annotated corpus of the norwegian parliament 19982016","In this work we present the Talk of Norway (ToN) data set, a collection of Norwegian Parliament speeches from 1998 to 2016.
 Every speech is richly annotated with metadata harvested from different sources, and augmented with language type, sentence, token, lemma, part-of-speech, and morphological feature annotations. We also present a pilot study on party classification in the Norwegian Parliament, carried out in the context of a cross-faculty collaboration involving researchers from both Political Science and Computer Science. Our initial experiments demonstrate how the linguistic and institutional annotations in ToN can be used to gather insights on how different aspects of the political process affect classification.
","Language Resources and Evaluation",2018,"No","comput polit scienc comput social scienc languag technolog natur languag process parliamentari proceed talk norway rich annot corpus norwegian parliament work present talk norway ton data set collect norwegian parliament speech speech rich annot metadata harvest sourc augment languag type sentenc token lemma part speech morpholog featur annot present pilot studi parti classif norwegian parliament carri context cross faculti collabor involv research polit scienc comput scienc initi experi demonstr linguist institut annot ton gather insight aspect polit process affect classif",0
"KeywordsNatural language generation Referring expressions  Content selection Corpora ","stars2 a corpus of object descriptions in a visual domain","This paper presents the Stars2 corpus of definite descriptions for referring expression generation (REG). The corpus was produced in collaborative communication involving speaker-hearer pairs, and includes situations of reference that are arguably under-represented in similar work. Stars2 is intended as an incremental contribution to the research in REG and related fields, and it may be used both as training/test data for algorithms of this kind, and also to gain further insights into reference phenomena in general, with a particular focus on the issue of attribute choice in referential overspecification.","Language Resources and Evaluation",2017,"No","natur languag generat refer express content select corpora star corpus object descript visual domain paper present star corpus definit descript refer express generat reg corpus produc collabor communic involv speaker hearer pair includ situat refer arguabl repres similar work star intend increment contribut research reg relat field trainingtest data algorithm kind gain insight refer phenomena general focus issu attribut choic referenti overspecif",0
"KeywordsPalestinian Arabic Palestinian corpus Arabic morphology Conventional Orthography for Dialectal Arabic Dialectal Arabic Word annotation ","curras an annotated corpus for the palestinian arabic dialect","
In this article we present Curras, the first morphologically annotated corpus of the Palestinian Arabic dialect. Palestinian Arabic is one of the many primarily spoken dialects of the Arabic language. Arabic dialects are generally under-resourced compared to Modern Standard Arabic, the primarily written and official form of Arabic. We start in the article with a background description that situates Palestinian Arabic linguistically and historically and compares it to Modern Standard Arabic and Egyptian Arabic in terms of phonological, morphological, orthographic, and lexical variations. We then describe the methodology we developed to collect Palestinian Arabic text to guarantee a variety of representative domains and genres. We also discuss the annotation process we used, which extended previous efforts for annotation guideline development, and utilized existing automatic annotation solutions for Standard Arabic and Egyptian Arabic. The annotation guidelines and annotation meta-data are described in detail. The Curras Palestinian Arabic corpus consists of more than 56 K tokens, which are annotated with rich morphological and lexical features. The inter-annotator agreement results indicate a high degree of consistency.","Language Resources and Evaluation",2017,"No","palestinian arab palestinian corpus arab morpholog convent orthographi dialect arab dialect arab word annot curra annot corpus palestinian arab dialect articl present curra morpholog annot corpus palestinian arab dialect palestinian arab primarili spoken dialect arab languag arab dialect general resourc compar modern standard arab primarili written offici form arab start articl background descript situat palestinian arab linguist histor compar modern standard arab egyptian arab term phonolog morpholog orthograph lexic variat describ methodolog develop collect palestinian arab text guarante varieti repres domain genr discuss annot process extend previous effort annot guidelin develop util exist automat annot solut standard arab egyptian arab annot guidelin annot meta data detail curra palestinian arab corpus consist token annot rich morpholog lexic featur inter annot agreement result high degre consist",0
"KeywordsRegional accents recognition Acoustic approaches Complex socio-linguistic environments Algerian Modern Colloquial Arabic Speech Corpus Code-switching Language contact phenomena ","algerian modern colloquial arabic speech corpus amcasc regional accents recognition within complex socio linguistic environments","The Algerian linguistic situation is very intricate due to the ethnic, geographical and colonial occupation influences which have lead to a complex sociolinguistic environment. As a result of the contact between different languages and accents, the Algerian speech community has acquired a distinctive sociolinguistic situation. In addition to the intra- and inter- lingual variations describing day-to-day linguistic behavior of the Algerian speakers, their speech is characterized by the presence of many linguistic phenomena such as bilingualism and code switching. The study of automatic regional accent recognition in such a type of environment is a new idea in the field of automatic languages, dialect and accent recognition especially that previous studies were conducted using monolingual evaluation data. The assessment of the effectiveness of GMM-UBM and i-vectors frameworks for accent recognition approaches through the use of the Algerian Modern Colloquial Arabic Speech Corpus (AMCASC), which is a linguistic resource collected for this purpose, shows that not only the recording conditions mismatch, channels mismatch, recordings length mismatch and the amplitude clipping which have a non-desirable effect on the effectiveness of these acoustic approaches but also language contact phenomena are other perturbation sources which should be taken into consideration especially in real life applications
.","Language Resources and Evaluation",2017,"No","region accent recognit acoust approach complex socio linguist environ algerian modern colloqui arab speech corpus code switch languag contact phenomena algerian modern colloqui arab speech corpus amcasc region accent recognit complex socio linguist environ algerian linguist situat intric due ethnic geograph coloni occup influenc lead complex sociolinguist environ result contact languag accent algerian speech communiti acquir distinct sociolinguist situat addit intra inter lingual variat describ day day linguist behavior algerian speaker speech character presenc linguist phenomena bilingu code switch studi automat region accent recognit type environ idea field automat languag dialect accent recognit previous studi conduct monolingu evalu data assess effect gmm ubm vector framework accent recognit approach algerian modern colloqui arab speech corpus amcasc linguist resourc collect purpos show record condit mismatch channel mismatch record length mismatch amplitud clip desir effect effect acoust approach languag contact phenomena perturb sourc consider real life applic",0
"KeywordsBCCWJ Japanese Balanced corpus Design Annotation Dual POS analysis Evaluation Shonagon Chunagon ","balanced corpus of contemporary written japanese","
The balanced corpus of contemporary written Japanese (BCCWJ) is Japan’s first 100 million words balanced corpus. It consists of three subcorpora (publication subcorpus, library subcorpus, and special-purpose subcorpus) and covers a wide range of text registers including books in general, magazines, newspapers, governmental white papers, best-selling books, an internet bulletin-board, a blog, school textbooks, minutes of the national diet, publicity newsletters of local governments, laws, and poetry verses. A random sampling technique is utilized whenever possible in order to maximize the representativeness of the corpus. The corpus is annotated in terms of dual POS analysis, document structure, and bibliographical information. The BCCWJ is currently accessible in three different ways including Chunagon a web-based interface to the dual POS analysis data. Lastly, results of some pilot evaluation of the corpus with respect to the textual diversity are reported. The analyses include POS distribution, word-class distribution, entropy of orthography, sentence length, and variation of the adjective predicate. High textual diversity is observed in all these analyses.","Language Resources and Evaluation",2014,"No","bccwj japanes balanc corpus design annot dual pos analysi evalu shonagon chunagon balanc corpus contemporari written japanes balanc corpus contemporari written japanes bccwj japan million word balanc corpus consist subcorpora public subcorpus librari subcorpus special purpos subcorpus cover wide rang text regist includ book general magazin newspap government white paper sell book internet bulletin board blog school textbook minut nation diet public newslett local govern law poetri vers random sampl techniqu util order maxim repres corpus corpus annot term dual pos analysi document structur bibliograph inform bccwj access way includ chunagon web base interfac dual pos analysi data last result pilot evalu corpus respect textual divers report analys includ pos distribut word class distribut entropi orthographi sentenc length variat adject predic high textual divers observ analys",0
"KeywordsDistributional semantic models Word embedding Word2vec Persian ","the impact of corpus domain on word representation a study on persian word embeddings","Word embedding, has been a great success story for natural language processing in recent years. The main purpose of this approach is providing a vector representation of words based on neural network language modeling. Using a large training corpus, the model most learns from co-occurrences of words, namely Skip-gram model, and capture semantic features of words. Moreover, adding the recently introduced character embedding model to the objective function, the model can also focus on morphological features of words. In this paper, we study the impact of training corpus on the results of word embedding and show how the genre of training data affects the type of information captured by word embedding models. We perform our experiments on the Persian language. In line of our experiments, providing two well-known evaluation datasets for Persian, namely Google semantic/syntactic analogy and Wordsim353, is also part of the contribution of this paper. The experiments include computation of word embedding from various public Persian corpora with different genres and sizes while considering comprehensive lexical and semantic comparison between them. We identify words whose usages differ between these datasets resulted totally different vector representation which ends to significant impact on different domains in which the results vary up to 9% on Google analogy and up to 6% on Wordsim353. The resulted word embedding for each of the individual corpora as well as their combinations will be publicly available for any further research based on word embedding for Persian.","Language Resources and Evaluation",2018,"No","distribut semant model word embed wordvec persian impact corpus domain word represent studi persian word embed word embed great success stori natur languag process recent year main purpos approach provid vector represent word base neural network languag model larg train corpus model learn occurr word skip gram model captur semant featur word ad recent introduc charact embed model object function model focus morpholog featur word paper studi impact train corpus result word embed show genr train data affect type inform captur word embed model perform experi persian languag line experi provid evalu dataset persian googl semanticsyntact analog wordsim part contribut paper experi includ comput word embed public persian corpora genr size comprehens lexic semant comparison identifi word usag differ dataset result total vector represent end signific impact domain result vari googl analog wordsim result word embed individu corpora combin public research base word embed persian",0
"KeywordsLearner corpus Error annotation Second language acquisition Czech ","evaluating and automating the annotation of a learner corpus","The paper describes a corpus of texts produced by non-native speakers of Czech. We discuss its annotation scheme, consisting of three interlinked tiers, designed to handle a wide range of error types present in the input. Each tier corrects different types of errors; links between the tiers allow capturing errors in word order and complex discontinuous expressions. Errors are not only corrected, but also classified. The annotation scheme is tested on a data set including approx. 175,000 words with fair inter-annotator agreement results. We also explore the possibility of applying automated linguistic annotation tools (taggers, spell checkers and grammar checkers) to the learner text to support or even substitute manual annotation.","Language Resources and Evaluation",2014,"No","learner corpus error annot languag acquisit czech evalu autom annot learner corpus paper describ corpus text produc nativ speaker czech discuss annot scheme consist interlink tier design handl wide rang error type present input tier correct type error link tier captur error word order complex discontinu express error correct classifi annot scheme test data set includ approx word fair inter annot agreement result explor possibl appli autom linguist annot tool tagger spell checker grammar checker learner text support substitut manual annot",0
"KeywordsImplicit argument Deverbal nominalizations Argument structure Thematic roles Semantic corpus annotation Linguistic resource ","iarg ancora spanish corpus annotated with implicit arguments","This article presents the Spanish Iarg-AnCora corpus (400 k-words, 13,883 sentences) annotated with the implicit arguments of deverbal nominalizations (18,397 occurrences). We describe the methodology used to create it, focusing on the annotation scheme and criteria adopted. The corpus was manually annotated and an interannotator agreement test was conducted (81 % observed agreement) in order to ensure the reliability of the final resource. The annotation of implicit arguments results in an important gain in argument and thematic role coverage (128 % on average). It is the first corpus annotated with implicit arguments for the Spanish language with a wide coverage that is freely available. This corpus can subsequently be used by machine learning-based semantic role labeling systems, and for the linguistic analysis of implicit arguments grounded on real data. Semantic analyzers are essential components of current language technology applications, which need to obtain a deeper understanding of the text in order to make inferences at the highest level to obtain qualitative improvements in the results.","Language Resources and Evaluation",2016,"No","implicit argument deverb nomin argument structur themat role semant corpus annot linguist resourc iarg ancora spanish corpus annot implicit argument articl present spanish iarg ancora corpus word sentenc annot implicit argument deverb nomin occurr describ methodolog creat focus annot scheme criteria adopt corpus manual annot interannot agreement test conduct observ agreement order ensur reliabl final resourc annot implicit argument result import gain argument themat role coverag averag corpus annot implicit argument spanish languag wide coverag freeli corpus subsequ machin learn base semant role label system linguist analysi implicit argument ground real data semant analyz essenti compon current languag technolog applic obtain deeper understand text order make infer highest level obtain qualit improv result",0
"KeywordsMutlimodal Corpus Annotation Evaluation Audio Video ","the chil audiovisual corpus for lecture and meeting analysis inside smart rooms","The analysis of lectures and meetings inside smart rooms has recently attracted much interest in the literature, being the focus of international projects and technology evaluations. A key enabler for progress in this area is the availability of appropriate multimodal and multi-sensory corpora, annotated with rich human activity information during lectures and meetings. This paper is devoted to exactly such a corpus, developed in the framework of the European project CHIL, “Computers in the Human Interaction Loop”. The resulting data set has the potential to drastically advance the state-of-the-art, by providing numerous synchronized audio and video streams of real lectures and meetings, captured in multiple recording sites over the past 4 years. It particularly overcomes typical shortcomings of other existing databases that may contain limited sensory or monomodal data, exhibit constrained human behavior and interaction patterns, or lack data variability. The CHIL corpus is accompanied by rich manual annotations of both its audio and visual modalities. These provide a detailed multi-channel verbatim orthographic transcription that includes speaker turns and identities, acoustic condition information, and named entities, as well as video labels in multiple camera views that provide multi-person 3D head and 2D facial feature location information. Over the past 3 years, the corpus has been crucial to the evaluation of a multitude of audiovisual perception technologies for human activity analysis in lecture and meeting scenarios, demonstrating its utility during internal evaluations of the CHIL consortium, as well as at the recent international CLEAR and Rich Transcription evaluations. The CHIL corpus is publicly available to the research community.","Language Resources and Evaluation",2007,"No","mutlimod corpus annot evalu audio video chil audiovisu corpus lectur meet analysi insid smart room analysi lectur meet insid smart room recent attract interest literatur focus intern project technolog evalu key enabl progress area avail multimod multi sensori corpora annot rich human activ inform lectur meet paper devot corpus develop framework european project chil comput human interact loop result data set potenti drastic advanc state art provid numer synchron audio video stream real lectur meet captur multipl record site past year overcom typic shortcom exist databas limit sensori monomod data exhibit constrain human behavior interact pattern lack data variabl chil corpus accompani rich manual annot audio visual modal provid detail multi channel verbatim orthograph transcript includ speaker turn ident acoust condit inform name entiti video label multipl camera view provid multi person d head d facial featur locat inform past year corpus crucial evalu multitud audiovisu percept technolog human activ analysi lectur meet scenario demonstr util intern evalu chil consortium recent intern clear rich transcript evalu chil corpus public research communiti",0
"KeywordsConceptual metaphor theory Corpus annotation Human experimentation ","conceptual metaphor theory meets the data a corpus based human annotation study","Metaphor makes our thoughts more vivid and fills our communication with richer imagery. Furthermore, according to the conceptual metaphor theory (CMT) of Lakoff and Johnson (Metaphors we live by. University of Chicago Press, Chicago, 1980), metaphor also plays an important structural role in the organization and processing of conceptual knowledge. According to this account, the phenomenon of metaphor is not restricted to similarity-based extensions of meanings of individual words, but instead involves activating fixed mappings that reconceptualize one whole area of experience in terms of another. CMT produced a significant resonance in the fields of philosophy, linguistics, cognitive science and artificial intelligence and still underlies a large proportion of modern research on metaphor. However, there has to date been no comprehensive corpus-based study of conceptual metaphor, which would provide an empirical basis for evaluating the CMT using real-world linguistic data. The annotation scheme and the empirical study we present in this paper is a step towards filling this gap. We test our annotation procedure in an experimental setting involving multiple annotators and estimate their agreement on the task. The goal of the study is to investigate (1) how intuitive the conceptual metaphor explanation of linguistic metaphors is for human annotators and whether it is possible to consistently annotate interconceptual mappings; (2) what are the main difficulties that the annotators experience during the annotation process; (3) whether one conceptual metaphor is sufficient to explain a linguistic metaphor or whether a chain of conceptual metaphors is needed. The resulting corpus annotated for conceptual mappings provides a new, valuable dataset for linguistic, computational and cognitive experiments on metaphor.","Language Resources and Evaluation",2013,"No","conceptu metaphor theori corpus annot human experiment conceptu metaphor theori meet data corpus base human annot studi metaphor make thought vivid fill communic richer imageri conceptu metaphor theori cmt lakoff johnson metaphor live univers chicago press chicago metaphor play import structur role organ process conceptu knowledg account phenomenon metaphor restrict similar base extens mean individu word involv activ fix map reconceptu area experi term cmt produc signific reson field philosophi linguist cognit scienc artifici intellig under larg proport modern research metaphor date comprehens corpus base studi conceptu metaphor provid empir basi evalu cmt real world linguist data annot scheme empir studi present paper step fill gap test annot procedur experiment set involv multipl annot estim agreement task goal studi investig intuit conceptu metaphor explan linguist metaphor human annot consist annot interconceptu map main difficulti annot experi annot process conceptu metaphor suffici explain linguist metaphor chain conceptu metaphor need result corpus annot conceptu map valuabl dataset linguist comput cognit experi metaphor",0
"KeywordsMultimodal corpora First acquaintance conversations Gestural annotation ","the danish nomco corpus multimodal interaction in first acquaintance conversations","This article presents the Danish NOMCO Corpus, an annotated multimodal collection of video-recorded first acquaintance conversations between Danish speakers. 
The annotation includes speech transcription including word boundaries, and formal as well as functional coding of gestural behaviours, specifically head movements, facial expressions, and body posture. 
The corpus has served as the empirical basis for a number of studies of communication phenomena related to turn management, feedback exchange, information packaging and the expression of emotional attitudes. 
We describe the annotation scheme, procedure, and annotation results. We then summarise a number of studies conducted on the corpus. The corpus is available for research and teaching purposes through the authors of this article.","Language Resources and Evaluation",2017,"No","multimod corpora acquaint convers gestur annot danish nomco corpus multimod interact acquaint convers articl present danish nomco corpus annot multimod collect video record acquaint convers danish speaker annot includ speech transcript includ word boundari formal function code gestur behaviour specif head movement facial express bodi postur corpus serv empir basi number studi communic phenomena relat turn manag feedback exchang inform packag express emot attitud describ annot scheme procedur annot result summaris number studi conduct corpus corpus research teach purpos author articl",0
"KeywordsCooperation Emotions Multimodal Dialogue Annotation ","the rovereto emotion and cooperation corpus a new resource to investigate cooperation and emotions","The Rovereto Emotion and Cooperation Corpus (RECC) is a new resource collected to investigate the relationship between cooperation and emotions in an interactive setting. Previous attempts at collecting corpora to study emotions have shown that this data are often quite difficult to classify and analyse, and coding schemes to analyse emotions are often found not to be reliable. We collected a corpus of task-oriented (MapTask-style) dialogues in Italian, in which the segments of emotional interest are identified using psycho-physiological indexes (Heart Rate and Galvanic Skin Conductance) which are highly reliable. We then annotated these segments in accordance with novel multimodal annotation schemes for cooperation (in terms of effort) and facial expressions (an indicator of emotional state). High agreement was obtained among coders on all the features. The RECC corpus is to our knowledge the first resource with psycho-physiological data aligned with verbal and nonverbal behaviour data.","Language Resources and Evaluation",2012,"No","cooper emot multimod dialogu annot rovereto emot cooper corpus resourc investig cooper emot rovereto emot cooper corpus recc resourc collect investig relationship cooper emot interact set previous attempt collect corpora studi emot shown data difficult classifi analys code scheme analys emot found reliabl collect corpus task orient maptask style dialogu italian segment emot interest identifi psycho physiolog index heart rate galvan skin conduct high reliabl annot segment accord multimod annot scheme cooper term effort facial express indic emot state high agreement obtain coder featur recc corpus knowledg resourc psycho physiolog data align verbal nonverb behaviour data",0
"KeywordsActive listening Multimodal feedback Backchannels Head gestures Attention Multimodal corpus ","the alico corpus analysing the active listener","The Active Listening Corpus (ALICO) is a multimodal data set of spontaneous dyadic conversations in German with diverse speech and gestural annotations of both dialogue partners. The annotations consist of short feedback expression transcriptions with corresponding communicative function interpretations as well as segmentations of interpausal units, words, rhythmic prominence intervals and vowel-to-vowel intervals. Additionally, ALICO contains head gesture annotations of both interlocutors. The corpus contributes to research on spontaneous human–human interaction, on functional relations between modalities, and timing variability in dialogue. It also provides data that differentiates between distracted and attentive listeners. We describe the main characteristics of the corpus and briefly present the most important results obtained from analyses in recent years.","Language Resources and Evaluation",2016,"No","activ listen multimod feedback backchannel head gestur attent multimod corpus alico corpus analys activ listen activ listen corpus alico multimod data set spontan dyadic convers german divers speech gestur annot dialogu partner annot consist short feedback express transcript communic function interpret segment interpaus unit word rhythmic promin interv vowel vowel interv addit alico head gestur annot interlocutor corpus contribut research spontan human human interact function relat modal time variabl dialogu data differenti distract attent listen describ main characterist corpus briefli present import result obtain analys recent year",0
"KeywordsEllipsis Annotation Evaluation VP ellipsis ","an annotated corpus for the analysis of vp ellipsis","Verb Phrase Ellipsis (VPE) has been studied in great depth in theoretical linguistics, but empirical studies of VPE are rare. We extend the few previous corpus studies with an annotated corpus of VPE in all 25 sections of the Wall Street Journal corpus (WSJ) distributed with the Penn Treebank. We annotated the raw files using a stand-off annotation scheme that codes the auxiliary verb triggering the elided verb phrase, the start and end of the antecedent, the syntactic type of antecedent (VP, TV, NP, PP or AP), and the type of syntactic pattern between the source and target clauses of the VPE and its antecedent. We found 487 instances of VPE (including predicative ellipsis, antecedent-contained deletion, comparative constructions, and pseudo-gapping) plus 67 cases of related phenomena such as do so anaphora. Inter-annotator agreement was high, with a 0.97 average F-score for three annotators for one section of the WSJ. Our annotation is theory neutral, and has better coverage than earlier efforts that relied on automatic methods, e.g. simply searching the parsed version of the Penn Treebank for empty VP’s achieves a high precision (0.95) but low recall (0.58) when compared with our manual annotation. The distribution of VPE source–target patterns deviates highly from the standard examples found in the theoretical linguistics literature on VPE, once more underlining the value of corpus studies. The resulting corpus will be useful for studying VPE phenomena as well as for evaluating natural language processing systems equipped with ellipsis resolution algorithms, and we propose evaluation measures for VPE detection and VPE antecedent selection. The stand-off annotation is freely available for research purposes.","Language Resources and Evaluation",2011,"No","ellipsi annot evalu vp ellipsi annot corpus analysi vp ellipsi verb phrase ellipsi vpe studi great depth theoret linguist empir studi vpe rare extend previous corpus studi annot corpus vpe section wall street journal corpus wsj distribut penn treebank annot raw file stand annot scheme code auxiliari verb trigger elid verb phrase start end anteced syntact type anteced vp tv np pp ap type syntact pattern sourc target claus vpe anteced found instanc vpe includ predic ellipsi anteced contain delet compar construct pseudo gap case relat phenomena anaphora inter annot agreement high averag score annot section wsj annot theori neutral coverag earlier effort reli automat method simpli search pars version penn treebank empti vp achiev high precis low recal compar manual annot distribut vpe sourc target pattern deviat high standard exampl found theoret linguist literatur vpe underlin corpus studi result corpus studi vpe phenomena evalu natur languag process system equip ellipsi resolut algorithm propos evalu measur vpe detect vpe anteced select stand annot freeli research purpos",0
"KeywordsAddressing Multi-party dialogues Multimodal corpora Annotation schemas Reliability analysis ","a corpus for studying addressing behaviour in multi party dialogues","This paper describes a multi-modal corpus of hand-annotated meeting dialogues that was designed for studying addressing behaviour in face-to-face conversations. The corpus contains annotated dialogue acts, addressees, adjacency pairs and gaze direction. First, we describe the corpus design where we present the meetings collection, annotation scheme and annotation tools. Then, we present the analysis of the reproducibility and stability of the annotation scheme.","Language Resources and Evaluation",2006,"No","address multi parti dialogu multimod corpora annot schema reliabl analysi corpus studi address behaviour multi parti dialogu paper describ multi modal corpus hand annot meet dialogu design studi address behaviour face face convers corpus annot dialogu act addresse adjac pair gaze direct describ corpus design present meet collect annot scheme annot tool present analysi reproduc stabil annot scheme",0
"KeywordsFrame-based terminology Knowledge visualization Multimodality Image-text interface Terminological resources ","a corpus based approach to the multimodal analysis of specialized knowledge","Modern communication environments have changed the cognitive patterns of individuals, who are now used to the interaction of information encoded in different semiotic modalities, especially visual and linguistic. Despite this, the main premise of Corpus Linguistics is still ruling: our perception of and experience with the world is conveyed in texts, which nowadays need to be studied from a multimodal perspective. Therefore, multimodal corpora are becoming extremely useful to extract specialized knowledge and explore the insights of specialized language and its relation to non-language-specific representations of knowledge. It is our assertion that the analysis of the image-text interface can help us understand the way visual and linguistic information converge in subject-field texts. In this article, we use Frame-based terminology to sketch a novel proposal to study images in a corpus rich in pictorial representations for their inclusion in a terminological resource on the environment. Our corpus-based approach provides the methodological underpinnings to create meaning within terminographic entries, thus facilitating specialized knowledge transfer and acquisition through images.","Language Resources and Evaluation",2013,"No","frame base terminolog knowledg visual multimod imag text interfac terminolog resourc corpus base approach multimod analysi special knowledg modern communic environ chang cognit pattern individu interact inform encod semiot modal visual linguist main premis corpus linguist rule percept experi world convey text nowaday studi multimod perspect multimod corpora extrem extract special knowledg explor insight special languag relat languag specif represent knowledg assert analysi imag text interfac understand visual linguist inform converg subject field text articl frame base terminolog sketch propos studi imag corpus rich pictori represent inclus terminolog resourc environ corpus base approach methodolog underpin creat mean terminograph entri facilit special knowledg transfer acquisit imag",0
"KeywordsTwitter n-gram corpus Social media Demographics Metadata Gender Time ","twitter n gram corpus with demographic metadata","Social media is a natural laboratory for linguistic and sociological purposes. In micro-blogging platforms such as Twitter, people share hundreds of millions of short messages about their lives and experiences on a daily basis. These messages, coupled with metadata about their authors, provide an opportunity to understand a wide variety of phenomena ranging from political polarization to geographic and demographic lexical variation. Lack of publicly available micro-blogging datasets has been a hindrance to replicable research. In this paper, I introduce Rovereto Twitter n-gram corpus, a publicly available n-gram dataset of Twitter messages, which contains gender-of-the-author and time-of-posting tags associated with the n-grams. I compare this dataset to a more traditional web-based corpus and present a case study which shows the potential of combining an n-gram corpus with demographic metadata.","Language Resources and Evaluation",2013,"No","twitter gram corpus social media demograph metadata gender time twitter gram corpus demograph metadata social media natur laboratori linguist sociolog purpos micro blog platform twitter peopl share hundr million short messag live experi daili basi messag coupl metadata author provid opportun understand wide varieti phenomena rang polit polar geograph demograph lexic variat lack public micro blog dataset hindranc replic research paper introduc rovereto twitter gram corpus public gram dataset twitter messag gender author time post tag gram compar dataset tradit web base corpus present case studi show potenti combin gram corpus demograph metadata",0
"KeywordsAnnotation of negation Scope of negation Polarity annotation Sentiment analysis ","sfu reviewsp neg a spanish corpus annotated with negation for sentiment analysis a typology of negation patterns","In this paper, we present SFU ReviewSP-NEG, the first Spanish corpus annotated with negation with a wide coverage freely available. We describe the methodology applied in the annotation of the corpus including the tagset, the linguistic criteria and the inter-annotator agreement tests.
 We also include a complete typology of negation patterns in Spanish. This typology has the advantage that it is easy to express in terms of a tagset for corpus annotation: the types are clearly defined, which avoids ambiguity in the annotation process, and they provide wide coverage (i.e. they resolved all the cases occurring in the corpus). We use the SFU ReviewSP as a base in order to make the annotations. 
The corpus consists of 400 reviews, 221,866 words and 9455 sentences, out of which 3022 sentences contain at least one negation structure.","Language Resources and Evaluation",2018,"No","annot negat scope negat polar annot sentiment analysi sfu reviewsp neg spanish corpus annot negat sentiment analysi typolog negat pattern paper present sfu reviewsp neg spanish corpus annot negat wide coverag freeli describ methodolog appli annot corpus includ tagset linguist criteria inter annot agreement test includ complet typolog negat pattern spanish typolog advantag easi express term tagset corpus annot type defin avoid ambigu annot process provid wide coverag resolv case occur corpus sfu reviewsp base order make annot corpus consist review word sentenc sentenc negat structur",0
"KeywordsCode-switching speech Spontaneous spoken corpus development Mandarin–English Speech recognition Language recognition ","mandarinenglish code switching speech corpus in south east asia seame","
This paper introduces the South East Asia Mandarin–English corpus, a 63-h spontaneous Mandarin–English code-switching transcribed speech corpus suitable for LVCSR and language change detection/identification research. The corpus is recorded under unscripted interview and conversational settings from 157 Singaporean and Malaysian speakers who spoke a mixture of Mandarin and English within a single sentence. About 82 % of the transcribed utterances are intra-sentential code-switching speech and the corpus will be release by LDC in 2015. This paper presents an analysis of the code-switching statistics of the corpus, such as the duration of monolingual segments and the frequency of language turns in code-switch utterances. We also summarize the development effort, details such as the processing time for transcription, validation and language boundary labelling. Lastly, we present textual analyses of code-switch segments examining the word length of monolingual segments in code-switch utterances and the most common single word and two-word phrase of such segments.
","Language Resources and Evaluation",2015,"No","code switch speech spontan spoken corpus develop mandarin english speech recognit languag recognit mandarinenglish code switch speech corpus south east asia seam paper introduc south east asia mandarin english corpus spontan mandarin english code switch transcrib speech corpus suitabl lvcsr languag chang detectionidentif research corpus record unscript interview convers set singaporean malaysian speaker spoke mixtur mandarin english singl sentenc transcrib utter intra sententi code switch speech corpus releas ldc paper present analysi code switch statist corpus durat monolingu segment frequenc languag turn code switch utter summar develop effort detail process time transcript valid languag boundari label last present textual analys code switch segment examin word length monolingu segment code switch utter common singl word word phrase segment",0
"KeywordsACL Anthology Network Bibliometrics Scientometrics Citation analysis Citation summaries ","the acl anthology network corpus","We introduce the ACL Anthology Network (AAN), a comprehensive manually curated networked database of citations, collaborations, and summaries in the field of Computational Linguistics. We also present a number of statistics about the network including the most cited authors, the most central collaborators, as well as network statistics about the paper citation, author citation, and author collaboration networks.","Language Resources and Evaluation",2013,"No","acl antholog network bibliometr scientometr citat analysi citat summari acl antholog network corpus introduc acl antholog network aan comprehens manual curat network databas citat collabor summari field comput linguist present number statist network includ cite author central collabor network statist paper citat author citat author collabor network",0
"Keywordsagreement between transcribers χ2 analysis corpus research effect size log odds power of a test sequential dependence unit dependence ","pitfalls in corpus research",". This paper discusses some pitfalls in corpus research and suggests solutions on the basis of examples and computer simulations. We first address reliability problems in language transcriptions, agreement between transcribers, and how disagreements can be dealt with. We then show that the frequencies of occurrence obtained from a corpus cannot always be analyzed with the traditional χ2 test, as corpus data are often not sequentially independent and unit independent. Next, we stress the relevance of the power of statistical tests, and the sizes of statistically significant effects. Finally, we point out that a t-test based on log odds often provides a better alternative to a χ2 analysis based on frequency counts.","Computers and the Humanities",2004,"No","agreement transcrib analysi corpus research effect size log odd power test sequenti depend unit depend pitfal corpus research paper discuss pitfal corpus research suggest solut basi exampl comput simul address reliabl problem languag transcript agreement transcrib disagr dealt show frequenc occurr obtain corpus analyz tradit test corpus data sequenti independ unit independ stress relev power statist test size statist signific effect final point test base log odd altern analysi base frequenc count",0
"KeywordsBlogs Sentiment analysis Corpus annotation Evaluation Polarity French language ","annotating opinionevaluation of blogs the blogoscopy corpus","The blog phenomenon is universal. Blogs are characterized by their evaluative use, in that they enable Internet users to express their opinion on a given subject. From this point of view, they are an ideal resource for the constitution of an annotated sentiment analysis corpus, crossing the subject and the opinion expressed on this subject. This paper presents the Blogoscopy corpus for the French language which was built up with personal thematic blogs. The annotation was governed by three principles: theoretical, as opinion is grounded in a linguistic theory of evaluation, practical, as every opinion is linked to an object, and methodological as annotation rules and successive phases are defined to ensure quality and thoroughness.","Language Resources and Evaluation",2011,"No","blog sentiment analysi corpus annot evalu polar french languag annot opinionevalu blog blogoscopi corpus blog phenomenon univers blog character evalu enabl internet user express opinion subject point view ideal resourc constitut annot sentiment analysi corpus cross subject opinion express subject paper present blogoscopi corpus french languag built person themat blog annot govern principl theoret opinion ground linguist theori evalu practic opinion link object methodolog annot rule success phase defin ensur qualiti thorough",0
"KeywordsDialog system Speech understanding Corpus Annotation Evaluation ","media a semantically annotated corpus of task oriented dialogs in french","The aim of the French Media project was to define a protocol for the evaluation of speech understanding modules for dialog systems. Accordingly, a corpus of 1,257 real spoken dialogs related to hotel reservation and tourist information was recorded, transcribed and semantically annotated, and a semantic attribute-value representation was defined in which each conceptual relationship was represented by the names of the attributes. Two semantic annotation levels are distinguished in this approach. At the first level, each utterance is considered separately and the annotation represents the meaning of the statement without taking into account the dialog context. The second level of annotation then corresponds to the interpretation of the meaning of the statement by taking into account the dialog context; in this way a semantic representation of the dialog context is defined. This paper discusses the data collection, the detailed definition of both annotation levels, and the annotation scheme. Then the paper comments on both evaluation campaigns which were carried out during the project and discusses some results.","Language Resources and Evaluation",2009,"No","dialog system speech understand corpus annot evalu media semant annot corpus task orient dialog french aim french media project defin protocol evalu speech understand modul dialog system corpus real spoken dialog relat hotel reserv tourist inform record transcrib semant annot semant attribut represent defin conceptu relationship repres name attribut semant annot level distinguish approach level utter consid separ annot repres mean statement take account dialog context level annot correspond interpret mean statement take account dialog context semant represent dialog context defin paper discuss data collect detail definit annot level annot scheme paper comment evalu campaign carri project discuss result",0
"KeywordsEvent factuality Modality Certainty Subjectivity analysis Corpus creation TimeBank ","factbank a corpus annotated with event factuality","Recent work in computational linguistics points out the need for systems to be sensitive to the veracity or factuality of events as mentioned in text; that is, to recognize whether events are presented as corresponding to actual situations in the world, situations that have not happened, or situations of uncertain interpretation. Event factuality is an important aspect of the representation of events in discourse, but the annotation of such information poses a representational challenge, largely because factuality is expressed through the interaction of numerous linguistic markers and constructions. Many of these markers are already encoded in existing corpora, albeit in a somewhat fragmented way. In this article, we present FactBank, a corpus annotated with information concerning the factuality of events. Its annotation has been carried out from a descriptive framework of factuality grounded on both theoretical findings and data analysis. FactBank is built on top of TimeBank, adding to it an additional level of semantic information.","Language Resources and Evaluation",2009,"No","event factual modal certainti subject analysi corpus creation timebank factbank corpus annot event factual recent work comput linguist point system sensit verac factual event mention text recogn event present actual situat world situat happen situat uncertain interpret event factual import aspect represent event discours annot inform pose represent challeng larg factual express interact numer linguist marker construct marker encod exist corpora albeit fragment articl present factbank corpus annot inform factual event annot carri descript framework factual ground theoret find data analysi factbank built top timebank ad addit level semant inform",0
"KeywordsParallel corpus Interactionist theory CALL Corpus CALL Data driven learning Language teaching/learning ","from input to output the potential of parallel corpora for call","The aim of this paper is to illustrate the potential of a parallel corpus in the context of (computer-assisted) language learning. In order to do so, we propose to answer two main questions (1) what corpus (data) to use and (2) how to use the corpus (data). We provide an answer to the what-question by describing the importance and particularities of compiling and processing a corpus for pedagogical purposes. In order to answer the how-question, we first investigate the central concepts of the interactionist theory of second language acquisition: comprehensible input, input enhancement, comprehensible output and output enhancement. By means of two case studies, we illustrate how the abovementioned concepts can be realized in concrete corpus-based language learning activities. We propose a design for a receptive and productive language task and describe how a parallel corpus can be at the basis of powerful language learning activities. The Dutch Parallel Corpus, a ten-million word sentence aligned and annotated parallel corpus, is used to develop these language tasks.","Language Resources and Evaluation",2014,"No","parallel corpus interactionist theori call corpus call data driven learn languag teachinglearn input output potenti parallel corpora call aim paper illustr potenti parallel corpus context comput assist languag learn order propos answer main question corpus data corpus data provid answer question describ import particular compil process corpus pedagog purpos order answer question investig central concept interactionist theori languag acquisit comprehens input input enhanc comprehens output output enhanc mean case studi illustr abovement concept realiz concret corpus base languag learn activ propos design recept product languag task describ parallel corpus basi power languag learn activ dutch parallel corpus ten million word sentenc align annot parallel corpus develop languag task",0
"KeywordsPhonetic corpus Phonetic transcription Transcription granularity Mexican Spanish Acoustic models ","the corpus dimex100 transcription and evaluation","In this paper the transcription and evaluation of the corpus DIMEx100 for Mexican Spanish is presented. First we describe the corpus and explain the linguistic and computational motivation for its design and collection process; then, the phonetic antecedents and the alphabet adopted for the transcription task are presented; the corpus has been transcribed at three different granularity levels, which are also specified in detail. The corpus statistics for each transcription level are also presented. A set of phonetic rules describing phonetic context observed empirically in spontaneous conversation is also validated with the transcription. The corpus has been used for the construction of acoustic models and a phonetic dictionary for the construction of a speech recognition system. Initial performance results suggest that the data can be used to train good quality acoustic models.","Language Resources and Evaluation",2010,"No","phonet corpus phonet transcript transcript granular mexican spanish acoust model corpus dimex transcript evalu paper transcript evalu corpus dimex mexican spanish present describ corpus explain linguist comput motiv design collect process phonet anteced alphabet adopt transcript task present corpus transcrib granular level detail corpus statist transcript level present set phonet rule describ phonet context observ empir spontan convers valid transcript corpus construct acoust model phonet dictionari construct speech recognit system initi perform result suggest data train good qualiti acoust model",0
"KeywordsDiscourse TreeBank Discourse relations Chinese Explicit and implicit discourse connectives ","the chinese discourse treebank a chinese corpus annotated with discourse relations","
The paper presents the Chinese Discourse TreeBank, a corpus annotated with Penn Discourse TreeBank style discourse relations that take the form of a predicate taking two arguments. We first characterize the syntactic and statistical distributions of Chinese discourse connectives as well as the role of Chinese punctuation marks in discourse annotation, and then describe how we design our annotation strategy procedure based on this characterization. The Chinese-specific features of our annotation strategy include annotating explicit and implicit discourse relations in one single pass, defining the argument labels on semantic, rather than syntactic, grounds, as well as annotating the semantic type of implicit discourse relations directly. We also introduce a flat, 11-valued semantic type classification scheme for discourse relations. We finally demonstrate the feasibility of our approach with evaluation results.","Language Resources and Evaluation",2015,"No","discours treebank discours relat chines explicit implicit discours connect chines discours treebank chines corpus annot discours relat paper present chines discours treebank corpus annot penn discours treebank style discours relat form predic take argument character syntact statist distribut chines discours connect role chines punctuat mark discours annot describ design annot strategi procedur base character chines specif featur annot strategi includ annot explicit implicit discours relat singl pass defin argument label semant syntact ground annot semant type implicit discours relat direct introduc flat valu semant type classif scheme discours relat final demonstr feasibl approach evalu result",0
"KeywordsCorpus construction Specialised corpus Web-derived corpus Virtual corpus Website ranking Boilerplate removal Term recognition ","constructing specialised corpora through analysing domain representativeness of websites","The role of the Web for text corpus construction is becoming increasingly significant. However, the contribution of the Web is largely confined to building a general virtual corpus or low quality specialised corpora. In this paper, we introduce a new technique called SPARTAN for constructing specialised corpora from the Web by systematically analysing website contents. Our evaluations show that the corpora constructed using our technique are independent of the search engines employed. In particular, SPARTAN-derived corpora outperform all corpora based on existing techniques for the task of term recognition.","Language Resources and Evaluation",2011,"No","corpus construct specialis corpus web deriv corpus virtual corpus websit rank boilerpl remov term recognit construct specialis corpora analys domain repres websit role web text corpus construct increas signific contribut web larg confin build general virtual corpus low qualiti specialis corpora paper introduc techniqu call spartan construct specialis corpora web systemat analys websit content evalu show corpora construct techniqu independ search engin employ spartan deriv corpora outperform corpora base exist techniqu task term recognit",0
"KeywordsWeb corpora Corpus evaluation Corpus similarity Varieties of English Canadian English ","building and evaluating web corpora representing national varieties of english","Corpora are essential resources for language studies, as well as for training statistical natural language processing systems. Although very large English corpora have been built, only relatively small corpora are available for many varieties of English. National top-level domains (e.g., .au, .ca) could be exploited to automatically build web corpora, but it is unclear whether such corpora would reflect the corresponding national varieties of English; i.e., would a web corpus built from the .ca domain correspond to Canadian English? In this article we build web corpora from national top-level domains corresponding to countries in which English is widely spoken. We then carry out statistical analyses of these corpora in terms of keywords, measures of corpus comparison based on the Chi-square test and spelling variants, and the frequencies of words known to be marked in particular varieties of English. We find evidence that the web corpora indeed reflect the corresponding national varieties of English. We then demonstrate, through a case study on the analysis of Canadianisms, that these corpora could be valuable lexicographical resources.","Language Resources and Evaluation",2017,"No","web corpora corpus evalu corpus similar varieti english canadian english build evalu web corpora repres nation varieti english corpora essenti resourc languag studi train statist natur languag process system larg english corpora built small corpora varieti english nation top level domain au ca exploit automat build web corpora unclear corpora reflect nation varieti english web corpus built ca domain correspond canadian english articl build web corpora nation top level domain countri english wide spoken carri statist analys corpora term keyword measur corpus comparison base chi squar test spell variant frequenc word mark varieti english find evid web corpora reflect nation varieti english demonstr case studi analysi canadian corpora valuabl lexicograph resourc",0
"KeywordsMultiparty conversation in L2 Proficiency Eye gaze Multimodal corpus Annotation ","multimodal corpus of multiparty conversations in l1 and l2 languages and findings obtained from it","To investigate the differences in communicative activities by the same interlocutors in Japanese (their L1) and in English (their L2), an 8-h multimodal corpus of multiparty conversations was collected. Three subjects participated in each conversational group, and they had conversations on free-flowing and goal-oriented topics in Japanese and in English. Their utterances, eye gazes, and gestures were recorded with microphones, eye trackers, and video cameras. The utterances and eye gazes were manually annotated. Their utterances were transcribed, and the transcriptions of each participant were aligned with those of the others along the time axis. Quantitative analyses were made to compare the communicative activities caused by the differences in conversational languages, the conversation types, and the levels of language expertise in L2. The results reveal different utterance characteristics and gaze patterns that reflect the differences in difficulty felt by the participants in each conversational condition. Both total and average durations of utterances were shorter in their L2 than in their L1 conversations. Differences in eye gazes were mainly found in those toward the information senders: Speakers were gazed at more in their second-language than in their native-language conversations. Our findings on the characteristics of conversations in the second language suggest possible directions for future research in psychology, cognitive science, and human–computer interaction technologies.","Language Resources and Evaluation",2015,"No","multiparti convers l profici eye gaze multimod corpus annot multimod corpus multiparti convers l l languag find obtain investig differ communic activ interlocutor japanes l english l multimod corpus multiparti convers collect subject particip convers group convers free flow goal orient topic japanes english utter eye gaze gestur record microphon eye tracker video camera utter eye gaze manual annot utter transcrib transcript particip align time axi quantit analys made compar communic activ caus differ convers languag convers type level languag expertis l result reveal utter characterist gaze pattern reflect differ difficulti felt particip convers condit total averag durat utter shorter l l convers differ eye gaze found inform sender speaker gaze languag nativ languag convers find characterist convers languag suggest direct futur research psycholog cognit scienc human comput interact technolog",0
"KeywordsAuthorship attribution Logistic regression Bray–Curtis distance Gallus Anonymous Monk of Lido Medieval Europe ","computational authorship attribution in medieval latin corpora the case of the monk of lido ca 110108 and gallus anonymous ca 111317","This paper applies computational methods of authorship attribution to shed light on a still open question concerning two Latin works of the twelfth century: are the anonymous authors of the Translatio s. Nicolai (ca. 1101–1108) and the Gesta principum polonorum (ca. 1113–1117) one and the same person? The Translatio was written by the so-called Monk of Lido and describes Venice’s role in the First Crusade. The Gesta were written by the so-called Gallus Anonymous and contain a panegyric of the contemporary Polish ruler, Bolesław III the Wry-Mouthed (r. 1102–1138). This study attributes authorship to these works within four corpora of Latin texts composed between the tenth and twelfth centuries, each with between 39 and 116 texts written by between 15 and 22 different authors. The goal of including four corpora is to see how robust the similarity between the target texts is to changes in text length, genre, and class balance in the corpora. In each corpus, nine different distance metrics and one machine-learning algorithm are used to classify the authors of the Translatio and Gesta. I conclude that it is highly likely that Gallus and Monk were indeed one and same anonymous author, and highlight the effectiveness of the Bray–Curtis distance and logistic regression as methods of attribution.
","Language Resources and Evaluation",2018,"No","authorship attribut logist regress bray curti distanc gallus anonym monk lido mediev europ comput authorship attribut mediev latin corpora case monk lido ca gallus anonym ca paper appli comput method authorship attribut shed light open question latin work twelfth centuri anonym author translatio nicolai ca gesta principum polonorum ca person translatio written call monk lido describ venic role crusad gesta written call gallus anonym panegyr contemporari polish ruler bole aw iii wri mouth studi attribut authorship work corpora latin text compos tenth twelfth centuri text written author goal includ corpora robust similar target text text length genr class balanc corpora corpus distanc metric machin learn algorithm classifi author translatio gesta conclud high gallus monk anonym author highlight effect bray curti distanc logist regress method attribut",0
"alignment evaluation lemmatization tagging translation equivalence ","extracting multilingual lexicons from parallel corpora","The paper describes our recent developments in automatic extraction of translation equivalents from parallel corpora. We describe three increasingly complex algorithms: a simple baseline iterative method, and two non-iterative more elaborated versions. While the baseline algorithm is mainly described for illustrative purposes, the non-iterative algorithms outline the use of different working hypotheses which may be motivated by different kinds of applications and to some extent by the languages concerned. The first two algorithms rely on cross-lingual POS preservation, while with the third one POS invariance is not an extraction condition. The evaluation of the algorithms was conducted on three different corpora and several pairs of languages.","Computers and the Humanities",2004,"No","align evalu lemmat tag translat equival extract multilingu lexicon parallel corpora paper describ recent develop automat extract translat equival parallel corpora describ increas complex algorithm simpl baselin iter method iter elabor version baselin algorithm illustr purpos iter algorithm outlin work hypothes motiv kind applic extent languag concern algorithm reli cross lingual pos preserv pos invari extract condit evalu algorithm conduct corpora pair languag",0
"KeywordsSpeech corpus Structural metadata extraction Speech disfluency Filler Slavic language ","design creation and analysis of czech corpora for structural metadata extraction from speech","Structural metadata extraction (MDE) research aims to develop techniques for automatic conversion of raw speech recognition output to forms that are more useful to humans and downstream automatic processes. The MDE annotation includes inserting boundaries of sentence-like units to the flow of speech, labeling non-content words like filled pauses and discourse markers for optional removal, and identifying sections of disfluent speech. This paper describes design, creation, and analysis of data resources for structural MDE from spoken Czech. The annotation is based on the LDC’s MDE annotation standard for English, with changes applied to accommodate specific phenomena of Czech. In addition to the necessary language-dependent modifications, we further proposed and applied several language-independent modifications slightly refining the original annotation scheme. We created two Czech MDE speech corpora—one in the domain of broadcast news and the other in the domain of broadcast conversations. Both corpora have already been published at LDC. The analysis section of this paper presents a variety of statistics about fillers, edit disfluencies, and sentence-like units. The two Czech corpora are not only compared with each other, but also with statistics relating to the available English MDE corpora. We also report the statistics indicating that edit disfluencies have a different part of speech (POS) distribution in comparison with the overall POS distribution. The findings from the corpus analysis should help guide strategies for developing automatic MDE systems.","Language Resources and Evaluation",2011,"No","speech corpus structur metadata extract speech disfluenc filler slavic languag design creation analysi czech corpora structur metadata extract speech structur metadata extract mde research aim develop techniqu automat convers raw speech recognit output form human downstream automat process mde annot includ insert boundari sentenc unit flow speech label content word fill paus discours marker option remov identifi section disfluent speech paper describ design creation analysi data resourc structur mde spoken czech annot base ldc mde annot standard english appli accommod specif phenomena czech addit languag depend modif propos appli languag independ modif slight refin origin annot scheme creat czech mde speech corpora domain broadcast news domain broadcast convers corpora publish ldc analysi section paper present varieti statist filler edit disfluenc sentenc unit czech corpora compar statist relat english mde corpora report statist indic edit disfluenc part speech pos distribut comparison pos distribut find corpus analysi guid strategi develop automat mde system",0
"corpus-based linguistics natural language processing SGML ","using sgml as a basis for data intensive natural language processing","This paper describes the LT NSL system (McKelvie et al., 1996), an architecture for writing corpus processing tools. This system is then compared with two other systems which address similar issues, the GATE system (Cunningham et al., 1995) and the IMS Corpus Workbench (Christ, 1994). In particular we address the advantages and disadvantages of an SGML approach compared with a non-sgml database approach.","Computers and the Humanities",1997,"No","corpus base linguist natur languag process sgml sgml basi data intens natur languag process paper describ lt nsl system mckelvi al architectur write corpus process tool system compar system address similar issu gate system cunningham al im corpus workbench christ address advantag disadvantag sgml approach compar sgml databas approach",0
"chronology hapax prediction vocabulary Yuless K ","stylistic constancy and change across literary corpora using measures of lexical richness to date works","The measure of the lexical richness of literary texts as a tool in thecomparative analysis of literary style has been hampered by the problem ofthe inequality of text lengths within and between literary corpora. Thispaper proposes an empirical method of description of lexical richness byaveraging measures on multiple chunks of text of a standard lengthwithin a literary work or corpus. A workss average vocabulary richness,average portion of hapax legomenaof the corpus from which it derives,and average repetition of frequently appearing vocabulary may thencharacterize that work relative to other works partitioned along withit. This method reveals the possibility of significant variance of thesemeasures of vocabulary among works of a single authorss corpus and warnsagainst the notion of some absolute authorial stylistic character. Weapply this method of vocabulary averaging to the corpora of threeplaywrights from classical antiquity whose works are chronologicallyrankable: Euripides, Aristophanes, and Terence. We look for trends in vocabulary richness over time, which we posit functions as anindicator of progressively changing authorial ability or inclination. This method then holds the potential of predicting datesfor undateable or tenuously dated works within a corpus of otherwisesecurely dated texts. From the results derived, a relatively late date forthe composition of the redrafted version ofAristophaness Clouds appearslikely; we predict an early composition date for the redraft of TerencessHecyra (and thus are inclined to think that the playwright did verylittle redrafting); and finally we findEuripidess Electra andSupplices exhibiting vocabulary characteristics of extremely latecomposition and we predict dates much later than those assigned based onmetrical considerations.","Computers and the Humanities",2002,"No","chronolog hapax predict vocabulari yuless stylist constanc chang literari corpora measur lexic rich date work measur lexic rich literari text tool thecompar analysi literari style hamper problem ofth inequ text length literari corpora thispap propos empir method descript lexic rich byaverag measur multipl chunk text standard lengthwithin literari work corpus workss averag vocabulari richnessaverag portion hapax legomenaof corpus deriv averag repetit frequent appear vocabulari thencharacter work relat work partit withit method reveal possibl signific varianc thesemeasur vocabulari work singl authorss corpus warnsagainst notion absolut authori stylist charact weappli method vocabulari averag corpora threeplaywright classic antiqu work chronologicallyrank euripid aristophan terenc trend vocabulari rich time posit function anind progress chang authori abil inclin method hold potenti predict datesfor undat tenuous date work corpus otherwisesecur date text result deriv late date forth composit redraft version ofaristopha cloud appearslik predict earli composit date redraft terencesshecyra inclin playwright verylittl redraft final findeuripidess electra andsupplic exhibit vocabulari characterist extrem latecomposit predict date assign base onmetr consider",0
"KeywordsAnnotated corpora Corpus construction General-purpose linguistic resources English German Italian Web as corpus WaCky! ","the wacky wide web a collection of very large linguistically processed web crawled corpora","This article introduces ukWaC, deWaC and itWaC, three very large corpora of English, German, and Italian built by web crawling, and describes the methodology and tools used in their construction. The corpora contain more than a billion words each, and are thus among the largest resources for the respective languages. The paper also provides an evaluation of their suitability for linguistic research, focusing on ukWaC and itWaC. A comparison in terms of lexical coverage with existing resources for the languages of interest produces encouraging results. Qualitative evaluation of ukWaC versus the British National Corpus was also conducted, so as to highlight differences in corpus composition (text types and subject matters). The article concludes with practical information about format and availability of corpora and tools.","Language Resources and Evaluation",2009,"No","annot corpora corpus construct general purpos linguist resourc english german italian web corpus wacki wacki wide web collect larg linguist process web crawl corpora articl introduc ukwac dewac itwac larg corpora english german italian built web crawl describ methodolog tool construct corpora billion word largest resourc respect languag paper evalu suitabl linguist research focus ukwac itwac comparison term lexic coverag exist resourc languag interest produc encourag result qualit evalu ukwac versus british nation corpus conduct highlight differ corpus composit text type subject matter articl conclud practic inform format avail corpora tool",0
"KeywordsGenres on the web Reliability testing Annotation guidelines Crowdsourcing ","crowdsourcing for web genre annotation","Recently, genre collection and automatic genre identification for the web has attracted much attention. However, currently there is no genre-annotated corpus of web pages where inter-annotator reliability has been established, i.e. the corpora are either not tested for inter-annotator reliability or exhibit low inter-coder agreement. Annotation has also mostly been carried out by a small number of experts, leading to concerns with regard to scalability of these annotation efforts and transferability of the schemes to annotators outside these small expert groups. In this paper, we tackle these problems by using crowd-sourcing for genre annotation, leading to the Leeds Web Genre Corpus—the first web corpus which is, demonstrably reliably annotated for genre and which can be easily and cost-effectively expanded using naive annotators. We also show that the corpus is source and topic diverse.","Language Resources and Evaluation",2016,"No","genr web reliabl test annot guidelin crowdsourc crowdsourc web genr annot recent genr collect automat genr identif web attract attent genr annot corpus web page inter annot reliabl establish corpora test inter annot reliabl exhibit low inter coder agreement annot carri small number expert lead concern regard scalabl annot effort transfer scheme annot small expert group paper tackl problem crowd sourc genr annot lead leed web genr corpus web corpus demonstr reliabl annot genr easili cost effect expand naiv annot show corpus sourc topic divers",0
"KeywordsModern Standard Arabic Speech corpus Text corpus Phonetically rich Phonetically balanced Automatic continuous speech recognition ","phonetically rich and balanced text and speech corpora for arabic language","This paper describes the preparation, recording, analyzing, and evaluation of a new speech corpus for Modern Standard Arabic (MSA). The speech corpus contains a total of 415 sentences recorded by 40 (20 male and 20 female) Arabic native speakers from 11 different Arab countries representing three major regions (Levant, Gulf, and Africa). Three hundred and sixty seven sentences are considered as phonetically rich and balanced, which are used for training Arabic Automatic Speech Recognition (ASR) systems. The rich characteristic is in the sense that it must contain all phonemes of Arabic language, whereas the balanced characteristic is in the sense that it must preserve the phonetic distribution of Arabic language. The remaining 48 sentences are created for testing purposes, which are mostly foreign to the training sentences and there are hardly any similarities in words. In order to evaluate the speech corpus, Arabic ASR systems were developed using the Carnegie Mellon University (CMU) Sphinx 3 tools at both training and testing/decoding levels. The speech engine uses 3-emitting state Hidden Markov Models (HMM) for tri-phone based acoustic models. Based on experimental analysis of about 8 h of training speech data, the acoustic model is best using continuous observation’s probability model of 16 Gaussian mixture distributions and the state distributions were tied to 500 senones. The language model contains uni-grams, bi-grams, and tri-grams. For same speakers with different sentences, Arabic ASR systems obtained average Word Error Rate (WER) of 9.70%. For different speakers with same sentences, Arabic ASR systems obtained average WER of 4.58%, whereas for different speakers with different sentences, Arabic ASR systems obtained average WER of 12.39%.","Language Resources and Evaluation",2012,"No","modern standard arab speech corpus text corpus phonet rich phonet balanc automat continu speech recognit phonet rich balanc text speech corpora arab languag paper describ prepar record analyz evalu speech corpus modern standard arab msa speech corpus total sentenc record male femal arab nativ speaker arab countri repres major region levant gulf africa hundr sixti sentenc consid phonet rich balanc train arab automat speech recognit asr system rich characterist sens phonem arab languag balanc characterist sens preserv phonet distribut arab languag remain sentenc creat test purpos foreign train sentenc similar word order evalu speech corpus arab asr system develop carnegi mellon univers cmu sphinx tool train testingdecod level speech engin emit state hidden markov model hmm tri phone base acoust model base experiment analysi train speech data acoust model continu observ probabl model gaussian mixtur distribut state distribut tie senon languag model uni gram bi gram tri gram speaker sentenc arab asr system obtain averag word error rate wer speaker sentenc arab asr system obtain averag wer speaker sentenc arab asr system obtain averag wer",0
"KeywordsParallel corpora Linguistic resources Highly multilingual European Union Translation memory JRC-Acquis DGT-Acquis DGT-TM DCEP ECDC-TM EAC-TM JRC EuroVoc Indexer JEX EuroVoc Eur-Lex ","an overview of the european unions highly multilingual parallel corpora","Starting in 2006, the European Commission’s Joint Research Centre and other European Union organisations have made available a number of large-scale highly-multilingual parallel language resources. In this article, we give a comparative overview of these resources and we explain the specific nature of each of them. This article provides answers to a number of question, including: What are these linguistic resources? What is the difference between them? Why were they originally created and why was the data released publicly? What can they be used for and what are the limitations of their usability? What are the text types, subject domains and languages covered? How to avoid overlapping document sets? How do they compare regarding the formatting and the translation alignment? What are their usage conditions? What other types of multilingual linguistic resources does the EU have? This article thus aims to clarify what the similarities and differences between the various resources are and what they can be used for. It will also serve as a reference publication for those resources, for which a more detailed description has been lacking so far (EAC-TM, ECDC-TM and DGT-Acquis).","Language Resources and Evaluation",2014,"No","parallel corpora linguist resourc high multilingu european union translat memori jrc acqui dgt acqui dgt tm dcep ecdc tm eac tm jrc eurovoc index jex eurovoc eur lex overview european union high multilingu parallel corpora start european commiss joint research centr european union organis made number larg scale high multilingu parallel languag resourc articl give compar overview resourc explain specif natur articl answer number question includ linguist resourc differ origin creat data releas public limit usabl text type subject domain languag cover avoid overlap document set compar format translat align usag condit type multilingu linguist resourc eu articl aim clarifi similar differ resourc serv refer public resourc detail descript lack eac tm ecdc tm dgt acqui",0
"KeywordsCollocation Collocation error Miscollocation CALL Collocation error detection Collocation error correction ","towards advanced collocation error correction in spanish learner corpora","Collocations in the sense of idiosyncratic binary lexical co-occurrences are one of the biggest challenges for any language learner. Even advanced learners make collocation mistakes in that they literally translate collocation elements from their native tongue, create new words as collocation elements, choose a wrong subcategorization for one of the elements, etc. Therefore, automatic collocation error detection and correction is increasingly in demand. However, while state-of-the-art models predict, with a reasonable accuracy, whether a given co-occurrence is a valid collocation or not, only few of them manage to suggest appropriate corrections with an acceptable hit rate. Most often, a ranked list of correction options is offered from which the learner has then to choose. This is clearly unsatisfactory. Our proposal focuses on this critical part of the problem in the context of the acquisition of Spanish as second language. For collocation error detection, we use a frequency-based technique. To improve on collocation error correction, we discuss three different metrics with respect to their capability to select the most appropriate correction of miscollocations found in our learner corpus.","Language Resources and Evaluation",2014,"No","colloc colloc error miscolloc call colloc error detect colloc error correct advanc colloc error correct spanish learner corpora colloc sens idiosyncrat binari lexic occurr biggest challeng languag learner advanc learner make colloc mistak liter translat colloc element nativ tongu creat word colloc element choos wrong subcategor element automat colloc error detect correct increas demand state art model predict reason accuraci occurr valid colloc manag suggest correct accept hit rate rank list correct option offer learner choos unsatisfactori propos focus critic part problem context acquisit spanish languag colloc error detect frequenc base techniqu improv colloc error correct discuss metric respect capabl select correct miscolloc found learner corpus",0
"KeywordsSpeech recognition Lwazi corpus Resource-scarce languages South African languages ","collecting and evaluating speech recognition corpora for 11 south african languages","We describe the Lwazi corpus for automatic speech recognition (ASR), a new telephone speech corpus which contains data from the eleven official languages of South Africa. Because of practical constraints, the amount of speech per language is relatively small compared to major corpora in world languages, and we report on our investigation of the stability of the ASR models derived from the corpus. We also report on phoneme distance measures across languages, and describe initial phone recognisers that were developed using this data. We find that a surprisingly small number of speakers (fewer than 50) and around 10 to 20 h of speech per language are sufficient for the purposes of acceptable phone-based recognition.","Language Resources and Evaluation",2011,"No","speech recognit lwazi corpus resourc scarc languag south african languag collect evalu speech recognit corpora south african languag describ lwazi corpus automat speech recognit asr telephon speech corpus data eleven offici languag south africa practic constraint amount speech languag small compar major corpora world languag report investig stabil asr model deriv corpus report phonem distanc measur languag describ initi phone recognis develop data find surpris small number speaker fewer speech languag suffici purpos accept phone base recognit",0
"KeywordsAnnotation evaluation Discourse analysis Rhetorical Structure Theory Translation strategies ","a qualitative comparison method for rhetorical structures identifying different discourse structures in multilingual corpora","Explaining why the same passage may have different rhetorical structures when conveyed in different languages remains an open question. Starting from a trilingual translation corpus, this paper aims to provide a new qualitative method for the comparison of rhetorical structures in different languages and to specify why translated texts may differ in their rhetorical structures. To achieve these aims we have carried out a contrastive analysis, comparing a corpus of parallel English, Spanish and Basque texts, using Rhetorical Structure Theory. We propose a method to describe the main linguistic differences among the rhetorical structures of the three languages in the two annotation stages (segmentation and rhetorical analysis). We show a new type of comparison that has important advantages with regard to the quantitative method usually employed: it provides an accurate measurement of inter-annotator agreement, and it pinpoints sources of disagreement among annotators. With the use of this new method, we show how translation strategies affect discourse structure.","Language Resources and Evaluation",2015,"No","annot evalu discours analysi rhetor structur theori translat strategi qualit comparison method rhetor structur identifi discours structur multilingu corpora explain passag rhetor structur convey languag remain open question start trilingu translat corpus paper aim provid qualit method comparison rhetor structur languag translat text differ rhetor structur achiev aim carri contrast analysi compar corpus parallel english spanish basqu text rhetor structur theori propos method describ main linguist differ rhetor structur languag annot stage segment rhetor analysi show type comparison import advantag regard quantit method employ accur measur inter annot agreement pinpoint sourc disagr annot method show translat strategi affect discours structur",0
"semantic tagging word sense disambiguation WSDS evaluation inter-annotator agreement Italian corpus annotation ","sensevalromanseval the framework for italian","In this paper we present some observations concerning an experiment of (manual/automatic) semantic tagging of a small Italian corpus performed within the framework of the SENSEVAL/ROMANSEVAL initiative. Themain goal of the initiative was to set up a framework for evaluation of Word Sense Disambiguation systems (WSDS) through the comparative analysis of their performance on the same type of data. In this experiment there are two aspects which are of relevance: first, the preparation of the reference annotated corpus, and, second, the evaluation of the systems against it. In both aspects we are mainly interested here in the analysis of the linguistic side which can lead to a better understanding of the problem of semantic annotation of a corpus, be itmanual or automatic annotation. In particular, we will investigate, firstly, the reasons for disagreement between human annotators, secondly, some linguistically relevant aspects of the performance of the Italian WSDS and, finally, the lessons learned from the present experiment.","Computers and the Humanities",2000,"No","semant tag word sens disambigu wsds evalu inter annot agreement italian corpus annot sensevalromansev framework italian paper present observ experi manualautomat semant tag small italian corpus perform framework sensevalromansev initi themain goal initi set framework evalu word sens disambigu system wsds compar analysi perform type data experi aspect relev prepar refer annot corpus evalu system aspect interest analysi linguist side lead understand problem semant annot corpus itmanu automat annot investig first reason disagr human annot linguist relev aspect perform italian wsds final lesson learn present experi",0
"alignment bilingual document generation bitext parallel corpus segmentation SGML TEI translation memories ","bitext generation through rich markup","This paper reports on a method for exploiting a bitext as the primary linguistic information source for the design of a generation environment for specialized bilingual documentation. The paper discusses such issues as Text Encoding Initiative (TEI), proposals for specialized corpus tagging, text segmentation and alignment of translation units and their allocation into translation memories, Document Type Definition (DTD), abstraction from tagged texts, and DTD deployment for bilingual text generation. The parallel corpus used for experimentation has two main features:","Computers and the Humanities",2004,"No","align bilingu document generat bitext parallel corpus segment sgml tei translat memori bitext generat rich markup paper report method exploit bitext primari linguist inform sourc design generat environ special bilingu document paper discuss issu text encod initi tei propos special corpus tag text segment align translat unit alloc translat memori document type definit dtd abstract tag text dtd deploy bilingu text generat parallel corpus experiment main featur",0
"KeywordsPartial annotation Domain adaptation Dictionary Word segmentation POS tagging Non-maleficence of language resources ","a comparative study of dictionaries and corpora as methods for language resource addition","In this paper, we investigate the relative effect of two strategies for language resource addition for Japanese morphological analysis, a joint task of word segmentation and part-of-speech tagging. The first strategy is adding entries to the dictionary and the second is adding annotated sentences to the training corpus. The experimental results showed that addition of annotated sentences to the training corpus is better than the addition of entries to the dictionary. In particular, adding annotated sentences is especially efficient when we add new words with contexts of several real occurrences as partially annotated sentences, i.e. sentences in which only some words are annotated with word boundary information. According to this knowledge, we performed real annotation experiments on invention disclosure texts and observed word segmentation accuracy. Finally we investigated various language resource addition cases and introduced the notion of non-maleficence, asymmetricity, and additivity of language resources for a task. In the WS case, we found that language resource addition is non-maleficent (adding new resources causes no harm in other domains) and sometimes additive (adding new resources helps other domains). We conclude that it is reasonable for us, NLP tool providers, to distribute only one general-domain model trained from all the language resources we have.","Language Resources and Evaluation",2016,"No","partial annot domain adapt dictionari word segment pos tag malefic languag resourc compar studi dictionari corpora method languag resourc addit paper investig relat effect strategi languag resourc addit japanes morpholog analysi joint task word segment part speech tag strategi ad entri dictionari ad annot sentenc train corpus experiment result show addit annot sentenc train corpus addit entri dictionari ad annot sentenc effici add word context real occurr partial annot sentenc sentenc word annot word boundari inform knowledg perform real annot experi invent disclosur text observ word segment accuraci final investig languag resourc addit case introduc notion malefic asymmetr addit languag resourc task ws case found languag resourc addit malefic ad resourc harm domain addit ad resourc help domain conclud reason nlp tool provid distribut general domain model train languag resourc",0
"KeywordsComputational Linguistic ","analysis of corpora of variations",NA,"Computers and the Humanities",1974,"No","comput linguist analysi corpora variat na",0
"KeywordsSentiment analysis Electronic lexica Corpus analysis Financial information extraction ","is there a language of sentiment an analysis of lexical resources for sentiment analysis","In recent years, sentiment analysis (SA) has emerged as a rapidly expanding field of application and research in the area of information retrieval. In order to facilitate the task of selecting lexical resources for automated SA systems, this paper sets out a detailed analysis of four widely used sentiment lexica. The analysis provides an overview of the coverage of each lexicon individually, the overlap and consistency of the four resources and a corpus analysis of the distribution of the resources’ lexical contents in general and specialised language. This work aims to explore the characteristics of affective language as represented by these lexica and the implications of the findings for developers of SA systems.","Language Resources and Evaluation",2013,"No","sentiment analysi electron lexica corpus analysi financi inform extract languag sentiment analysi lexic resourc sentiment analysi recent year sentiment analysi sa emerg rapid expand field applic research area inform retriev order facilit task select lexic resourc autom sa system paper set detail analysi wide sentiment lexica analysi overview coverag lexicon individu overlap consist resourc corpus analysi distribut resourc lexic content general specialis languag work aim explor characterist affect languag repres lexica implic find develop sa system",0
"KeywordsDiscourse markers Speech corpora Annotating Conversation Discourse analysis Speech-to-speech translation Spontaneous speech Slovenian language ","annotating discourse markers in spontaneous speech corpora on an example for the slovenian language","Speech-to-speech translation technology has difficulties processing elements of spontaneity in conversation. We propose a discourse marker attribute in speech corpora to help overcome some of these problems. There have already been some attempts to annotate discourse markers in speech corpora. However, as there is no consistency on what expressions count as discourse markers, we have to reconsider how to set a framework for annotating, and, in order to better understand what we gain by introducing a discourse marker category, we have to analyse their characteristics and functions in discourse. This is especially important for languages such as Slovenian where no or little research on the topic of discourse markers has been carried out. The aims of this paper are to present a scheme for annotating discourse markers based on the analysis of a corpus of telephone conversations in the tourism domain in the Slovenian language, and to give some additional arguments based on the characteristics and functions of discourse markers that confirm their special status in conversation.","Language Resources and Evaluation",2007,"No","discours marker speech corpora annot convers discours analysi speech speech translat spontan speech slovenian languag annot discours marker spontan speech corpora slovenian languag speech speech translat technolog difficulti process element spontan convers propos discours marker attribut speech corpora overcom problem attempt annot discours marker speech corpora consist express count discours marker reconsid set framework annot order understand gain introduc discours marker categori analys characterist function discours import languag slovenian research topic discours marker carri aim paper present scheme annot discours marker base analysi corpus telephon convers tourism domain slovenian languag give addit argument base characterist function discours marker confirm special status convers",0
"KeywordsMonolingual treebank Semantic similarity Corpus Alignment Semantic relations Parallel text Comparable text Tree alignment Paraphrase ","construction of an aligned monolingual treebank for studying semantic similarity","Modern paraphrase research would benefit from large corpora with detailed annotations. However, currently these corpora are still thin on the ground. In this paper, we describe the development of such a corpus for Dutch, which takes the form of a parallel monolingual treebank consisting of over 2 million tokens and covering various text genres, including both parallel and comparable text. This publicly available corpus is richly annotated with alignments between syntactic nodes, which are also classified using five different semantic similarity relations. A quarter of the corpus is manually annotated, and this informs the development of an automatic tree aligner used to annotate the remainder of the corpus. We argue that this corpus is the first of this size and kind, and offers great potential for paraphrasing research.","Language Resources and Evaluation",2014,"No","monolingu treebank semant similar corpus align semant relat parallel text compar text tree align paraphras construct align monolingu treebank studi semant similar modern paraphras research benefit larg corpora detail annot corpora thin ground paper describ develop corpus dutch take form parallel monolingu treebank consist million token cover text genr includ parallel compar text public corpus rich annot align syntact node classifi semant similar relat quarter corpus manual annot inform develop automat tree align annot remaind corpus argu corpus size kind offer great potenti paraphras research",0
"KeywordsTerminology extraction Statistical machine translation Phrase-based statistical machine translation Log-likelihood Dice coefficient ","termfinder log likelihood comparison and phrase based statistical machine translation models for bilingual terminology extraction","Bilingual termbanks are important for many natural language processing applications, especially in translation workflows in industrial settings. In this paper, we apply a log-likelihood comparison method to extract monolingual terminology from the source and target sides of a parallel corpus. The initial candidate terminology list is prepared by taking all arbitrary n-gram word sequences from the corpus. Then, a well-known statistical measure (the Dice coefficient) is employed in order to remove any multi-word terms with weak associations from the candidate term list. Thereafter, the log-likelihood comparison method is applied to rank the phrasal candidate term list. Then, using a phrase-based statistical machine translation model, we create a bilingual terminology with the extracted monolingual term lists. We integrate an external knowledge source—the Wikipedia cross-language link databases—into the terminology extraction (TE) model to assist two processes: (a) the ranking of the extracted terminology list, and (b) the selection of appropriate target terms for a source term. First, we report the performance of our monolingual TE model compared to a number of the state-of-the-art TE models on English-to-Turkish and English-to-Hindi data sets. Then, we evaluate our novel bilingual TE model on an English-to-Turkish data set, and report the automatic evaluation results. We also manually evaluate our novel TE model on English-to-Spanish and English-to-Hindi data sets, and observe excellent performance for all domains.","Language Resources and Evaluation",2018,"No","terminolog extract statist machin translat phrase base statist machin translat log likelihood dice coeffici termfind log likelihood comparison phrase base statist machin translat model bilingu terminolog extract bilingu termbank import natur languag process applic translat workflow industri set paper appli log likelihood comparison method extract monolingu terminolog sourc target side parallel corpus initi candid terminolog list prepar take arbitrari gram word sequenc corpus statist measur dice coeffici employ order remov multi word term weak associ candid term list log likelihood comparison method appli rank phrasal candid term list phrase base statist machin translat model creat bilingu terminolog extract monolingu term list integr extern knowledg sourc wikipedia cross languag link databas terminolog extract te model assist process rank extract terminolog list select target term sourc term report perform monolingu te model compar number state art te model english turkish english hindi data set evalu bilingu te model english turkish data set report automat evalu result manual evalu te model english spanish english hindi data set observ excel perform domain",0
"KeywordsTurkish language resources Morphological parser Morphological disambiguation Web corpus ","resources for turkish morphological processing","We present a set of language resources and tools—a morphological parser, a morphological disambiguator, and a text corpus—for exploiting Turkish morphology in natural language processing applications. The morphological parser is a state-of-the-art finite-state transducer-based implementation of Turkish morphology. The disambiguator is based on the averaged perceptron algorithm and has the best accuracy reported for Turkish in the literature. The text corpus has been compiled from the web and contains about 500 million tokens. This is the largest Turkish web corpus published.","Language Resources and Evaluation",2011,"No","turkish languag resourc morpholog parser morpholog disambigu web corpus resourc turkish morpholog process present set languag resourc tool morpholog parser morpholog disambigu text corpus exploit turkish morpholog natur languag process applic morpholog parser state art finit state transduc base implement turkish morpholog disambigu base averag perceptron algorithm accuraci report turkish literatur text corpus compil web million token largest turkish web corpus publish",0
"KeywordsTimeML TimeBank Corpus analysis Temporal information extraction ","timebank evolution as a community resource for timeml parsing","TimeBank is the only reference corpus for TimeML, an expressive language for annotating complex temporal information. It is a rich resource for a broad range of research into various aspects of the expression of time and temporally related events. This paper traces the development of TimeBank from its initial—and somewhat noisy—version (1.1) to a substantially revised release (1.2), now available via the Linguistic Data Consortium. The development path is motivated by the encouraging empirical results of TimeML-compliant annotators developed on the basis of TimeBank 1.1, and is informed by a detailed study of the characteristics of that initial release, which guides a clean-up process turning TimeBank 1.2 into a consistent and robust community resource.","Language Resources and Evaluation",2007,"No","timeml timebank corpus analysi tempor inform extract timebank evolut communiti resourc timeml pars timebank refer corpus timeml express languag annot complex tempor inform rich resourc broad rang research aspect express time tempor relat event paper trace develop timebank initi noisi version substanti revis releas linguist data consortium develop path motiv encourag empir result timeml compliant annot develop basi timebank inform detail studi characterist initi releas guid clean process turn timebank consist robust communiti resourc",0
"KeywordsChinese chat language Phonetic mapping Chat language Modelling Chat term normalization Natural language processing ","normalization of chinese chat language","Real-time communication platforms such as ICQ, MSN and online chat rooms are getting more popular than ever on the Internet. There are, however, real risks where criminals and terrorists can perpetrate illegal and criminal abuses. This highlights the security significance of accurate detection and translation of the chat language to its stand language counterpart. The language used on these platforms differs significantly from the standard language. This language, referred to as chat language, is comparatively informal, anomalous and dynamic. Such features render conventional language resources such as dictionaries, and processing tools such as parsers ineffective. In this paper, we present the NIL corpus, a chat language text collection annotated to facilitate training and testing of chat language processing algorithms. We analyse the NIL corpus to study the linguistic characteristics and contextual behaviour of a chat language. First we observe that majority of the chat terms, i.e. informal words in a chat text, is formed by phonetic mapping. We then propose the eXtended Source Channel Model (XSCM) for the normalization of the chat language, which is a process to convert messages expressed in a chat language to its standard language counterpart. Experimental results indicate that the performance of XSCM in terms of chat term recognition and normalization accuracy is superior to its Source Channel Model (SCM) counterparts, and is also more consistent over time.","Language Resources and Evaluation",2008,"No","chines chat languag phonet map chat languag model chat term normal natur languag process normal chines chat languag real time communic platform icq msn onlin chat room popular internet real risk crimin terrorist perpetr illeg crimin abus highlight secur signific accur detect translat chat languag stand languag counterpart languag platform differ signific standard languag languag refer chat languag compar inform anomal dynam featur render convent languag resourc dictionari process tool parser ineffect paper present nil corpus chat languag text collect annot facilit train test chat languag process algorithm analys nil corpus studi linguist characterist contextu behaviour chat languag observ major chat term inform word chat text form phonet map propos extend sourc channel model xscm normal chat languag process convert messag express chat languag standard languag counterpart experiment result perform xscm term chat term recognit normal accuraci superior sourc channel model scm counterpart consist time",0
"KeywordsResource-scarce Data selection Corpus design  Speech recognition ","efficient data selection for asr","Automatic speech recognition (ASR) technology has matured over the past few decades and has made significant impacts in a variety of fields, from assistive technologies to commercial products. However, ASR system development is a resource intensive activity and requires language resources in the form of text annotated audio recordings and pronunciation dictionaries. Unfortunately, many languages found in the developing world fall into the resource-scarce category and due to this resource scarcity the deployment of ASR systems in the developing world is severely inhibited. One approach to assist with resource-scarce ASR system development, is to select “useful” training samples which could reduce the resources needed to collect new corpora. In this work, we propose a new data selection framework which can be used to design a speech recognition corpus. We show for limited data sets, independent of language and bandwidth, the most effective strategy for data selection is frequency-matched selection and that the widely-used maximum entropy methods generally produced the least promising results. In our model, the frequency-matched selection method corresponds to a logarithmic relationship between accuracy and corpus size; we also investigated other model relationships, and found that a hyperbolic relationship (as suggested from simple asymptotic arguments in learning theory) may lead to somewhat better performance under certain conditions.","Language Resources and Evaluation",2015,"No","resourc scarc data select corpus design speech recognit effici data select asr automat speech recognit asr technolog matur past decad made signific impact varieti field assist technolog commerci product asr system develop resourc intens activ requir languag resourc form text annot audio record pronunci dictionari languag found develop world fall resourc scarc categori due resourc scarciti deploy asr system develop world sever inhibit approach assist resourc scarc asr system develop select train sampl reduc resourc need collect corpora work propos data select framework design speech recognit corpus show limit data set independ languag bandwidth effect strategi data select frequenc match select wide maximum entropi method general produc promis result model frequenc match select method correspond logarithm relationship accuraci corpus size investig model relationship found hyperbol relationship suggest simpl asymptot argument learn theori lead perform condit",0
"Key Wordsstylometry literary detection Thisted-Efron tests Shakespearean canon ","are the thisted efron authorship tests valid","We assess the validity of the Thisted-Efron author-ship tests in two stages. First, we construct simulated texts in accordance with the assumptions implicit in the underlying model and use these to validate the basic computations, to determine their range of applicability, and to evaluate their sensitivity to basic lexical parameters. Second, we experiment with actual texts from the Shakespearean canon and the plays of Christopher Marlowe. The results of the tests are mixed, showing good consistency for the Shakespeare plays (with some discrimination among early, middle and late works) but poor consistency between Shakespeare's poems and plays, or among Marlowe's plays.","Computers and the Humanities",1991,"No","key wordsstylometri literari detect thist efron test shakespearean canon thist efron authorship test valid assess valid thist efron author ship test stage construct simul text accord assumpt implicit under model valid basic comput determin rang applic evalu sensit basic lexic paramet experi actual text shakespearean canon play christoph marlow result test mix show good consist shakespear play discrimin earli middl late work poor consist shakespear poem play marlow play",0
"automatic term recognition special languages special language subcorpora terms term extraction verb subcategorisation patterns ","an analysis of verb subcategorization frames in three special language corpora with a view towards automatic term recognition","Current term recognition algorithms havecentred mostly on the notion of term based onthe assumption that terms are monoreferentialand as such independent of context. Thecharacteristics and behaviour of terms in realtexts are however far removed from this idealbecause factors such as text type orcommunicative situation greatly influence thelinguistic realisation of a concept. Context,therefore, is important for the correctidentification of terms (Dubuc and Lauriston,1997). Based on this assumption, we haveshifted our emphasis from terms towardssurrounding linguistic context, namely verbs,as verbs are considered the central elements inthe sentence. More specifically, we have setout to examine whether verbs and verbal syntaxin particular, could help us towards the taskof automatic term recognition. Our findingssuggest that term occurrence variessignificantly in different argument structuresand different syntactic positions. Additionally, deviant grammatical structureshave proved rich environments for terms. Theanalysis was carried out in three differentspecialised subcorpora in order to explore howthe effectiveness of verbal syntax as apotential indicator of term occurrence can beconstrained by factors such as subject matterand text type.","Computers and the Humanities",2004,"No","automat term recognit special languag special languag subcorpora term term extract verb subcategoris pattern analysi verb subcategor frame special languag corpora view automat term recognit current term recognit algorithm havecentr notion term base onth assumpt term monoreferentialand independ context thecharacterist behaviour term realtext remov idealbecaus factor text type orcommun situat great influenc thelinguist realis concept context import correctidentif term dubuc lauriston base assumpt haveshift emphasi term towardssurround linguist context verb verb consid central element inth sentenc specif setout examin verb verbal syntaxin taskof automat term recognit findingssuggest term occurr variessignific argument structuresand syntact posit addit deviant grammat structureshav prove rich environ term theanalysi carri differentspecialis subcorpora order explor howth effect verbal syntax apotenti indic term occurr beconstrain factor subject matterand text type",0
"KeywordsMorphosyntactic annotation Multilinguality Language encoding standards ","multext east morphosyntactic resources for central and eastern european languages","The paper presents the MULTEXT-East language resources, a multilingual dataset for language engineering research, focused on the morphosyntactic level of linguistic description. The MULTEXT-East dataset includes the morphosyntactic specifications, morphosyntactic lexica, and a parallel corpus, the novel “1984” by George Orwell, which is sentence aligned and contains hand-validated morphosyntactic descriptions and lemmas. The resources are uniformly encoded in XML, using the Text Encoding Initiative Guidelines, TEI P5, and cover 16 languages, mainly from Central and Eastern Europe: Bulgarian, Croatian, Czech, English, Estonian, Hungarian, Macedonian, Persian, Polish, Resian, Romanian, Russian, Serbian, Slovak, Slovene, and Ukrainian. This dataset, unique in terms of languages covered and the wealth of encoding, is extensively documented, and freely available for research purposes. The paper overviews the MULTEXT-East resources by type and language and gives some conclusions and directions for further work.","Language Resources and Evaluation",2012,"No","morphosyntact annot multilingu languag encod standard multext east morphosyntact resourc central eastern european languag paper present multext east languag resourc multilingu dataset languag engin research focus morphosyntact level linguist descript multext east dataset includ morphosyntact specif morphosyntact lexica parallel corpus georg orwel sentenc align hand valid morphosyntact descript lemma resourc uniform encod xml text encod initi guidelin tei p cover languag central eastern europ bulgarian croatian czech english estonian hungarian macedonian persian polish resian romanian russian serbian slovak sloven ukrainian dataset uniqu term languag cover wealth encod extens document freeli research purpos paper overview multext east resourc type languag conclus direct work",0
"KeywordsCorpus construction Web corpora Boilerplate Non-destructive corpus normalization ","accurate and efficient general purpose boilerplate detection for crawled web corpora","Removal of boilerplate is one of the essential tasks in web corpus construction and web indexing. Boilerplate (redundant and automatically inserted material like menus, copyright notices, navigational elements, etc.) is usually considered to be linguistically unattractive for inclusion in a web corpus. Also, search engines should not index such material because it can lead to spurious results for search terms if these terms appear in boilerplate regions of the web page. In this paper, I present and evaluate a supervised machine-learning approach to general-purpose boilerplate detection for languages based on Latin alphabets using Multi-Layer Perceptrons (MLPs). It is both very efficient and very accurate (between 95 % and \(99\,\%\) correct classifications, depending on the input language). I show that language-specific classifiers greatly improve the accuracy of boilerplate detectors. The single features used for the classification are evaluated with regard to the merit they contribute to the classification. Furthermore, I show that the accuracy of the MLP is on a par with that of a wide range of other classifiers. My approach has been implemented in the open-source texrex web page cleaning software, and large corpora constructed using it are available from the COW initiative, including the CommonCOW corpora created from CommonCrawl datasets.
","Language Resources and Evaluation",2017,"No","corpus construct web corpora boilerpl destruct corpus normal accur effici general purpos boilerpl detect crawl web corpora remov boilerpl essenti task web corpus construct web index boilerpl redund automat insert materi menus copyright notic navig element consid linguist unattract inclus web corpus search engin index materi lead spurious result search term term boilerpl region web page paper present evalu supervis machin learn approach general purpos boilerpl detect languag base latin alphabet multi layer perceptron mlps effici accur correct classif depend input languag show languag specif classifi great improv accuraci boilerpl detector singl featur classif evalu regard merit contribut classif show accuraci mlp par wide rang classifi approach implement open sourc texrex web page clean softwar larg corpora construct cow initi includ commoncow corpora creat commoncrawl dataset",0
"KeywordsInformation Extraction Evaluation Message understanding conferences ","fact distribution in information extraction","Several recent Information Extraction (IE) systems have been restricted to the identification facts which are described within a single sentence. It is not clear what effect this has on the difficulty of the extraction task or how the performance of systems which consider only single sentences should be compared with those which consider multiple sentences. This paper compares three IE evaluation corpora, from the Message Understanding Conferences, and finds that a significant proportion of the facts mentioned therein are not described within a single sentence. Therefore systems which are evaluated only on facts described within single sentences are being tested against a limited portion of the relevant information in the text and it is difficult to compare their performance with other systems. Further analysis demonstrates that anaphora resolution and world knowledge are required to combine information described across multiple sentences. This result has implications for the development and evaluation of IE systems.","Language Resources and Evaluation",2006,"No","inform extract evalu messag understand confer fact distribut inform extract recent inform extract system restrict identif fact singl sentenc clear effect difficulti extract task perform system singl sentenc compar multipl sentenc paper compar evalu corpora messag understand confer find signific proport fact mention singl sentenc system evalu fact singl sentenc test limit portion relev inform text difficult compar perform system analysi demonstr anaphora resolut world knowledg requir combin inform multipl sentenc result implic develop evalu system",0
"KeywordsMachine learning Arabic text categorization Arabic text classification ","comparative evaluation of text classification techniques using a large diverse arabic dataset","A vast amount of valuable human knowledge is recorded in documents. The rapid growth in the number of machine-readable documents for public or private access necessitates the use of automatic text classification. While a lot of effort has been put into Western languages—mostly English—minimal experimentation has been done with Arabic. This paper presents, first, an up-to-date review of the work done in the field of Arabic text classification and, second, a large and diverse dataset that can be used for benchmarking Arabic text classification algorithms. The different techniques derived from the literature review are illustrated by their application to the proposed dataset. The results of various feature selections, weighting methods, and classification algorithms show, on average, the superiority of support vector machine, followed by the decision tree algorithm (C4.5) and Naïve Bayes. The best classification accuracy was 97 % for the Islamic Topics dataset, and the least accurate was 61 % for the Arabic Poems dataset.","Language Resources and Evaluation",2013,"No","machin learn arab text categor arab text classif compar evalu text classif techniqu larg divers arab dataset vast amount valuabl human knowledg record document rapid growth number machin readabl document public privat access necessit automat text classif lot effort put western languag english minim experiment arab paper present date review work field arab text classif larg divers dataset benchmark arab text classif algorithm techniqu deriv literatur review illustr applic propos dataset result featur select weight method classif algorithm show averag superior support vector machin decis tree algorithm c na ve bay classif accuraci islam topic dataset accur arab poem dataset",0
"KeywordsVerb subcategorization Hebrew Lexicography ","a hebrew verbcomplement dictionary","We present a verb–complement dictionary of Modern Hebrew, automatically extracted from text corpora. Carefully examining a large set of examples, we defined ten types of verb complements that cover the vast majority of the occurrences of verb complements in the corpora. We explored several collocation measures as indicators of the strength of the association between the verb and its complement. We then used these measures to automatically extract verb complements from corpora. The result is a wide-coverage, accurate dictionary that lists not only the likely complements for each verb, but also the likelihood of each complement. We evaluated the quality of the extracted dictionary both intrinsically and extrinsically. Intrinsically, we showed high precision and recall on randomly (but systematically) selected verbs. Extrinsically, we showed that using the extracted information is beneficial for two applications, prepositional phrase attachment disambiguation and Arabic-to-Hebrew machine translation.","Language Resources and Evaluation",2014,"No","verb subcategor hebrew lexicographi hebrew verbcompl dictionari present verb complement dictionari modern hebrew automat extract text corpora care examin larg set exampl defin ten type verb complement cover vast major occurr verb complement corpora explor colloc measur indic strength associ verb complement measur automat extract verb complement corpora result wide coverag accur dictionari list complement verb likelihood complement evalu qualiti extract dictionari intrins extrins intrins show high precis recal random systemat select verb extrins show extract inform benefici applic preposit phrase attach disambigu arab hebrew machin translat",0
"KeywordsCoreference Anaphora Corpus annotation Annotation scheme Reliability study ","ancora co coreferentially annotated corpora for spanish and catalan","This article describes the enrichment of the AnCora corpora of Spanish and Catalan (400 k each) with coreference links between pronouns (including elliptical subjects and clitics), full noun phrases (including proper nouns), and discourse segments. The coding scheme distinguishes between identity links, predicative relations, and discourse deixis. Inter-annotator agreement on the link types is 85–89% above chance, and we provide an analysis of the sources of disagreement. The resulting corpora make it possible to train and test learning-based algorithms for automatic coreference resolution, as well as to carry out bottom-up linguistic descriptions of coreference relations as they occur in real data.","Language Resources and Evaluation",2010,"No","corefer anaphora corpus annot annot scheme reliabl studi ancora coreferenti annot corpora spanish catalan articl describ enrich ancora corpora spanish catalan corefer link pronoun includ ellipt subject clitic full noun phrase includ proper noun discours segment code scheme distinguish ident link predic relat discours deixi inter annot agreement link type chanc provid analysi sourc disagr result corpora make train test learn base algorithm automat corefer resolut carri bottom linguist descript corefer relat occur real data",0
"KeywordsComputational Linguistic ","framework and results for french",NA,"Computers and the Humanities",2000,"No","comput linguist framework result french na",0
"KeywordsDetailed Analysis Publication Date Computational Linguistic Quantitative Inquiry Newspaper Text ","computing historical consciousness a quantitative inquiry into the presence of the past in newspaper texts","In this paper, some electronically gathered data arepresented and analyzed about the presence of the pastin newspaper texts. In ten large text corpora of sixdifferent languages, all dates in the form of yearsbetween 1930 and 1990 were counted. For six of thesecorpora this was done for all the years between 1200and 1993. Depicting these frequencies on the timeline,we find an underlying regularly declining curve,deviations at regular places and culturally determinedpeaks at irregular points. These three phenomena areanalyzed.","Computers and the Humanities",2001,"No","detail analysi public date comput linguist quantit inquiri newspap text comput histor conscious quantit inquiri presenc past newspap text paper electron gather data arepres analyz presenc pastin newspap text ten larg text corpora sixdiffer languag date form yearsbetween count thesecorpora year and depict frequenc timelin find under regular declin curvedevi regular place cultur determinedpeak irregular point phenomena areanalyz",0
"KeywordsLarge comparable corpora Translation equivalents Multiword expressions Distributional similarity ","irrefragable answers using comparable corpora to retrieve translation equivalents","In this paper we present a tool that uses comparable corpora to find appropriate translation equivalents for expressions that are considered by translators as difficult. For a phrase in the source language the tool identifies a range of possible expressions used in similar contexts in target language corpora and presents them to the translator as a list of suggestions. In the paper we discuss the method and present results of human evaluation of the performance of the tool, which highlight its usefulness when dictionary solutions are lacking.","Language Resources and Evaluation",2009,"No","larg compar corpora translat equival multiword express distribut similar irrefrag answer compar corpora retriev translat equival paper present tool compar corpora find translat equival express consid translat difficult phrase sourc languag tool identifi rang express similar context target languag corpora present translat list suggest paper discuss method present result human evalu perform tool highlight use dictionari solut lack",0
"KeywordsJapanese idiom Corpus Idiom identification Language resources ","compilation of an idiom example database for supervised idiom identification","Some phrases can be interpreted in their context either idiomatically (figuratively) or literally. The precise identification of idioms is essential in order to achieve full-fledged natural language processing. Because of this, the authors of this paper have created an idiom corpus for Japanese. This paper reports on the corpus itself and the results of an idiom identification experiment conducted using the corpus. The corpus targeted 146 ambiguous idioms, and consists of 102,856 examples, each of which is annotated with a literal/idiomatic label. All sentences were collected from the World Wide Web. For idiom identification, 90 out of the 146 idioms were targeted and a word sense disambiguation (WSD) method was adopted using both common WSD features and idiom-specific features. The corpus and the experiment are both, as far as can be determined, the largest of their kinds. It was discovered that a standard supervised WSD method works well for idiom identification and it achieved accuracy levels of 89.25 and 88.86%, with and without idiom-specific features, respectively. It was also found that the most effective idiom-specific feature is the one that involves the adjacency of idiom constituents.","Language Resources and Evaluation",2009,"No","japanes idiom corpus idiom identif languag resourc compil idiom databas supervis idiom identif phrase interpret context idiomat figur liter precis identif idiom essenti order achiev full fledg natur languag process author paper creat idiom corpus japanes paper report corpus result idiom identif experi conduct corpus corpus target ambigu idiom consist exampl annot literalidiomat label sentenc collect world wide web idiom identif idiom target word sens disambigu wsd method adopt common wsd featur idiom specif featur corpus experi determin largest kind discov standard supervis wsd method work idiom identif achiev accuraci level idiom specif featur found effect idiom specif featur involv adjac idiom constitu",0
"Key Wordsintertextuality Old French scribal practice manuscript traditions Marie de France Chrétien de Troyes ","intertextuality and large corpora a medievalist approach","This paper concurs with Mark Olsen's premise that computer-aided literature studies should take a different direction, one that is more suited to the computer's strength in analyzing large corpora of texts. However, the authors take issue with his conclusion that a reorientation of the notions of textual analysis is necessary in order to exploit the computer's capabilities. Contemporary medieval studies already provides us with models of textual analysis which are well suited to computer development. Though they stem from the particularities of medieval textual production, these models can perhaps be useful in the study of modern literatures.","Computers and the Humanities",1993,"No","key wordsintertextu french scribal practic manuscript tradit mari de franc chr tien de troy intertextu larg corpora medievalist approach paper concur mark olsen premis comput aid literatur studi direct suit comput strength analyz larg corpora text author issu conclus reorient notion textual analysi order exploit comput capabl contemporari mediev studi model textual analysi suit comput develop stem particular mediev textual product model studi modern literatur",0
"KeywordsThai ontology learning Lexico-syntactic patterns Taxonomic list ","automatic building of an ontology on the basis of text corpora in thai","This paper presents a methodology for automatic learning of ontologies from Thai text corpora, by extraction of terms and relations. A shallow parser is used to chunk texts on which we identify taxonomic relations with the help of cues: lexico-syntactic patterns and item lists. The main advantage of the approach is that it simplify the task of concept and relation labeling since cues help for identifying the ontological concept and hinting their relation. However, these techniques pose certain problems, i.e. cue word ambiguity, item list identification, and numerous candidate terms. We also propose the methodology to solve these problems by using lexicon and co-occurrence features and weighting them with information gain. The precision, recall and F-measure of the system are 0.74, 0.78 and 0.76, respectively.","Language Resources and Evaluation",2008,"No","thai ontolog learn lexico syntact pattern taxonom list automat build ontolog basi text corpora thai paper present methodolog automat learn ontolog thai text corpora extract term relat shallow parser chunk text identifi taxonom relat cue lexico syntact pattern item list main advantag approach simplifi task concept relat label cue identifi ontolog concept hint relat techniqu pose problem cue word ambigu item list identif numer candid term propos methodolog solv problem lexicon occurr featur weight inform gain precis recal measur system",0
"KeywordsStatistical machine translation Web crawling Crowdsourcing ","crawl and crowd to bring machine translation to under resourced languages","We present a widely applicable methodology to bring machine translation (MT) to under-resourced languages in a cost-effective and rapid manner. Our proposal relies on web crawling to automatically acquire parallel data to train statistical MT systems if any such data can be found for the language pair and domain of interest. If that is not the case, we resort to (1) crowdsourcing to translate small amounts of text (hundreds of sentences), which are then used to tune statistical MT models, and (2) web crawling of vast amounts of monolingual data (millions of sentences), which are then used to build language models for MT. We apply these to two respective use-cases for Croatian, an under-resourced language that has gained relevance since it recently attained official 
status in the European Union. The first use-case regards tourism, given the importance of this sector to Croatia’s economy, while the second has to do with tweets, due to the growing importance of social media. For tourism, we crawl parallel data from 20 web domains using two state-of-the-art crawlers and explore how to combine the crawled data with bigger amounts of general-domain data. Our domain-adapted system is evaluated on a set of three additional tourism web domains and it outperforms the baseline in terms of automatic metrics and/or vocabulary coverage. In the social media use-case, we deal with tweets from the 2014 edition of the soccer World Cup. We build domain-adapted systems by (1) translating small amounts of tweets to be used for tuning by means of crowdsourcing and (2) crawling vast amounts of monolingual tweets. These systems outperform the baseline (Microsoft Bing) by 7.94 BLEU points (5.11 TER) for Croatian-to-English and by 2.17 points (1.94 TER) for English-to-Croatian on a test set translated by means of crowdsourcing. A complementary manual analysis sheds further light on these results.","Language Resources and Evaluation",2017,"No","statist machin translat web crawl crowdsourc crawl crowd bring machin translat resourc languag present wide applic methodolog bring machin translat mt resourc languag cost effect rapid manner propos reli web crawl automat acquir parallel data train statist mt system data found languag pair domain interest case resort crowdsourc translat small amount text hundr sentenc tune statist mt model web crawl vast amount monolingu data million sentenc build languag model mt appli respect case croatian resourc languag gain relev recent attain offici status european union case tourism import sector croatia economi tweet due grow import social media tourism crawl parallel data web domain state art crawler explor combin crawl data bigger amount general domain data domain adapt system evalu set addit tourism web domain outperform baselin term automat metric vocabulari coverag social media case deal tweet edit soccer world cup build domain adapt system translat small amount tweet tune mean crowdsourc crawl vast amount monolingu tweet system outperform baselin microsoft bing bleu point ter croatian english point ter english croatian test set translat mean crowdsourc complementari manual analysi shed light result",0
"KeywordsResources Summarisation Arabic Under-resourced languages ","creating language resources for under resourced languages methodologies and experiments with arabic","Language resources are important for those working on computational methods to analyse and study languages. These resources are needed to help advancing the research in fields such as natural language processing, machine learning, information retrieval and text analysis in general. We describe the creation of useful resources for languages that currently lack them, taking resources for Arabic summarisation as a case study. We illustrate three different paradigms for creating language resources, namely: (1) using crowdsourcing to produce a small resource rapidly and relatively cheaply; (2) translating an existing gold-standard dataset, which is relatively easy but potentially of lower quality; and (3) using manual effort with appropriately skilled human participants to create a resource that is more expensive but of high quality. The last of these was used as a test collection for TAC-2011. An evaluation of the resources is also presented.","Language Resources and Evaluation",2015,"No","resourc summaris arab resourc languag creat languag resourc resourc languag methodolog experi arab languag resourc import work comput method analys studi languag resourc need advanc research field natur languag process machin learn inform retriev text analysi general describ creation resourc languag lack take resourc arab summaris case studi illustr paradigm creat languag resourc crowdsourc produc small resourc rapid cheapli translat exist gold standard dataset easi potenti lower qualiti manual effort appropri skill human particip creat resourc expens high qualiti test collect tac evalu resourc present",0
"KeywordsLanguage model Spoken dialogue systems User simulation Example-based generation ","automatic induction of language model data for a spoken dialogue system","In this paper, we address the issue of generating in-domain language model training data when little or no real user data are available. The two-stage approach taken begins with a data induction phase whereby linguistic constructs from out-of-domain sentences are harvested and integrated with artificially constructed in-domain phrases. After some syntactic and semantic filtering, a large corpus of synthetically assembled user utterances is induced. In the second stage, two sampling methods are explored to filter the synthetic corpus to achieve a desired probability distribution of the semantic content, both on the sentence level and on the class level. The first method utilizes user simulation technology, which obtains the probability model via an interplay between a probabilistic user model and the dialogue system. The second method synthesizes novel dialogue interactions from the raw data by modelling after a small set of dialogues produced by the developers during the course of system refinement. Evaluation is conducted on recognition performance in a restaurant information domain. We show that a partial match to usage-appropriate semantic content distribution can be achieved via user simulations. Furthermore, word error rate can be reduced when limited amounts of in-domain training data are augmented with synthetic data derived by our methods.","Language Resources and Evaluation",2006,"No","languag model spoken dialogu system user simul base generat automat induct languag model data spoken dialogu system paper address issu generat domain languag model train data real user data stage approach begin data induct phase linguist construct domain sentenc harvest integr artifici construct domain phrase syntact semant filter larg corpus synthet assembl user utter induc stage sampl method explor filter synthet corpus achiev desir probabl distribut semant content sentenc level class level method util user simul technolog obtain probabl model interplay probabilist user model dialogu system method synthes dialogu interact raw data model small set dialogu produc develop system refin evalu conduct recognit perform restaur inform domain show partial match usag semant content distribut achiev user simul word error rate reduc limit amount domain train data augment synthet data deriv method",0
"KeywordsCorpus annotation Polarity Sentiment analysis  Natural language processing ","the good the bad and the implicit a comprehensive approach to annotating explicit and implicit sentiment","We present a fine-grained scheme for the annotation of polar sentiment in text, that accounts for explicit sentiment (so-called private states), as well as implicit expressions of sentiment (polar facts). Polar expressions are annotated below sentence level and classified according to their subjectivity status. Additionally, they are linked to one or more targets with a specific polar orientation and intensity. Other components of the annotation scheme include source attribution and the identification and classification of expressions that modify polarity. In previous research, little attention has been given to implicit sentiment, which represents a substantial amount of the polar expressions encountered in our data. An English and Dutch corpus of financial newswire text, consisting of over 45,000 words each, was annotated using our scheme. A subset of this corpus was used to conduct an inter-annotator agreement study, which demonstrated that the proposed scheme can be used to reliably annotate explicit and implicit sentiment in real-world textual data, making the created corpora a useful resource for sentiment analysis.","Language Resources and Evaluation",2015,"No","corpus annot polar sentiment analysi natur languag process good bad implicit comprehens approach annot explicit implicit sentiment present fine grain scheme annot polar sentiment text account explicit sentiment call privat state implicit express sentiment polar fact polar express annot sentenc level classifi subject status addit link target specif polar orient intens compon annot scheme includ sourc attribut identif classif express modifi polar previous research attent implicit sentiment repres substanti amount polar express encount data english dutch corpus financi newswir text consist word annot scheme subset corpus conduct inter annot agreement studi demonstr propos scheme reliabl annot explicit implicit sentiment real world textual data make creat corpora resourc sentiment analysi",0
"KeywordsInformation extraction Word similarity Comparable corpora Singular value decomposition ","is singular value decomposition useful for word similarity extraction","In this paper, we analyze the behaviour of Singular Value Decomposition in a number of word similarity extraction tasks, namely acquisition of translation equivalents from comparable corpora. Special attention is paid to two different aspects: computational efficiency and extraction quality. The main objective of the paper is to describe several experiments comparing methods based on Singular Value Decomposition (SVD) to other strategies. The results lead us to conclude that SVD makes the extraction less computationally efficient and much less precise than other more basic models for the task of extracting translation equivalents from comparable corpora.","Language Resources and Evaluation",2011,"No","inform extract word similar compar corpora singular decomposit singular decomposit word similar extract paper analyz behaviour singular decomposit number word similar extract task acquisit translat equival compar corpora special attent paid aspect comput effici extract qualiti main object paper describ experi compar method base singular decomposit svd strategi result lead conclud svd make extract comput effici precis basic model task extract translat equival compar corpora",0
"KeywordsTask-performance evaluation Referring expressions Demonstrative pronouns Situated dialogue Japanese ","a task performance evaluation of referring expressions in situated collaborative task dialogues","Appropriate evaluation of referring expressions is critical for the design of systems that can effectively collaborate with humans. A widely used method is to simply evaluate the degree to which an algorithm can reproduce the same expressions as those in previously collected corpora. Several researchers, however, have noted the need of a task-performance evaluation measuring the effectiveness of a referring expression in the achievement of a given task goal. This is particularly important in collaborative situated dialogues. Using referring expressions used by six pairs of Japanese speakers collaboratively solving Tangram puzzles, we conducted a task-performance evaluation of referring expressions with 36 human evaluators. Particularly we focused on the evaluation of demonstrative pronouns generated by a machine learning-based algorithm. Comparing the results of this task-performance evaluation with the results of a previously conducted corpus-matching evaluation (Spanger et al. in Lang Resour Eval, 2010b), we confirmed the limitation of a corpus-matching evaluation and discuss the need for a task-performance evaluation.","Language Resources and Evaluation",2013,"No","task perform evalu refer express demonstr pronoun situat dialogu japanes task perform evalu refer express situat collabor task dialogu evalu refer express critic design system effect collabor human wide method simpli evalu degre algorithm reproduc express previous collect corpora research note task perform evalu measur effect refer express achiev task goal import collabor situat dialogu refer express pair japanes speaker collabor solv tangram puzzl conduct task perform evalu refer express human evalu focus evalu demonstr pronoun generat machin learn base algorithm compar result task perform evalu result previous conduct corpus match evalu spanger al lang resour eval b confirm limit corpus match evalu discuss task perform evalu",0
"KeywordsCall-centre data Automatic speech recognition system Opinion detection Business concept detection Disfluency ","spontaneous speech and opinion detection mining call centre transcripts","Opinion mining on conversational telephone speech tackles two challenges: the robustness of speech transcriptions and the relevance of opinion models. The two challenges are critical in an industrial context such as marketing. The paper addresses jointly these two issues by analyzing the influence of speech transcription errors on the detection of opinions and business concepts. We present both modules: the speech transcription system, which consists in a successful adaptation of a conversational speech transcription system to call-centre data and the information extraction module, which is based on a semantic modeling of business concepts, opinions and sentiments with complex linguistic rules. Three models of opinions are implemented based on the discourse theory, the appraisal theory and the marketers’ expertise, respectively. The influence of speech recognition errors on the information extraction module is evaluated by comparing its outputs on manual versus automatic transcripts. The F-scores obtained are 0.79 for business concepts detection, 0.74 for opinion detection and 0.67 for the extraction of relations between opinions and their target. This result and the in-depth analysis of the errors show the feasibility of opinion detection based on complex rules on call-centre transcripts.","Language Resources and Evaluation",2013,"No","call centr data automat speech recognit system opinion detect busi concept detect disfluenc spontan speech opinion detect mine call centr transcript opinion mine convers telephon speech tackl challeng robust speech transcript relev opinion model challeng critic industri context market paper address joint issu analyz influenc speech transcript error detect opinion busi concept present modul speech transcript system consist success adapt convers speech transcript system call centr data inform extract modul base semant model busi concept opinion sentiment complex linguist rule model opinion implement base discours theori apprais theori market expertis influenc speech recognit error inform extract modul evalu compar output manual versus automat transcript score obtain busi concept detect opinion detect extract relat opinion target result depth analysi error show feasibl opinion detect base complex rule call centr transcript",0
"KeywordsNamed entity recognition Annotation Classifier ensembles Subtype classification ","fine grained dutch named entity recognition","This paper describes the creation of a fine-grained named entity annotation scheme and corpus for Dutch, and experiments on automatic main type and subtype named entity recognition. We give an overview of existing named entity annotation schemes, and motivate our own, which describes six main types (persons, organizations, locations, products, events and miscellaneous named entities) and finer-grained information on subtypes and metonymic usage. This was applied to a one-million-word subset of the Dutch SoNaR reference corpus. The classifier for main type named entities achieves a micro-averaged F-score of 84.91 %, and is publicly available, along with the corpus and annotations.","Language Resources and Evaluation",2014,"No","name entiti recognit annot classifi ensembl subtyp classif fine grain dutch name entiti recognit paper describ creation fine grain name entiti annot scheme corpus dutch experi automat main type subtyp name entiti recognit give overview exist name entiti annot scheme motiv describ main type person organ locat product event miscellan name entiti finer grain inform subtyp metonym usag appli million word subset dutch sonar refer corpus classifi main type name entiti achiev micro averag score public corpus annot",0
"KeywordsConceptual Modeling Problem Domain Computational Linguistic Computing Technology Recent Introduction ","conceptual modeling versus visual modeling a technological key to building consensus","Debate has long been a hallmark of the academic endeavor. The recent introduction of computers into academic life has not been the deus ex machina to bring sudden resolution to these debates. There is a new computing technology, however, that has some promise in this regard. It is called conceptual modeling. This paper' demonstrates how a computer-based model of a problem domain can lead to consensus when competing approaches to the domain can be encapsulated in different visual models that are applied to the same underlying conceptual model.","Computers and the Humanities",1996,"No","conceptu model problem domain comput linguist comput technolog recent introduct conceptu model versus visual model technolog key build consensus debat long hallmark academ endeavor recent introduct comput academ life deus machina bring sudden resolut debat comput technolog promis regard call conceptu model paper demonstr comput base model problem domain lead consensus compet approach domain encapsul visual model appli under conceptu model",0
NA,"review essay",NA,"Computers and the Humanities",1984,"No","review essay na",0
"education gender language proficiency lexical statistics parts of speech socioeconomic background speech vocabulary vocabulary richness ","vocabulary in interviews as related to respondent characteristics","Responses in personalinterviews about education and career with 415Swedish men and women (age 34) forms the basisof a speech corpus with 1.8 million words. Thevocabulary is described by means of two sets ofvariables. One is based on the number of tokensand types, word length and sectioning of therunning text. The other set divides the corpusinto grammatical categories. Both sets ofvariables are related to a number of backgroundvariables such as gender, socioeconomicbackground, education, and indicators of verbalproficiency at age 13 and 32. This possibilityto study the relationship between vocabularyand a broad set of respondent characteristicsis a unique feature of this corpus.","Computers and the Humanities",2003,"No","educ gender languag profici lexic statist part speech socioeconom background speech vocabulari vocabulari rich vocabulari interview relat respond characterist respons personalinterview educ career swedish men women age form basisof speech corpus million word thevocabulari mean set ofvari base number tokensand type word length section therun text set divid corpusinto grammat categori set ofvari relat number backgroundvari gender socioeconomicbackground educ indic verbalprofici age possibilityto studi relationship vocabularyand broad set respond characteristicsi uniqu featur corpus",0
"automatic document encoding corpus linguistics TEI World Wide Web ","taking snapshots of the web with a tei camera","Electronic texts are claimed to exhibit features distinct from their more tangible cousins. The Snapshot project aims to observe and capture language usage in an electronic medium by creating an open corpus of World Wide Web documents. These documents are re-encoded using the TEI guidelines to create a flexible, persistent and portable data repository. This report gives an overview of the decisions made with respect to the re-encoding of HTML documents, and with the structuring the overall corpus.","Computers and the Humanities",1999,"No","automat document encod corpus linguist tei world wide web take snapshot web tei camera electron text claim exhibit featur distinct tangibl cousin snapshot project aim observ captur languag usag electron medium creat open corpus world wide web document document encod tei guidelin creat flexibl persist portabl data repositori report overview decis made respect encod html document structur corpus",0
"KeywordsOperational interoperability Conceptual interoperability Parsing Tagging Adverbial clause Speech Writing The International Corpus of English ","creating an interoperable language resource for interoperable linguistic studies","There are two different levels of interoperability for language resources: operational interoperability and conceptual interoperability. The former refers to the standardization of the formal aspects of language resources so that different resources can work together. The latter refers to the standardization of the notional representation of the semantic content of the analysis. This article addresses both issues but focuses on the latter through a description of the annotation and analysis of the International Corpus of English, which is a corpus for the study of English as a global language. The project is parameterised by component, regional sub-corpora and a set of pre-defined textual categories. The one-million-word British component has been constructed, grammatically tagged, and syntactically parsed. This article is first of all a description of steps taken to ensure conformity within the project. These include corpus design, part-of-speech tagging, and syntactic parsing. The article will then present a study that examines the use of adverbial clauses across speech and writing, illustrating the imminent necessity for interoperable analysis of linguistic data.","Language Resources and Evaluation",2012,"No","oper interoper conceptu interoper pars tag adverbi claus speech write intern corpus english creat interoper languag resourc interoper linguist studi level interoper languag resourc oper interoper conceptu interoper refer standard formal aspect languag resourc resourc work refer standard notion represent semant content analysi articl address issu focus descript annot analysi intern corpus english corpus studi english global languag project parameteris compon region corpora set pre defin textual categori million word british compon construct grammat tag syntact pars articl descript step ensur conform project includ corpus design part speech tag syntact pars articl present studi examin adverbi claus speech write illustr immin necess interoper analysi linguist data",0
"Keywordsarabic automatic diacritics generation envelope geographic names N-gram romanization security stochastic transliteration ","stochastic models for automatic diacritics generation of arabic names","In this paper, two new models for generating diacritics for Arabic names are proposed. The first proposed model is called N-gram model. It is a stochastic model that is based on generating a corpus database of N-grams extracted from a large database of names with their corresponding probability according to an N-gram position in a text (Bhal et al., 1983). i.e., the probability that an N-gram has happened in a position x, where x can be the first, second,... or ith position in the text. Replacing the N-grams with their patterns extends the first model to the second proposed stochastic model. It is called the Envelope model. These two proposed models are unique in being the first attempt to solve the problem in Arabic text diacritics generation using linguistic constraints stochastic approaches that are neither grammatical nor pure lexical based (Merialdo, 1991; Ney and Essen, 1991; Schukat-Talamazzini et al., 1992; Witschel and Niedermair, 1992). This methodology helps in reducing size and complexity of software implementation of the proposed models and makes it easier to update and port across different platforms.","Computers and the Humanities",2004,"No","arab automat diacrit generat envelop geograph name gram roman secur stochast transliter stochast model automat diacrit generat arab name paper model generat diacrit arab name propos propos model call gram model stochast model base generat corpus databas gram extract larg databas name probabl gram posit text bhal al probabl gram happen posit ith posit text replac gram pattern extend model propos stochast model call envelop model propos model uniqu attempt solv problem arab text diacrit generat linguist constraint stochast approach grammat pure lexic base merialdo ney essen schukat talamazzini al witschel niedermair methodolog help reduc size complex softwar implement propos model make easier updat port platform",0
"KeywordsParsing Dependency grammar Child language Syntactic annotation ","parsing hebrew childes transcripts","We present a syntactic parser of (transcripts of) spoken Hebrew: a dependency parser of the Hebrew CHILDES database. CHILDES is a corpus of child–adult linguistic interactions. Its Hebrew section has recently been morphologically analyzed and disambiguated, paving the way for syntactic annotation. This paper describes a novel annotation scheme of dependency relations reflecting constructions of child and child-directed Hebrew utterances. A subset of the corpus was annotated with dependency relations according to this scheme, and was used to train two parsers (MaltParser and MEGRASP) with which the rest of the data were parsed. The adequacy of the annotation scheme to the CHILDES data is established through numerous evaluation scenarios. The paper also discusses different annotation approaches to several linguistic phenomena, as well as the contribution of morphological features to the accuracy of parsing.","Language Resources and Evaluation",2015,"No","pars depend grammar child languag syntact annot pars hebrew child transcript present syntact parser transcript spoken hebrew depend parser hebrew child databas child corpus child adult linguist interact hebrew section recent morpholog analyz disambigu pave syntact annot paper describ annot scheme depend relat reflect construct child child direct hebrew utter subset corpus annot depend relat scheme train parser maltpars megrasp rest data pars adequaci annot scheme child data establish numer evalu scenario paper discuss annot approach linguist phenomena contribut morpholog featur accuraci pars",0
"KeywordsEvaluation methodology Information extraction Machine learning ","evaluation of machine learning based information extraction algorithms criticisms and recommendations","We survey the evaluation methodology adopted in information extraction (IE), as defined in a few different efforts applying machine learning (ML) to IE. We identify a number of critical issues that hamper comparison of the results obtained by different researchers. Some of these issues are common to other NLP-related tasks: e.g., the difficulty of exactly identifying the effects on performance of the data (sample selection and sample size), of the domain theory (features selected), and of algorithm parameter settings. Some issues are specific to IE: how leniently to assess inexact identification of filler boundaries, the possibility of multiple fillers for a slot, and how the counting is performed. We argue that, when specifying an IE task, these issues should be explicitly addressed, and a number of methodological characteristics should be clearly defined. To empirically verify the practical impact of the issues mentioned above, we perform a survey of the results of different algorithms when applied to a few standard datasets. The survey shows a serious lack of consensus on these issues, which makes it difficult to draw firm conclusions on a comparative evaluation of the algorithms. Our aim is to elaborate a clear and detailed experimental methodology and propose it to the IE community. Widespread agreement on this proposal should lead to future IE comparative evaluations that are fair and reliable. To demonstrate the way the methodology is to be applied we have organized and run a comparative evaluation of ML-based IE systems (the Pascal Challenge on ML-based IE) where the principles described in this article are put into practice. In this article we describe the proposed methodology and its motivations. The Pascal evaluation is then described and its results presented.","Language Resources and Evaluation",2008,"No","evalu methodolog inform extract machin learn evalu machin learn base inform extract algorithm critic recommend survey evalu methodolog adopt inform extract defin effort appli machin learn ml identifi number critic issu hamper comparison result obtain research issu common nlp relat task difficulti identifi effect perform data sampl select sampl size domain theori featur select algorithm paramet set issu specif lenient assess inexact identif filler boundari possibl multipl filler slot count perform argu task issu explicit address number methodolog characterist defin empir verifi practic impact issu mention perform survey result algorithm appli standard dataset survey show lack consensus issu make difficult draw firm conclus compar evalu algorithm aim elabor clear detail experiment methodolog propos communiti widespread agreement propos lead futur compar evalu fair reliabl demonstr methodolog appli organ run compar evalu ml base system pascal challeng ml base principl articl put practic articl describ propos methodolog motiv pascal evalu result present",0
"KeywordsSentiment analysis Twitter Machine-learning Corpora for the Spanish language ","spanish sentiment analysis in twitter at the tass workshop","This paper describes a support vector machine-based approach to different tasks related to sentiment analysis in Twitter for Spanish. We focus on parameter optimization of the models and the combination of several models by means of voting techniques. We evaluate the proposed approach in all the tasks that were defined in the five editions of the TASS workshop, between 2012 and 2016. TASS has become a framework for sentiment analysis tasks that are focused on the Spanish language. We describe our participation in this competition and the results achieved, and then we provide an analysis of and comparison with the best approaches of the teams who participated in all the tasks defined in the TASS workshops. To our knowledge, our results exceed those published to date in the sentiment analysis tasks of the TASS workshops.","Language Resources and Evaluation",2018,"No","sentiment analysi twitter machin learn corpora spanish languag spanish sentiment analysi twitter tass workshop paper describ support vector machin base approach task relat sentiment analysi twitter spanish focus paramet optim model combin model mean vote techniqu evalu propos approach task defin edit tass workshop tass framework sentiment analysi task focus spanish languag describ particip competit result achiev provid analysi comparison approach team particip task defin tass workshop knowledg result exceed publish date sentiment analysi task tass workshop",0
"KeywordsAutomatic annotation Intrinsic and extrinsic evaluation Syntactic alternation Dative alternation Logistic regression ","evaluating automatic annotation automatically detecting and enriching instances of the dative alternation","In this article, we automatically create two large and richly annotated data sets for studying the English dative alternation. With an intrinsic and an extrinsic evaluation, we address the question of whether such data sets that are obtained and enriched automatically are suitable for linguistic research, even if they contain errors. The extrinsic evaluation consists of building logistic regression models with these data sets. We conclude that the automatic approach for detecting instances of the dative alternation still needs human intervention, but that it is indeed possible to annotate the instances with features that are syntactic, semantic and discourse-related in nature. Only the automatic classification of the concreteness of nouns is problematic.","Language Resources and Evaluation",2012,"No","automat annot intrins extrins evalu syntact altern dativ altern logist regress evalu automat annot automat detect enrich instanc dativ altern articl automat creat larg rich annot data set studi english dativ altern intrins extrins evalu address question data set obtain enrich automat suitabl linguist research error extrins evalu consist build logist regress model data set conclud automat approach detect instanc dativ altern human intervent annot instanc featur syntact semant discours relat natur automat classif concret noun problemat",0
"KeywordsText annotation Web-based annotation tool GATE Cloud-based text annotation service ","gate teamware a web based collaborative text annotation framework","This paper presents GATE Teamware—an open-source, web-based, collaborative text annotation framework. It enables users to carry out complex corpus annotation projects, involving distributed annotator teams. Different user roles are provided (annotator, manager, administrator) with customisable user interface functionalities, in order to support the complex workflows and user interactions that occur in corpus annotation projects. Documents may be pre-processed automatically, so that human annotators can begin with text that has already been pre-annotated and thus making them more efficient. The user interface is simple to learn, aimed at non-experts, and runs in an ordinary web browser, without need of additional software installation. GATE Teamware has been evaluated through the creation of several gold standard corpora and internal projects, as well as through external evaluation in commercial and EU text annotation projects. It is available as on-demand service on GateCloud.net, as well as open-source for self-installation.","Language Resources and Evaluation",2013,"No","text annot web base annot tool gate cloud base text annot servic gate teamwar web base collabor text annot framework paper present gate teamwar open sourc web base collabor text annot framework enabl user carri complex corpus annot project involv distribut annot team user role provid annot manag administr customis user interfac function order support complex workflow user interact occur corpus annot project document pre process automat human annot begin text pre annot make effici user interfac simpl learn aim expert run ordinari web browser addit softwar instal gate teamwar evalu creation gold standard corpora intern project extern evalu commerci eu text annot project demand servic gatecloudnet open sourc instal",0
"Key wordsLinguistic annotation Multi-modal language corpora Software tools ","the nite xml toolkit data model and query language","The NITE XML Toolkit (NXT) is open source software for working with language corpora, with particular strengths for multimodal and heavily cross-annotated data sets. In NXT, annotations are described by types and attribute value pairs, and can relate to signal via start and end times, to representations of the external environment, and to each other via either an arbitrary graph structure or a multi-rooted tree structure characterized by both temporal and structural orderings. Simple queries in NXT express variable bindings for n-tuples of objects, optionally constrained by type, and give a set of conditions on the n-tuples combined with boolean operators. The defined operators for the condition tests allow full access to the timing and structural properties of the data model. A complex query facility passes variable bindings from one query to another for filtering, returning a tree structure. In addition to describing NXTȁ9s core data handling and search capabilities, we explain the stand-off XML data storage format that it employs and illustrate its use with examples from an early adopter of the technology.","Language Resources and Evaluation",2005,"No","key wordslinguist annot multi modal languag corpora softwar tool nite xml toolkit data model queri languag nite xml toolkit nxt open sourc softwar work languag corpora strength multimod heavili cross annot data set nxt annot type attribut pair relat signal start end time represent extern environ arbitrari graph structur multi root tree structur character tempor structur order simpl queri nxt express variabl bind tupl object option constrain type give set condit tupl combin boolean oper defin oper condit test full access time structur properti data model complex queri facil pass variabl bind queri filter return tree structur addit describ nxt s core data handl search capabl explain stand xml data storag format employ illustr exampl earli adopt technolog",0
"corpus annotation reusability ","the feasibility of incremental linguistic annotation","This paper examines the feasibility of incremental annotation, i.e. using existing annotation on a text as the basis for further annotation rather than starting the new annotation from scratch. It contains a theoretical component, describing basic methodology and potential obstacles, as well as a practical component, describing an experiment which tests the efficiency of incremental annotation. Apart from guidelines for the execution of such pilot experiments, the experiment demonstrates that incremental annotation is most effective when supported by thorough pre-planning and documentation. Unplanned, opportunistic use of existing annotation is much less effective in its reduction of annotation time and furthermore increases the development time of the annotation software, so that this type of incremental annotation appears only practical for large amounts of heritage data.","Computers and the Humanities",1998,"No","corpus annot reusabl feasibl increment linguist annot paper examin feasibl increment annot exist annot text basi annot start annot scratch theoret compon describ basic methodolog potenti obstacl practic compon describ experi test effici increment annot guidelin execut pilot experi experi demonstr increment annot effect support pre plan document unplan opportunist exist annot effect reduct annot time increas develop time annot softwar type increment annot appear practic larg amount heritag data",0
"text encoding syntactic tagging syntactic tagset ","tei encoding and syntactic tagging of an old french text","This paper report on some of the concrete outcomes of a larger research project on the study of syntactic change. In this part of the project, we are collecting and encoding historical texts and tagging them for syntactic analysis. We have so far produced a TEI-conformant version of an Old French text, La Vie de Saint Louis written by Jehan de Joinville around 1305, and we are in the process of adding syntactic tags to this text. Those syntactic tags are derived from the Penn-Helsinki coding scheme, which had been devised for the syntactic encoding of Middle English texts, and have been translated into TEI.","Computers and the Humanities",1999,"No","text encod syntact tag syntact tagset tei encod syntact tag french text paper report concret outcom larger research project studi syntact chang part project collect encod histor text tag syntact analysi produc tei conform version french text la vie de saint loui written jehan de joinvill process ad syntact tag text syntact tag deriv penn helsinki code scheme devis syntact encod middl english text translat tei",0
"aggregate methods association measures multidimensional scaling profile-based analysis variational linguistics ","profile based linguistic uniformity as a generic method for comparing language varieties","In this text we present``profile-based linguistic uniformity'', a methoddesigned to compare language varieties on thebasis of a wide range of potentiallyheterogeneous linguistic variables. In manyrespects a parallel can be drawn with currentmethods in dialectometry (for an overview, see,Nerbonne and Heeringa, 2001; Heeringa, Nerbonneand Kleiweg, 2002): in both casesdissimilarities between varieties on the basisof individual variables are summarized inglobal dissimilarities, and a series oflanguage varieties are subsequently clusteredor charted using multivariate techniques suchas cluster analysis or multidimensionalscaling. This global similarity between themethods makes it possible to compare them andto investigate the implications of notabledifferences. In this text we specifically focuson, and defend one characteristic of ourmethodology, its profile-based nature.","Computers and the Humanities",2003,"No","aggreg method associ measur multidimension scale profil base analysi variat linguist profil base linguist uniform generic method compar languag varieti text presentprofil base linguist uniform methoddesign compar languag varieti thebasi wide rang potentiallyheterogen linguist variabl manyrespect parallel drawn currentmethod dialectometri overview nerbonn heeringa heeringa nerbonneand kleiweg casesdissimilar varieti basisof individu variabl summar inglob dissimilar seri oflanguag varieti subsequ clusteredor chart multivari techniqu sucha cluster analysi multidimensionalsc global similar themethod make compar andto investig implic notablediffer text specif focuson defend characterist ourmethodolog profil base natur",0
"Key Wordslinguistic variation genre style register dimension factor analysis cluster analysis historical change English Somali ","the multi dimensional approach to linguistic analyses of genre variation an overview of methodology and findings","The present paper summarizes the major methods and results of the multi-dimensional approach to genre variation. The approach combines the resources of computational tools, large text corpora, and multivariate statistical tools (such as factor analysis and cluster analysis). It has been used to address issues such as the relations among spoken and written genres in English, and the historical development of genres and styles. The approach has also been applied to other languages; in this regard it has been used to address broader theoretical issues, such as the extent to which genre, and style variation are comparable cross-linguistically, and the linguistic consequences of literacy.","Computers and the Humanities",1992,"No","key wordslinguist variat genr style regist dimens factor analysi cluster analysi histor chang english somali multi dimension approach linguist analys genr variat overview methodolog find present paper summar major method result multi dimension approach genr variat approach combin resourc comput tool larg text corpora multivari statist tool factor analysi cluster analysi address issu relat spoken written genr english histor develop genr style approach appli languag regard address broader theoret issu extent genr style variat compar cross linguist linguist consequ literaci",0
"KeywordsMachine Translation Name Entity Recognition Language Resource Asian Language Ontology Building ","asian language resources the state of the art","This special issue of Language Resources and Evaluation, entitled “New Frontiers in Asian Language Resources”, complements the earlier special double issue on Asian Language Processing: State of the Art Resources and Processing (Huang et al. 2006) by presenting eight papers describing specific Asian language resources. As Bird and Simons (2003) explain, research on language resources must deal with how the resources can be acquired and documented as well as how the resources can be accessed and used. Among the eight papers in this issue, the first four papers focus on resources, while the latter four target specific application tasks and describe resource building in the contexts of these applications.","Language Resources and Evaluation",2008,"No","machin translat entiti recognit languag resourc asian languag ontolog build asian languag resourc state art special issu languag resourc evalu entitl frontier asian languag resourc complement earlier special doubl issu asian languag process state art resourc process huang al present paper describ specif asian languag resourc bird simon explain research languag resourc deal resourc acquir document resourc access paper issu paper focus resourc target specif applic task describ resourc build context applic",0
"KeywordsAudio-visual database Dyadic interaction Emotion Emotional assessment Motion capture system ","iemocap interactive emotional dyadic motion capture database","Since emotions are expressed through a combination of verbal and non-verbal channels, a joint analysis of speech and gestures is required to understand expressive human communication. To facilitate such investigations, this paper describes a new corpus named the “interactive emotional dyadic motion capture database” (IEMOCAP), collected by the Speech Analysis and Interpretation Laboratory (SAIL) at the University of Southern California (USC). This database was recorded from ten actors in dyadic sessions with markers on the face, head, and hands, which provide detailed information about their facial expressions and hand movements during scripted and spontaneous spoken communication scenarios. The actors performed selected emotional scripts and also improvised hypothetical scenarios designed to elicit specific types of emotions (happiness, anger, sadness, frustration and neutral state). The corpus contains approximately 12 h of data. The detailed motion capture information, the interactive setting to elicit authentic emotions, and the size of the database make this corpus a valuable addition to the existing databases in the community for the study and modeling of multimodal and expressive human communication.","Language Resources and Evaluation",2008,"No","audio visual databas dyadic interact emot emot assess motion captur system iemocap interact emot dyadic motion captur databas emot express combin verbal verbal channel joint analysi speech gestur requir understand express human communic facilit investig paper describ corpus name interact emot dyadic motion captur databas iemocap collect speech analysi interpret laboratori sail univers southern california usc databas record ten actor dyadic session marker face head hand provid detail inform facial express hand movement script spontan spoken communic scenario actor perform select emot script improvis hypothet scenario design elicit specif type emot happi anger sad frustrat neutral state corpus approxim data detail motion captur inform interact set elicit authent emot size databas make corpus valuabl addit exist databas communiti studi model multimod express human communic",0
"KeywordsAnnotation Corpus annotation Machine assistance Syriac studies Bayesian data analysis User study Language resource evaluation ","evaluating machine assisted annotation in under resourced settings","Machine assistance is vital to managing the cost of corpus annotation projects. Identifying effective forms of machine assistance through principled evaluation is particularly important and challenging in under-resourced domains and highly heterogeneous corpora, as the quality of machine assistance varies. We perform a fine-grained evaluation of two machine-assistance techniques in the context of an under-resourced corpus annotation project. This evaluation requires a carefully controlled user study crafted to test a number of specific hypotheses. We show that human annotators performing morphological analysis of text in a Semitic language perform their task significantly more accurately and quickly when even mediocre pre-annotations are provided. When pre-annotations are at least 70 % accurate, annotator speed and accuracy show statistically significant relative improvements of 25–35 and 5–7 %, respectively. However, controlled user studies are too costly to be suitable for under-resourced corpus annotation projects. Thus, we also present an alternative analysis methodology that models the data as a combination of latent variables in a Bayesian framework. We show that modeling the effects of interesting confounding factors can generate useful insights. In particular, correction propagation appears to be most effective for our task when implemented with minimal user involvement. More importantly, by explicitly accounting for confounding variables, this approach has the potential to yield fine-grained evaluations using data collected in a natural environment outside of costly controlled user studies.","Language Resources and Evaluation",2014,"No","annot corpus annot machin assist syriac studi bayesian data analysi user studi languag resourc evalu evalu machin assist annot resourc set machin assist vital manag cost corpus annot project identifi effect form machin assist principl evalu import challeng resourc domain high heterogen corpora qualiti machin assist vari perform fine grain evalu machin assist techniqu context resourc corpus annot project evalu requir care control user studi craft test number specif hypothes show human annot perform morpholog analysi text semit languag perform task signific accur quick mediocr pre annot provid pre annot accur annot speed accuraci show statist signific relat improv control user studi cost suitabl resourc corpus annot project present altern analysi methodolog model data combin latent variabl bayesian framework show model effect interest confound factor generat insight correct propag appear effect task implement minim user involv import explicit account confound variabl approach potenti yield fine grain evalu data collect natur environ cost control user studi",0
"KeywordsPOS tagging Lemmatization Spelling variation Orthographic variation Historical text Middle Dutch ","dealing with orthographic variation in a tagger lemmatizer for fourteenth century dutch charters","In this paper we describe a tagger-lemmatizer for fourteenth century Dutch charters (as found in the corpus van Reenen/Mulder), with a special focus on the treatment of the extensive orthographic variation in this material. We show that despite the difficulties caused by the variation, we are still able to reach about 95 % accuracy in a tenfold cross-validation experiment for both tagging and lemmatization. We can deal effectively with the variation in tokenization (as applied by the authors) by pre-normalization (retokenization). For variation in spelling, however, we choose to expand our lexicon with predicted spelling variants. For those forms which can also not be found in this expanded lexicon, we first derive the word class and subsequently search for the most similar lexicon word. Interestingly, our techniques for recognizing spelling variants turn out to be vital for lemmatization accuracy, but much less important for tagging accuracy.","Language Resources and Evaluation",2013,"No","pos tag lemmat spell variat orthograph variat histor text middl dutch deal orthograph variat tagger lemmat fourteenth centuri dutch charter paper describ tagger lemmat fourteenth centuri dutch charter found corpus van reenenmuld special focus treatment extens orthograph variat materi show difficulti caus variat reach accuraci tenfold cross valid experi tag lemmat deal effect variat token appli author pre normal retoken variat spell choos expand lexicon predict spell variant form found expand lexicon deriv word class subsequ search similar lexicon word interest techniqu recogn spell variant turn vital lemmat accuraci import tag accuraci",0
"Keywordsalignment error rate bilingual evaluation gold standard manual alignment parallel corpus precision recall word alignment ","guidelines for word alignment evaluation and manual alignment","The purpose of this paper is to provide guidelines for building a word alignment evaluation scheme. The notion of word alignment quality depends on the application: here we review standard scoring metrics for full text alignment and give explanations on how to use them better. We discuss strategies to build a reference corpus, and show that the ratio between ambiguous and unambiguous links in the reference has a great impact on scores measured with these metrics. In particular, automatically computed alignments with higher precision or higher recall can be favoured depending on the value of this ratio. Finally, we suggest a strategy to build a reference corpus particularly adapted to applications where recall plays a significant role, like in machine translation. The manually aligned corpus we built for the Spanish-English European Parliament corpus is also described. This corpus is freely available.","Language Resources and Evaluation",2005,"No","align error rate bilingu evalu gold standard manual align parallel corpus precis recal word align guidelin word align evalu manual align purpos paper provid guidelin build word align evalu scheme notion word align qualiti depend applic review standard score metric full text align give explan discuss strategi build refer corpus show ratio ambigu unambigu link refer great impact score measur metric automat comput align higher precis higher recal favour depend ratio final suggest strategi build refer corpus adapt applic recal play signific role machin translat manual align corpus built spanish english european parliament corpus corpus freeli",0
"KeywordsAppraisal Parameters Corpus linguistics Evaluative language Customer review texts ","how products are evaluated evaluation in customer review texts","This study, drawing on insights from the Appraisal framework, the parameter-based approach to evaluation and corpus linguistics, investigates the evaluative language used in customer review texts. The primary goal of this investigation is to develop a framework of evaluation that can be used to account adequately for evaluative expressions in customer review texts, and the ultimate goal is to support the argument that the modelling and theorising of evaluation is context-specific. Based on the investigation into a corpus compiled of review texts retrieved from www.amazon.co.uk, this study proposes a data-driven, parameter-based and appraisal-informed framework of evaluation which comprises four parameters—Quality, Satisfactoriness, Recommendability and Worthiness. Since these parameters are not thought-up, but are generalised from real data, it is arguable that the proposed framework of evaluation is certainly valid and thus can be used to describe and analyse evaluative language used in this particular context. This in turn indicates that the description and theorising of evaluation is indeed highly dependent on the discourse type that is under examination.","Language Resources and Evaluation",2016,"No","apprais paramet corpus linguist evalu languag custom review text product evalu evalu custom review text studi draw insight apprais framework paramet base approach evalu corpus linguist investig evalu languag custom review text primari goal investig develop framework evalu account adequ evalu express custom review text ultim goal support argument model theoris evalu context specif base investig corpus compil review text retriev wwwamazonuk studi propos data driven paramet base apprais inform framework evalu compris paramet qualiti satisfactori recommend worthi paramet thought generalis real data arguabl propos framework evalu valid describ analys evalu languag context turn descript theoris evalu high depend discours type examin",0
"KeywordsLexical acquisition Subcategorisation Verb classes Catalan Clustering ","automatic acquisition of syntactic verb classes with basic resources","This paper describes a methodology aimed at grouping Catalan verbs according to their syntactic behavior. Our goal is to acquire a small number of basic classes with a high level of accuracy, using minimal resources. Information on syntactic class, expensive and slow to compile by hand, is useful for any NLP task requiring specific lexical information. We show that it is possible to acquire this kind of information using only a POS-tagged corpus. We perform two clustering experiments. The first one aims at classifying verbs into transitive, intransitive and verbs alternating with a se-construction. Our system achieves an average 0.84 F-score, for a task with a 0.33 baseline. The second experiment aims at further distinguishing among pure intransitives and verbs bearing a prepositional object. The baseline for the task is 0.51 and the upperbound 0.98. The system achieves an average 0.88 F-score.","Language Resources and Evaluation",2006,"No","lexic acquisit subcategoris verb class catalan cluster automat acquisit syntact verb class basic resourc paper describ methodolog aim group catalan verb syntact behavior goal acquir small number basic class high level accuraci minim resourc inform syntact class expens slow compil hand nlp task requir specif lexic inform show acquir kind inform pos tag corpus perform cluster experi aim classifi verb transit intransit verb altern se construct system achiev averag score task baselin experi aim distinguish pure intransit verb bear preposit object baselin task upperbound system achiev averag score",0
"KeywordsMultiword expressions Spoken language Transcription Pronunciation reduction Identification ","analyzing and identifying multiword expressions in spoken language","The present paper investigates multiword expressions (MWEs) in spoken language and possible ways of identifying MWEs automatically in speech corpora. Two MWEs that emerged from previous studies and that occur frequently in Dutch are analyzed to study their pronunciation characteristics and compare them to those of other utterances in a large speech corpus. The analyses reveal that these MWEs display extreme pronunciation variation and reduction, i.e., many phonemes and even syllables are deleted. Several measures of pronunciation reduction are calculated for these two MWEs and for all other utterances in the corpus. Five of these measures are more than twice as high for the MWEs, thus indicating considerable reduction. One overall measure of pronunciation deviation is then calculated and used to automatically identify MWEs in a large speech corpus. The results show that neither this overall measure, nor frequency of co-occurrence alone are suitable for identifying MWEs. The best results are obtained by using a metric that combines overall pronunciation reduction with weighted frequency. In this way, recurring “islands of pronunciation reduction” that contain (potential) MWEs can be identified in a large speech corpus.","Language Resources and Evaluation",2010,"No","multiword express spoken languag transcript pronunci reduct identif analyz identifi multiword express spoken languag present paper investig multiword express mwes spoken languag way identifi mwes automat speech corpora mwes emerg previous studi occur frequent dutch analyz studi pronunci characterist compar utter larg speech corpus analys reveal mwes display extrem pronunci variat reduct phonem syllabl delet measur pronunci reduct calcul mwes utter corpus measur high mwes indic consider reduct measur pronunci deviat calcul automat identifi mwes larg speech corpus result show measur frequenc occurr suitabl identifi mwes result obtain metric combin pronunci reduct weight frequenc recur island pronunci reduct potenti mwes identifi larg speech corpus",0
"KeywordsUnder-resourced language Rule-based Grapheme-to-phoneme conversion Automatic speech recognition Tunisian dialect ","automatic speech recognition system for tunisian dialect","Although Modern Standard Arabic is taught in schools and used in written communication and TV/radio broadcasts, all informal communication is typically carried out in dialectal Arabic.
 In this work, we focus on the design of speech tools and resources required for the development of an Automatic Speech Recognition system for the Tunisian dialect.
 The development of such a system faces the challenges of the lack of annotated resources and tools, apart from the lack of standardization at all linguistic levels (phonological, morphological, syntactic and lexical) together with the mispronunciation dictionary needed for ASR development.
 In this paper, we present a historical overview of the Tunisian dialect and its linguistic characteristics. We also describe and evaluate our rule-based phonetic tool. Next, we go deeper into the details of Tunisian dialect corpus creation. This corpus is finally approved and used to build the first ASR system for Tunisian dialect with a Word Error Rate of 22.6%.
","Language Resources and Evaluation",2018,"No","resourc languag rule base graphem phonem convers automat speech recognit tunisian dialect automat speech recognit system tunisian dialect modern standard arab taught school written communic tvradio broadcast inform communic typic carri dialect arab work focus design speech tool resourc requir develop automat speech recognit system tunisian dialect develop system face challeng lack annot resourc tool lack standard linguist level phonolog morpholog syntact lexic mispronunci dictionari need asr develop paper present histor overview tunisian dialect linguist characterist describ evalu rule base phonet tool deeper detail tunisian dialect corpus creation corpus final approv build asr system tunisian dialect word error rate",0
"context/kwd> corpus evaluation lexicography part-of-speech tagging word sense disambiguation sense-tagging ","peeling an onion the lexicographers experience ofmanual sense tagging","SENSEVAL set itself the task of evaluating automaticword sense disambiguation programs (see Kilgarriff andRosenzweig, this volume, for an overview of theframework and results). In order to do this, it wasnecessary to provide a `gold standard' dataset of `correct' answers. This paper will describe thelexicographic part of the process involved in creatingthat dataset. The primary objective was for a group oflexicographers to manually examine keywords in a largenumber of corpus contexts, and assign to each contexta sense-tag for the keyword, taken from the Hectordictionary. Corpus contexts also had to be manuallypart-of-speech (POS) tagged. Various observationsmade and insights gained by the lexicographers duringthis process will be presented, including a critiqueof the resources and the methodology.","Computers and the Humanities",2000,"No","contextkwd corpus evalu lexicographi part speech tag word sens disambigu sens tag peel onion lexicograph experi ofmanu sens tag sensev set task evalu automaticword sens disambigu program kilgarriff androsenzweig volum overview theframework result order wasnecessari provid gold standard dataset correct answer paper describ thelexicograph part process involv creatingthat dataset primari object group oflexicograph manual examin keyword largenumb corpus context assign contexta sens tag keyword hectordictionari corpus context manuallypart speech pos tag observationsmad insight gain lexicograph duringthi process present includ critiqueof resourc methodolog",0
"Constraint Grammar corpus investigation nominative pronouns in English and Norwegian statistic infrequency Subject position ","tagging and the case of pronouns","Using a corpus to investigate empirically grammatical phenomena prior to writing grammatical rules or constraints for a disambiguating tagger is important. The paper shows how even case distinctions on pronouns are used more diversely than is usually assumed. Both in English and Norwegian nominative pronouns are used in more positions than the expected Subject one. Although the other uses are statistically less frequent, they may be important to the users of the resulting tagged corpus – who are often theoretical linguists. A tagger should therefore tag correctly also the more infrequent constructions. The paper shows how this can be done in a Constraint Grammar type tagger.","Computers and the Humanities",1998,"No","constraint grammar corpus investig nomin pronoun english norwegian statist infrequ subject posit tag case pronoun corpus investig empir grammat phenomena prior write grammat rule constraint disambigu tagger import paper show case distinct pronoun divers assum english norwegian nomin pronoun posit expect subject statist frequent import user result tag corpus theoret linguist tagger tag correct infrequ construct paper show constraint grammar type tagger",0
"KeywordsSuperlatives Annotation Wikipedia Resources Adjectives ","textwiki a superlative resource","Although superlatives are commonly used in natural language, so far there has been no large-scale computational investigation of the types of comparisons they express. This article describes a comprehensive annotation scheme for superlatives, which classifies superlatives according to their surface forms and motivates an initial focus on so-called “ISA superlatives”. This type of superlative comparison is especially suitable for a computational approach because both their targets and comparison sets are explicitly realised in the text, and the proposed annotation scheme offers guidelines for annotating the spans of such comparative elements. The annotations are tested and evaluated on 500 tokens of superlatives with good inter-annotator agreement. In addition to providing a platform for investigating superlatives on a larger scale, this research also introduces a new text-based Wikipedia corpus in which all superlative instances have been annotated according to the proposed annotation scheme, and which has been used to develop a tool that can reliably distinguish between different superlative types, and identify the comparative components of ISA superlatives.","Language Resources and Evaluation",2012,"No","superl annot wikipedia resourc adject textwiki superl resourc superl common natur languag larg scale comput investig type comparison express articl describ comprehens annot scheme superl classifi superl surfac form motiv initi focus call isa superl type superl comparison suitabl comput approach target comparison set explicit realis text propos annot scheme offer guidelin annot span compar element annot test evalu token superl good inter annot agreement addit provid platform investig superl larger scale research introduc text base wikipedia corpus superl instanc annot propos annot scheme develop tool reliabl distinguish superl type identifi compar compon isa superl",0
"KeywordsText simplification Aligned monolingual corpora Simplification operations ","text simplification resources for spanish","In this paper we present the development of a text simplification system for Spanish. Text simplification is the adaptation of a text for the special needs of certain groups of readers, such as language learners, people with cognitive difficulties, and elderly people, among others. There is a clear need for simplified texts, but manual production and adaptation of existing text is labour-intensive and costly. Automatic simplification is a field which attracts growing attention in Natural Language Processing, but, to the best of our knowledge, there are no existing simplification tools for Spanish. We present a corpus study which aims to identify the operations a text simplification system needs to carry out in order to produce an output similar to what human editors produce when they simplify news texts. We also present a first prototype for automatic simplification, which shows that the most important simplification operations can be successfully treated.","Language Resources and Evaluation",2014,"No","text simplif align monolingu corpora simplif oper text simplif resourc spanish paper present develop text simplif system spanish text simplif adapt text special group reader languag learner peopl cognit difficulti elder peopl clear simplifi text manual product adapt exist text labour intens cost automat simplif field attract grow attent natur languag process knowledg exist simplif tool spanish present corpus studi aim identifi oper text simplif system carri order produc output similar human editor produc simplifi news text present prototyp automat simplif show import simplif oper success treat",0
"KeywordsCross-lingual word sense disambiguation Word sense induction Sense clustering Parallel corpora WordNet ","data driven synset induction and disambiguation for wordnet development","Automatic methods for wordnet development in languages other than English generally exploit information found in Princeton WordNet (PWN) and translations extracted from parallel corpora. A common approach consists in preserving the structure of PWN and transferring its content in new languages using alignments, possibly combined with information extracted from multilingual semantic resources. Even if the role of PWN remains central in this process, these automatic methods offer an alternative to the manual elaboration of new wordnets. However, their limited coverage has a strong impact on that of the resulting resources. Following this line of research, we apply a cross-lingual word sense disambiguation method to wordnet development. Our approach exploits the output of a data-driven sense induction method that generates sense clusters in new languages, similar to wordnet synsets, by identifying word senses and relations in parallel corpora. We apply our cross-lingual word sense disambiguation method to the task of enriching a French wordnet resource, the WOLF, and show how it can be efficiently used for increasing its coverage. Although our experiments involve the English–French language pair, the proposed methodology is general enough to be applied to the development of wordnet resources in other languages for which parallel corpora are available. Finally, we show how the disambiguation output can serve to reduce the granularity of new wordnets and the degree of polysemy present in PWN.","Language Resources and Evaluation",2014,"No","cross lingual word sens disambigu word sens induct sens cluster parallel corpora wordnet data driven synset induct disambigu wordnet develop automat method wordnet develop languag english general exploit inform found princeton wordnet pwn translat extract parallel corpora common approach consist preserv structur pwn transfer content languag align possibl combin inform extract multilingu semant resourc role pwn remain central process automat method offer altern manual elabor wordnet limit coverag strong impact result resourc line research appli cross lingual word sens disambigu method wordnet develop approach exploit output data driven sens induct method generat sens cluster languag similar wordnet synset identifi word sens relat parallel corpora appli cross lingual word sens disambigu method task enrich french wordnet resourc wolf show effici increas coverag experi involv english french languag pair propos methodolog general appli develop wordnet resourc languag parallel corpora final show disambigu output serv reduc granular wordnet degre polysemi present pwn",0
"KeywordsEnlargement of morphological dictionaries Knowledge elicitation Resource development for under-resourced languages Machine translation ","assisting non expert speakers of under resourced languages in assigning stems and inflectional paradigms to new word entries of morphological dictionaries","This paper presents a new method with which to assist individuals with no background in linguistics to create monolingual dictionaries such as those used by the morphological analysers of many natural language processing applications.
 The involvement of non-expert users is especially critical for under-resourced languages which either lack or cannot afford the recruitment of a skilled workforce. Adding a word to a morphological dictionary usually requires identifying its stem along with the inflection paradigm that can be used in order to generate all the word forms of the new entry. Our method works under the assumption that the average speakers of a language can successfully answer the polar question “is x a valid form of the word w to be inserted?”, where x represents tentative alternative (inflected) forms of the new word w. The experiments show that with a small number of polar questions the correct stem and paradigm can be obtained from non-experts with high success rates. We study the impact of different heuristic and probabilistic approaches on the actual number of questions.","Language Resources and Evaluation",2017,"No","enlarg morpholog dictionari knowledg elicit resourc develop resourc languag machin translat assist expert speaker resourc languag assign stem inflect paradigm word entri morpholog dictionari paper present method assist individu background linguist creat monolingu dictionari morpholog analys natur languag process applic involv expert user critic resourc languag lack afford recruit skill workforc ad word morpholog dictionari requir identifi stem inflect paradigm order generat word form entri method work assumpt averag speaker languag success answer polar question valid form word insert repres tentat altern inflect form word experi show small number polar question correct stem paradigm obtain expert high success rate studi impact heurist probabilist approach actual number question",0
"Key Wordskey word model statistics stylometry vocabulary ","where have all the key words gone","A key word with regard to a sub-corpus is a word of which the frequency in that sub-corpus is significantly higher than expected under the hypothesis that its use and the variable “part of the corpus” are mutually independent. A study in literary statistics almost invariably includes a chapter devoted to key words. However, a strong attack has been recently launched upon the way stylometry has been modelling texts since the classical works of Herdan, Guiraud or Muller. In fact statistical modelling seems as valid in stylistics as in any other field of the humanities and social sciences. What is questionable is the fact that many studies in literary statistics are more satisfied with the easy identification of monsters, i.e. literary phenomena unexplained by wrong models, than with the laborious research of models fitting the textual data well. A short examination of the mentioned controversy and the quantitative analysis of an example provided by Laclos' novelLes Liaisons dangereuses endeavour to support this argument.","Computers and the Humanities",1989,"No","key wordskey word model statist stylometri vocabulari key word key word regard corpus word frequenc corpus signific higher expect hypothesi variabl part corpus mutual independ studi literari statist invari includ chapter devot key word strong attack recent launch stylometri model text classic work herdan guiraud muller fact statist model valid stylist field human social scienc question fact studi literari statist satisfi easi identif monster literari phenomena unexplain wrong model labori research model fit textual data short examin mention controversi quantit analysi provid laclo novell liaison dangereus endeavour support argument",0
"Key Wordsword frequency distribution lognormal generalized inverse Gauss-Poisson extended generalized Zipf's law vocabulary richness morphological productivity goodness of fit ","statistical models for word frequency distributions a linguistic evaluation","Three models for word frequency distributions, the lognormal law, the generalized inverse Gauss-Poisson law and the extended generalized Zipf's law are compared and evaluated with respect to goodness of fit and rationale. Application of these models to frequency distributions of a text, a corpus and morphological data reveals that no model can lay claim to exclusive validity, while inspection of the extrapolated theoretical vocabulary sizes raises doubts as to whether the urn scheme with independent trials is the correct underlying model for word frequency data. The role of morphology in shaping word frequency distributions is discussed, as well as parallelisms between vocabulary richness in literary studies and morphological productivity in linguistics.","Computers and the Humanities",1992,"No","key wordsword frequenc distribut lognorm general invers gauss poisson extend general zipf law vocabulari rich morpholog product good fit statist model word frequenc distribut linguist evalu model word frequenc distribut lognorm law general invers gauss poisson law extend general zipf law compar evalu respect good fit rational applic model frequenc distribut text corpus morpholog data reveal model lay claim exclus valid inspect extrapol theoret vocabulari size rais doubt urn scheme independ trial correct under model word frequenc data role morpholog shape word frequenc distribut discuss parallel vocabulari rich literari studi morpholog product linguist",0
"KeywordsForeign Language Natural Language Processing Language Learner Language Resource Parallel Corpus ","introduction to the special issue on resources and tools for language learners","This special issue of Language Resources and Evaluation is devoted to Resources and Tools for Language Learners.","Language Resources and Evaluation",2014,"No","foreign languag natur languag process languag learner languag resourc parallel corpus introduct special issu resourc tool languag learner special issu languag resourc evalu devot resourc tool languag learner",0
"KeywordsCorpora Language resources Language tools Lexicon Machine translation Morphology ","building language resources for a multi engine english filipino machine translation system","In this paper, we present the building of various language resources for a multi-engine bi-directional English-Filipino Machine Translation (MT) system. Since linguistics information on Philippine languages are available, but as of yet, the focus has been on theoretical linguistics and little is done on the computational aspects of these languages, attempts are reported here on the manual construction of these language resources such as the grammar, lexicon, morphological information, and the corpora which were literally built from almost non-existent digital forms. Due to the inherent difficulties of manual construction, we also discuss our experiments on various technologies for automatic extraction of these resources to handle the intricacies of the Filipino language, designed with the intention of using them for the MT system. To implement the different MT engines and to ensure the improvement of translation quality, other language tools (such as the morphological analyzer and generator, and the part of speech tagger) were developed.","Language Resources and Evaluation",2008,"No","corpora languag resourc languag tool lexicon machin translat morpholog build languag resourc multi engin english filipino machin translat system paper present build languag resourc multi engin bi direct english filipino machin translat mt system linguist inform philippin languag focus theoret linguist comput aspect languag attempt report manual construct languag resourc grammar lexicon morpholog inform corpora liter built exist digit form due inher difficulti manual construct discuss experi technolog automat extract resourc handl intricaci filipino languag design intent mt system implement mt engin ensur improv translat qualiti languag tool morpholog analyz generat part speech tagger develop",0
"KeywordsTreebank Error detection Entropy ","vietnamese treebank construction and entropy based error detection","Treebanks, especially the Penn treebank for natural language processing (NLP) in English, play an essential role in both research into and the application of NLP. However, many languages still lack treebanks and building a treebank can be very complicated and difficult. This work has a twofold objective. Firstly, to share our results in constructing a large Vietnamese treebank (VTB) with three levels of annotation including word segmentation, part-of-speech tagging, and syntactic analysis. Major steps in the treebank construction process are described with particular regard to specific Vietnamese properties such as lack of word delimiter and isolation. Those properties make sentences highly syntactically ambiguous, and therefore it is difficult to ensure a high level of agreement among annotators. Various studies of Vietnamese syntax were employed not only to define annotations but also to systematically deal with ambiguities. Annotators were supported by automatic labelling tools, which are based on statistical machine learning methods, for sentence pre-processing and a tree editor for supporting manual annotation. As a result, an annotation agreement of around 90 % was achieved. Our second objective is to present our method for automatically finding errors and inconsistencies in treebank corpora and its application to the construction of the VTB. This method employs the Shannon entropy measure in a manner that the more reduced entropy the more corrected errors in a treebank. The method ranks error candidates by using a scoring function based on conditional entropy. Our experiments showed that this method detected high-error-density subsets of original error candidate sets, and that the corpus entropy was significantly reduced after error correction. The size of these subsets was only about one third of the whole set, while these subsets contained 80–90 % of the total errors. This method can also be applied to languages similar to Vietnamese.","Language Resources and Evaluation",2015,"No","treebank error detect entropi vietnames treebank construct entropi base error detect treebank penn treebank natur languag process nlp english play essenti role research applic nlp languag lack treebank build treebank complic difficult work twofold object first share result construct larg vietnames treebank vtb level annot includ word segment part speech tag syntact analysi major step treebank construct process regard specif vietnames properti lack word delimit isol properti make sentenc high syntact ambigu difficult ensur high level agreement annot studi vietnames syntax employ defin annot systemat deal ambigu annot support automat label tool base statist machin learn method sentenc pre process tree editor support manual annot result annot agreement achiev object present method automat find error inconsist treebank corpora applic construct vtb method employ shannon entropi measur manner reduc entropi correct error treebank method rank error candid score function base condit entropi experi show method detect high error densiti subset origin error candid set corpus entropi signific reduc error correct size subset set subset contain total error method appli languag similar vietnames",0
"KeywordsAutomatic identification Word alignment Machine translation Terminology Multiword expressions Lexical acquisition Statistical methods ","alignment based extraction of multiword expressions","Due to idiosyncrasies in their syntax, semantics or frequency, Multiword Expressions (MWEs) have received special attention from the NLP community, as the methods and techniques developed for the treatment of simplex words are not necessarily suitable for them. This is certainly the case for the automatic acquisition of MWEs from corpora. A lot of effort has been directed to the task of automatically identifying them, with considerable success. In this paper, we propose an approach for the identification of MWEs in a multilingual context, as a by-product of a word alignment process, that not only deals with the identification of possible MWE candidates, but also associates some multiword expressions with semantics. The results obtained indicate the feasibility and low costs in terms of tools and resources demanded by this approach, which could, for example, facilitate and speed up lexicographic work.","Language Resources and Evaluation",2010,"No","automat identif word align machin translat terminolog multiword express lexic acquisit statist method align base extract multiword express due idiosyncrasi syntax semant frequenc multiword express mwes receiv special attent nlp communiti method techniqu develop treatment simplex word necessarili suitabl case automat acquisit mwes corpora lot effort direct task automat identifi consider success paper propos approach identif mwes multilingu context product word align process deal identif mwe candid associ multiword express semant result obtain feasibl low cost term tool resourc demand approach facilit speed lexicograph work",0
"KeywordsWordnet development Multilingual lexicon extraction  Word-sense disambiguation Distributional similarity ","constructing a poor mans wordnet in a resource rich world","In this paper we present a language-independent, fully modular and automatic approach to bootstrap a wordnet for a new language by recycling different types of already existing language resources, such as machine-readable dictionaries, parallel corpora, and Wikipedia. The approach, which we apply here to Slovene, takes into account monosemous and polysemous words, general and specialised vocabulary as well as simple and multi-word lexemes. The extracted words are then assigned one or several synset ids, based on a classifier that relies on several features including distributional similarity. Finally, we identify and remove highly dubious (literal, synset) pairs, based on simple distributional information extracted from a large corpus in an unsupervised way. Automatic, manual and task-based evaluations show that the resulting resource, the latest version of the Slovene wordnet, is already a valuable source of lexico-semantic information.","Language Resources and Evaluation",2015,"No","wordnet develop multilingu lexicon extract word sens disambigu distribut similar construct poor man wordnet resourc rich world paper present languag independ fulli modular automat approach bootstrap wordnet languag recycl type exist languag resourc machin readabl dictionari parallel corpora wikipedia approach appli sloven take account monosem polysem word general specialis vocabulari simpl multi word lexem extract word assign synset id base classifi reli featur includ distribut similar final identifi remov high dubious liter synset pair base simpl distribut inform extract larg corpus unsupervis automat manual task base evalu show result resourc latest version sloven wordnet valuabl sourc lexico semant inform",0
"KeywordsText summarization Evaluation Content evaluation Readability Task-based evaluation ","the challenging task of summary evaluation an overview","Evaluation is crucial in the research and development of automatic summarization applications, in order to determine the appropriateness of a summary based on different criteria, such as the content it contains, and the way it is presented.
 To perform an adequate evaluation is of great relevance to ensure that automatic summaries can be useful for the context and/or application they are generated for.
 To this end, researchers must be aware of the evaluation metrics, approaches, and datasets that are available, in order to decide which of them would be the most suitable to use, or to be able to propose new ones, overcoming the possible limitations that existing methods may present. In this article, a critical and historical analysis of evaluation metrics, methods, and datasets for automatic summarization systems is presented, where the strengths and weaknesses of evaluation efforts are discussed and the major challenges to solve are identified. Therefore, a clear up-to-date overview of the evolution and progress of summarization evaluation is provided, giving the reader useful insights into the past, present and latest trends in the automatic evaluation of summaries.","Language Resources and Evaluation",2018,"No","text summar evalu content evalu readabl task base evalu challeng task summari evalu overview evalu crucial research develop automat summar applic order determin appropri summari base criteria content present perform adequ evalu great relev ensur automat summari context applic generat end research awar evalu metric approach dataset order decid suitabl propos overcom limit exist method present articl critic histor analysi evalu metric method dataset automat summar system present strength weak evalu effort discuss major challeng solv identifi clear date overview evolut progress summar evalu provid give reader insight past present latest trend automat evalu summari",0
"KeywordsTerminology mining Comparable corpora Lexical alignment Compositional translation ","compositionality and lexical alignment of multi word terms","The automatic compilation of bilingual lists of terms from specialized comparable corpora using lexical alignment has been successful for single-word terms (SWTs), but remains disappointing for multi-word terms (MWTs). The low frequency and the variability of the syntactic structures of MWTs in the source and the target languages are the main reported problems. This paper defines a general framework dedicated to the lexical alignment of MWTs from comparable corpora that includes a compositional translation process and the standard lexical context analysis. The compositional method which is based on the translation of lexical items being restrictive, we introduce an extended compositional method that bridges the gap between MWTs of different syntactic structures through morphological links. We experimented with the two compositional methods for the French–Japanese alignment task. The results show a significant improvement for the translation of MWTs and advocate further morphological analysis in lexical alignment.","Language Resources and Evaluation",2010,"No","terminolog mine compar corpora lexic align composit translat composit lexic align multi word term automat compil bilingu list term special compar corpora lexic align success singl word term swts remain disappoint multi word term mwts low frequenc variabl syntact structur mwts sourc target languag main report problem paper defin general framework dedic lexic align mwts compar corpora includ composit translat process standard lexic context analysi composit method base translat lexic item restrict introduc extend composit method bridg gap mwts syntact structur morpholog link experi composit method french japanes align task result show signific improv translat mwts advoc morpholog analysi lexic align",0
"KeywordsGeoparsing Geotagging Geocoding NER NLP NEL NED ","whats missing in geographical parsing","Geographical data can be obtained by converting place names from free-format text into geographical coordinates. The ability to geo-locate events in textual reports represents a valuable source of information in many real-world applications such as emergency responses, real-time social media geographical event analysis, understanding location instructions in auto-response systems and more. However, geoparsing is still widely regarded as a challenge because of domain language diversity, place name ambiguity, metonymic language and limited leveraging of context as we show in our analysis. Results to date, whilst promising, are on laboratory data and unlike in wider NLP are often not cross-compared. In this study, we evaluate and analyse the performance of a number of leading geoparsers on a number of corpora and highlight the challenges in detail. We also publish an automatically geotagged Wikipedia corpus to alleviate the dearth of (open source) corpora in this domain.
","Language Resources and Evaluation",2018,"No","geopars geotag geocod ner nlp nel ned what miss geograph pars geograph data obtain convert place name free format text geograph coordin abil geo locat event textual report repres valuabl sourc inform real world applic emerg respons real time social media geograph event analysi understand locat instruct auto respons system geopars wide regard challeng domain languag divers place ambigu metonym languag limit leverag context show analysi result date whilst promis laboratori data unlik wider nlp cross compar studi evalu analys perform number lead geopars number corpora highlight challeng detail publish automat geotag wikipedia corpus allevi dearth open sourc corpora domain",0
"Keywordsaffect attitudes corpus annotation emotion natural language processing opinions sentiment subjectivity ","annotating expressions of opinions and emotions in language","This paper describes a corpus annotation project to study issues in the manual annotation of opinions, emotions, sentiments, speculations, evaluations and other private states in language. The resulting corpus annotation scheme is described, as well as examples of its use. In addition, the manual annotation process and the results of an inter-annotator agreement study on a 10,000-sentence corpus of articles drawn from the world press are presented.","Language Resources and Evaluation",2005,"No","affect attitud corpus annot emot natur languag process opinion sentiment subject annot express opinion emot languag paper describ corpus annot project studi issu manual annot opinion emot sentiment specul evalu privat state languag result corpus annot scheme exampl addit manual annot process result inter annot agreement studi sentenc corpus articl drawn world press present",0
"Key Wordselectronic text machine-readable text database on-line corpora humanities microcomputer SGML electronic publishing text-analysis tools ","the very pulse of the machine three trends toward improvement in electronic versions of humanities texts","Since April 1989, the Center for Text and Technology at Georgetown University has gathered information on the structure of projects that produce electronic text in the humanities. This report — based on the April, 1991 version of the Georgetown Catalogue and emphasizing its full-text projects in humanities disciplines other than linguistics —surveys the countries in which projects are found, the languages encoded, the disciplines served, and the auspices represented. Then the report explores three trends toward the improvement of electronic texts: increased scope of the new projects, improved quality of the editions used, and greater sophistication in the text-analysis tools added. Included among the notes is a list of titles and contacts for 42 projects cited in the report.","Computers and the Humanities",1991,"No","key wordselectron text machin readabl text databas line corpora human microcomput sgml electron publish text analysi tool puls machin trend improv electron version human text april center text technolog georgetown univers gather inform structur project produc electron text human report base april version georgetown catalogu emphas full text project human disciplin linguist survey countri project found languag encod disciplin serv auspic repres report explor trend improv electron text increas scope project improv qualiti edit greater sophist text analysi tool ad includ note list titl contact project cite report",0
"KeywordsInformation Processing Computational Linguistic Technical Guideline ","information processing in dictionary making some technical guidelines",NA,"Computers and the Humanities",1976,"No","inform process comput linguist technic guidelin inform process dictionari make technic guidelin na",0
"KeywordsComputational Linguistic Newspaper Text German Newspaper ","lemmatizing german newspaper texts with the aid of an algorithm",NA,"Computers and the Humanities",1981,"No","comput linguist newspap text german newspap lemmat german newspap text aid algorithm na",0
"KeywordsStatistical machine translation Word reordering Statistical classification Automatic evaluation ","recursive alignment block classification technique for word reordering in statistical machine translation","Statistical machine translation (SMT) is based on alignment models which learn from bilingual corpora the word correspondences between source and target language. These models are assumed to be capable of learning reorderings. However, the difference in word order between two languages is one of the most important sources of errors in SMT. In this paper, we show that SMT can take advantage of inductive learning in order to solve reordering problems. Given a word alignment, we identify those pairs of consecutive source blocks (sequences of words) whose translation is swapped, i.e. those blocks which, if swapped, generate a correct monotonic translation. Afterwards, we classify these pairs into groups, following recursively a co-occurrence block criterion, in order to infer reorderings. Inside the same group, we allow new internal combination in order to generalize the reorder to unseen pairs of blocks. Then, we identify the pairs of blocks in the source corpora (both training and test) which belong to the same group. We swap them and we use the modified source training corpora to realign and to build the final translation system. We have evaluated our reordering approach both in alignment and translation quality. In addition, we have used two state-of-the-art SMT systems: a Phrased-based and an Ngram-based. Experiments are reported on the EuroParl task, showing improvements almost over 1 point in the standard MT evaluation metrics (mWER and BLEU).","Language Resources and Evaluation",2011,"No","statist machin translat word reorder statist classif automat evalu recurs align block classif techniqu word reorder statist machin translat statist machin translat smt base align model learn bilingu corpora word correspond sourc target languag model assum capabl learn reorder differ word order languag import sourc error smt paper show smt advantag induct learn order solv reorder problem word align identifi pair consecut sourc block sequenc word translat swap block swap generat correct monoton translat classifi pair group recurs occurr block criterion order infer reorder insid group intern combin order general reorder unseen pair block identifi pair block sourc corpora train test belong group swap modifi sourc train corpora realign build final translat system evalu reorder approach align translat qualiti addit state art smt system phrase base ngram base experi report europarl task show improv point standard mt evalu metric mwer bleu",0
"KeywordsLanguage Resources Interoperability ","multilingual language resources and interoperability","This article introduces the topic of “Multilingual language resources and interoperability”. We start with a taxonomy and parameters for classifying language resources. Later we provide examples and issues of interoperatability, and resource architectures to solve such issues. Finally we discuss aspects of linguistic formalisms and interoperability.","Language Resources and Evaluation",2009,"No","languag resourc interoper multilingu languag resourc interoper articl introduc topic multilingu languag resourc interoper start taxonomi paramet classifi languag resourc provid exampl issu interoperat resourc architectur solv issu final discuss aspect linguist formal interoper",0
"KeywordsPartial cognates Word sense disambiguation Monolingual bootstrapping Bilingual bootstrapping ","disambiguation of partial cognates","Partial cognates are pairs of words in two languages that have the same meaning in some, but not all contexts. Detecting the actual meaning of a partial cognate in context can be useful for Machine Translation tools and for Computer-Assisted Language Learning tools. We propose a supervised and a semi-supervised method to disambiguate partial cognates between two languages: French and English. The methods use only automatically-labeled data; therefore they can be applied to other pairs of languages as well. The aim of our work is to automatically detect the meaning of a French partial cognate word in a specific context.","Language Resources and Evaluation",2008,"No","partial cognat word sens disambigu monolingu bootstrap bilingu bootstrap disambigu partial cognat partial cognat pair word languag mean context detect actual mean partial cognat context machin translat tool comput assist languag learn tool propos supervis semi supervis method disambigu partial cognat languag french english method automat label data appli pair languag aim work automat detect mean french partial cognat word specif context",0
"KeywordsMorphological parsing Word segmentation Data annotation Unsupervised learning Asian language processing Bengali ","unsupervised morphological parsing of bengali","Unsupervised morphological analysis is the task of segmenting words into prefixes, suffixes and stems without prior knowledge of language-specific morphotactics and morpho-phonological rules. This paper introduces a simple, yet highly effective algorithm for unsupervised morphological learning for Bengali, an Indo–Aryan language that is highly inflectional in nature. When evaluated on a set of 4,110 human-segmented Bengali words, our algorithm achieves an F-score of 83%, substantially outperforming Linguistica, one of the most widely-used unsupervised morphological parsers, by about 23%.","Language Resources and Evaluation",2006,"No","morpholog pars word segment data annot unsupervis learn asian languag process bengali unsupervis morpholog pars bengali unsupervis morpholog analysi task segment word prefix suffix stem prior knowledg languag specif morphotact morpho phonolog rule paper introduc simpl high effect algorithm unsupervis morpholog learn bengali indo aryan languag high inflect natur evalu set human segment bengali word algorithm achiev score substanti outperform linguistica wide unsupervis morpholog parser",0
"KeywordsEstonian language Idioms Multi-word expressions Particle verbs Support verb constructions ","the variability of multi word verbal expressions in estonian","This article focuses on the variability of one of the subtypes of multi-word expressions, namely those consisting of a verb and a particle or a verb and its complement(s). We build on evidence from Estonian, an agglutinative language with free word order, analysing the behaviour of verbal multi-word expressions (opaque and transparent idioms, support verb constructions and particle verbs). Using this data we analyse such phenomena as the order of the components of a multi-word expression, lexical substitution and morphosyntactic flexibility.","Language Resources and Evaluation",2010,"No","estonian languag idiom multi word express particl verb support verb construct variabl multi word verbal express estonian articl focus variabl subtyp multi word express consist verb particl verb complement build evid estonian agglutin languag free word order analys behaviour verbal multi word express opaqu transpar idiom support verb construct particl verb data analys phenomena order compon multi word express lexic substitut morphosyntact flexibl",0
"KeywordsStatistical machine translation N-gram-based translation Linguistic knowledge Grammatical categories ","overcoming statistical machine translation limitations error analysis and proposed solutions for the catalanspanish language pair","This work aims to improve an N-gram-based statistical machine translation system between the Catalan and Spanish languages, trained with an aligned Spanish–Catalan parallel corpus consisting of 1.7 million sentences taken from El Periódico newspaper. Starting from a linguistic error analysis above this baseline system, orthographic, morphological, lexical, semantic and syntactic problems are approached using a set of techniques. The proposed solutions include the development and application of additional statistical techniques, text pre- and post-processing tasks, and rules based on the use of grammatical categories, as well as lexical categorization. The performance of the improved system is clearly increased, as is shown in both human and automatic evaluations of the system, with a gain of about 1.1 points BLEU observed in the Spanish-to-Catalan direction of translation, and a gain of about 0.5 points in the reverse direction. The final system is freely available online as a linguistic resource.","Language Resources and Evaluation",2011,"No","statist machin translat gram base translat linguist knowledg grammat categori overcom statist machin translat limit error analysi propos solut catalanspanish languag pair work aim improv gram base statist machin translat system catalan spanish languag train align spanish catalan parallel corpus consist million sentenc el peri dico newspap start linguist error analysi baselin system orthograph morpholog lexic semant syntact problem approach set techniqu propos solut includ develop applic addit statist techniqu text pre post process task rule base grammat categori lexic categor perform improv system increas shown human automat evalu system gain point bleu observ spanish catalan direct translat gain point revers direct final system freeli onlin linguist resourc",0
"KeywordsDialogue act Language model Sentence structure Speech act Speech recognition Syntax ","automatic dialogue act recognition with syntactic features","This work studies the usefulness of syntactic information in the context of automatic dialogue act recognition in Czech. Several pieces of evidence are presented in this work that support our claim that syntax might bring valuable information for dialogue act recognition. In particular, a parallel is drawn with the related domain of automatic punctuation generation and a set of syntactic features derived from a deep parse tree is further proposed and successfully used in a Czech dialogue act recognition system based on conditional random fields. We finally discuss the possible reasons why so few works have exploited this type of information before and propose future research directions to further progress in this area.","Language Resources and Evaluation",2014,"No","dialogu act languag model sentenc structur speech act speech recognit syntax automat dialogu act recognit syntact featur work studi use syntact inform context automat dialogu act recognit czech piec evid present work support claim syntax bring valuabl inform dialogu act recognit parallel drawn relat domain automat punctuat generat set syntact featur deriv deep pars tree propos success czech dialogu act recognit system base condit random field final discuss reason work exploit type inform propos futur research direct progress area",0
"KeywordsLoanwords Transliteration Detection N-gram EM algorithm Korean ","an unsupervised method for identifying loanwords in korean","This paper presents an unsupervised method for developing a character-based n-gram classifier that identifies loanwords or transliterated foreign words in Korean text. The classifier is trained on an unlabeled corpus using the Expectation Maximization algorithm, building on seed words extracted from the corpus. Words with high token frequency serve as native seed words. Words with seeming traces of vowel insertion to repair consonant clusters serve as foreign seed words. What counts as a trace of insertion is determined using phoneme co-occurrence statistics in conjunction with ideas and findings in phonology. Experiments show that the method can produce an unsupervised classifier that performs at a level comparable to that of a supervised classifier. In a cross-validation experiment using a corpus of about 9.2 million words and a lexicon of about 71,000 words, mean F-scores of the best unsupervised classifier and the corresponding supervised classifier were 94.77 and 96.67 %, respectively. Experiments also suggest that the method can be readily applied to other languages with similar phonotactics such as Japanese.","Language Resources and Evaluation",2015,"No","loanword transliter detect gram em algorithm korean unsupervis method identifi loanword korean paper present unsupervis method develop charact base gram classifi identifi loanword transliter foreign word korean text classifi train unlabel corpus expect maxim algorithm build seed word extract corpus word high token frequenc serv nativ seed word word trace vowel insert repair conson cluster serv foreign seed word count trace insert determin phonem occurr statist conjunct idea find phonolog experi show method produc unsupervis classifi perform level compar supervis classifi cross valid experi corpus million word lexicon word score unsupervis classifi supervis classifi experi suggest method readili appli languag similar phonotact japanes",0
"KeywordsTimeML Temporal information Event ordering Temporal expressions Temporal expression recognition Temporal expression normalization Natural Language Processing Annotation schemes Temporal reasoning ","automatic transformation from tides to timeml annotation","Until recently, most systems performing temporal extraction and reasoning from text have focused on recognizing and normalizing temporal expressions alone, for which the TIDES annotation scheme has been adopted. Temporal awareness of a text, however, involves not only identifying the temporal expressions, but the events which these expressions anchor, as well as other events which must be ordered relative to them. Because of these broader concerns, TimeML has been developed as an annotation specification that encompasses not only temporal expressions, but all temporally relevant aspects of a text. The annotation schemes, however, are not interchangeable, resulting in incompatible corpora and accompanying extraction algorithms for each standard. In this paper, we describe an automatic migration process from the  TIMEX2  tags of TIDES to the  TIMEX3  tags of TimeML. This transformation procedure has been implemented and evaluated with two different corpora, obtaining 93.3 and 89.2% overall F-Measure respectively.","Language Resources and Evaluation",2011,"No","timeml tempor inform event order tempor express tempor express recognit tempor express normal natur languag process annot scheme tempor reason automat transform tide timeml annot recent system perform tempor extract reason text focus recogn normal tempor express tide annot scheme adopt tempor awar text involv identifi tempor express event express anchor event order relat broader concern timeml develop annot specif encompass tempor express tempor relev aspect text annot scheme interchang result incompat corpora accompani extract algorithm standard paper describ automat migrat process timex tag tide timex tag timeml transform procedur implement evalu corpora obtain measur",0
"KeywordsInformation extraction Product named entity recognition Hierarchical hidden Markov model ","product named entity recognition in chinese text","There are many expressive and structural differences between product names and general named entities such as person names, location names and organization names. To date, there has been little research on product named entity recognition (NER), which is crucial and valuable for information extraction in the field of market intelligence. This paper focuses on product NER (PRO NER) in Chinese text. First, we describe our efforts on data annotation, including well-defined specifications, data analysis and development of a corpus with annotated product named entities. Second, a hierarchical hidden Markov model-based approach to PRO NER is proposed and evaluated. Extensive experiments show that the proposed method outperforms the cascaded maximum entropy model and obtains promising results on the data sets of two different electronic product domains (digital and cell phone).","Language Resources and Evaluation",2008,"No","inform extract product name entiti recognit hierarch hidden markov model product name entiti recognit chines text express structur differ product name general name entiti person name locat name organ name date research product name entiti recognit ner crucial valuabl inform extract field market intellig paper focus product ner pro ner chines text describ effort data annot includ defin specif data analysi develop corpus annot product name entiti hierarch hidden markov model base approach pro ner propos evalu extens experi show propos method outperform cascad maximum entropi model obtain promis result data set electron product domain digit cell phone",0
"KeywordsSemEval 2010 Cross lingual Lexical substitution ","the cross lingual lexical substitution task","In this paper we provide an account of the cross-lingual lexical substitution task run as part of SemEval-2010. In this task both annotators (native Spanish speakers, proficient in English) and participating systems had to find Spanish translations for target words in the context of an English sentence. Because only translations of a single lexical unit were required, this task does not necessitate a full blown translation system. This we hope encouraged those working specifically on lexical semantics to participate without a requirement for them to use machine translation software, though they were free to use whatever resources they chose. In this paper we pay particular attention to the resources used by the various participating systems and present analyses to demonstrate the relative strengths of the systems as well as the requirements they have in terms of resources. In addition to the analyses of individual systems we also present the results of a combined system based on voting from the individual systems. We demonstrate that the system produces better results at finding the most frequent translation from the annotators compared to the highest ranked translation provided by individual systems. This supports our other analyses that the systems are heterogeneous, with different strengths and weaknesses.","Language Resources and Evaluation",2013,"No","semev cross lingual lexic substitut cross lingual lexic substitut task paper provid account cross lingual lexic substitut task run part semev task annot nativ spanish speaker profici english particip system find spanish translat target word context english sentenc translat singl lexic unit requir task necessit full blown translat system hope encourag work specif lexic semant particip requir machin translat softwar free resourc chose paper pay attent resourc particip system present analys demonstr relat strength system requir term resourc addit analys individu system present result combin system base vote individu system demonstr system produc result find frequent translat annot compar highest rank translat provid individu system support analys system heterogen strength weak",0
"KeywordsVerbal lexicon WordNet VerbNet FrameNet  PropBank SemLink ","predicate matrix automatically extending the semantic interoperability between predicate resources","This paper presents a novel approach to improve the interoperability between four semantic resources that incorporate predicate information. Our proposal defines a set of automatic methods for mapping the semantic knowledge included in WordNet, VerbNet, PropBank and FrameNet. We use advanced graph-based word sense disambiguation algorithms and corpus alignment methods to automatically establish the appropriate mappings among their lexical entries and roles. We study different settings for each method using SemLink as a gold-standard for evaluation. The results show that the new approach provides productive and reliable mappings. In fact, the mappings obtained automatically outnumber the set of original mappings in SemLink. Finally, we also present a new version of the Predicate Matrix, a lexical-semantic resource resulting from the integration of the mappings obtained by our automatic methods and SemLink.","Language Resources and Evaluation",2016,"No","verbal lexicon wordnet verbnet framenet propbank semlink predic matrix automat extend semant interoper predic resourc paper present approach improv interoper semant resourc incorpor predic inform propos defin set automat method map semant knowledg includ wordnet verbnet propbank framenet advanc graph base word sens disambigu algorithm corpus align method automat establish map lexic entri role studi set method semlink gold standard evalu result show approach product reliabl map fact map obtain automat outnumb set origin map semlink final present version predic matrix lexic semant resourc result integr map obtain automat method semlink",0
"KeywordsLexical semantics Lexical knowledge bases Wordnet ","methodology and construction of the basque wordnet","Semantic interpretation of language requires extensive and rich lexical knowledge bases (LKB). The Basque WordNet is a LKB based on WordNet and its multilingual counterparts EuroWordNet and the Multilingual Central Repository. This paper reviews the theoretical and practical aspects of the Basque WordNet lexical knowledge base, as well as the steps and methodology followed in its construction. Our methodology is based on the joint development of wordnets and annotated corpora. The Basque WordNet contains 32,456 synsets and 26,565 lemmas, and is complemented by a hand-tagged corpus comprising 59,968 annotations.","Language Resources and Evaluation",2011,"No","lexic semant lexic knowledg base wordnet methodolog construct basqu wordnet semant interpret languag requir extens rich lexic knowledg base lkb basqu wordnet lkb base wordnet multilingu counterpart eurowordnet multilingu central repositori paper review theoret practic aspect basqu wordnet lexic knowledg base step methodolog construct methodolog base joint develop wordnet annot corpora basqu wordnet synset lemma complement hand tag corpus compris annot",0
"KeywordsText data mining Language modeling Topic identification Duplicity detection ","general framework for mining processing and storing large amounts of electronic texts for language modeling purposes","The paper describes a general framework for mining large amounts of text data from a defined set of Web pages. The acquired data are meant to constitute a corpus for training robust and reliable language models and thus the framework needs to also incorporate algorithms for appropriate text processing and duplicity detection in order to secure quality and consistency of the data. As we expect the resulting corpus to be very large, we have also implemented topic detection algorithms that allow us to automatically select subcorpora for domain-specific language models. The description of the framework architecture and the implemented algorithms is complemented with a detailed evaluation section. It analyses the basic properties of the gathered Czech corpus containing more than one billion text tokens collected using the described framework, shows the results of the topic detection methods and finally also describes the design and outcomes of the automatic speech recognition experiments with domain-specific language models estimated from the collected data.","Language Resources and Evaluation",2014,"No","text data mine languag model topic identif duplic detect general framework mine process store larg amount electron text languag model purpos paper describ general framework mine larg amount text data defin set web page acquir data meant constitut corpus train robust reliabl languag model framework incorpor algorithm text process duplic detect order secur qualiti consist data expect result corpus larg implement topic detect algorithm automat select subcorpora domain specif languag model descript framework architectur implement algorithm complement detail evalu section analys basic properti gather czech corpus billion text token collect framework show result topic detect method final describ design outcom automat speech recognit experi domain specif languag model estim collect data",0
"KeywordsText classification Food domain Social media  Linguistically informed feature engineering Polarity classification ","detecting conditional healthiness of food items from natural language text","In this article, we explore the feasibility of extracting suitable and unsuitable food items for particular health conditions from natural language text. We refer to this task as conditional healthiness classification. For that purpose, we annotate a corpus extracted from forum entries of a food-related website. We identify different relation types that hold between food items and health conditions going beyond a binary distinction of suitability and unsuitability and devise various supervised classifiers using different types of features. We examine the impact of different task-specific resources, such as a healthiness lexicon that lists the healthiness status of a food item and a sentiment lexicon. Moreover, we also consider task-specific linguistic features that disambiguate a context in which mentions of a food item and a health condition co-occur and compare them with standard features using bag of words, part-of-speech information and syntactic parses. We also investigate in how far individual food items and health conditions correlate with specific relation types and try to harness this information for classification.","Language Resources and Evaluation",2015,"No","text classif food domain social media linguist inform featur engin polar classif detect condit healthi food item natur languag text articl explor feasibl extract suitabl unsuit food item health condit natur languag text refer task condit healthi classif purpos annot corpus extract forum entri food relat websit identifi relat type hold food item health condit binari distinct suitabl unsuit devis supervis classifi type featur examin impact task specif resourc healthi lexicon list healthi status food item sentiment lexicon task specif linguist featur disambigu context mention food item health condit occur compar standard featur bag word part speech inform syntact pars investig individu food item health condit correl specif relat type har inform classif",0
"KeywordsComputational Linguistic Text Variant ","using the computer to identify differences among text variants",NA,"Computers and the Humanities",1971,"No","comput linguist text variant comput identifi differ text variant na",0
"corpus contemporary de electronic texts Golden-Age language que Spanish word frequency y ","spanish word frequency a historical surprise","This article compares the word frequencies of the few most commonwords in Spanish as revealed by a modern corpus of over fivethousand words with a corpus of Golden-Age Spanish texts of overa million words, and finds that although de is by far themost common word in contemporary Spanish, in the 16thand 17th Centuries it was considerably less frequent, and in many texts was less frequent than y, or quefor which shared very similar frequency figures. It is arguedthat this significant change in the Spanish language comes aboutin the 20th Century.","Computers and the Humanities",2001,"No","corpus contemporari de electron text golden age languag spanish word frequenc spanish word frequenc histor surpris articl compar word frequenc commonword spanish reveal modern corpus fivethousand word corpus golden age spanish text overa million word find de themost common word contemporari spanish thand th centuri consider frequent text frequent quefor share similar frequenc figur arguedthat signific chang spanish languag aboutin th centuri",0
"KeywordsComputational Linguistic Verbal Material ","verbal materials in machine readable form",NA,"Computers and the Humanities",1971,"No","comput linguist verbal materi verbal materi machin readabl form na",0
NA,"rapport dactivit sur le traitement informatique des textes mdivaux",NA,"Computers and the Humanities",1986,"No","rapport dactivit sur le traitement informatiqu des text mdivaux na",0
"Key Wordsballad “Mary Hamilton” phatic language textual variation collocation stylized language epithet concordance verbal echoes ","the way stylized language means pattern matching in the child ballads","This paper suggests ways in which the pattern-matching capability of the computer can be used to further our understanding of stylized ballad language. The study is based upon a computer-aided analysis of the entire 595,000- word corpus of Francis James Child'sThe English and Scottish Popular Ballads (1882–1892), a collection of 305 textual traditions, most of which are represented by a variety of texts. The paper focuses on the “Mary Hamilton” tradition as a means of discussing the function of phatic language in the ballad genre and the significance of textual variation.","Computers and the Humanities",1989,"No","key wordsballad mari hamilton phatic languag textual variat colloc styliz languag epithet concord verbal echo styliz languag mean pattern match child ballad paper suggest way pattern match capabl comput understand styliz ballad languag studi base comput aid analysi entir word corpus franci jame childsth english scottish popular ballad collect textual tradit repres varieti text paper focus mari hamilton tradit mean discuss function phatic languag ballad genr signific textual variat",0
"agglutinative languages morphological disambiguation n-gram language models statistical natural language processing Turkish ","statistical morphological disambiguation for agglutinative languages","We present statistical models for morphological disambiguation in agglutinative languages, with a specific application to Turkish. Turkish presents an interesting problem for statistical models as the potential tag set size is very large because of the productive derivational morphology. We propose to handle this by breaking up the morhosyntactic tags into inflectional groups, each of which contains the inflectional features for each (intermediate) derived form. Our statistical models score the probability of each morhosyntactic tag by considering statistics over the individual inflectional groups and surface roots in trigram models. Among the four models that we have developed and tested, the simplest model ignoring the local morphotactics within words performs the best. Our best trigram model performs with 93.95% accuracy on our test data getting all the morhosyntactic and semantic features correct. If we are just interested in syntactically relevant features and ignore a very small set of semantic features, then the accuracy increases to 95.07%.","Computers and the Humanities",2002,"No","agglutin languag morpholog disambigu gram languag model statist natur languag process turkish statist morpholog disambigu agglutin languag present statist model morpholog disambigu agglutin languag specif applic turkish turkish present interest problem statist model potenti tag set size larg product deriv morpholog propos handl break morhosyntact tag inflect group inflect featur intermedi deriv form statist model score probabl morhosyntact tag statist individu inflect group surfac root trigram model model develop test simplest model ignor local morphotact word perform trigram model perform accuraci test data morhosyntact semant featur correct interest syntact relev featur ignor small set semant featur accuraci increas",0
"Key WordsLatin Latin grammarians concordances Heinrich Keil ","a concordance to keils latin grammarians","The following is a description of a computerized version of the corpus of Latin grammarians published by Heinrich Keil in Leipzig between 1855 and 1880. The intent was to prepare an instrument which would serve both as a key to Keil's corpus and as the basis for a re-edition of the work itself. We discuss the corpus itself, the ways in which it was encoded, the pre-editing work, and how the material was organized for analysis by computer.","Computers and the Humanities",1990,"No","key wordslatin latin grammarian concord heinrich keil concord keil latin grammarian descript computer version corpus latin grammarian publish heinrich keil leipzig intent prepar instrument serv key keil corpus basi edit work discuss corpus way encod pre edit work materi organ analysi comput",0
"Key wordsstylometry Shakespeare authorship Shakespeare canon Shakespeare Apocrypha Elizabethan poems Elizabethan plays ","and then there were none winnowing the shakespeare claimants","The Shakespeare Clinic has developed 51 computer tests of Shakespeare play authorship and 14 of poem authorship, and applied them to 37 claimed “true Shakespeares,” to 27 plays of the Shakespeare Apocrypha, and to several poems of unknown or disputed authorship. No claimant, and none of the apocryphal plays or poems, matched Shakespeare. Two plays and one poem from the Shakespeare Canon,Titus Andronicus, Henry VI, Part 3, and “A Lover's Complaint,” do not match the others.","Computers and the Humanities",1996,"No","key wordsstylometri shakespear authorship shakespear canon shakespear apocrypha elizabethan poem elizabethan play winnow shakespear claimant shakespear clinic develop comput test shakespear play authorship poem authorship appli claim true shakespear play shakespear apocrypha poem unknown disput authorship claimant apocryph play poem match shakespear play poem shakespear canontitus andronicus henri vi part lover complaint match",0
"Key Wordstree-analysis additive trees tree-topology tree-representation algorithms computational linguistics English syntax English poetry ","unrooted trees revisited topology and poetic data","Scholars in the humanities often have to account exhaustively for the structure of large masses of data. Tree-diagrams implemented by means of suitable computer programs can be of considerable assistance in achieving a cohesive representation of the data. This paper discusses the respective merits of the two main approaches to tree representation and introduces a new method based on the use of unrooted trees. After a detailed examination of the topological properties of such trees, two algorithms are described. The second part of the paper consists in practical applications of the method of tree representation to a corpus of contemporary English poetry. Several sets of data made up of both lexical and grammatical items (adjectives, modals, auxiliaries and personal pronouns) have been submitted to the method. The findings are assessed in terms of their heuristic value in the light of modern linguistic theory and compared with the results obtained by means of more traditional statistical procedures.","Computers and the Humanities",1989,"No","key wordstre analysi addit tree tree topolog tree represent algorithm comput linguist english syntax english poetri unroot tree revisit topolog poetic data scholar human account exhaust structur larg mass data tree diagram implement mean suitabl comput program consider assist achiev cohes represent data paper discuss respect merit main approach tree represent introduc method base unroot tree detail examin topolog properti tree algorithm part paper consist practic applic method tree represent corpus contemporari english poetri set data made lexic grammat item adject modal auxiliari person pronoun submit method find assess term heurist light modern linguist theori compar result obtain mean tradit statist procedur",0
"KeywordsInformation extraction OWL reasoning Ontologies ","semanticlean","In our research on using information extraction to help populate semantic web resources, we have encountered significant obstacles to interoperability between the technologies. We believe these obstacles to be endemic to the basic paradigms and not quirks of the specific implementations we have worked with. In particular, we identify five dimensions of interoperability that must be addressed to successfully employ information extraction systems to populate semantic web resources that are suitable for reasoning. We call the task of transforming IE data into knowledge-based resources knowledge integration and we report results of experiments in which the knowledge integration process uses the deeper semantics of OWL ontologies to improve by between 8% and 13% the precision of relation extraction from text.","Language Resources and Evaluation",2008,"No","inform extract owl reason ontolog semanticlean research inform extract popul semant web resourc encount signific obstacl interoper technolog obstacl endem basic paradigm quirk specif implement work identifi dimens interoper address success employ inform extract system popul semant web resourc suitabl reason call task transform data knowledg base resourc knowledg integr report result experi knowledg integr process deeper semant owl ontolog improv precis relat extract text",0
"KeywordsJapanese Treebank Sensebank HPSG Ontology ","the hinoki syntactic and semantic treebank of japanese","In this paper we describe the current state of a new Japanese lexical resource: the Hinoki treebank. The treebank is built from dictionary definitions, examples and news text, and uses an HPSG based Japanese grammar to encode both syntactic and semantic information. It is combined with an ontology based on the definition sentences to give a detailed sense level description of the most familiar 28,000 words of Japanese.","Language Resources and Evaluation",2008,"No","japanes treebank sensebank hpsg ontolog hinoki syntact semant treebank japanes paper describ current state japanes lexic resourc hinoki treebank treebank built dictionari definit exampl news text hpsg base japanes grammar encod syntact semant inform combin ontolog base definit sentenc give detail sens level descript familiar word japanes",0
"KeywordsCombination of taggers Integration of taggers Linguistically motivated rules Simple voting Tagging accuracy ","tagging icelandic text an experiment with integrations and combinations of taggers","We use integrations and combinations of taggers to improve the tagging accuracy of Icelandic text. The accuracy of the best performing integrated tagger, which consists of our linguistic rule-based tagger for initial disambiguation and a trigram tagger for full disambiguation, is 91.80%. Combining five different taggers, using simple voting, results in 93.34% accuracy. By adding two linguistically motivated rules to the combined tagger, we obtain an accuracy of 93.48%. This method reduces the error rate by 20.5%, with respect to the best performing tagger in the combination pool.","Language Resources and Evaluation",2006,"No","combin tagger integr tagger linguist motiv rule simpl vote tag accuraci tag iceland text experi integr combin tagger integr combin tagger improv tag accuraci iceland text accuraci perform integr tagger consist linguist rule base tagger initi disambigu trigram tagger full disambigu combin tagger simpl vote result accuraci ad linguist motiv rule combin tagger obtain accuraci method reduc error rate respect perform tagger combin pool",0
"KeywordsFrameNet Grammatical Framework Multilinguality  Natural language generation Controlled natural language ","a multilingual framenet based grammar and lexicon for controlled natural language","Berkeley FrameNet is a lexico-semantic resource for English based on the theory of frame semantics. It has been exploited in a range of natural language processing applications and has inspired the development of framenets for many languages. We present a methodological approach to the extraction and generation of a computational multilingual FrameNet-based grammar and lexicon. The approach leverages FrameNet-annotated corpora to automatically extract a set of cross-lingual semantico-syntactic valence patterns. Based on data from Berkeley FrameNet and Swedish FrameNet, the proposed approach has been implemented in Grammatical Framework (GF), a categorial grammar formalism specialized for multilingual grammars. The implementation of the grammar and lexicon is supported by the design of FrameNet, providing a frame semantic abstraction layer, an interlingual semantic application programming interface (API), over the interlingual syntactic API already provided by GF Resource Grammar Library. The evaluation of the acquired grammar and lexicon shows the feasibility of the approach. Additionally, we illustrate how the FrameNet-based grammar and lexicon are exploited in two distinct multilingual controlled natural language applications. The produced resources are available under an open source license.","Language Resources and Evaluation",2017,"No","framenet grammat framework multilingu natur languag generat control natur languag multilingu framenet base grammar lexicon control natur languag berkeley framenet lexico semant resourc english base theori frame semant exploit rang natur languag process applic inspir develop framenet languag present methodolog approach extract generat comput multilingu framenet base grammar lexicon approach leverag framenet annot corpora automat extract set cross lingual semantico syntact valenc pattern base data berkeley framenet swedish framenet propos approach implement grammat framework gf categori grammar formal special multilingu grammar implement grammar lexicon support design framenet provid frame semant abstract layer interlingu semant applic program interfac api interlingu syntact api provid gf resourc grammar librari evalu acquir grammar lexicon show feasibl approach addit illustr framenet base grammar lexicon exploit distinct multilingu control natur languag applic produc resourc open sourc licens",0
"KeywordsLexical association measures Collocations Multiword expressions Evaluation ","lexical association measures and collocation extraction","We present an extensive empirical evaluation of collocation extraction methods based on lexical association measures and their combination. The experiments are performed on three sets of collocation candidates extracted from the Prague Dependency Treebank with manual morphosyntactic annotation and from the Czech National Corpus with automatically assigned lemmas and part-of-speech tags. The collocation candidates were manually labeled as collocational or non-collocational. The evaluation is based on measuring the quality of ranking the candidates according to their chance to form collocations. Performance of the methods is compared by precision-recall curves and mean average precision scores. The work is focused on two-word (bigram) collocations only. We experiment with bigrams extracted from sentence dependency structure as well as from surface word order. Further, we study the effect of corpus size on the performance of the individual methods and their combination.","Language Resources and Evaluation",2010,"No","lexic associ measur colloc multiword express evalu lexic associ measur colloc extract present extens empir evalu colloc extract method base lexic associ measur combin experi perform set colloc candid extract pragu depend treebank manual morphosyntact annot czech nation corpus automat assign lemma part speech tag colloc candid manual label colloc colloc evalu base measur qualiti rank candid chanc form colloc perform method compar precis recal curv averag precis score work focus word bigram colloc experi bigram extract sentenc depend structur surfac word order studi effect corpus size perform individu method combin",0
"KeywordsLanguage identification Tweets Short texts Multilingualism Similar languages ","tweetlid a benchmark for tweet language identification","Language identification, as the task of determining the language a given text is written in, has progressed substantially in recent decades. However, three main issues remain still unresolved: (1) distinction of similar languages, (2) detection of multilingualism in a single document, and (3) identifying the language of short texts. In this paper, we describe our work on the development of a benchmark to encourage further research in these three directions, set forth an evaluation framework suitable for the task, and make a dataset of annotated tweets publicly available for research purposes. We also describe the shared task we organized to validate and assess the evaluation framework and dataset with systems submitted by seven different participants, and analyze the performance of these systems. The evaluation of the results submitted by the participants of the shared task helped us shed some light on the shortcomings of state-of-the-art language identification systems, and gives insight into the extent to which the brevity, multilingualism, and language similarity found in texts exacerbate the performance of language identifiers. Our dataset with nearly 35,000 tweets and the evaluation framework provide researchers and practitioners with suitable resources to further study the aforementioned issues on language identification within a common setting that enables to compare results with one another.","Language Resources and Evaluation",2016,"No","languag identif tweet short text multilingu similar languag tweetlid benchmark tweet languag identif languag identif task determin languag text written progress substanti recent decad main issu remain unresolv distinct similar languag detect multilingu singl document identifi languag short text paper describ work develop benchmark encourag research direct set evalu framework suitabl task make dataset annot tweet public research purpos describ share task organ valid assess evalu framework dataset system submit particip analyz perform system evalu result submit particip share task help shed light shortcom state art languag identif system insight extent breviti multilingu languag similar found text exacerb perform languag identifi dataset tweet evalu framework provid research practition suitabl resourc studi aforement issu languag identif common set enabl compar result",0
"Keywords(Semi)-automatic annotation Gesture analysis Video analysis Hand annotation Gesture space Motion analysis ","a semi automatic annotation tool for unobtrusive gesture analysis","In a variety of research fields, including linguistics, human–computer interaction research, psychology, sociology and behavioral studies, there is a growing interest in the role of gestural behavior related to speech and other modalities. The analysis of multimodal communication requires high-quality video data and detailed annotation of the different semiotic resources under scrutiny. In the majority of cases, the annotation of hand position, hand motion, gesture type, etc. is done manually, which is a time-consuming enterprise requiring multiple annotators and substantial resources. In this paper we present a semi-automatic alternative, in which the focus lies on minimizing the manual workload while guaranteeing highly accurate annotations. First, we discuss our approach, which consists of several processing steps such as identifying the hands in images, calculating motion of the hands, segmenting the recording in gesture and non-gesture events, etc. Second, we validate our approach against existing corpora in terms of accuracy and usefulness. The proposed approach is designed to provide annotations according to the McNeill (Hand and mind: what gestures reveal about thought, University of Chicago Press, Chicago, 1992) gesture space and the output is compatible with annotation tools such as ELAN or ANVIL.","Language Resources and Evaluation",2018,"No","semi automat annot gestur analysi video analysi hand annot gestur space motion analysi semi automat annot tool unobtrus gestur analysi varieti research field includ linguist human comput interact research psycholog sociolog behavior studi grow interest role gestur behavior relat speech modal analysi multimod communic requir high qualiti video data detail annot semiot resourc scrutini major case annot hand posit hand motion gestur type manual time consum enterpris requir multipl annot substanti resourc paper present semi automat altern focus lie minim manual workload guarante high accur annot discuss approach consist process step identifi hand imag calcul motion hand segment record gestur gestur event valid approach exist corpora term accuraci use propos approach design provid annot mcneill hand mind gestur reveal thought univers chicago press chicago gestur space output compat annot tool elan anvil",0
"KeywordsCross-language Plagiarism detection Similarity Retrieval model Evaluation ","cross language plagiarism detection","Cross-language plagiarism detection deals with the automatic identification and extraction of plagiarism in a multilingual setting. In this setting, a suspicious document is given, and the task is to retrieve all sections from the document that originate from a large, multilingual document collection. Our contributions in this field are as follows: (1) a comprehensive retrieval process for cross-language plagiarism detection is introduced, highlighting the differences to monolingual plagiarism detection, (2) state-of-the-art solutions for two important subtasks are reviewed, (3) retrieval models for the assessment of cross-language similarity are surveyed, and, (4) the three models CL-CNG, CL-ESA and CL-ASA are compared. Our evaluation is of realistic scale: it relies on 120,000 test documents which are selected from the corpora JRC-Acquis and Wikipedia, so that for each test document highly similar documents are available in all of the six languages English, German, Spanish, French, Dutch, and Polish. The models are employed in a series of ranking tasks, and more than 100 million similarities are computed with each model. The results of our evaluation indicate that CL-CNG, despite its simple approach, is the best choice to rank and compare texts across languages if they are syntactically related. CL-ESA almost matches the performance of CL-CNG, but on arbitrary pairs of languages. CL-ASA works best on “exact” translations but does not generalize well.","Language Resources and Evaluation",2011,"No","cross languag plagiar detect similar retriev model evalu cross languag plagiar detect cross languag plagiar detect deal automat identif extract plagiar multilingu set set suspici document task retriev section document origin larg multilingu document collect contribut field comprehens retriev process cross languag plagiar detect introduc highlight differ monolingu plagiar detect state art solut import subtask review retriev model assess cross languag similar survey model cl cng cl esa cl asa compar evalu realist scale reli test document select corpora jrc acqui wikipedia test document high similar document languag english german spanish french dutch polish model employ seri rank task million similar comput model result evalu cl cng simpl approach choic rank compar text languag syntact relat cl esa match perform cl cng arbitrari pair languag cl asa work exact translat general",0
"KeywordsLarge vocabulary speech recognition Statistical language modeling Subword units Data filtering Adaptation ","modeling under resourced languages for speech recognition","One particular problem in large vocabulary continuous speech recognition for low-resourced languages is finding relevant training data for the statistical language models. Large amount of data is required, because models should estimate the probability for all possible word sequences. For Finnish, Estonian and the other fenno-ugric languages a special problem with the data is the huge amount of different word forms that are common in normal speech. The same problem exists also in other language technology applications such as machine translation, information retrieval, and in some extent also in other morphologically rich languages. In this paper we present methods and evaluations in four recent language modeling topics: selecting conversational data from the Internet, adapting models for foreign words, multi-domain and adapted neural network language modeling, and decoding with subword units. Our evaluations show that the same methods work in more than one language and that they scale down to smaller data resources.","Language Resources and Evaluation",2017,"No","larg vocabulari speech recognit statist languag model subword unit data filter adapt model resourc languag speech recognit problem larg vocabulari continu speech recognit low resourc languag find relev train data statist languag model larg amount data requir model estim probabl word sequenc finnish estonian fenno ugric languag special problem data huge amount word form common normal speech problem exist languag technolog applic machin translat inform retriev extent morpholog rich languag paper present method evalu recent languag model topic select convers data internet adapt model foreign word multi domain adapt neural network languag model decod subword unit evalu show method work languag scale smaller data resourc",0
"KeywordsWord sense induction Graph clustering Pseudowords Evaluation ","a comparison of graph based word sense induction clustering algorithms in a pseudoword evaluation framework","
This article presents a comparison of different Word Sense Induction (wsi) clustering algorithms on two novel pseudoword data sets of semantic-similarity and co-occurrence-based word graphs, with a special focus on the detection of homonymic polysemy. We follow the original definition of a pseudoword as the combination of two monosemous terms and their contexts to simulate a polysemous word. The evaluation is performed comparing the algorithm’s output on a pseudoword’s ego word graph (i.e., a graph that represents the pseudoword’s context in the corpus) with the known subdivision given by the components corresponding to the monosemous source words forming the pseudoword. The main contribution of this article is to present a self-sufficient pseudoword-based evaluation framework for wsi graph-based clustering algorithms, thereby defining a new evaluation measure (top2) and a secondary clustering process (hyperclustering). To our knowledge, we are the first to conduct and discuss a large-scale systematic pseudoword evaluation targeting the induction of coarse-grained homonymous word senses across a large number of graph clustering algorithms.","Language Resources and Evaluation",2018,"No","word sens induct graph cluster pseudoword evalu comparison graph base word sens induct cluster algorithm pseudoword evalu framework articl present comparison word sens induct wsi cluster algorithm pseudoword data set semant similar occurr base word graph special focus detect homonym polysemi follow origin definit pseudoword combin monosem term context simul polysem word evalu perform compar algorithm output pseudoword ego word graph graph repres pseudoword context corpus subdivis compon monosem sourc word form pseudoword main contribut articl present suffici pseudoword base evalu framework wsi graph base cluster algorithm defin evalu measur top secondari cluster process hyperclust knowledg conduct discuss larg scale systemat pseudoword evalu target induct coars grain homonym word sens larg number graph cluster algorithm",0
"KeywordsWordnet Dictionary Lexical semantics Semantic relations Hyponymy Nouns Verbs ","dannet the challenge of compiling a wordnet for danish by reusing a monolingual dictionary","This paper is a contribution to the discussion on compiling computational lexical resources from conventional dictionaries. It describes the theoretical as well as practical problems that are encountered when reusing a conventional dictionary for compiling a lexical-semantic resource in terms of a wordnet. More specifically, it describes the methodological issues of compiling a wordnet for Danish, DanNet, from a monolingual basis, and not—as is often seen—by applying the translational expansion method with Princeton WordNet as the English source. Thus, we apply as our basis a large, corpus-based printed dictionary of modern Danish. Using this approach, we discuss the issues of readjusting inconsistent and/or underspecified hyponymy hierarchies taken from the conventional dictionary, sense distinctions as opposed to the synonym sets of wordnets, generating semantic wordnet relations on the basis of sense definitions, and finally, supplementing missing or implicit information.","Language Resources and Evaluation",2009,"No","wordnet dictionari lexic semant semant relat hyponymi noun verb dannet challeng compil wordnet danish reus monolingu dictionari paper contribut discuss compil comput lexic resourc convent dictionari describ theoret practic problem encount reus convent dictionari compil lexic semant resourc term wordnet specif describ methodolog issu compil wordnet danish dannet monolingu basi appli translat expans method princeton wordnet english sourc appli basi larg corpus base print dictionari modern danish approach discuss issu readjust inconsist underspecifi hyponymi hierarchi convent dictionari sens distinct oppos synonym set wordnet generat semant wordnet relat basi sens definit final supplement miss implicit inform",0
"KeywordsWeb genre identification Information retrieval Natural language processing Random feature selection ","open set evaluation of web genre identification","Web genre detection is a task that can enhance information retrieval systems by providing rich descriptions of documents and enabling more specialized queries. Most of previous studies in this field adopt the closed-set scenario where a given palette comprises all available genre labels. However this is not a realistic setup since web genres are constantly enriched with new labels and existing web genres are evolving in time. Open-set classification, where some pages used in the evaluation phase do not belong to any of the known genres, is a more realistic setup for this task. In this case, all pages not belonging to known genres can be seen as noise. This paper focuses on systematic evaluation of open-set web genre identification when the noise is either structured or unstructured. Two open-set methods combined with alternative text representation schemes and similarity measures are tested based on two benchmark corpora. Moreover, we adopt the openness test for web genre identification that enables the observation of effectiveness for a varying number of known/unknown labels.","Language Resources and Evaluation",2018,"No","web genr identif inform retriev natur languag process random featur select open set evalu web genr identif web genr detect task enhanc inform retriev system provid rich descript document enabl special queri previous studi field adopt close set scenario palett compris genr label realist setup web genr constant enrich label exist web genr evolv time open set classif page evalu phase belong genr realist setup task case page belong genr nois paper focus systemat evalu open set web genr identif nois structur unstructur open set method combin altern text represent scheme similar measur test base benchmark corpora adopt open test web genr identif enabl observ effect vari number unknown label",0
"KeywordsWeb spam filtering Statistical language models Artificial languages ","filtering artificial texts with statistical machine learning techniques","Fake content is flourishing on the Internet, ranging from basic random word salads to web scraping. Most of this fake content is generated for the purpose of nourishing fake web sites aimed at biasing search engine indexes: at the scale of a search engine, using automatically generated texts render such sites harder to detect than using copies of existing pages. In this paper, we present three methods aimed at distinguishing natural texts from artificially generated ones: the first method uses basic lexicometric features, the second one uses standard language models and the third one is based on a relative entropy measure which captures short range dependencies between words. Our experiments show that lexicometric features and language models are efficient to detect most generated texts, but fail to detect texts that are generated with high order Markov models. By comparison our relative entropy scoring algorithm, especially when trained on a large corpus, allows us to detect these “hard” text generators with a high degree of accuracy.","Language Resources and Evaluation",2011,"No","web spam filter statist languag model artifici languag filter artifici text statist machin learn techniqu fake content flourish internet rang basic random word salad web scrape fake content generat purpos nourish fake web site aim bias search engin index scale search engin automat generat text render site harder detect copi exist page paper present method aim distinguish natur text artifici generat method basic lexicometr featur standard languag model base relat entropi measur captur short rang depend word experi show lexicometr featur languag model effici detect generat text fail detect text generat high order markov model comparison relat entropi score algorithm train larg corpus detect hard text generat high degre accuraci",0
"browsing support cross-language information retrieval partial translation term list ","a method for supporting document selection in cross language information retrieval and its evaluation","It is important to give useful clues for selecting desiredcontent from a number of retrieval results obtained (usually) from avague search request. Compared with monolingual retrieval, such asupport framework is inevitable and much more significant for filteringgiven translingual retrieval results. This paper describes an attempt toprovide appropriate translation of major keywords in each document in across-language information retrieval (CLIR) result, as a browsingsupport for users. Our idea of determining appropriate translation ofmajor keywords is based on word co-occurrence distribution in thetranslation target language, considering the actual situation of WWWcontent where it is difficult to obtain aligned parallel (multilingual)corpora. The proposed method provides higher quality of keywordtranslation to yield a more effective support in identifying the targetdocuments in the retrieval result. We report the advantage of thisbrowsing support technique through evaluation experiments includingcomparison with conditions of referring to a translated documentsummary, and discuss related issues to be examined towards moreeffective cross-language information extraction.","Computers and the Humanities",2001,"No","brows support cross languag inform retriev partial translat term list method support document select cross languag inform retriev evalu import give clue select desiredcont number retriev result obtain avagu search request compar monolingu retriev asupport framework inevit signific filteringgiven translingu retriev result paper describ attempt toprovid translat major keyword document languag inform retriev clir result browsingsupport user idea determin translat ofmajor keyword base word occurr distribut thetransl target languag actual situat wwwcontent difficult obtain align parallel multilingualcorpora propos method higher qualiti keywordtransl yield effect support identifi targetdocu retriev result report advantag thisbrows support techniqu evalu experi includingcomparison condit refer translat documentsummari discuss relat issu examin moreeffect cross languag inform extract",0
"Key Wordscomputer-assisted discourse analysis content analysis political discourse theme functional grammar ","automated syntactic text description enhancement the thematic structure of discourse utterances","Our work aims at the optimization of existing tools for computer-assisted description and analysis of textual data. More specifically, we have been involved in the thematic description of clauses and clause complexes of Quebec budget speeches from 1934 to 1960. Our main objective is to enhance the work already done in this direction by elaborating the analytic framework through a study of the thematic structure of these discourses. We first set out the general context of our work by briefly explaining the research project on political discourse under the Duplessis Regime in Quebec (1936–60) and giving a brief survey of the parsing strategy applied to the corpus. Second, we present the theoretical background of thematic analysis and the operational model that we are using here. Finally, we try to illustrate the relevance of such methodological work on research data.","Computers and the Humanities",1992,"No","key wordscomput assist discours analysi content analysi polit discours theme function grammar autom syntact text descript enhanc themat structur discours utter work aim optim exist tool comput assist descript analysi textual data specif involv themat descript claus claus complex quebec budget speech main object enhanc work direct elabor analyt framework studi themat structur discours set general context work briefli explain research project polit discours duplessi regim quebec give survey pars strategi appli corpus present theoret background themat analysi oper model final illustr relev methodolog work research data",0
"word sense disambiguation evaluation SENSEVAL ","introduction to the special issue on senseval","Senseval was the first open, community-based evaluation exercise for WordSense Disambiguation programs. It took place in the summer of 1998,with tasks for English, French and Italian. There were participating systems from 23 researchgroups. This special issueis an account of the exercise. In addition to describing the contentsof the volume, this introduction considers how the exercise has shedlight on some general questions about wordsenses and evaluation.","Computers and the Humanities",2000,"No","word sens disambigu evalu sensev introduct special issu sensev sensev open communiti base evalu exercis wordsens disambigu program place summer task english french italian particip system researchgroup special issuei account exercis addit describ contentsof volum introduct consid exercis shedlight general question wordsens evalu",0
"KeywordsDialogue act tagsets Conversational corpora Tagset dimensionality ","dimensionality of dialogue act tagsets","This article compares one-dimensional and multi-dimensional dialogue act tagsets used for automatic labeling of utterances. The influence of tagset dimensionality on tagging accuracy is first discussed theoretically, then based on empirical data from human and automatic annotations of large scale resources, using four existing tagsets: damsl, swbd-damsl, icsi-mrda and maltus. The Dominant Function Approximation proposes that automatic dialogue act taggers could focus initially on finding the main dialogue function of each utterance, which is empirically acceptable and has significant practical relevance.","Language Resources and Evaluation",2008,"No","dialogu act tagset convers corpora tagset dimension dimension dialogu act tagset articl compar dimension multi dimension dialogu act tagset automat label utter influenc tagset dimension tag accuraci discuss theoret base empir data human automat annot larg scale resourc exist tagset damsl swbd damsl icsi mrda maltus domin function approxim propos automat dialogu act tagger focus initi find main dialogu function utter empir accept signific practic relev",0
NA,"barbara mcgillivray methods in latin computational linguistics brills studies in historical linguistics","The book under review opens the series of Brill’s Studies in Historical Linguistics with a new methodological approach to the observation of diachronic phenomena which moves the focus from qualitative to quantitative aspects. For this reason the author aims to illustrate how to apply computational methods to historical language data, in particular to a corpus of lemmatized and morpho-syntactically annotated Latin texts. The volume is addressed to a heterogeneous audience composed of computational linguists, Latin linguists and those within the growing community of Latin computational linguists (for an overview on methods and tools at disposition of Latin computational linguists, see Passarotti 2010; Babeu 2011; Spinazzè 2015). Such communities are likely to have different backgrounds and cultural gaps which need to be filled in at least in a cursory fashion in order to understand the overall structure of the work presented here. This difficult task is achieved through a prudent...","Language Resources and Evaluation",2015,"No","barbara mcgillivray method latin comput linguist brill studi histor linguist book review open seri brill studi histor linguist methodolog approach observ diachron phenomena move focus qualit quantit aspect reason author aim illustr appli comput method histor languag data corpus lemmat morpho syntact annot latin text volum address heterogen audienc compos comput linguist latin linguist grow communiti latin comput linguist overview method tool disposit latin comput linguist passarotti babeu spinazz communiti background cultur gap fill cursori fashion order understand structur work present difficult task achiev prudent",0
"KeywordsDigital Humanities Web-based linguistic tools Language data management IGT Shared linguistic methodologies Single system usage evaluation ","typecraft collaborative databasing and resource sharing for linguists","Interlinear Glossed Text (IGT) is a well established data format within philology and the structural and generative fields of linguistics. The best known format for an IGT is the one found in linguistic publications, where one line of text is followed by one line of glosses and one line of free translation. Although used in different functions, IGTs are ubiquitous in linguistic research and publications. Yet they also have been criticised for being fabricated and unreliable in some of their uses. However that might be, IGTs represent linguistic knowledge, and in particular for less-resourced languages, they are not rarely the only structured data available. Under the auspices of the Digital Humanities, linguists increasingly focus on the advantages of Semantic Web technologies. Presenting the modules and procedures of the web-based linguistic application TypeCraft (TC), we outline how the creation of IGTs can become an integral part of a shared linguistic methodology. Linguistic services have the potential of allowing efficient data management, and their strength lies in facilitating new forms of collaboration beyond social networking. They pave the way towards what one might call shared methodologies. In this paper we would like to discuss the linguistic value of web-based technology. By presenting the functionalities of TC and giving a detailed summary of online linguistic data creation and retrieval, we will present external and internal criteria for a single system evaluation of TC centred on usage objectives.","Language Resources and Evaluation",2014,"No","digit human web base linguist tool languag data manag igt share linguist methodolog singl system usag evalu typecraft collabor databas resourc share linguist interlinear gloss text igt establish data format philolog structur generat field linguist format igt found linguist public line text line gloss line free translat function igt ubiquit linguist research public criticis fabric unreli igt repres linguist knowledg resourc languag rare structur data auspic digit human linguist increas focus advantag semant web technolog present modul procedur web base linguist applic typecraft tc outlin creation igt integr part share linguist methodolog linguist servic potenti allow effici data manag strength lie facilit form collabor social network pave call share methodolog paper discuss linguist web base technolog present function tc give detail summari onlin linguist data creation retriev present extern intern criteria singl system evalu tc centr usag object",0
"KeywordsAmazon Turk Lexical substitution Word sense disambiguation Language resource creation Crowdsourcing ","creating a system for lexical substitutions from scratch using crowdsourcing","This article describes the creation and application of the Turk Bootstrap Word Sense Inventory for 397 frequent nouns, which is a publicly available resource for lexical substitution. This resource was acquired using Amazon Mechanical Turk. In a bootstrapping process with massive collaborative input, substitutions for target words in context are elicited and clustered by sense; then, more contexts are collected. Contexts that cannot be assigned to a current target word’s sense inventory re-enter the bootstrapping loop and get a supply of substitutions. This process yields a sense inventory with its granularity determined by substitutions as opposed to psychologically motivated concepts. It comes with a large number of sense-annotated target word contexts. Evaluation on data quality shows that the process is robust against noise from the crowd, produces a less fine-grained inventory than WordNet and provides a rich body of high precision substitution data at low cost. Using the data to train a system for lexical substitutions, we show that amount and quality of the data is sufficient for producing high quality substitutions automatically. In this system, co-occurrence cluster features are employed as a means to cheaply model topicality.","Language Resources and Evaluation",2013,"No","amazon turk lexic substitut word sens disambigu languag resourc creation crowdsourc creat system lexic substitut scratch crowdsourc articl describ creation applic turk bootstrap word sens inventori frequent noun public resourc lexic substitut resourc acquir amazon mechan turk bootstrap process massiv collabor input substitut target word context elicit cluster sens context collect context assign current target word sens inventori enter bootstrap loop suppli substitut process yield sens inventori granular determin substitut oppos psycholog motiv concept larg number sens annot target word context evalu data qualiti show process robust nois crowd produc fine grain inventori wordnet rich bodi high precis substitut data low cost data train system lexic substitut show amount qualiti data suffici produc high qualiti substitut automat system occurr cluster featur employ mean cheapli model topic",0
"KeywordsAnnotation Guidelines Spatial language Geography Information extraction Evaluation Adaptation ","spatialml annotation scheme resources and evaluation","SpatialML is an annotation scheme for marking up references to places in natural language. It covers both named and nominal references to places, grounding them where possible with geo-coordinates, and characterizes relationships among places in terms of a region calculus. A freely available annotation editor has been developed for SpatialML, along with several annotated corpora. Inter-annotator agreement on SpatialML extents is 91.3 F-measure on a corpus of SpatialML-annotated ACE documents released by the Linguistic Data Consortium. Disambiguation agreement on geo-coordinates on ACE is 87.93 F-measure. An automatic tagger for SpatialML extents scores 86.9 F on ACE, while a disambiguator scores 93.0 F on it. Results are also presented for two other corpora. In adapting the extent tagger to new domains, merging the training data from the ACE corpus with annotated data in the new domain provides the best performance.","Language Resources and Evaluation",2010,"No","annot guidelin spatial languag geographi inform extract evalu adapt spatialml annot scheme resourc evalu spatialml annot scheme mark refer place natur languag cover name nomin refer place ground geo coordin character relationship place term region calculus freeli annot editor develop spatialml annot corpora inter annot agreement spatialml extent measur corpus spatialml annot ace document releas linguist data consortium disambigu agreement geo coordin ace measur automat tagger spatialml extent score ace disambigu score result present corpora adapt extent tagger domain merg train data ace corpus annot data domain perform",0
"KeywordsDutch Lexicon Multiword expressions ","duelme a dutch electronic lexicon of multiword expressions","This article describes the design and implementation of a Dutch Electronic Lexicon of Multiword Expressions (DuELME). DuELME describes the core properties of over 5,000 Dutch multiword expressions. This article gives an overview of the decisions made in order to come to a standard lexical representation and discusses the description fields this representation comprises. We discuss the approach taken, which is innovative since it is based on the Equivalence Class Method (ECM). It is shown that introducing parameters to the ECM optimizes the method. The selection of the lexical entries and their properties is corpus-based. We describe the extraction of candidate expressions from corpora and discuss the selection criteria of the lexical entries. Moreover, we present the results of an evaluation of the standard representation in Alpino, a Dutch dependency parser.","Language Resources and Evaluation",2010,"No","dutch lexicon multiword express duelm dutch electron lexicon multiword express articl describ design implement dutch electron lexicon multiword express duelm duelm describ core properti dutch multiword express articl overview decis made order standard lexic represent discuss descript field represent compris discuss approach innov base equival class method ecm shown introduc paramet ecm optim method select lexic entri properti corpus base describ extract candid express corpora discuss select criteria lexic entri present result evalu standard represent alpino dutch depend parser",0
"KeywordsSemEval Null instantiation Semantic roles Frame semantics ","beyond sentence level semantic role labeling linking argument structures in discourse","Semantic role labeling is traditionally viewed as a sentence-level task concerned with identifying semantic arguments that are overtly realized in a fairly local context (i.e., a clause or sentence). However, this local view potentially misses important information that can only be recovered if local argument structures are linked across sentence boundaries. One important link concerns semantic arguments that remain locally unrealized (null instantiations) but can be inferred from the context. In this paper, we report on the SemEval 2010 Task-10 on “Linking Events and Their Participants in Discourse”, that addressed this problem. We discuss the corpus that was created for this task, which contains annotations on multiple levels: predicate argument structure (FrameNet and PropBank), null instantiations, and coreference. We also provide an analysis of the task and its difficulties.","Language Resources and Evaluation",2013,"No","semev null instanti semant role frame semant sentenc level semant role label link argument structur discours semant role label tradit view sentenc level task concern identifi semant argument overt realiz fair local context claus sentenc local view potenti miss import inform recov local argument structur link sentenc boundari import link concern semant argument remain local unreal null instanti infer context paper report semev task link event particip discours address problem discuss corpus creat task annot multipl level predic argument structur framenet propbank null instanti corefer provid analysi task difficulti",0
"KeywordsStudent response analysis Short answer scoring Recognizing textual entailment Semantic textual similarity ","the joint student response analysis and recognizing textual entailment challenge making sense of student responses in educational applications","We present the results of the joint student response analysis (SRA) and 8th recognizing textual entailment challenge. The goal of this challenge was to bring together researchers from the educational natural language processing and computational semantics communities. The goal of the SRA task is to assess student responses to questions in the science domain, focusing on correctness and completeness of the response content. Nine teams took part in the challenge, submitting a total of 18 runs using methods and features adapted from previous research on automated short answer grading, recognizing textual entailment and semantic textual similarity. We provide an extended analysis of the results focusing on the impact of evaluation metrics, application scenarios and the methods and features used by the participants. We conclude that additional research is required to be able to leverage syntactic dependency features and external semantic resources for this task, possibly due to limited coverage of scientific domains in existing resources. However, each of three approaches to using features and models adjusted to application scenarios achieved better system performance, meriting further investigation by the research community.","Language Resources and Evaluation",2016,"No","student respons analysi short answer score recogn textual entail semant textual similar joint student respons analysi recogn textual entail challeng make sens student respons educ applic present result joint student respons analysi sra th recogn textual entail challeng goal challeng bring research educ natur languag process comput semant communiti goal sra task assess student respons question scienc domain focus correct complet respons content team part challeng submit total run method featur adapt previous research autom short answer grade recogn textual entail semant textual similar provid extend analysi result focus impact evalu metric applic scenario method featur particip conclud addit research requir leverag syntact depend featur extern semant resourc task possibl due limit coverag scientif domain exist resourc approach featur model adjust applic scenario achiev system perform merit investig research communiti",0
"Keywordsannotation inference temporal closure temporal information TimeML ","the role of inference in the temporal annotation and analysis of text","In this paper we argue for the importance of doing inference over the information expressed by the annotations of temporally annotated corpora. We describe the process of inferential closure which can be applied to determine the full temporal content that follows from an annotation. We illustrate the importance of temporal inference and temporal closure in relation to three tasks, which are: (a) the comparison of different temporal annotations, (b) facilitating the manual annotation process needed to create temporally annotated corpora and (c) empirical investigations done over temporally annotated data.","Language Resources and Evaluation",2005,"No","annot infer tempor closur tempor inform timeml role infer tempor annot analysi text paper argu import infer inform express annot tempor annot corpora describ process inferenti closur appli determin full tempor content annot illustr import tempor infer tempor closur relat task comparison tempor annot facilit manual annot process need creat tempor annot corpora empir investig tempor annot data",0
"KeywordsBroad phonetic transcriptions Validation Automatic speech recognition ","validation of phonetic transcriptions in the context of automatic speech recognition","Some of the speech databases and large spoken language corpora that have been collected during the last fifteen years have been (at least partly) annotated with a broad phonetic transcription. Such phonetic transcriptions are often validated in terms of their resemblance to a handcrafted reference transcription. However, there are at least two methodological issues questioning this validation method. First, no reference transcription can fully represent the phonetic truth. This calls into question the status of such a transcription as a single reference for the quality of other phonetic transcriptions. Second, phonetic transcriptions are often generated to serve various purposes, none of which are considered when the transcriptions are compared to a reference transcription that was not made with the same purpose in mind. Since phonetic transcriptions are often used for the development of automatic speech recognition (ASR) systems, and since the relationship between ASR performance and a transcription’s resemblance to a reference transcription does not seem to be straightforward, we verified whether phonetic transcriptions that are to be used for ASR development can be justifiably validated in terms of their similarity to a purpose-independent reference transcription. To this end, we validated canonical representations and manually verified broad phonetic transcriptions of read speech and spontaneous telephone dialogues in terms of their resemblance to a handcrafted reference transcription on the one hand, and in terms of their suitability for ASR development on the other hand. Whereas the manually verified phonetic transcriptions resembled the reference transcription much closer than the canonical representations, the use of both transcription types yielded similar recognition results. The difference between the outcomes of the two validation methods has two implications. First, ASR developers can save themselves the effort of collecting expensive reference transcriptions in order to validate phonetic transcriptions of speech databases or spoken language corpora. Second, phonetic transcriptions should preferably be validated in terms of the application they will serve because a higher resemblance to a purpose-independent reference transcription is no guarantee for a transcription to be better suited for ASR development.","Language Resources and Evaluation",2007,"No","broad phonet transcript valid automat speech recognit valid phonet transcript context automat speech recognit speech databas larg spoken languag corpora collect fifteen year part annot broad phonet transcript phonet transcript valid term resembl handcraft refer transcript methodolog issu question valid method refer transcript fulli repres phonet truth call question status transcript singl refer qualiti phonet transcript phonet transcript generat serv purpos consid transcript compar refer transcript made purpos mind phonet transcript develop automat speech recognit asr system relationship asr perform transcript resembl refer transcript straightforward verifi phonet transcript asr develop justifi valid term similar purpos independ refer transcript end valid canon represent manual verifi broad phonet transcript read speech spontan telephon dialogu term resembl handcraft refer transcript hand term suitabl asr develop hand manual verifi phonet transcript resembl refer transcript closer canon represent transcript type yield similar recognit result differ outcom valid method implic asr develop save effort collect expens refer transcript order valid phonet transcript speech databas spoken languag corpora phonet transcript prefer valid term applic serv higher resembl purpos independ refer transcript guarante transcript suit asr develop",0
"KeywordsMultilingualism Translation divergence Syntactic projection ","capturing divergence in dependency trees to improve syntactic projection","Obtaining syntactic parses is an important step in many NLP pipelines. However, most of the world’s languages do not have a large amount of syntactically annotated data available for building parsers. Syntactic projection techniques attempt to address this issue by using parallel corpora consisting of resource-poor and resource-rich language pairs, taking advantage of a parser for the resource-rich language and word alignment between the languages to project the parses onto the data for the resource-poor language. These projection methods can suffer, however, when syntactic structures for some sentence pairs in the two languages look quite different. In this paper, we investigate the use of small, parallel, annotated corpora to automatically detect divergent structural patterns between two languages. We then use these detected patterns to improve projection algorithms and dependency parsers, allowing for better performing NLP tools for resource-poor languages, particularly those that may not have large amounts of annotated data necessary for traditional, fully-supervised methods. While this detection process is not exhaustive, we demonstrate that common patterns of divergence can be identified automatically without prior knowledge of a given language pair, and the patterns can be used to improve performance of syntactic projection and parsing.","Language Resources and Evaluation",2014,"No","multilingu translat diverg syntact project captur diverg depend tree improv syntact project obtain syntact pars import step nlp pipelin world languag larg amount syntact annot data build parser syntact project techniqu attempt address issu parallel corpora consist resourc poor resourc rich languag pair take advantag parser resourc rich languag word align languag project pars data resourc poor languag project method suffer syntact structur sentenc pair languag paper investig small parallel annot corpora automat detect diverg structur pattern languag detect pattern improv project algorithm depend parser allow perform nlp tool resourc poor languag larg amount annot data tradit fulli supervis method detect process exhaust demonstr common pattern diverg identifi automat prior knowledg languag pair pattern improv perform syntact project pars",0
"KeywordsSpanish Treebank Dependency annotation Technical corpus ","dependency structure annotation in the iula spanish lsp treebank","This paper presents the IULA Spanish LSP Treebank, an open-source treebank of over 40,000 sentences, developed in the framework of the European project METANET4U. The IULA Spanish LSP Treebank is the first technical corpus of Spanish annotated at surface syntactic level, following the dependency grammar theory. We present the method we used to create the resource and the linguistic annotations that the treebank provides, using examples and comparing with similar resources. We also provide the statistics of the treebank and the evaluation results.","Language Resources and Evaluation",2015,"No","spanish treebank depend annot technic corpus depend structur annot iula spanish lsp treebank paper present iula spanish lsp treebank open sourc treebank sentenc develop framework european project metanetu iula spanish lsp treebank technic corpus spanish annot surfac syntact level depend grammar theori present method creat resourc linguist annot treebank exampl compar similar resourc provid statist treebank evalu result",0
"KeywordsComputational Linguistic ","computerized lemmatization without the use of a dictionary a case study from swedish lexicology",NA,"Computers and the Humanities",1972,"No","comput linguist computer lemmat dictionari case studi swedish lexicolog na",0
"Key wordsTEI SGML text encoding spoken texts transcription temporal alignment ","the encoding of spoken texts","There is a great deal of variation in the encoding of spoken texts in electronic form, both with respect to the types of features represented and the way particular features are rendered. This paper surveys problems in the electronic representation of speech and presents the solutions proposed by the Text Encoding Initiative. The special tags needed for the encoding of spoken texts are discussed, including a mechanism for temporal alignment. Further work is needed on phonological aspects, parallel representation, and on the development of software which connects the systematic underlying representation with a workable format for input and display.","Computers and the Humanities",1995,"No","key wordstei sgml text encod spoken text transcript tempor align encod spoken text great deal variat encod spoken text electron form respect type featur repres featur render paper survey problem electron represent speech present solut propos text encod initi special tag need encod spoken text discuss includ mechan tempor align work need phonolog aspect parallel represent develop softwar connect systemat under represent workabl format input display",0
"GATE infrastructure language engineering software architecture ","gate a general architecture for text engineering","This paper presents the design, implementation and evaluation of GATE, a General Architecture for Text Engineering.GATE lies at the intersection of human language computation and software engineering, and constitutes aninfrastructural system supporting research and development of languageprocessing software.","Computers and the Humanities",2002,"No","gate infrastructur languag engin softwar architectur gate general architectur text engin paper present design implement evalu gate general architectur text engineeringg lie intersect human languag comput softwar engin constitut aninfrastructur system support research develop languageprocess softwar",0
"KeywordsLarge Body Significant Part Computer Processing Computational Linguistic Linguistic Theory ","a computer assisted study of the vocabulary of young navajo children","Computer processing made it feasible for us to base our study of the vocabulary of six-year-old Navajo children on as large a corpus as we could collect in the time available. Our difficulties, as so often in computational linguistics, were with matters of linguistic theory rather than of computing. What we ran into was the as-yet unsolved question of the nature of the word in Navajo: how many affixes should be written as part of the verb and how many as separate words; and how does one handle the unbelievably complex morphophonemics in choosing headwords. The computer once again showed its ability, not just as an aid in handling large bodies of data, but as a heuristic device that makes clear to the researcher the limitations of his understanding of the material he is working with.","Computers and the Humanities",1973,"No","larg bodi signific part comput process comput linguist linguist theori comput assist studi vocabulari young navajo children comput process made feasibl base studi vocabulari year navajo children larg corpus collect time difficulti comput linguist matter linguist theori comput ran unsolv question natur word navajo affix written part verb separ word handl unbeliev complex morphophonem choos headword comput show abil aid handl larg bodi data heurist devic make clear research limit understand materi work",0
"KeywordsLiterature Research Computational Linguistic Computing Activity Current Computing ","current computing activity in scandinavia related to language and literature research",NA,"Computers and the Humanities",1968,"No","literatur research comput linguist comput activ current comput current comput activ scandinavia relat languag literatur research na",0
"KeywordsDevelopment Time Information Source Computational Linguistic Close Match Fast Development ","memory based word sense disambiguation","We describe a memory-based classification architecture for word sense disambiguation and its application to the SENSEVAL evaluationtask. For each ambiguous word, a semantic word expert isautomatically trained using a memory-based approach. In each expert,selecting the correct sense of a word in a new context is achieved byfinding the closest match to stored examples of this task. Advantagesof the approach include (i) fast development time for word experts,(ii) easy and elegant automatic integration of information sources,(iii) use of all available data for training the experts, and (iv)relatively high accuracy with minimal linguistic engineering.","Computers and the Humanities",2000,"No","develop time inform sourc comput linguist close match fast develop memori base word sens disambigu describ memori base classif architectur word sens disambigu applic sensev evaluationtask ambigu word semant word expert isautomat train memori base approach expertselect correct sens word context achiev byfind closest match store exampl task advantagesof approach includ fast develop time word expertsii easi eleg automat integr inform sourcesiii data train expert iv high accuraci minim linguist engin",0
"KeywordsComputational Linguistic Belgian Initiative ","vox latina belgian initiatives in data processing the intellectual language of europe ad 1971965",NA,"Computers and the Humanities",1978,"No","comput linguist belgian initi vox latina belgian initi data process intellectu languag europ ad na",0
"KeywordsDiscriminant Function Production Area Classification Algorithm Computational Linguistic Morphological Attribute ","typologie damphores romaines par une methode logique de classification","For classifying wine amphoras used at the end of the Roman Republic and the beginning of the Empire (the so-called Dressel 2–4), we present a typological approach which combines a classification algorithm with the archeological reasoning. At the first step, clusters contain only nuclei based on the different production areas. To assign a corpus of artifacts to them, it is divided for each cluster into a context (artifacts which certainly do not belong to the cluster) and a residue. For each cluster, we built characteristic definitions with logical discriminant function of morphological attributes. Each definition cuts the residue in two classes: one containing the artifacts assigned to the cluster by the definition and the complementary one in the residue. Assignment and choices of cluster definitions and context remain with the archeological expert, who submits those typological constructions to a validation process founded on archeological knowledge. Such an approach focuses on a very common situation in human sciences: the construction of a cognitive typology beginning with a partially clustered set. Clustering must be done with descriptive attributes, without knowing if they can be connected with the wanted cluster.","Computers and the Humanities",1983,"No","discrimin function product area classif algorithm comput linguist morpholog attribut typologi damphor romain par une method logiqu de classif classifi wine amphora end roman republ begin empir call dressel present typolog approach combin classif algorithm archeolog reason step cluster nuclei base product area assign corpus artifact divid cluster context artifact belong cluster residu cluster built characterist definit logic discrimin function morpholog attribut definit cut residu class artifact assign cluster definit complementari residu assign choic cluster definit context remain archeolog expert submit typolog construct valid process found archeolog knowledg approach focus common situat human scienc construct cognit typolog begin partial cluster set cluster descript attribut know connect want cluster",0
"discourse processing finite state methods machine translation speech act assignment ","modeling task oriented dialogue","A common tool for improving theperformance quality of natural languageprocessing systems is the use of contextualinformation for disambiguation. Here I describethe use of a finite state machine (FSM) todisambiguate speech acts in a machinetranslation system. The FSM has two layers thatmodel, respectively, the global and localstructures found in naturally-occurringconversations. The FSM has been modeled on acorpus of task-oriented dialogues in a travelplanning situation. In the dialogues, one ofthe interactants is a travel agent or hotelclerk, and the other a client requestinginformation or services. A discourse processorbased on the FSM was implemented in order toprocess contextual information in a machinetranslation system. Evaluation results showthat the discourse processor is able todisambiguate and improve the quality of thedialogue translation. Other applicationsinclude human-computer interaction andcomputer-assisted language learning.","Computers and the Humanities",2003,"No","discours process finit state method machin translat speech act assign model task orient dialogu common tool improv theperform qualiti natur languageprocess system contextualinform disambigu describeth finit state machin fsm todisambigu speech act machinetransl system fsm layer thatmodel global localstructur found natur occurringconvers fsm model acorpus task orient dialogu travelplan situat dialogu ofth interact travel agent hotelclerk client requestinginform servic discours processorbas fsm implement order toprocess contextu inform machinetransl system evalu result showthat discours processor todisambigu improv qualiti thedialogu translat applicationsinclud human comput interact andcomput assist languag learn",0
"KeywordsMarkov Decision Process Data Resource Dialogue System Dialogue Model Influence Diagram ","introduction to special issue on data resources evaluation and dialogue interaction","This special issue on Data Resources, Evaluation, and Dialogue Interaction is based on five thoroughly revised and extended papers from the sixth SIGdial Workshop held in Lisbon, Portugal, in September 2005. SIGdial is a special interest group on discourse and dialogue whose parent organisations are the Association for Computational Linguistics (ACL) and the International Speech Communication Association (ISCA). SIGdial workshops accommodate a broad range of topics related to discourse and dialogue. Among these topics are data resources, evaluation, and dialogue interaction. The papers selected for this special issue have in common that they all deal with aspects of these topics and each paper has its focus on at least one of them.","Language Resources and Evaluation",2006,"No","markov decis process data resourc dialogu system dialogu model influenc diagram introduct special issu data resourc evalu dialogu interact special issu data resourc evalu dialogu interact base revis extend paper sixth sigdial workshop held lisbon portug septemb sigdial special interest group discours dialogu parent organis associ comput linguist acl intern speech communic associ isca sigdial workshop accommod broad rang topic relat discours dialogu topic data resourc evalu dialogu interact paper select special issu common deal aspect topic paper focus",0
"KeywordsSpeech Recognition Machine Translation Statistical Machine Translation Language Technology Human Language Technology ","introduction to the special issue on african language technology","In today’s digital multilingual world, language technology is crucial for providing access to information and opportunities for economic development. With approximately two thousand different languages, Africa is a multilingual continent par excellence, presenting acute challenges for those seeking to promote and use African languages in the areas of business development, education and relief aid. In recent times a number of researchers and institutions, both from Africa and elsewhere, have come forward to share the common goal of developing capabilities in language technology for African languages. In 2009 and 2010, the first two workshops on African Language Technology were organized (De Pauw et al. 2009, 2010a) as a forum to bring together a wide range of researchers working in this domain.","Language Resources and Evaluation",2011,"No","speech recognit machin translat statist machin translat languag technolog human languag technolog introduct special issu african languag technolog today digit multilingu world languag technolog crucial provid access inform opportun econom develop approxim thousand languag africa multilingu contin par excel present acut challeng seek promot african languag area busi develop educ relief aid recent time number research institut africa forward share common goal develop capabl languag technolog african languag workshop african languag technolog organ de pauw al a forum bring wide rang research work domain",0
"KeywordsAbbreviation Atomic abbreviation Single character recovery model ","mining atomic chinese abbreviations with a probabilistic single character recovery model","An HMM-based single character recovery (SCR) model is proposed in this paper to extract a large set of atomic abbreviations and their full forms from a text corpus. By an “atomic abbreviation,” it refers to an abbreviated word consisting of a single Chinese character. This task is important since Chinese abbreviations cannot be enumerated exhaustively but the abbreviation process for compound words seems to be compositional. One can often decode an abbreviated word character by character to its full form. With a large atomic abbreviation dictionary, one may be able to handle multiple character abbreviation problems more easily based on the compositional property of abbreviations.","Language Resources and Evaluation",2006,"No","abbrevi atom abbrevi singl charact recoveri model mine atom chines abbrevi probabilist singl charact recoveri model hmm base singl charact recoveri scr model propos paper extract larg set atom abbrevi full form text corpus atom abbrevi refer abbrevi word consist singl chines charact task import chines abbrevi enumer exhaust abbrevi process compound word composit decod abbrevi word charact charact full form larg atom abbrevi dictionari handl multipl charact abbrevi problem easili base composit properti abbrevi",0
"KeywordsComputational Linguistic Question Answering ","introduction to special issue on advances in question answering",NA,"Language Resources and Evaluation",2005,"No","comput linguist question answer introduct special issu advanc question answer na",0
"KeywordsErrors Dyslexia Visual Phonetics Resource Spanish ","a resource of errors written in spanish by people with dyslexia and its linguistic phonetic and visual analysis","In this work we introduce the analysis of DysList, a language resource for Spanish composed of a list of unique spelling errors extracted from a collection of texts written by people with dyslexia. Each of the errors was annotated with a set of characteristics as well as with visual and phonetic features. To the best of our knowledge, this is the largest resource of this kind in Spanish. We also analyzed all the features of Spanish errors and our main finding is that dyslexic errors are phonetically and visually motivated.","Language Resources and Evaluation",2017,"No","error dyslexia visual phonet resourc spanish resourc error written spanish peopl dyslexia linguist phonet visual analysi work introduc analysi dyslist languag resourc spanish compos list uniqu spell error extract collect text written peopl dyslexia error annot set characterist visual phonet featur knowledg largest resourc kind spanish analyz featur spanish error main find dyslex error phonet visual motiv",0
"KeywordsSynonymy networks Semantic relatedness Collaboratively constructed resources Wiktionary Semi-automatic enrichment Random walks Small worlds ","semi automatic enrichment of crowdsourced synonymy networks the wisigoth system applied to wiktionary","Semantic lexical resources are a mainstay of various Natural Language Processing applications. However, comprehensive and reliable resources are rare and not often freely available. Handcrafted resources are too costly for being a general solution while automatically-built resources need to be validated by experts or at least thoroughly evaluated. We propose in this paper a picture of the current situation with regard to lexical resources, their building and their evaluation. We give an in-depth description of Wiktionary, a freely available and collaboratively built multilingual dictionary. Wiktionary is presented here as a promising raw resource for NLP. We propose a semi-automatic approach based on random walks for enriching Wiktionary synonymy network that uses both endogenous and exogenous data. We take advantage of the wiki infrastructure to propose a validation “by crowds”. Finally, we present an implementation called WISIGOTH, which supports our approach.","Language Resources and Evaluation",2013,"No","synonymi network semant related collabor construct resourc wiktionari semi automat enrich random walk small world semi automat enrich crowdsourc synonymi network wisigoth system appli wiktionari semant lexic resourc mainstay natur languag process applic comprehens reliabl resourc rare freeli handcraft resourc cost general solut automat built resourc valid expert evalu propos paper pictur current situat regard lexic resourc build evalu give depth descript wiktionari freeli collabor built multilingu dictionari wiktionari present promis raw resourc nlp propos semi automat approach base random walk enrich wiktionari synonymi network endogen exogen data advantag wiki infrastructur propos valid crowd final present implement call wisigoth support approach",0
"KeywordsDiacritic restoration Unicodification Under-resourced languages African languages Machine learning ","statistical unicodification of african languages","Many languages in Africa are written using Latin-based scripts, but often with extra diacritics (e.g. dots below in Igbo: \({\d i}, {\d o}, {\d u}\)) or modifications to the letters themselves (e.g. open vowels “e” and “o” in Lingala: ɛ, ɔ). While it is possible to render these characters accurately in Unicode, oftentimes keyboard input methods are not easily accessible or are cumbersome to use, and so the vast majority of electronic texts in many African languages are written in plain ASCII. We call the process of converting an ASCII text to its proper Unicode form unicodification. This paper describes an open-source package which performs automatic unicodification, implementing a variant of an algorithm described in previous work of De Pauw, Wagacha, and de Schryver. We have trained models for more than 100 languages using web data, and have evaluated each language using a range of feature sets.","Language Resources and Evaluation",2011,"No","diacrit restor unicodif resourc languag african languag machin learn statist unicodif african languag languag africa written latin base script extra diacrit dot igbo modif letter open vowel lingala render charact accur unicod oftentim keyboard input method easili access cumbersom vast major electron text african languag written plain ascii call process convert ascii text proper unicod form unicodif paper describ open sourc packag perform automat unicodif implement variant algorithm previous work de pauw wagacha de schryver train model languag web data evalu languag rang featur set",0
"KeywordsCrowdsourcing Evaluation Semantic annotation Cross-language transfer ","cross language transfer of semantic annotation via targeted crowdsourcing task design and evaluation","Modern data-driven spoken language systems (SLS) require manual semantic annotation for training spoken language understanding parsers. Multilingual porting of SLS demands significant manual effort and language resources, as this manual annotation has to be replicated. Crowdsourcing is an accessible and cost-effective alternative to traditional methods of collecting and annotating data.
 The application of crowdsourcing to simple tasks has been well investigated. However, complex tasks, like cross-language semantic annotation transfer, may generate low judgment agreement and/or poor performance. The most serious issue in cross-language porting is the absence of reference annotations in the target language; thus, crowd quality control and the evaluation of the collected annotations is difficult. In this paper we investigate targeted crowdsourcing for semantic annotation transfer that delegates to crowds a complex task such as segmenting and labeling of concepts taken from a domain ontology; and evaluation using source language annotation. To test the applicability and effectiveness of the crowdsourced annotation transfer we have considered the case of close and distant language pairs: Italian–Spanish and Italian–Greek. The corpora annotated via crowdsourcing are evaluated against source and target language expert annotations. We demonstrate that the two evaluation references (source and target) highly correlate with each other; thus, drastically reduce the need for the target language reference annotations.","Language Resources and Evaluation",2018,"No","crowdsourc evalu semant annot cross languag transfer cross languag transfer semant annot target crowdsourc task design evalu modern data driven spoken languag system sls requir manual semant annot train spoken languag understand parser multilingu port sls demand signific manual effort languag resourc manual annot replic crowdsourc access cost effect altern tradit method collect annot data applic crowdsourc simpl task investig complex task cross languag semant annot transfer generat low judgment agreement poor perform issu cross languag port absenc refer annot target languag crowd qualiti control evalu collect annot difficult paper investig target crowdsourc semant annot transfer deleg crowd complex task segment label concept domain ontolog evalu sourc languag annot test applic effect crowdsourc annot transfer consid case close distant languag pair italian spanish italian greek corpora annot crowdsourc evalu sourc target languag expert annot demonstr evalu refer sourc target high correl drastic reduc target languag refer annot",0
"KeywordsAbstract anaphora Abstract entities Coreference annotation Semantic annotation ","annotating abstract anaphora","In this paper, we present first results from annotating abstract (discourse-deictic) anaphora in German. Our annotation guidelines provide linguistic tests for identifying the antecedent, and for determining the semantic types of both the antecedent and the anaphor. The corpus consists of selected speaker turns from the Europarl corpus. To date, 100 texts have been annotated according to these guidelines. The annotations show that anaphoric personal and demonstrative pronouns differ with respect to the distance to their antecedents. A semantic analysis reveals that, contrary to suggestions put forward in the literature, referents of anaphors do not tend to be more abstract than the referents of their antecedents.","Language Resources and Evaluation",2012,"No","abstract anaphora abstract entiti corefer annot semant annot annot abstract anaphora paper present result annot abstract discours deictic anaphora german annot guidelin provid linguist test identifi anteced determin semant type anteced anaphor corpus consist select speaker turn europarl corpus date text annot guidelin annot show anaphor person demonstr pronoun differ respect distanc anteced semant analysi reveal contrari suggest put forward literatur refer anaphor tend abstract refer anteced",0
"KeywordsElectronic health records Semantic textual similarity Natural language processing Clinical semantic textual similarity resource ","medsts a resource for clinical semantic textual similarity","The adoption of electronic health records (EHRs) has enabled a wide range of applications leveraging EHR data. However, the meaningful use of EHR data largely depends on our ability to efficiently extract and consolidate information embedded in clinical text where natural language processing (NLP) techniques are essential. Semantic textual similarity (STS) that measures the semantic similarity between text snippets plays a significant role in many NLP applications. In the general NLP domain, STS shared tasks have made available a huge collection of text snippet pairs with manual annotations in various domains. In the clinical domain, STS can enable us to detect and eliminate redundant information that may lead to a reduction in cognitive burden and an improvement in the clinical decision-making process. This paper elaborates our efforts to assemble a resource for STS in the medical domain, MedSTS. It consists of a total of 174,629 sentence pairs gathered from a clinical corpus at Mayo Clinic. A subset of MedSTS (MedSTS_ann) containing 1068 sentence pairs was annotated by two medical experts with semantic similarity scores of 0–5 (low to high similarity). We further analyzed the medical concepts in the MedSTS corpus, and tested four STS systems on the MedSTS_ann corpus. In the future, we will organize a shared task by releasing the MedSTS_ann corpus to motivate the community to tackle the real world clinical problems.","Language Resources and Evaluation",2018,"No","electron health record semant textual similar natur languag process clinic semant textual similar resourc medst resourc clinic semant textual similar adopt electron health record ehr enabl wide rang applic leverag ehr data meaning ehr data larg depend abil effici extract consolid inform embed clinic text natur languag process nlp techniqu essenti semant textual similar sts measur semant similar text snippet play signific role nlp applic general nlp domain sts share task made huge collect text snippet pair manual annot domain clinic domain sts enabl detect elimin redund inform lead reduct cognit burden improv clinic decis make process paper elabor effort assembl resourc sts medic domain medst consist total sentenc pair gather clinic corpus mayo clinic subset medst medstsann sentenc pair annot medic expert semant similar score low high similar analyz medic concept medst corpus test sts system medstsann corpus futur organ share task releas medstsann corpus motiv communiti tackl real world clinic problem",0
"KeywordsLatent semantic analysis WordNet Term alignment  Semantic similarity ","robust semantic text similarity using lsa machine learning and linguistic resources","Semantic textual similarity is a measure of the degree of semantic equivalence between two pieces of text. We describe the SemSim system and its performance in the *SEM 2013 and SemEval-2014 tasks on semantic textual similarity. At the core of our system lies a robust distributional word similarity component that combines latent semantic analysis and machine learning augmented with data from several linguistic resources. We used a simple term alignment algorithm to handle longer pieces of text. Additional wrappers and resources were used to handle task specific challenges that include processing Spanish text, comparing text sequences of different lengths, handling informal words and phrases, and matching words with sense definitions. In the *SEM 2013 task on Semantic Textual Similarity, our best performing system ranked first among the 89 submitted runs. In the SemEval-2014 task on Multilingual Semantic Textual Similarity, we ranked a close second in both the English and Spanish subtasks. In the SemEval-2014 task on Cross-Level Semantic Similarity, we ranked first in Sentence–Phrase, Phrase–Word, and Word–Sense subtasks and second in the Paragraph–Sentence subtask.","Language Resources and Evaluation",2016,"No","latent semant analysi wordnet term align semant similar robust semant text similar lsa machin learn linguist resourc semant textual similar measur degre semant equival piec text describ semsim system perform sem semev task semant textual similar core system lie robust distribut word similar compon combin latent semant analysi machin learn augment data linguist resourc simpl term align algorithm handl longer piec text addit wrapper resourc handl task specif challeng includ process spanish text compar text sequenc length handl inform word phrase match word sens definit sem task semant textual similar perform system rank submit run semev task multilingu semant textual similar rank close english spanish subtask semev task cross level semant similar rank sentenc phrase phrase word word sens subtask paragraph sentenc subtask",0
NA,"news and notes",NA,"Computers and the Humanities",1971,"No","news note na",0
"combining knowledge sources word sense disambiguation ","combining supervised and unsupervised lexical knowledge methods for word sense disambiguation","This work combines a set of available techniques – whichcould be further extended – to perform noun sense disambiguation. We use several unsupervised techniques (Rigau et al., 1997) that draw knowledge from a variety of sources. In addition, we also apply a supervised technique in order to show that supervised and unsupervised methods can be combined to obtain better results. This paper tries to prove that using an appropriate method to combine those heuristics we can disambiguate words in free running text with reasonable precision.","Computers and the Humanities",2000,"No","combin knowledg sourc word sens disambigu combin supervis unsupervis lexic knowledg method word sens disambigu work combin set techniqu whichcould extend perform noun sens disambigu unsupervis techniqu rigau al draw knowledg varieti sourc addit appli supervis techniqu order show supervis unsupervis method combin obtain result paper prove method combin heurist disambigu word free run text reason precis",0
"analogy-based NLP semantic similarity word sense disambiguation ","romanseval results for italian by sense","The paper describes SENSE, a word sense disambiguation system thatmakes use of different types of cues to infer the most likelysense of a word given its context. Architecture and functioning ofthe system are briefly illustrated. Results are given for theROMANSEVAL Italian test corpus of verbs.","Computers and the Humanities",2000,"No","analog base nlp semant similar word sens disambigu romansev result italian sens paper describ sens word sens disambigu system thatmak type cue infer likelysens word context architectur function ofth system briefli illustr result theromansev italian test corpus verb",0
"KeywordsLanguage Acquisition Selectional Constraint Minimalist Program Language Change Distributional Regularity ","michael r brent computational approaches to language acquisition",NA,"Computers and the Humanities",1999,"No","languag acquisit select constraint minimalist program languag chang distribut regular michael brent comput approach languag acquisit na",0
"KeywordsComputational Linguistic ","solar a semantically oriented lexical archive current status and plans",NA,"Computers and the Humanities",1974,"No","comput linguist solar semant orient lexic archiv current status plan na",0
"corpus processing electronic dictionaries finite state technology natural language processing ","text indexation with intex","INTEX is a linguistic development environment that includes large-coverage dictionaries and grammars, and parses texts of several million words in real time. INTEX has tools to create and maintain large-coverage lexical resources as well as morphological and syntactic grammars. Dictionaries and grammars are applied to texts in order to locate morphological, lexical and syntactic patterns, remove ambiguities, and tag simple and compound words. INTEX can build lemmatized concordances and indices of large texts with respect to all types of Finite State patterns. INTEX is used as a corpus processor, to analyze literary, journalistic and technical texts. I describe here the subset of tools used to perform advanced search requests on large texts.","Computers and the Humanities",1999,"No","corpus process electron dictionari finit state technolog natur languag process text index intex intex linguist develop environ includ larg coverag dictionari grammar pars text million word real time intex tool creat maintain larg coverag lexic resourc morpholog syntact grammar dictionari grammar appli text order locat morpholog lexic syntact pattern remov ambigu tag simpl compound word intex build lemmat concord indic larg text respect type finit state pattern intex corpus processor analyz literari journalist technic text describ subset tool perform advanc search request larg text",0
"automatic text categorization text analysis text classification ","categorisation techniques in computer assisted reading and analysis of texts carat in the humanities","There are two important strategies incomputer-assisted reading and analysis of text(CARAT). The first relates to theclassification process, and the second pertainsto the categorisation process. These twooften-interrelated operations have beenregularly recognised as essential components oftext analysis. However, the two operations arehighly time-consuming. A possible solution tothis problem calls upon more inductive orbottom-up strategies that are numerical andstatistical in nature. In our own research, wehave been exploring a few of these techniquesand their combination. We now know, through ourown past research and others' work, that theclassification methods allow a good empiricalthematic exploration of a corpus. Morespecifically, in this paper we shallconcentrate on the problem of assisting theautomatic categorisation of small segments of aphilosophical text into a set of thematiccategories.","Computers and the Humanities",2003,"No","automat text categor text analysi text classif categoris techniqu comput assist read analysi text carat human import strategi incomput assist read analysi textcarat relat theclassif process pertainsto categoris process twooften interrel oper beenregular recognis essenti compon oftext analysi oper arehigh time consum solut tothi problem call induct orbottom strategi numer andstatist natur research wehav explor techniquesand combin ourown past research work theclassif method good empiricalthemat explor corpus morespecif paper shallconcentr problem assist theautomat categoris small segment aphilosoph text set thematiccategori",0
"KeywordsMultilingual morphology Loanwords Assimilation Neologism ","google the verb","The verb google is intriguing for the study of morphology, loanwords, assimilation, language contrast and neologisms. We present data for it for nineteen languages from nine language families.","Language Resources and Evaluation",2010,"No","multilingu morpholog loanword assimil neolog googl verb verb googl intrigu studi morpholog loanword assimil languag contrast neolog present data nineteen languag languag famili",0
"KeywordsTurkish Natural language processing Language resources ","turkish and its challenges for language processing","We present a short survey and exposition of some of the important aspects of Turkish that have proven challenging for natural language processing. Most of the challenges stem from the complex morphology of Turkish and how morphology interacts with syntax. We also provide a short overview of the major tools and resources developed for Turkish natural language processing over the last two decades.","Language Resources and Evaluation",2014,"No","turkish natur languag process languag resourc turkish challeng languag process present short survey exposit import aspect turkish proven challeng natur languag process challeng stem complex morpholog turkish morpholog interact syntax provid short overview major tool resourc develop turkish natur languag process decad",0
"KeywordsDepression screening Depression lexicon Lexicon evaluation Lexicon expansion Text analysis Natural language processing ","evaluating and improving lexical resources for detecting signs of depression in text","While considerable attention has been given to the analysis of texts written by depressed individuals, few studies were interested in evaluating and improving lexical resources for supporting the detection of signs of depression in text.
 In this paper, we present a search-based methodology to evaluate existing depression lexica. To meet this aim, we exploit existing resources for depression and language use and we analyze which elements of the lexicon are the most effective at revealing depression symptoms. Furthermore, we propose innovative expansion strategies able to further enhance the quality of the lexica.
","Language Resources and Evaluation",2018,"No","depress screen depress lexicon lexicon evalu lexicon expans text analysi natur languag process evalu improv lexic resourc detect sign depress text consider attent analysi text written depress individu studi interest evalu improv lexic resourc support detect sign depress text paper present search base methodolog evalu exist depress lexica meet aim exploit exist resourc depress languag analyz element lexicon effect reveal depress symptom propos innov expans strategi enhanc qualiti lexica",0
"KeywordsArabic language Text mining Named Entity Recognition Event detection Morphological analysis Root extraction ","a real time named entity recognition system for arabic text mining","Arabic is the most widely spoken language in the Arab World. Most people of the Islamic World understand the Classic Arabic language because it is the language of the Qur’an. Despite the fact that in the last decade the number of Arabic Internet users (Middle East and North and East of Africa) has increased considerably, systems to analyze Arabic digital resources automatically are not as easily available as they are for English. Therefore, in this work, an attempt is made to build a real time Named Entity Recognition system that can be used in web applications to detect the appearance of specific named entities and events in news written in Arabic. Arabic is a highly inflectional language, thus we will try to minimize the impact of Arabic affixes on the quality of the pattern recognition model applied to identify named entities. These patterns are built up by processing and integrating different gazetteers, from DBPedia (http://dbpedia.org/About, 2009) to GATE (A general architecture for text engineering, 2009) and ANERGazet (http://users.dsic.upv.es/grupos/nle/?file=kop4.php).","Language Resources and Evaluation",2012,"No","arab languag text mine name entiti recognit event detect morpholog analysi root extract real time name entiti recognit system arab text mine arab wide spoken languag arab world peopl islam world understand classic arab languag languag qur fact decad number arab internet user middl east north east africa increas consider system analyz arab digit resourc automat easili english work attempt made build real time name entiti recognit system web applic detect appear specif name entiti event news written arab arab high inflect languag minim impact arab affix qualiti pattern recognit model appli identifi name entiti pattern built process integr gazett dbpedia httpdbpediaorg gate general architectur text engin anergazet httpusersdsicupvesgruposnlefilekopphp",0
"KeywordsResource-poor languages Interlinear glossed text ODIN ","enriching a massively multilingual database of interlinear glossed text","The majority of the world’s languages have little to no NLP resources or tools. This is due to a lack of training data (“resources”) over which tools, such as taggers or parsers, can be trained. In recent years, there have been increasing efforts to apply NLP methods to a much broader swath of the world’s languages. In many cases this involves bootstrapping the learning process with enriched or partially enriched resources. We propose that Interlinear Glossed Text (IGT), a very common form of annotated data used in the field of linguistics, has great potential for bootstrapping NLP tools for resource-poor languages. Although IGT is generally very richly annotated, and can be enriched even further (e.g., through structural projection), much of the content is not easily consumable by machines since it remains “trapped” in linguistic scholarly documents and in human readable form. In this paper, we describe the expansion of the ODIN resource—a database containing many thousands of instances of IGT for over a thousand languages. We enrich the original IGT data by adding word alignment and syntactic structure. To make the data in ODIN more readily consumable by tool developers and NLP researchers, we adopt and extend a new XML format for IGT, called Xigt. We also develop two packages for manipulating IGT data: one, INTENT, enriches raw IGT automatically, and the other, XigtEdit, is a graphical IGT editor.","Language Resources and Evaluation",2016,"No","resourc poor languag interlinear gloss text odin enrich massiv multilingu databas interlinear gloss text major world languag nlp resourc tool due lack train data resourc tool tagger parser train recent year increas effort appli nlp method broader swath world languag case involv bootstrap learn process enrich partial enrich resourc propos interlinear gloss text igt common form annot data field linguist great potenti bootstrap nlp tool resourc poor languag igt general rich annot enrich structur project content easili consum machin remain trap linguist scholar document human readabl form paper describ expans odin resourc databas thousand instanc igt thousand languag enrich origin igt data ad word align syntact structur make data odin readili consum tool develop nlp research adopt extend xml format igt call xigt develop packag manipul igt data intent enrich raw igt automat xigtedit graphic igt editor",0
"KeywordsUnicode Font Devanagari South Asian languages/scripts Legacy text Encoding Conversion Virama Conjunct consonant Vowel diacritic ","from legacy encodings to unicode the graphical and logical principles in the scripts of south asia","Much electronic text in the languages of South Asia has been published on the Internet. However, while Unicode has emerged as the favoured encoding system of corpus and computational linguists, most South Asian language data on the web uses one of a wide range of non-standard legacy encodings. This paper describes the difficulties inherent in converting text in these encodings to Unicode. Among the various legacy encodings for South Asian scripts, the most problematic are 8-bit fonts based on graphical principles (as opposed to the logical principles of Unicode). Graphical fonts typically encode several features in ways highly incompatible with Unicode. For instance, half-form glyphs used to construct conjunct consonants are typically separate code points in 8-bit fonts; in Unicode they are represented by the full consonant followed by virama. There are many more such cases. The solution described here is an approach to text conversion based on mapping rules. A small number of generalised rules (plus the capacity for more specialised rules) captures the behaviour of each character in a font, building up a conversion algorithm for that encoding. This system is embedded in a font-mapping program, outputting CES-compliant SGML Unicode. This program, a generalised text-conversion tool, has been employed extensively in corpus-building for South Asian languages.","Language Resources and Evaluation",2007,"No","unicod font devanagari south asian languagesscript legaci text encod convers virama conjunct conson vowel diacrit legaci encod unicod graphic logic principl script south asia electron text languag south asia publish internet unicod emerg favour encod system corpus comput linguist south asian languag data web wide rang standard legaci encod paper describ difficulti inher convert text encod unicod legaci encod south asian script problemat bit font base graphic principl oppos logic principl unicod graphic font typic encod featur way high incompat unicod instanc half form glyph construct conjunct conson typic separ code point bit font unicod repres full conson virama case solut approach text convers base map rule small number generalis rule capac specialis rule captur behaviour charact font build convers algorithm encod system embed font map program output ces compliant sgml unicod program generalis text convers tool employ extens corpus build south asian languag",0
"KeywordsSpelling correction Real-word error Context-sensitive Language model ","real word error correction with trigrams correcting multiple errors in a sentence","Spelling correction is a fundamental task in text mining. In this study, we assess the real-word error correction model proposed by Mays, Damerau and Mercer and describe several drawbacks of the model. We propose a new variation which focuses on detecting and correcting multiple real-word errors in a sentence, by manipulating a probabilistic context-free grammar to discriminate between items in the search space. We test our approach on the Wall Street Journal corpus and show that it outperforms Hirst and Budanitsky’s WordNet-based method and Wilcox-O’Hearn, Hirst, and Budanitsky’s fixed windows size method.","Language Resources and Evaluation",2018,"No","spell correct real word error context sensit languag model real word error correct trigram correct multipl error sentenc spell correct fundament task text mine studi assess real word error correct model propos may damerau mercer describ drawback model propos variat focus detect correct multipl real word error sentenc manipul probabilist context free grammar discrimin item search space test approach wall street journal corpus show outperform hirst budanitski wordnet base method wilcox hearn hirst budanitski fix window size method",0
"KeywordsLexical acquisition Corpus-based statistical measures Verb semantics Multiword predicates Light verb constructions ","automatically learning semantic knowledge about multiword predicates","Highly frequent and highly polysemous verbs, such as give, take, and make, pose a challenge to automatic lexical acquisition methods. These verbs widely participate in multiword predicates (such as light verb constructions, or LVCs), in which they contribute a broad range of figurative meanings that must be recognized. Here we focus on two properties that are key to the computational treatment of LVCs. First, we consider the degree of figurativeness of the semantic contribution of such a verb to the various LVCs it participates in. Second, we explore the patterns of acceptability of LVCs, and their productivity over semantically related combinations. To assess these properties, we develop statistical measures of figurativeness and acceptability that draw on linguistic properties of LVCs. We demonstrate that these corpus-based measures correlate well with human judgments of the relevant property. We also use the acceptability measure to estimate the degree to which a semantic class of nouns can productively form LVCs with a given verb. The linguistically-motivated measures outperform a standard measure for capturing the strength of collocation of these multiword expressions.","Language Resources and Evaluation",2007,"No","lexic acquisit corpus base statist measur verb semant multiword predic light verb construct automat learn semant knowledg multiword predic high frequent high polysem verb give make pose challeng automat lexic acquisit method verb wide particip multiword predic light verb construct lvcs contribut broad rang figur mean recogn focus properti key comput treatment lvcs degre figur semant contribut verb lvcs particip explor pattern accept lvcs product semant relat combin assess properti develop statist measur figur accept draw linguist properti lvcs demonstr corpus base measur correl human judgment relev properti accept measur estim degre semant class noun product form lvcs verb linguist motiv measur outperform standard measur captur strength colloc multiword express",0
"KeywordsTreebank Dependency grammar Indo-European Greek Latin Romance Germanic Slavic Armenian ","the proiel treebank family a standard for early attestations of indo european languages","This article describes a family of dependency treebanks of early attestations of Indo-European languages originating in the parallel treebank built by the members of the project pragmatic resources in old Indo-European languages. The treebanks all share a set of open-source software tools, including a web annotation interface, and a set of annotation schemes and guidelines developed especially for the project languages. The treebanks use an enriched dependency grammar scheme complemented by detailed morphological tags, which have proved sufficient to give detailed descriptions of these richly inflected languages, and which have been easy to adapt to new languages. We describe the tools and annotation schemes and discuss some challenges posed by the various languages that have been annotated. We also discuss problems with tokenisation, sentence division and lemmatisation, commonly encountered in ancient and mediaeval texts, and challenges associated with low levels of standardisation and ongoing morphological and syntactic change.","Language Resources and Evaluation",2018,"No","treebank depend grammar indo european greek latin romanc german slavic armenian proiel treebank famili standard earli attest indo european languag articl describ famili depend treebank earli attest indo european languag origin parallel treebank built member project pragmat resourc indo european languag treebank share set open sourc softwar tool includ web annot interfac set annot scheme guidelin develop project languag treebank enrich depend grammar scheme complement detail morpholog tag prove suffici give detail descript rich inflect languag easi adapt languag describ tool annot scheme discuss challeng pose languag annot discuss problem tokenis sentenc divis lemmatis common encount ancient mediaev text challeng low level standardis ongo morpholog syntact chang",0
"KeywordsSentiment lexicon Greek language Word embeddings Sentiment analysis Natural language processing Opinion mining Emotion analysis Sarcasm detection ","building and evaluating resources for sentiment analysis in the greek language","
Sentiment lexicons and word embeddings constitute well-established sources of information for sentiment analysis in online social media. Although their effectiveness has been demonstrated in state-of-the-art sentiment analysis and related tasks in the English language, such publicly available resources are much less developed and evaluated for the Greek language. In this paper, we tackle the problems arising when analyzing text in such an under-resourced language. We present and make publicly available a rich set of such resources, ranging from a manually annotated lexicon, to semi-supervised word embedding vectors and annotated datasets for different tasks. Our experiments using different algorithms and parameters on our resources show promising results over standard baselines; on average, we achieve a 24.9% relative improvement in F-score on the cross-domain sentiment analysis task when training the same algorithms with our resources, compared to training them on more traditional feature sources, such as n-grams. Importantly, while our resources were built with the primary focus on the cross-domain sentiment analysis task, they also show promising results in related tasks, such as emotion analysis and sarcasm detection.","Language Resources and Evaluation",2018,"No","sentiment lexicon greek languag word embed sentiment analysi natur languag process opinion mine emot analysi sarcasm detect build evalu resourc sentiment analysi greek languag sentiment lexicon word embed constitut establish sourc inform sentiment analysi onlin social media effect demonstr state art sentiment analysi relat task english languag public resourc develop evalu greek languag paper tackl problem aris analyz text resourc languag present make public rich set resourc rang manual annot lexicon semi supervis word embed vector annot dataset task experi algorithm paramet resourc show promis result standard baselin averag achiev relat improv score cross domain sentiment analysi task train algorithm resourc compar train tradit featur sourc gram import resourc built primari focus cross domain sentiment analysi task show promis result relat task emot analysi sarcasm detect",0
NA,"lurl9 etude statistique des textes littraires",NA,"Computers and the Humanities",1986,"No","lurl etud statistiqu des text littrair na",0
"KeywordsComputational Linguistic ","lexicography and disambiguation the size of the problem",NA,"Computers and the Humanities",2000,"No","comput linguist lexicographi disambigu size problem na",0
"KeywordsPedagogical Approach Computational Linguistic ","a new pedagogical approach to the study of texts with a microcomputer",NA,"Computers and the Humanities",1986,"No","pedagog approach comput linguist pedagog approach studi text microcomput na",0
NA,"letter from nancy",NA,"Computers and the Humanities",1979,"No","letter nanci na",0
"authorship New York Tribune Stephen Crane stylometry ","stephen crane and the new york tribune a case study in traditional and non traditional authorship attribution","This paper describes how traditional andnon-traditional methods were used to identifyseventeen previously unknown articles that webelieve to be by Stephen Crane, published inthe New-York Tribune between 1889 and1892. The articles, printed without byline inwhat was at the time New York City's mostprestigious newspaper, report on activities ina string of summer resort towns on New Jersey'snorthern shore. Scholars had previouslyidentified fourteen shore reports as Crane's;these possible attributions more than doublethat corpus. The seventeen articles confirmhow remarkably early Stephen Crane set hisdistinctive writing style and artistic agenda. In addition, the sheer quantity of the articlesfrom the summer of 1892 reveals how vigorouslythe twenty-year-old Crane sought to establishhimself in the role of professional writer. Finally, our discovery of an article about theNew Jersey National Guard's summer encampmentreveals another way in which Crane immersedhimself in nineteenth-century military cultureand help to explain how a young man who hadnever seen a battle could write so convincinglyof war in his soon-to-come masterpiece,The Red Badge of Courage. We argue that thejoint interdisciplinary approach employed inthis paper should be the way in whichattributional research is conducted.","Computers and the Humanities",2001,"No","authorship york tribun stephen crane stylometri stephen crane york tribun case studi tradit tradit authorship attribut paper describ tradit andnon tradit method identifyseventeen previous unknown articl webeliev stephen crane publish inth york tribun and articl print bylin inwhat time york citi mostprestigi newspap report activ ina string summer resort town jerseysnorthern shore scholar previouslyidentifi fourteen shore report crane attribut doublethat corpus seventeen articl confirmhow remark earli stephen crane set hisdistinct write style artist agenda addit sheer quantiti articlesfrom summer reveal vigorouslyth twenti year crane sought establishhimself role profession writer final discoveri articl thenew jersey nation guard summer encampmentrev crane immersedhimself nineteenth centuri militari cultureand explain young man hadnev battl write convincinglyof war masterpiec red badg courag argu thejoint interdisciplinari approach employ inthi paper whichattribut research conduct",0
"KeywordsLiterature Research Computational Linguistic ","current scandinavian computer assisted language and literature research",NA,"Computers and the Humanities",1971,"No","literatur research comput linguist current scandinavian comput assist languag literatur research na",0
"Key wordstext encoding initiative TEI header header electronic documentation cataloging SGML ","the tei header and the documentation of electronic texts","The article gives a general introduction to the form and function of the TEI header, points out some of the reasoning of the Text Documentation Committee that went into its design, and discusses some of its limitations. The TEI header's major strength is that it gives encoders the ability to document the electronic text itself, its source, its encoding principles, revisions, and characteristics of the text in an interchange format. Its bibliographical descriptions can be loaded into standard remote bibliographic databases, which should make electronic texts as easy to find for researchers as texts in other media, including print. Its major weakness is that it does not yet provide the ability for retrieval across texts in a networked environment, which users may want now or in the future.","Computers and the Humanities",1995,"No","key wordstext encod initi tei header header electron document catalog sgml tei header document electron text articl general introduct form function tei header point reason text document committe design discuss limit tei header major strength encod abil document electron text sourc encod principl revis characterist text interchang format bibliograph descript load standard remot bibliograph databas make electron text easi find research text media includ print major weak provid abil retriev text network environ user futur",0
NA,"annual bibliography for 1977 and supplement to preceding years",NA,"Computers and the Humanities",1979,"No","annual bibliographi supplement preced year na",0
"KeywordsComputer System Computational Linguistic Final Design ","the dictionary of old english and the final design of its computer system",NA,"Computers and the Humanities",1985,"No","comput system comput linguist final design dictionari english final design comput system na",0
"KeywordsRoot System Computational Linguistic ","establishing a german root system by computer",NA,"Computers and the Humanities",1973,"No","root system comput linguist establish german root system comput na",0
"Key Wordsdictionary lexicography concording lemmatizing editing CD-ROM database ","unseen users unknown systems computer design for a scholars dictionary","The Dictionary of Old English computing systems have provided access since the 1970s to a database of approximately three million running words. These systems, designed for a variety of machines and written in a variety of languages, have until recently been planned with computing center billing algorithms in mind. With personal workstations emphasis has shifted to building more elegant user interfaces and to providing the entire DOE database to editors around the world. While the shift from sequential files to random access files and the provision of extensive development tools have changed some of the design process, error checking and protection of the database against accidental intrusion have remained as central issues.","Computers and the Humanities",1988,"No","key wordsdictionari lexicographi concord lemmat edit cd rom databas unseen user unknown system comput design scholar dictionari dictionari english comput system provid access s databas approxim million run word system design varieti machin written varieti languag recent plan comput center bill algorithm mind person workstat emphasi shift build eleg user interfac provid entir doe databas editor world shift sequenti file random access file provis extens develop tool chang design process error check protect databas accident intrus remain central issu",0
"Key WordsDynamic programming Indus texts Harappan civilization segmentation ","segmentation of indus texts a dynamic programming approach","The Indus valley civilization, also known as the Harappan civilization flourished from 2600 B.C. to 1750 B.C. in the Indian subcontinent. The writings of that civilization are in a pictorial type of script which has not been deciphered so far.","Computers and the Humanities",1988,"No","key wordsdynam program indus text harappan civil segment segment indus text dynam program approach indus valley civil harappan civil flourish indian subcontin write civil pictori type script deciph",0
"KeywordsComputational Linguistic Chinese Dialect Dialect Dictionary ","doc 1971 a chinese dialect dictionary on computer"," Project on Linguistic Analysis, Berkeley.","Computers and the Humanities",1972,"No","comput linguist chines dialect dialect dictionari doc chines dialect dictionari comput project linguist analysi berkeley",0
NA,"quelques rsultats dune analyse automatique du discours duplessiste",NA,"Computers and the Humanities",1986,"No","quelqu rsultat dune analys automatiqu du discour duplessist na",0
"Key WordsLatin Greek Justinian Corpus Iuris lexicography databases text archives lemmatization ","justinian lexicography","This paper describes two research projects, both involving Latin and Greek lexicography. They are undertaken at the University of Florence and at the Italian National Research Council respectively. The one involves the creation of a Dictionary of Justinian's constitutions based on the emperor's legislative lexicon formed in the Corpus Iuris and elsewhere. The most demanding aspect of this task has been the creation of the Dictionary of the Novellae. The other project involves the creation of a Lexicon of the Novellae in the Authenticum version.","Computers and the Humanities",1990,"No","key wordslatin greek justinian corpus iuri lexicographi databas text archiv lemmat justinian lexicographi paper describ research project involv latin greek lexicographi undertaken univers florenc italian nation research council involv creation dictionari justinian constitut base emperor legisl lexicon form corpus iuri demand aspect task creation dictionari novella project involv creation lexicon novella authenticum version",0
"KeywordsComputational Linguistic Semantic Description ","a shakespeare dictionary shad some preliminaries for a semantic description",NA,"Computers and the Humanities",1975,"No","comput linguist semant descript shakespear dictionari shad preliminari semant descript na",0
"Key wordscomputational lexicography natural language processing computational linguistics syntactic parsing lexicon dictionary machine tractable dictionary ","a full and efficient machine tractable dictionary for natural language processing a revised version of the cuvoald","A lexicon is an essential part of any natural language processing system. The size, content and format of the lexicon is crucial in determining the power and sophistication of a natural language processing system. However, a lexicon which provides comprehensive, consistent and accurate lexical information and which is in a format facilitating fast retrieval is not easily available. This paper reports on a project which aims at the development of such a lexicon. The resulting lexicon is actually the modified and extended version of the machine tractable version of the Oxford Advanced Learner's Dictionary. The modification and extension concentrate mainly on the aspects of comprehensiveness, consistency, explicitness, accuracy and the dictionary format. The modified and extended version is considered a desirable source of lexical information for any natural language processing system.","Computers and the Humanities",1994,"No","key wordscomput lexicographi natur languag process comput linguist syntact pars lexicon dictionari machin tractabl dictionari full effici machin tractabl dictionari natur languag process revis version cuvoald lexicon essenti part natur languag process system size content format lexicon crucial determin power sophist natur languag process system lexicon comprehens consist accur lexic inform format facilit fast retriev easili paper report project aim develop lexicon result lexicon modifi extend version machin tractabl version oxford advanc learner dictionari modif extens concentr aspect comprehens consist explicit accuraci dictionari format modifi extend version consid desir sourc lexic inform natur languag process system",0
"KeywordsComputational Linguistic Numerical Taxonomy ","a numerical taxonomy of merovingian coins",NA,"Computers and the Humanities",1978,"No","comput linguist numer taxonomi numer taxonomi merovingian coin na",0
"KeywordsRetrieval System Computational Linguistic Responsum Project ","computerized full text retrieval systems and research in the humanities the responsa project",NA,"Computers and the Humanities",1980,"No","retriev system comput linguist responsum project computer full text retriev system research human responsa project na",0
"KeywordsTense/aspect/modality Support vector machine Machine translation system On the market ","japanese to english translations of tense aspect and modality using machine learning methods and comparison with machine translation systems on market","This paper describes experiments carried out utilizing a variety of machine-learning methods (the k-nearest neighborhood, decision list, maximum entropy, and support vector machine), and using six machine-translation (MT) systems available on the market for translating tense, aspect, and modality. We found that all these, including the simple string-matching-based k-nearest neighborhood used in a previous study, obtained higher accuracy rates than the MT systems currently available on the market. We also found that the support vector machine obtained the best accuracy rates (98.8%) of these methods. Finally, we analyzed errors against the machine-learning methods and commercially available MT systems and obtained error patterns that should be useful for making future improvements.","Language Resources and Evaluation",2006,"No","tenseaspectmod support vector machin machin translat system market japanes english translat tens aspect modal machin learn method comparison machin translat system market paper describ experi carri util varieti machin learn method nearest neighborhood decis list maximum entropi support vector machin machin translat mt system market translat tens aspect modal found includ simpl string match base nearest neighborhood previous studi obtain higher accuraci rate mt system market found support vector machin obtain accuraci rate method final analyz error machin learn method commerci mt system obtain error pattern make futur improv",0
NA,"alexander mehler serge sharoff and marina santini eds genres on the web computational models and emprical studies","This comprehensive book makes many original contributions to the field of genres on the web. The identification and characterization of genres is of obvious interest to “pure” linguistics, but as this book makes clear, there are some important practical applications. Chief amongst these will be the advent of genre-aware search engines, where users will be able to specify not only their topics of interest, but the desired genre of the returned web pages, as in the WEGA search engine described in this book by Stein et al. Crowston et al. give the example of someone wishing to buy a digital camera. A traditional search engine would return pages on the topic of the specified brand of digital cameras, most of which will just be the web sites of sellers. But what the buyer really wants is information about this type of camera in certain genres only, such as product reviews and opinion-bearing blogs, which provide the opinions of people who have already bought that camera. The...","Language Resources and Evaluation",2012,"No","alexand mehler serg sharoff marina santini ed genr web comput model empric studi comprehens book make origin contribut field genr web identif character genr obvious interest pure linguist book make clear import practic applic chief advent genr awar search engin user topic interest desir genr return web page wega search engin book stein al crowston al give wish buy digit camera tradit search engin return page topic brand digit camera web site seller buyer inform type camera genr product review opinion bear blog provid opinion peopl bought camera",0
"KeywordsChinese processing Copy detection Ferret Plagiarism Word definition ","copy detection in chinese documents using ferret","The Ferret copy detector has been used since 2001 to find plagiarism in large collections of students’ coursework in English. This article reports on extending its application to Chinese, with experiments on corpora of coursework collected from two Chinese universities. Our experiments show that Ferret can find both artificially constructed plagiarism and actually occurring, previously undetected plagiarism. We discuss issues of representation, focus on the effectiveness of a sub-symbolic approach, and show that Ferret does not need to find word boundaries first.","Language Resources and Evaluation",2006,"No","chines process copi detect ferret plagiar word definit copi detect chines document ferret ferret copi detector find plagiar larg collect student coursework english articl report extend applic chines experi corpora coursework collect chines univers experi show ferret find artifici construct plagiar occur previous undetect plagiar discuss issu represent focus effect symbol approach show ferret find word boundari",0
"Keywordsemotion neuro-psychology non-verbal speech paralinguistic information speech technology ","getting to the heart of the matter speech as the expression of affect rather than just text or language","This paper addresses the current needs for so-called emotion in speech, but points out that the issue is better described as the expression of relationships and attitudes rather than the currently held raw (or big-six) emotional states. From an analysis of more than three years of daily conversational speech, we find the direct expression of emotion to be extremely rare, and contend that when speech technologists say that what we need now is more ‘emotion’ in speech, what they really mean is that the current technologies are too text-based, and that more expression of speaker attitude, affect, and discourse relationships is required.","Language Resources and Evaluation",2005,"No","emot neuro psycholog verbal speech paralinguist inform speech technolog heart matter speech express affect text languag paper address current call emot speech point issu express relationship attitud held raw big emot state analysi year daili convers speech find direct express emot extrem rare contend speech technologist emot speech current technolog text base express speaker attitud affect discours relationship requir",0
"KeywordsVerb-particle construction Multiword expression Identification ","how to pick out token instances of english verb particle constructions","We propose a method for automatically identifying individual instances of English verb-particle constructions (VPCs) in raw text. Our method employs the RASP parser and analysis of the sentential context of each VPC candidate to differentiate VPCs from simple combinations of a verb and prepositional phrase. We show that our proposed method has an F-score of 0.974 at VPC identification over the Brown Corpus and Wall Street Journal.","Language Resources and Evaluation",2010,"No","verb particl construct multiword express identif pick token instanc english verb particl construct propos method automat identifi individu instanc english verb particl construct vpcs raw text method employ rasp parser analysi sententi context vpc candid differenti vpcs simpl combin verb preposit phrase show propos method score vpc identif brown corpus wall street journal",0
"KeywordsTimeML Temporal annotation Temporal relations Information extraction Evaluation Corpus creation ","the tempeval challenge identifying temporal relations in text","TempEval is a framework for evaluating systems that automatically annotate texts with temporal relations. It was created in the context of the SemEval 2007 workshop and uses the TimeML annotation language. The evaluation consists of three subtasks of temporal annotation: anchoring an event to a time expression in the same sentence, anchoring an event to the document creation time, and ordering main events in consecutive sentences. In this paper we describe the TempEval task and the systems that participated in the evaluation. In addition, we describe how further task decomposition can bring even more structure to the evaluation of temporal relations.","Language Resources and Evaluation",2009,"No","timeml tempor annot tempor relat inform extract evalu corpus creation tempev challeng identifi tempor relat text tempev framework evalu system automat annot text tempor relat creat context semev workshop timeml annot languag evalu consist subtask tempor annot anchor event time express sentenc anchor event document creation time order main event consecut sentenc paper describ tempev task system particip evalu addit describ task decomposit bring structur evalu tempor relat",0
"KeywordsNamed entity recognition Information extraction Rule-based approach Machine learning Hybrid approach Natural language processing ","studying the impact of language independent and language specific features on hybrid arabic person name recognition","In this paper, extensive experiments are conducted to study the impact of features of different categories, in isolation and gradually in an incremental manner, on Arabic Person name recognition. We present an integrated system that employs the rule-based approach with the machine learning (ML)-based approach in order to develop a consolidated hybrid system. Our feature space is comprised of language-independent and language-specific features. The explored features are naturally grouped under six categories: Person named entity tags predicted by the rule-based component, word-level features, POS features, morphological features, gazetteer features, and other contextual features. As decision tree algorithm has proved comparatively higher efficiency as a classifier in current state-of-the-art hybrid Named Entity Recognition for Arabic, it is adopted in this study as the ML technique utilized by the hybrid system. Therefore, the experiments are focused on two dimensions: the standard dataset used and the set of selected features. A number of standard datasets are used for the training and testing of the hybrid system, including ACE (2003–2004) and ANERcorp. The experimental analysis indicates that both language-independent and language-specific features play an important role in overcoming the challenges posed by Arabic language and have demonstrated critical impact on optimizing the performance of the hybrid system.","Language Resources and Evaluation",2017,"No","name entiti recognit inform extract rule base approach machin learn hybrid approach natur languag process studi impact languag independ languag specif featur hybrid arab person recognit paper extens experi conduct studi impact featur categori isol gradual increment manner arab person recognit present integr system employ rule base approach machin learn ml base approach order develop consolid hybrid system featur space compris languag independ languag specif featur explor featur natur group categori person name entiti tag predict rule base compon word level featur pos featur morpholog featur gazett featur contextu featur decis tree algorithm prove compar higher effici classifi current state art hybrid name entiti recognit arab adopt studi ml techniqu util hybrid system experi focus dimens standard dataset set select featur number standard dataset train test hybrid system includ ace anercorp experiment analysi languag independ languag specif featur play import role overcom challeng pose arab languag demonstr critic impact optim perform hybrid system",0
"KeywordsPropBank Semantic role annotation Derivational morphology Turkish Crowdsourcing Semantic role labeling ","annotation of semantic roles for the turkish proposition bank","In this work, we report large-scale semantic role annotation of arguments in the Turkish dependency treebank, and present the first comprehensive Turkish semantic role labeling (SRL) resource: Turkish Proposition Bank (PropBank). We present our annotation workflow that harnesses crowd intelligence, and discuss the procedures for ensuring annotation consistency and quality control. Our discussion focuses on syntactic variations in realization of predicate-argument structures, and the large lexicon problem caused by complex derivational morphology. We describe our approach that exploits framesets of root verbs to abstract away from syntax and increase self-consistency of the Turkish PropBank. The issues that arise in the annotation of verbs derived via valency changing morphemes, verbal nominals, and nominal verbs are explored, and evaluation results for inter-annotator agreement are provided. Furthermore, semantic layer described here is aligned with universal dependency (UD) compliant treebank and released to enable more researchers to work on the problem. Finally, we use PropBank to establish a baseline score of 79.10 F1 for Turkish SRL using the mate-tool (an open-source SRL tool based on supervised machine learning) enhanced with basic morphological features. Turkish PropBank and the extended SRL system are made publicly available.
","Language Resources and Evaluation",2018,"No","propbank semant role annot deriv morpholog turkish crowdsourc semant role label annot semant role turkish proposit bank work report larg scale semant role annot argument turkish depend treebank present comprehens turkish semant role label srl resourc turkish proposit bank propbank present annot workflow har crowd intellig discuss procedur ensur annot consist qualiti control discuss focus syntact variat realiz predic argument structur larg lexicon problem caus complex deriv morpholog describ approach exploit frameset root verb abstract syntax increas consist turkish propbank issu aris annot verb deriv valenc chang morphem verbal nomin nomin verb explor evalu result inter annot agreement provid semant layer align univers depend ud compliant treebank releas enabl research work problem final propbank establish baselin score f turkish srl mate tool open sourc srl tool base supervis machin learn enhanc basic morpholog featur turkish propbank extend srl system made public",0
"KeywordsText classification Language identification Microblogs Content analysis ","microblog language identification overcoming the limitations of short unedited and idiomatic text","Multilingual posts can potentially affect the outcomes of content analysis on microblog platforms. To this end, language identification can provide a monolingual set of content for analysis. We find the unedited and idiomatic language of microblogs to be challenging for state-of-the-art language identification methods. To account for this, we identify five microblog characteristics that can help in language identification: the language profile of the blogger (blogger), the content of an attached hyperlink (link), the language profile of other users mentioned (mention) in the post, the language profile of a tag (tag), and the language of the original post (conversation), if the post we examine is a reply. Further, we present methods that combine these priors in a post-dependent and post-independent way. We present test results on 1,000 posts from five languages (Dutch, English, French, German, and Spanish), which show that our priors improve accuracy by 5 % over a domain specific baseline, and show that post-dependent combination of the priors achieves the best performance. When suitable training data does not exist, our methods still outperform a domain unspecific baseline. We conclude with an examination of the language distribution of a million tweets, along with temporal analysis, the usage of twitter features across languages, and a correlation study between classifications made and geo-location and language metadata fields.","Language Resources and Evaluation",2013,"No","text classif languag identif microblog content analysi microblog languag identif overcom limit short unedit idiomat text multilingu post potenti affect outcom content analysi microblog platform end languag identif provid monolingu set content analysi find unedit idiomat languag microblog challeng state art languag identif method account identifi microblog characterist languag identif languag profil blogger blogger content attach hyperlink link languag profil user mention mention post languag profil tag tag languag origin post convers post examin repli present method combin prior post depend post independ present test result post languag dutch english french german spanish show prior improv accuraci domain specif baselin show post depend combin prior achiev perform suitabl train data exist method outperform domain unspecif baselin conclud examin languag distribut million tweet tempor analysi usag twitter featur languag correl studi classif made geo locat languag metadata field",0
"KeywordsDependency structure Syntactico–semantic relation  Paninian karak Modern Bangla grammar and language ","a dependency annotation scheme for bangla treebank","Dependency grammar is considered appropriate for many Indian languages. In this paper, we present a study of the dependency relations in Bangla language. We have categorized these relations in three different levels, namely intrachunk relations, interchunk relations and interclause relations. Each of these levels is further categorized and an annotation scheme has been developed. Both syntactic and semantic features have been taken into consideration for describing the relations. In our scheme, there are 63 such syntactico–semantic relations. We have verified the scheme by tagging a corpus of 4167 Bangla sentences to create a treebank (KGPBenTreebank).","Language Resources and Evaluation",2014,"No","depend structur syntactico semant relat paninian karak modern bangla grammar languag depend annot scheme bangla treebank depend grammar consid indian languag paper present studi depend relat bangla languag categor relat level intrachunk relat interchunk relat interclaus relat level categor annot scheme develop syntact semant featur consider describ relat scheme syntactico semant relat verifi scheme tag corpus bangla sentenc creat treebank kgpbentreebank",0
"KeywordsCollocation extraction Evaluation Hybrid methods Multilingual issues Syntactic parsing ","multilingual collocation extraction with a syntactic parser","An impressive amount of work was devoted over the past few decades to collocation extraction. The state of the art shows that there is a sustained interest in the morphosyntactic preprocessing of texts in order to better identify candidate expressions; however, the treatment performed is, in most cases, limited (lemmatization, POS-tagging, or shallow parsing). This article presents a collocation extraction system based on the full parsing of source corpora, which supports four languages: English, French, Spanish, and Italian. The performance of the system is compared against that of the standard mobile-window method. The evaluation experiment investigates several levels of the significance lists, uses a fine-grained annotation schema, and covers all the languages supported. Consistent results were obtained for these languages: parsing, even if imperfect, leads to a significant improvement in the quality of results, in terms of collocational precision (between 16.4 and 29.7%, depending on the language; 20.1% overall), MWE precision (between 19.9 and 35.8%; 26.1% overall), and grammatical precision (between 47.3 and 67.4%; 55.6% overall). This positive result bears a high importance, especially in the perspective of the subsequent integration of extraction results in other NLP applications.","Language Resources and Evaluation",2009,"No","colloc extract evalu hybrid method multilingu issu syntact pars multilingu colloc extract syntact parser impress amount work devot past decad colloc extract state art show sustain interest morphosyntact preprocess text order identifi candid express treatment perform case limit lemmat pos tag shallow pars articl present colloc extract system base full pars sourc corpora support languag english french spanish italian perform system compar standard mobil window method evalu experi investig level signific list fine grain annot schema cover languag support consist result obtain languag pars imperfect lead signific improv qualiti result term colloc precis depend languag mwe precis grammat precis posit result bear high import perspect subsequ integr extract result nlp applic",0
"KeywordsIrony detection Figurative language processing Negation Web text analysis ","a multidimensional approach for detecting irony in twitter","Irony is a pervasive aspect of many online texts, one made all the more difficult by the absence of face-to-face contact and vocal intonation. As our media increasingly become more social, the problem of irony detection will become even more pressing. We describe here a set of textual features for recognizing irony at a linguistic level, especially in short texts created via social media such as Twitter postings or “tweets”. Our experiments concern four freely available data sets that were retrieved from Twitter using content words (e.g. “Toyota”) and user-generated tags (e.g. “#irony”). We construct a new model of irony detection that is assessed along two dimensions: representativeness and relevance. Initial results are largely positive, and provide valuable insights into the figurative issues facing tasks such as sentiment analysis, assessment of online reputations, or decision making.","Language Resources and Evaluation",2013,"No","ironi detect figur languag process negat web text analysi multidimension approach detect ironi twitter ironi pervas aspect onlin text made difficult absenc face face contact vocal inton media increas social problem ironi detect press describ set textual featur recogn ironi linguist level short text creat social media twitter post tweet experi concern freeli data set retriev twitter content word toyota user generat tag ironi construct model ironi detect assess dimens repres relev initi result larg posit provid valuabl insight figur issu face task sentiment analysi assess onlin reput decis make",0
"KeywordsVietnamese treebank Quality control Consistent annotation Linguistic challenges ","ensuring annotation consistency and accuracy for vietnamese treebank","Treebanks are important resources for researchers in natural language processing. They provide training and testing materials so that different algorithms can be compared. However, it is not a trivial task to construct high-quality treebanks. We have not yet had a proper treebank for such a low-resource language as Vietnamese, which has probably lowered the performance of Vietnamese language processing. We have been building a consistent and accurate Vietnamese treebank to alleviate such situations. Our treebank is annotated with three layers: word segmentation, part-of-speech tagging, and bracketing. We developed detailed annotation guidelines for each layer by presenting Vietnamese linguistic issues as well as methods of addressing them. Here, we also describe approaches to controlling annotation quality while ensuring a reasonable annotation speed. We specifically designed an appropriate annotation process and an effective process to train annotators. In addition, we implemented several support tools to improve annotation speed and to control the consistency of the treebank. The results from experiments revealed that both inter-annotator agreement and accuracy were higher than 90%, which indicated that the treebank is reliable.
","Language Resources and Evaluation",2018,"No","vietnames treebank qualiti control consist annot linguist challeng ensur annot consist accuraci vietnames treebank treebank import resourc research natur languag process provid train test materi algorithm compar trivial task construct high qualiti treebank proper treebank low resourc languag vietnames lower perform vietnames languag process build consist accur vietnames treebank allevi situat treebank annot layer word segment part speech tag bracket develop detail annot guidelin layer present vietnames linguist issu method address describ approach control annot qualiti ensur reason annot speed specif design annot process effect process train annot addit implement support tool improv annot speed control consist treebank result experi reveal inter annot agreement accuraci higher treebank reliabl",0
"KeywordsComputational Linguistic ","contemporary literary lexicology and terminology an inventory",NA,"Computers and the Humanities",1986,"No","comput linguist contemporari literari lexicolog terminolog inventori na",0
"KeywordsComputational Linguistic Mutual Benefit Recent Conference ","recent conferences in italy","The conferences were very different in nature, each being valuable in its own way. The first brought together many people from all parts of the world making it possible to gain an overall view on the state of the art in the growing field of computational linguistics, and provided a forum for the interchange of ideas, and also enabled those who are new to such research to benefit from communication with those who are acknowledged leaders in the field. The LIE colloquium was more akin to a working group of experts who had been invited to participate in the LIE project to ensure the end product is of the highest possible standard by consultation and planning of a type that can only be achieved under such “interactive” conditions. Communication was two-way to the mutual benefit of all who attended.","Computers and the Humanities",1974,"No","comput linguist mutual benefit recent confer recent confer itali confer natur valuabl brought peopl part world make gain view state art grow field comput linguist provid forum interchang idea enabl research benefit communic acknowledg leader field lie colloquium akin work group expert invit particip lie project ensur end product highest standard consult plan type achiev interact condit communic mutual benefit attend",0
"KeywordsCluster Analysis Computational Linguistic ","cluster analysis for the computer assisted statistical analysis of melodies",NA,"Computers and the Humanities",1986,"No","cluster analysi comput linguist cluster analysi comput assist statist analysi melodi na",0
"Key WordsChinese language sinological research Mandarin Chinese text encoding non-Roman characters lexical distributions lexicology ","lexical distribution in the guomin xiaoxue guoyu a computer assisted analysis of morpholexical elements","The research described in this paper was originally presented as a dissertation at the Università di Venezia. The objective was to establish the kind of language to which Chinese students are exposed during their primary education. The analysis was based on twelve texts of Guomin Xiaoxue Guoyu. This manuscript concentrates mainly on the techniques used to encode Chinese characters.","Computers and the Humanities",1990,"No","key wordschines languag sinolog research mandarin chines text encod roman charact lexic distribut lexicolog lexic distribut guomin xiaoxu guoyu comput assist analysi morpholex element research paper origin present dissert universit di venezia object establish kind languag chines student expos primari educ analysi base twelv text guomin xiaoxu guoyu manuscript concentr techniqu encod chines charact",0
"word sense disambiguation information filtering SENSEVAL ","word sense disambiguation by information filtering and extraction","We describe a simple approach to word sensedisambiguation using information filtering andextraction. The method fully exploits and extends theinformation available in the Hector dictionary. Thealgorithm proceeds by the application of severalfilters to prune the candidate set of word sensesreturning the most frequent if more than one remains.The experimental methodology and its implication arealso discussed.","Computers and the Humanities",2000,"No","word sens disambigu inform filter sensev word sens disambigu inform filter extract describ simpl approach word sensedisambigu inform filter andextract method fulli exploit extend theinform hector dictionari thealgorithm proceed applic severalfilt prune candid set word sensesreturn frequent remain experiment methodolog implic arealso discuss",0
"Keywordscombination mapping WordNet word sense disambiguation ","multiple heuristics and their combination for automatic wordnet mapping","This paper presents an automatic construction of Korean WordNet from pre-existing lexical resources. We develop a set of automatic word sense disambiguation techniques to link a Korean word sense collected from a bilingual machine-readable dictionary to a single corresponding English WordNet synset. We show how individual links provided by each word sense disambiguation method can be non-linearly combined to produce a Korean WordNet from existing English WordNet for nouns.","Computers and the Humanities",2004,"No","combin map wordnet word sens disambigu multipl heurist combin automat wordnet map paper present automat construct korean wordnet pre exist lexic resourc develop set automat word sens disambigu techniqu link korean word sens collect bilingu machin readabl dictionari singl english wordnet synset show individu link provid word sens disambigu method linear combin produc korean wordnet exist english wordnet noun",0
"KeywordsComputational Linguistic ","some of my best friends are linguists",NA,"Language Resources and Evaluation",2005,"No","comput linguist friend linguist na",0
"KeywordsNatural Language Processing Natural Language Processing Application Multilingual Context Multiword Expression Natural Language Processing Community ","multiword expressions hard going or plain sailing","Over the past two decades or so, Multi-Word Expressions (MWEs; also called Multi-word Units) have been an increasingly important concern for Computational Linguistics and Natural Language Processing (NLP). The term MWE has been used to refer to various types of linguistic units and expressions, including idioms, noun compounds, phrasal verbs, light verbs and other habitual collocations. However, while there is no universally agreed definition for MWE as yet, most researchers use the term to refer to those frequently occurring phrasal units which are subject to certain level of semantic opaqueness, or non-compositionality. Non-compositional MWEs pose tough challenges for automatic analysis because their interpretation cannot be achieved by directly combining the semantics of their constituents, thereby causing the “pain in the neck of NLP” (Sag et al. 2001).","Language Resources and Evaluation",2010,"No","natur languag process natur languag process applic multilingu context multiword express natur languag process communiti multiword express hard plain sail past decad multi word express mwes call multi word unit increas import concern comput linguist natur languag process nlp term mwe refer type linguist unit express includ idiom noun compound phrasal verb light verb habitu colloc univers agre definit mwe research term refer frequent occur phrasal unit subject level semant opaqu composit composit mwes pose tough challeng automat analysi interpret achiev direct combin semant constitu caus pain neck nlp sag al",0
"KeywordsSlang words Sentiment lexicon Social media Sentiment classification ","slangsd building expanding and using a sentiment dictionary of slang words for short text sentiment classification","Sentiment information about social media posts is increasingly considered an important resource for customer segmentation, market understanding, and tackling other socio-economic issues.
 However, sentiment in social media is difficult to measure since user-generated content is usually short and informal. Although many traditional sentiment analysis methods have been proposed, identifying slang sentiment words remains a challenging task for practitioners. Though some slang words are available in existing sentiment lexicons, with new slang being generated with emerging memes, a dedicated lexicon will be useful for researchers and practitioners. To this end, we propose to build a slang sentiment dictionary to aid sentiment analysis.
 It is laborious and time-consuming to collect a comprehensive list of slang words and label the sentiment polarity. We present an approach to leverage web resources to construct a Slang Sentiment Dictionary (SlangSD) that is easy to expand. SlangSD is publicly available for research purposes. We empirically show the advantages of using SlangSD, the newly-built slang sentiment word dictionary for sentiment classification, and provide examples demonstrating its ease of use with a sentiment analysis system.","Language Resources and Evaluation",2018,"No","slang word sentiment lexicon social media sentiment classif slangsd build expand sentiment dictionari slang word short text sentiment classif sentiment inform social media post increas consid import resourc custom segment market understand tackl socio econom issu sentiment social media difficult measur user generat content short inform tradit sentiment analysi method propos identifi slang sentiment word remain challeng task practition slang word exist sentiment lexicon slang generat emerg meme dedic lexicon research practition end propos build slang sentiment dictionari aid sentiment analysi labori time consum collect comprehens list slang word label sentiment polar present approach leverag web resourc construct slang sentiment dictionari slangsd easi expand slangsd public research purpos empir show advantag slangsd newli built slang sentiment word dictionari sentiment classif provid exampl demonstr eas sentiment analysi system",0
"KeywordsWikipedia Infobox Attributes Temporal data ","whad wikipedia historical attributes data","This paper describes the generation of temporally anchored infobox attribute data from the Wikipedia history of revisions. By mining (attribute, value) pairs from the revision history of the English Wikipedia we are able to collect a comprehensive knowledge base that contains data on how attributes change over time. When dealing with the Wikipedia edit history, vandalic and erroneous edits are a concern for data quality. We present a study of vandalism identification in Wikipedia edits that uses only features from the infoboxes, and show that we can obtain, on this dataset, an accuracy comparable to a state-of-the-art vandalism identification method that is based on the whole article. Finally, we discuss different characteristics of the extracted dataset, which we make available for further study.","Language Resources and Evaluation",2013,"No","wikipedia infobox attribut tempor data whad wikipedia histor attribut data paper describ generat tempor anchor infobox attribut data wikipedia histori revis mine attribut pair revis histori english wikipedia collect comprehens knowledg base data attribut chang time deal wikipedia edit histori vandal erron edit concern data qualiti present studi vandal identif wikipedia edit featur infobox show obtain dataset accuraci compar state art vandal identif method base articl final discuss characterist extract dataset make studi",0
"KeywordsCoreference resolution and evaluation NLP system analysis Machine learning based NLP tools SemEval-2010 (Task 1) Discourse entities ","coreference resolution an empirical study based on semeval 2010 shared task 1","This paper presents an empirical evaluation of coreference resolution that covers several interrelated dimensions. The main goal is to complete the comparative analysis from the SemEval-2010 task on Coreference Resolution in Multiple Languages. To do so, the study restricts the number of languages and systems involved, but extends and deepens the analysis of the system outputs, including a more qualitative discussion. The paper compares three automatic coreference resolution systems for three languages (English, Catalan and Spanish) in four evaluation settings, and using four evaluation measures. Given that our main goal is not to provide a comparison between resolution algorithms, these are merely used as tools to shed light on the different conditions under which coreference resolution is evaluated. Although the dimensions are strongly interdependent, making it very difficult to extract general principles, the study reveals a series of interesting issues in relation to coreference resolution: the portability of systems across languages, the influence of the type and quality of input annotations, and the behavior of the scoring measures.","Language Resources and Evaluation",2013,"No","corefer resolut evalu nlp system analysi machin learn base nlp tool semev task discours entiti corefer resolut empir studi base semev share task paper present empir evalu corefer resolut cover interrel dimens main goal complet compar analysi semev task corefer resolut multipl languag studi restrict number languag system involv extend deepen analysi system output includ qualit discuss paper compar automat corefer resolut system languag english catalan spanish evalu set evalu measur main goal provid comparison resolut algorithm tool shed light condit corefer resolut evalu dimens strong interdepend make difficult extract general principl studi reveal seri interest issu relat corefer resolut portabl system languag influenc type qualiti input annot behavior score measur",0
"KeywordsWord sense disambiguation Knowledge sources Inductive logic programming ","assessing the contribution of shallow and deep knowledge sources for word sense disambiguation","Corpus-based techniques have proved to be very beneficial in the development of efficient and accurate approaches to word sense disambiguation (WSD) despite the fact that they generally represent relatively shallow knowledge. It has always been thought, however, that WSD could also benefit from deeper knowledge sources. We describe a novel approach to WSD using inductive logic programming to learn theories from first-order logic representations that allows corpus-based evidence to be combined with any kind of background knowledge. This approach has been shown to be effective over several disambiguation tasks using a combination of deep and shallow knowledge sources. Is it important to understand the contribution of the various knowledge sources used in such a system. This paper investigates the contribution of nine knowledge sources to the performance of the disambiguation models produced for the SemEval-2007 English lexical sample task. The outcome of this analysis will assist future work on WSD in concentrating on the most useful knowledge sources.","Language Resources and Evaluation",2010,"No","word sens disambigu knowledg sourc induct logic program assess contribut shallow deep knowledg sourc word sens disambigu corpus base techniqu prove benefici develop effici accur approach word sens disambigu wsd fact general repres shallow knowledg thought wsd benefit deeper knowledg sourc describ approach wsd induct logic program learn theori order logic represent corpus base evid combin kind background knowledg approach shown effect disambigu task combin deep shallow knowledg sourc import understand contribut knowledg sourc system paper investig contribut knowledg sourc perform disambigu model produc semev english lexic sampl task outcom analysi assist futur work wsd concentr knowledg sourc",0
"Keywordsevaluation architecture MT architecture Machine Translation ","hybrid architectures for machine translation systems","Although some progress has been made on the quality of Machine Translation in recent years, there is still a significant potential for quality improvement. There has also been a shift in paradigm of machine translation, from “classical” rule-based systems like METAL or LMT1 towards example-based or statistical MT.2 It seems to be time now to evaluate the progress and compare the results of these efforts, and draw conclusions for further improvements of MT quality.","Language Resources and Evaluation",2005,"No","evalu architectur mt architectur machin translat hybrid architectur machin translat system progress made qualiti machin translat recent year signific potenti qualiti improv shift paradigm machin translat classic rule base system metal lmt base statist mt time evalu progress compar result effort draw conclus improv mt qualiti",0
"KeywordsWord sense disambiguation Sense granularity Maximum entropy Linguistically motivated features Linear regression ","improving english verb sense disambiguation performance with linguistically motivated features and clear sense distinction boundaries","This paper presents a high-performance broad-coverage supervised word sense disambiguation (WSD) system for English verbs that uses linguistically motivated features and a smoothed maximum entropy machine learning model. We describe three specific enhancements to our system’s treatment of linguistically motivated features which resulted in the best published results on SENSEVAL-2 verbs. We then present the results of training our system on OntoNotes data, both the SemEval-2007 task and additional data. OntoNotes data is designed to provide clear sense distinctions, based on using explicit syntactic and semantic criteria to group WordNet senses, with sufficient examples to constitute high quality, broad coverage training data. Using similar syntactic and semantic features for WSD, we achieve performance comparable to that of human taggers, and competitive with the top results for the SemEval-2007 task. Empirical analysis of our results suggests that clarifying sense boundaries and/or increasing the number of training instances for certain verbs could further improve system performance.","Language Resources and Evaluation",2009,"No","word sens disambigu sens granular maximum entropi linguist motiv featur linear regress improv english verb sens disambigu perform linguist motiv featur clear sens distinct boundari paper present high perform broad coverag supervis word sens disambigu wsd system english verb linguist motiv featur smooth maximum entropi machin learn model describ specif enhanc system treatment linguist motiv featur result publish result sensev verb present result train system ontonot data semev task addit data ontonot data design provid clear sens distinct base explicit syntact semant criteria group wordnet sens suffici exampl constitut high qualiti broad coverag train data similar syntact semant featur wsd achiev perform compar human tagger competit top result semev task empir analysi result suggest clarifi sens boundari increas number train instanc verb improv system perform",0
"KeywordsComputational paralinguistics Affective computing Political speech Machine learning Charisma Speaker ability ","a longitudinal database of irish political speech with annotations of speaker ability","This paper presents the Irish Political Speech Database, an English-language database collected from Irish political recordings. The database is collected with automated indexing and content retrieval in mind, and thus is gathered from real-world recordings (such as television interviews and election rallies) which represent the nature and quality of recordings which will be encountered in practical applications. The database is labelled for six speaker attributes: boring; charismatic; enthusiastic; inspiring; likeable; and persuasive. Each of these traits is linked to the perceived ability or appeal of the speaker, and as such are relevant to a range of content retrieval and speech analysis tasks. The six base attributes are combined to form a metric of Overall Speaker Appeal. A set of baseline experiments is presented, which demonstrate the potential of this database for affective computing studies. Classification accuracies of up to 76% are achieved, with little feature or system optimisation.","Language Resources and Evaluation",2018,"No","comput paralinguist affect comput polit speech machin learn charisma speaker abil longitudin databas irish polit speech annot speaker abil paper present irish polit speech databas english languag databas collect irish polit record databas collect autom index content retriev mind gather real world record televis interview elect ralli repres natur qualiti record encount practic applic databas label speaker attribut bore charismat enthusiast inspir likeabl persuas trait link perceiv abil appeal speaker relev rang content retriev speech analysi task base attribut combin form metric speaker appeal set baselin experi present demonstr potenti databas affect comput studi classif accuraci achiev featur system optimis",0
"KeywordsWeakly supervised learning POS tagging Cross-lingual transfer ","reassessing the value of resources for cross lingual transfer of pos tagging models","When linguistically annotated data is scarce, as is the case for many under-resourced languages, one has to resort to less complete forms of annotations obtained from crawled dictionaries and/or through cross-lingual transfer. Several recent works have shown that learning from such partially supervised data can be effective in many practical situations. In this work, we review two existing proposals for learning with ambiguous labels which extend conventional learners to the weakly supervised setting: a history-based model using a variant of the perceptron, on the one hand; an extension of the Conditional Random Fields model on the other hand. Focusing on the part-of-speech tagging task, but considering a large set of ten languages, we show (a) that good performance can be achieved even in the presence of ambiguity, provided however that both monolingual and bilingual resources are available; (b) that our two learners exploit different characteristics of the training set, and are successful in different situations; (c) that in addition to the choice of an adequate learning algorithm, many other factors are critical for achieving good performance in a cross-lingual transfer setting.","Language Resources and Evaluation",2017,"No","weak supervis learn pos tag cross lingual transfer reassess resourc cross lingual transfer pos tag model linguist annot data scarc case resourc languag resort complet form annot obtain crawl dictionari cross lingual transfer recent work shown learn partial supervis data effect practic situat work review exist propos learn ambigu label extend convent learner weak supervis set histori base model variant perceptron hand extens condit random field model hand focus part speech tag task larg set ten languag show good perform achiev presenc ambigu provid monolingu bilingu resourc learner exploit characterist train set success situat addit choic adequ learn algorithm factor critic achiev good perform cross lingual transfer set",0
"Keywordslinguistic features self-organized document maps semantic space SENSEVAL-2 word sense disambiguation ","evaluation of linguistic features for word sense disambiguation with self organized document maps","Word sense disambiguation automatically determines the appropriate senses of a word in context. We have previously shown that self-organized document maps have properties similar to a large-scale semantic structure that is useful for word sense disambiguation. This work evaluates the impact of different linguistic features on self-organized document maps for word sense disambiguation. The features evaluated are various qualitative features, e.g. part-of-speech and syntactic labels, and quantitative features, e.g. cut-off levels for word frequency. It is shown that linguistic features help make contextual information explicit. If the training corpus is large even contextually weak features, such as base forms, will act in concert to produce sense distinctions in a statistically significant way. However, the most important features are syntactic dependency relations and base forms annotated with part of speech or syntactic labels. We achieve 62.9% ± 0.73% correct results on the fine grained lexical task of the English SENSEVAL-2 data. On the 96.7% of the test cases which need no back-off to the most frequent sense we achieve 65.7% correct results.","Computers and the Humanities",2004,"No","linguist featur organ document map semant space sensev word sens disambigu evalu linguist featur word sens disambigu organ document map word sens disambigu automat determin sens word context previous shown organ document map properti similar larg scale semant structur word sens disambigu work evalu impact linguist featur organ document map word sens disambigu featur evalu qualit featur part speech syntact label quantit featur cut level word frequenc shown linguist featur make contextu inform explicit train corpus larg contextu weak featur base form act concert produc sens distinct statist signific import featur syntact depend relat base form annot part speech syntact label achiev correct result fine grain lexic task english sensev data test case back frequent sens achiev correct result",0
"KeywordsNormalization of short texts Statistical machine translation Automatic extraction of rules Perplexity ","automatic normalization of short texts by combining statistical and rule based techniques","Short texts are typically composed of small number of words, most of which are abbreviations, typos and other kinds of noise. This makes the noise to signal ratio relatively high for this specific category of text. A high proportion of noise in the data is undesirable for analysis procedures as well as machine learning applications. Text normalization techniques are used to reduce the noise and improve the quality of text for processing and analysis purposes. In this work, we propose a combination of statistical and rule-based techniques to normalize short texts. More specifically, we focus our attention on SMS messages. We base our normalization approach on a statistical machine translation system which translates from noisy data to clean data. This system is trained on a small manually annotated set. Then, we study several automatic methods to extract more general rules from the normalizations generated with the statistical machine translation system. We illustrate the proposed methodology by conducting some experiments with a SMS Haitian-Créole data collection. In order to evaluate the performance of our methodology we use several Haitian-Créole dictionaries, the well-known perplexity criteria and the achieved reduction of vocabulary.","Language Resources and Evaluation",2013,"No","normal short text statist machin translat automat extract rule perplex automat normal short text combin statist rule base techniqu short text typic compos small number word abbrevi typo kind nois make nois signal ratio high specif categori text high proport nois data undesir analysi procedur machin learn applic text normal techniqu reduc nois improv qualiti text process analysi purpos work propos combin statist rule base techniqu normal short text specif focus attent sms messag base normal approach statist machin translat system translat noisi data clean data system train small manual annot set studi automat method extract general rule normal generat statist machin translat system illustr propos methodolog conduct experi sms haitian cr ole data collect order evalu perform methodolog haitian cr ole dictionari perplex criteria achiev reduct vocabulari",0
"KeywordsComputational Linguistic ","letter to the editor",NA,"Computers and the Humanities",1977,"No","comput linguist letter editor na",0
NA,"call for papers",NA,"Computers and the Humanities",1990,"No","call paper na",0
"Senseval statistical WSD word sense disambiguation ","simple word sense discrimination","Wisdom is a system for performing word sense disambiguation (WSD)using a limited number of linguistic features and a simplesupervised learning algorithm. The most likely sense tag for aword is determined by calculating co-occurrence statistics forwords appearing within a small window. This paper gives abrief description of the components in the Wisdom system and thealgorithm used to predict the correct sense tag. Some results forWisdom from the Senseval competition are presented, and directionsfor future work are also explored.","Computers and the Humanities",2000,"No","sensev statist wsd word sens disambigu simpl word sens discrimin wisdom system perform word sens disambigu wsd limit number linguist featur simplesupervis learn algorithm sens tag aword determin calcul occurr statist forword appear small window paper abrief descript compon wisdom system thealgorithm predict correct sens tag result forwisdom sensev competit present directionsfor futur work explor",0
NA,"construction dun prototype de systme expert dans le domaine historique",NA,"Computers and the Humanities",1986,"No","construct dun prototyp de systm expert dan le domain historiqu na",0
"dictionary entry Korean markup SGML TEI ","modifying the tei dtd the case of korean dictionaries","Dictionary markup is one of the concerns of the Text Encoding Initiative (TEI), an international project for text encoding. In this paper, we investigate ways to use and extend the TEI encoding scheme for the markup of Korean dictionary entries. Since TEI suggestions for dictionary markup are mainly for western language dictionaries, we need to cope with problems to be encountered in encoding Korean dictionary entries. We try to extend and modify the TEI encoding scheme in the way suggested by the TEI. Also, we restrict the content model so that the encoded dictionary might be viewed as a database as well as a computerized, originally printed, dictionary.","Computers and the Humanities",1997,"No","dictionari entri korean markup sgml tei modifi tei dtd case korean dictionari dictionari markup concern text encod initi tei intern project text encod paper investig way extend tei encod scheme markup korean dictionari entri tei suggest dictionari markup western languag dictionari cope problem encount encod korean dictionari entri extend modifi tei encod scheme suggest tei restrict content model encod dictionari view databas computer origin print dictionari",0
"KeywordsPrincipal Component Analysis Computational Linguistic ","an application of principal component analysis to the works of molire",NA,"Computers and the Humanities",1973,"No","princip compon analysi comput linguist applic princip compon analysi work molir na",0
"KeywordsComputational Linguistic ","abstracts and brief reports",NA,"Computers and the Humanities",1981,"No","comput linguist abstract report na",0
"KeywordsComputational Linguistic ","computational linguistics in denmark a report on the third aila congress",NA,"Computers and the Humanities",1973,"No","comput linguist comput linguist denmark report aila congress na",0
"KeywordsComputational Linguistic ","computers and philosophical lexicography the activities of the lessico intellettuale europeo",NA,"Computers and the Humanities",1982,"No","comput linguist comput philosoph lexicographi activ lessico intellettual europeo na",0
"KeywordsComputational Linguistic Oral Poetry ","thought clusters in early greek oral poetry",NA,"Computers and the Humanities",1974,"No","comput linguist oral poetri thought cluster earli greek oral poetri na",0
"KeywordsComputational Linguistic ","sasanian pahlavi inscriptions a concordance",NA,"Computers and the Humanities",1974,"No","comput linguist sasanian pahlavi inscript concord na",0
"KeywordsComputational Linguistic ","the berkeley late egyptian dictionary",NA,"Computers and the Humanities",1977,"No","comput linguist berkeley late egyptian dictionari na",0
"KeywordsVerse Computational Linguistic High Coefficient Strong Similarity Conceptual Level ","phonological patterning in german verse","While numerous tentative conclusions might be drawn from the data presented here, the following generalizations are consistently supported by the results of the Phonological Frames Program:","Computers and the Humanities",1976,"No","vers comput linguist high coeffici strong similar conceptu level phonolog pattern german vers numer tentat conclus drawn data present general consist support result phonolog frame program",0
"KeywordsComputational Linguistic ","from co occurrences to concepts",NA,"Computers and the Humanities",1977,"No","comput linguist occurr concept na",0
"KeywordsComputational Linguistic Short Context ","disambiguation by short contexts",NA,"Computers and the Humanities",1985,"No","comput linguist short context disambigu short context na",0
"KeywordsComputational Linguistic ","literary works in machine readable form",NA,"Computers and the Humanities",1967,"No","comput linguist literari work machin readabl form na",0
"KeywordsComputational Linguistic Computer Study English Poem ","a poetic formula inbeowulf and seven other old english poems a computer study",NA,"Computers and the Humanities",1985,"No","comput linguist comput studi english poem poetic formula inbeowulf english poem comput studi na",0
"KeywordsComputational Linguistic Word Index ","automated concordances and word indexes the fifties",NA,"Computers and the Humanities",1981,"No","comput linguist word index autom concord word index fifti na",0
"KeywordsComputational Linguistic Computer Generation ","computer generation of melodies further proposals",NA,"Computers and the Humanities",1983,"No","comput linguist comput generat comput generat melodi propos na",0
"information theory KL-distance language change linguistic distance mathematics of language ","the time course of language change","This paper presents a numeric and information theoretic model for themeasuring of language change, without specifying the particular type ofchange. It is shown that this measurement is intuitively plausibleand that meaningful measurements canbe made from as few as 1000 characters. This measurement techniqueis extended to the task of determining the ``rate'' of language changebased on an examination of brief excerpts from the NationalGeographic Magazine and determining both their linguistic distancefrom one another as well as the number of years of temporal separation.A statistical analysis of these results shows, first, that language changecan be measured, and second, that the rate of languagechange has not been uniform, and that in particular, the period 1939-;1948had particularly slow change, while 1949-;1958 and 1959-;1968 hadparticularly rapid changes.","Computers and the Humanities",2003,"No","inform theori kl distanc languag chang linguist distanc mathemat languag time languag chang paper present numer inform theoret model themeasur languag chang type ofchang shown measur intuit plausibleand meaning measur canb made charact measur techniquei extend task determin rate languag changebas examin excerpt nationalgeograph magazin determin linguist distancefrom number year tempor separ statist analysi result show languag changecan measur rate languagechang uniform period had slow chang hadparticular rapid",0
"Key wordsTEI electronic texts text encoding encoding standards SGML tagging ","the tei history goals and future","This paper traces the history of the Text Encoding Initiative, through the Vassar Conference and the Poughkeepsie Principles to the publication, in May 1994, of theGuidelines for the Electronic Text Encoding and Interchange. The authors explain the types of questions that were raised, the attempts made to resolve them, the TEI project's aims, the general organization of the TEI committees, and they discuss the project's future.","Computers and the Humanities",1995,"No","key wordstei electron text text encod encod standard sgml tag tei histori goal futur paper trace histori text encod initi vassar confer poughkeepsi principl public theguidelin electron text encod interchang author explain type question rais attempt made resolv tei project aim general organ tei committe discuss project futur",0
"KeywordsSemantic Relation Language Resource Lexical Unit Deduction Rule Lexical Meaning ","introduction to the special issue on wordnets and relations","Since its inception a quarter century ago, Princeton WordNet [PWN] (Miller 1995; Fellbaum 1998) has had a profound influence on research and applications in lexical semantics, computational linguistics and natural language processing. The numerous uses of this lexical resource have motivated the building of wordnets1 in several dozen languages, including even a “dead” language, Latin. This special issue looks at certain aspects of wordnet construction and organisation.","Language Resources and Evaluation",2013,"No","semant relat languag resourc lexic unit deduct rule lexic mean introduct special issu wordnet relat incept quarter centuri ago princeton wordnet pwn miller fellbaum profound influenc research applic lexic semant comput linguist natur languag process numer lexic resourc motiv build wordnet dozen languag includ dead languag latin special issu aspect wordnet construct organis",0
"KeywordsLexical substitution Word sense disambiguation SemEval-2007 ","the english lexical substitution task","Since the inception of the Senseval series there has been a great deal of debate in the word sense disambiguation (WSD) community on what the right sense distinctions are for evaluation, with the consensus of opinion being that the distinctions should be relevant to the intended application. A solution to the above issue is lexical substitution, i.e. the replacement of a target word in context with a suitable alternative substitute. In this paper, we describe the English lexical substitution task and report an exhaustive evaluation of the systems participating in the task organized at SemEval-2007. The aim of this task is to provide an evaluation where the sense inventory is not predefined and where performance on the task would bode well for applications. The task not only reflects WSD capabilities, but also can be used to compare lexical resources, whether man-made or automatically created, and has the potential to benefit several natural-language applications.","Language Resources and Evaluation",2009,"No","lexic substitut word sens disambigu semev english lexic substitut task incept sensev seri great deal debat word sens disambigu wsd communiti sens distinct evalu consensus opinion distinct relev intend applic solut issu lexic substitut replac target word context suitabl altern substitut paper describ english lexic substitut task report exhaust evalu system particip task organ semev aim task provid evalu sens inventori predefin perform task bode applic task reflect wsd capabl compar lexic resourc man made automat creat potenti benefit natur languag applic",0
"KeywordsWord similarity Word embeddings Count-based models Dependency-based semantic models ","comparing explicit and predictive distributional semantic models endowed with syntactic contexts","In this article, we introduce an explicit count-based strategy to build word space models with syntactic contexts (dependencies). A filtering method is defined to reduce explicit word-context vectors. 
This traditional strategy is compared with a neural embedding (predictive) model also based on syntactic dependencies. The comparison was performed using the same parsed corpus for both models. Besides,
 the dependency-based methods are also compared with bag-of-words strategies, both count-based and predictive ones. The results show that our traditional count-based model with syntactic dependencies outperforms other strategies, including dependency-based embeddings, but just for the tasks focused on discovering similarity between words with the same function (i.e. near-synonyms).","Language Resources and Evaluation",2017,"No","word similar word embed count base model depend base semant model compar explicit predict distribut semant model endow syntact context articl introduc explicit count base strategi build word space model syntact context depend filter method defin reduc explicit word context vector tradit strategi compar neural embed predict model base syntact depend comparison perform pars corpus model depend base method compar bag word strategi count base predict result show tradit count base model syntact depend outperform strategi includ depend base embed task focus discov similar word function synonym",0
"KeywordsSimilarity Evaluation Semantic textual similarity ","cross level semantic similarity an evaluation framework for universal measures of similarity","Semantic similarity has typically been measured across items of approximately similar sizes. As a result, similarity measures have largely ignored the fact that different types of linguistic item can potentially have similar or even identical meanings, and therefore are designed to compare only one type of linguistic item. Furthermore, nearly all current similarity benchmarks within NLP contain pairs of approximately the same size, such as word or sentence pairs, preventing the evaluation of methods that are capable of comparing different sized items. To address this, we introduce a new semantic evaluation called cross-level semantic similarity (CLSS), which measures the degree to which the meaning of a larger linguistic item, such as a paragraph, is captured by a smaller item, such as a sentence. Our pilot CLSS task was presented as part of SemEval-2014, which attracted 19 teams who submitted 38 systems. CLSS data contains a rich mixture of pairs, spanning from paragraphs to word senses to fully evaluate similarity measures that are capable of comparing items of any type. Furthermore, data sources were drawn from diverse corpora beyond just newswire, including domain-specific texts and social media. We describe the annotation process and its challenges, including a comparison with crowdsourcing, and identify the factors that make the dataset a rigorous assessment of a method’s quality. Furthermore, we examine in detail the systems participating in the SemEval task to identify the common factors associated with high performance and which aspects proved difficult to all systems. Our findings demonstrate that CLSS poses a significant challenge for similarity methods and provides clear directions for future work on universal similarity methods that can compare any pair of items.","Language Resources and Evaluation",2016,"No","similar evalu semant textual similar cross level semant similar evalu framework univers measur similar semant similar typic measur item approxim similar size result similar measur larg fact type linguist item potenti similar ident mean design compar type linguist item current similar benchmark nlp pair approxim size word sentenc pair prevent evalu method capabl compar size item address introduc semant evalu call cross level semant similar clss measur degre mean larger linguist item paragraph captur smaller item sentenc pilot clss task present part semev attract team submit system clss data rich mixtur pair span paragraph word sens fulli evalu similar measur capabl compar item type data sourc drawn divers corpora newswir includ domain specif text social media describ annot process challeng includ comparison crowdsourc identifi factor make dataset rigor assess method qualiti examin detail system particip semev task identifi common factor high perform aspect prove difficult system find demonstr clss pose signific challeng similar method clear direct futur work univers similar method compar pair item",0
"KeywordsMetonymy Selectional restrictions Shared task evaluation ","data and models for metonymy resolution","We describe the first shared task for figurative language resolution, which was organised within SemEval-2007 and focused on metonymy. The paper motivates the linguistic principles of data sampling and annotation and shows the task’s feasibility via human agreement. The five participating systems mainly used supervised approaches exploiting a variety of features, of which grammatical relations proved to be the most useful. We compare the systems’ performance to automatic baselines as well as to a manually simulated approach based on selectional restriction violations, showing some limitations of this more traditional approach to metonymy recognition. The main problem supervised systems encountered is data sparseness, since metonymies in general tend to occur more rarely than literal uses. Also, within metonymies, the reading distribution is skewed towards a few frequent metonymy types. Future task developments should focus on addressing this issue.","Language Resources and Evaluation",2009,"No","metonymi select restrict share task evalu data model metonymi resolut describ share task figur languag resolut organis semev focus metonymi paper motiv linguist principl data sampl annot show task feasibl human agreement particip system supervis approach exploit varieti featur grammat relat prove compar system perform automat baselin manual simul approach base select restrict violat show limit tradit approach metonymi recognit main problem supervis system encount data spars metonymi general tend occur rare liter metonymi read distribut skew frequent metonymi type futur task develop focus address issu",0
"KeywordsIntonation Automatic intonation recognition Sp_ToBI Cat_ToBI ","a tool for automatic transcription of intonation etitobi a tobi transcriber for spanish and catalan","This article presents Eti_ToBI, a tool that automatically labels intonational events in Spanish and Catalan utterances according to the Sp_ToBI and Cat_ToBI current conventions. The system consists in a Praat script that assigns ToBI labels to pitch movements basing the assignments on lexical data introduced by the researcher and the acoustical data that it extracts from sound files. The first part of the article explains the methodological approach that has made possible the automatisation and describes the algorithms used by the script to perform the analysis. The second part presents the reliability results for both Catalan and Spanish corpora showing a level of agreement equal to the one shown by human transcribers among them in the literature.","Language Resources and Evaluation",2016,"No","inton automat inton recognit sptobi cattobi tool automat transcript inton etitobi tobi transcrib spanish catalan articl present etitobi tool automat label inton event spanish catalan utter sptobi cattobi current convent system consist praat script assign tobi label pitch movement base assign lexic data introduc research acoust data extract sound file part articl explain methodolog approach made automatis describ algorithm script perform analysi part present reliabl result catalan spanish corpora show level agreement equal shown human transcrib literatur",0
"KeywordsSemantic role Predicate-argument structure Chinese Proposition Bank Semantic role labeling ","generalizing the semantic roles in the chinese proposition bank","The Chinese Proposition Bank (CPB) is a 
corpus annotated with semantic roles for the arguments of verbal and nominalized predicates. The semantic roles for the core arguments are defined in a predicate-specific manner. That is, a set of semantic roles, numerically identified, are defined for each sense of a predicate lemma and recorded in a valency lexicon called frame files. The predicate-specific manner in which the semantic roles are defined reduces the cognitive burden on the annotators since they only need to internalize a few roles at a time and this has contributed to the consistency in annotation. It was also a sensible approach given the contentious issue of how many semantic roles are needed if one were to adopt of set of global semantic roles that apply to all predicates. A downside of this approach, however, is that the predicate-specific roles may not be consistent across predicates, and this inconsistency has a negative impact on training automatic systems. Given the progress that has been made in defining semantic roles in the last decade or so, time is ripe for adopting a set of general semantic roles. In this article, we describe our effort to “re-annotate” the CPB with a set of “global” semantic roles that are predicate-independent and investigate their impact on automatic semantic role labeling systems. When defining these global semantic roles, we strive to make them compatible with a recently published ISO standards on the annotation of semantic roles (ISO 24617-4:2014 SemAF-SR) while taking the linguistic characteristics of the Chinese language into account. We show that in spite of the much larger number of global semantic roles, the accuracy of an off-the-shelf semantic role labeling system retrained on the data re-annotated with global semantic roles is comparable to that trained on the data set with the original predicate-specific semantic roles. We also argue that the re-annotated data set, together with the original data, provides the user with more flexibility when using the corpus.","Language Resources and Evaluation",2016,"No","semant role predic argument structur chines proposit bank semant role label general semant role chines proposit bank chines proposit bank cpb corpus annot semant role argument verbal nomin predic semant role core argument defin predic specif manner set semant role numer identifi defin sens predic lemma record valenc lexicon call frame file predic specif manner semant role defin reduc cognit burden annot intern role time contribut consist annot approach contenti issu semant role need adopt set global semant role appli predic downsid approach predic specif role consist predic inconsist negat impact train automat system progress made defin semant role decad time ripe adopt set general semant role articl describ effort annot cpb set global semant role predic independ investig impact automat semant role label system defin global semant role strive make compat recent publish iso standard annot semant role iso semaf sr take linguist characterist chines languag account show spite larger number global semant role accuraci shelf semant role label system retrain data annot global semant role compar train data set origin predic specif semant role argu annot data set origin data user flexibl corpus",0
"KeywordsVerb valence extraction EM algorithm Co-occurrence matrices Polish language ","valence extraction using em selection and co occurrence matrices","This paper discusses two new procedures for extracting verb valences from raw texts, with an application to the Polish language. The first novel technique, the EM selection algorithm, performs unsupervised disambiguation of valence frame forests, obtained by applying a non-probabilistic deep grammar parser and some post-processing to the text. The second new idea concerns filtering of incorrect frames detected in the parsed text and is motivated by an observation that verbs which take similar arguments tend to have similar frames. This phenomenon is described in terms of newly introduced co-occurrence matrices. Using co-occurrence matrices, we split filtering into two steps. The list of valid arguments is first determined for each verb, whereas the pattern according to which the arguments are combined into frames is computed in the following stage. Our best extracted dictionary reaches an F-score of 45%, compared to an F-score of 39% for the standard frame-based BHT filtering.","Language Resources and Evaluation",2009,"No","verb valenc extract em algorithm occurr matric polish languag valenc extract em select occurr matric paper discuss procedur extract verb valenc raw text applic polish languag techniqu em select algorithm perform unsupervis disambigu valenc frame forest obtain appli probabilist deep grammar parser post process text idea concern filter incorrect frame detect pars text motiv observ verb similar argument tend similar frame phenomenon term newli introduc occurr matric occurr matric split filter step list valid argument determin verb pattern argument combin frame comput stage extract dictionari reach score compar score standard frame base bht filter",0
"KeywordsDependency structure Parsing accuracy Parsing time Sentence segmentation Speech corpus Speech understanding Spoken language Stochastic parsing Syntactically annotated corpus ","dependency parsing of japanese monologue using clause boundaries","Spoken monologues feature greater sentence length and structural complexity than spoken dialogues. To achieve high-parsing performance for spoken monologues, simplifying the structure by dividing a sentence into suitable language units could prove effective. This paper proposes a method for dependency parsing of Japanese spoken monologues based on sentence segmentation. In this method, dependency parsing is executed in two stages: at the clause level and the sentence level. First, dependencies within a clause are identified by dividing a sentence into clauses and executing stochastic dependency parsing for each clause. Next, dependencies across clause boundaries are identified stochastically, and the dependency structure of the entire sentence is thus completed. An experiment using a spoken monologue corpus shows the effectiveness of this method for efficient dependency parsing of Japanese monologue sentences.","Language Resources and Evaluation",2006,"No","depend structur pars accuraci pars time sentenc segment speech corpus speech understand spoken languag stochast pars syntact annot corpus depend pars japanes monologu claus boundari spoken monologu featur greater sentenc length structur complex spoken dialogu achiev high pars perform spoken monologu simplifi structur divid sentenc suitabl languag unit prove effect paper propos method depend pars japanes spoken monologu base sentenc segment method depend pars execut stage claus level sentenc level depend claus identifi divid sentenc claus execut stochast depend pars claus depend claus boundari identifi stochast depend structur entir sentenc complet experi spoken monologu corpus show effect method effici depend pars japanes monologu sentenc",0
"KeywordsSpeech database Noise database Spoken dialogue interaction Open-air noise environment ","the moveon database motorcycle environment speech and noise database for command and control applications","The MoveOn speech and noise database was purposely designed and implemented in support of research on spoken dialogue interaction in a motorcycle environment. The distinctiveness of the MoveOn database results from the requirements of the application domain—an information support and operational command and control system for the two-wheel police force—and also from the specifics of the adverse open-air acoustic environment. In this article, we first outline the target application, motivating the database design and purpose, and then report on the implementation details. The main challenges related to the choice of equipment, the organization of recording sessions, and some difficulties that were experienced during this effort, are discussed. We offer a detailed account of the database statistics, the suggested data splits in subsets, and discuss results from automatic speech recognition experiments which illustrate the degree of complexity of the operational environment.","Language Resources and Evaluation",2013,"No","speech databas nois databas spoken dialogu interact open air nois environ moveon databas motorcycl environ speech nois databas command control applic moveon speech nois databas purpos design implement support research spoken dialogu interact motorcycl environ distinct moveon databas result requir applic domain inform support oper command control system wheel polic forc specif advers open air acoust environ articl outlin target applic motiv databas design purpos report implement detail main challeng relat choic equip organ record session difficulti experienc effort discuss offer detail account databas statist suggest data split subset discuss result automat speech recognit experi illustr degre complex oper environ",0
"KeywordsMultilingual and multigenre corpus Crowdsourcing Speech transcription Automatic speech recognition ","creating a ground truth multilingual dataset of news and talk show transcriptions through crowdsourcing","This paper describes the development of a multilingual and multigenre manually annotated speech dataset, freely available to the research community as ground truth for the evaluation of automatic transcription systems and spoken language translation systems. The dataset includes two video genres—television broadcast news and talk-shows—and covers Flemish, English, German, and Italian, for a total of about 35 h of television speech. Besides segmentation and orthographic transcription, we added a very rich annotation on the audio signal, both at the linguistic level (e.g. filled pauses, pronunciation errors, disfluencies, speech in a foreign language) and at the acoustic level (e.g. background noise and different types of non-speech events). Furthermore, a subset of the transcriptions is translated in four directions, namely Flemish to English, German to English, German to Italian and English to Italian. The development of this dataset was organized in several phases, relying on expert transcribers as well as involving non-expert contributors through crowdsourcing. We first conducted a feasibility study to test and compare two methods for crowdsourcing speech transcription on broadcast news data. These methods are based on different transcription processes (i.e. parallel vs. iterative) and incorporate two different quality control mechanisms. With both methods, we achieved near-expert transcription quality—in terms of word error rate—for English, German and Italian data. Instead, for Flemish data we were not able to get a sufficient response from the crowd to complete the offered transcription tasks. The results obtained demonstrate that the viability of methods for crowdsourcing speech transcription significantly depends on the target language. This paper provides a detailed comparison of the results obtained with the two crowdsourcing methods tested, describes the main characteristics of the final ground truth resource created as well as the methodology adopted, and the guidelines prepared for its development.","Language Resources and Evaluation",2017,"No","multilingu multigenr corpus crowdsourc speech transcript automat speech recognit creat ground truth multilingu dataset news talk show transcript crowdsourc paper describ develop multilingu multigenr manual annot speech dataset freeli research communiti ground truth evalu automat transcript system spoken languag translat system dataset includ video genr televis broadcast news talk show cover flemish english german italian total televis speech segment orthograph transcript ad rich annot audio signal linguist level fill paus pronunci error disfluenc speech foreign languag acoust level background nois type speech event subset transcript translat direct flemish english german english german italian english italian develop dataset organ phase reli expert transcrib involv expert contributor crowdsourc conduct feasibl studi test compar method crowdsourc speech transcript broadcast news data method base transcript process parallel iter incorpor qualiti control mechan method achiev expert transcript qualiti term word error rate english german italian data flemish data suffici respons crowd complet offer transcript task result obtain demonstr viabil method crowdsourc speech transcript signific depend target languag paper detail comparison result obtain crowdsourc method test describ main characterist final ground truth resourc creat methodolog adopt guidelin prepar develop",0
"Key WordsSociete de 1789 collocation cahiers de doléances French Revolution political lexicology ","the language of enlightened politics thesocit de 1789 in the french revolution","The Société de 1789 was a political club founded in early 1790 to propagate the ideals of the Revolution and the Enlightenment. A systematic analysis of the language found in the public discourse of the Société using simple quantitative techniques suggests important distinctions in comparison to the language found in a baseline sample, a selection of the General Cahiers de doléances of 1789. It is further argued that these differences represent an Enlightened reforming tradition that carried into the French Revolution.","Computers and the Humanities",1989,"No","key wordssociet de colloc cahier de dol anc french revolut polit lexicolog languag enlighten polit thesocit de french revolut soci de polit club found earli propag ideal revolut enlighten systemat analysi languag found public discours soci simpl quantit techniqu suggest import distinct comparison languag found baselin sampl select general cahier de dol anc argu differ repres enlighten reform tradit carri french revolut",0
"authorship Elegy by W.S. Shakespeare stylometry ","the professor doth protest too much methinks problems with the foster response","In “Response to Elliott and Valenza, 'And Then There Were None'”, (1996) Donald Foster has taken strenuous issue with our Shakespeare Clinic's final report, which concluded that none of the testable Shakespeare claimants, and none of the Shakespeare Apocrypha poems and plays – including Funeral Elegy by W.S. – match Shakespeare. Though he seems to accept most of our exclusions – notably excepting those of the Elegy and A Lover's Complaint – he believes that our methodology is nonetheless fatally flawed by “worthless figures ... wrong more often than right”, “rigorous cherry–picking”, “playing with a stacked deck”, and “conveniently exil[ing] ... inconvenient data.” He describes our tests as “foul vapor” and “methodological madness.”","Computers and the Humanities",1998,"No","authorship elegi shakespear stylometri professor doth protest methink problem foster respons respons elliott valenza donald foster strenuous issu shakespear clinic final report conclud testabl shakespear claimant shakespear apocrypha poem play includ funer elegi match shakespear accept exclus notabl except elegi lover complaint believ methodolog nonetheless fatal flaw worthless figur wrong rigor cherri pick play stack deck conveni exil inconveni data describ test foul vapor methodolog mad",0
"KeywordsComputational Linguistic Computer Format Collegiate Dictionary ","a new computer format for websters seventh collegiate dictionary",NA,"Computers and the Humanities",1974,"No","comput linguist comput format collegi dictionari comput format webster seventh collegi dictionari na",0
"KeywordsLinguistic annotation Standards Language resources Annotation processing software ","bridging the gaps interoperability for language engineering architectures using graf","This paper explores interoperability for data represented using the Graph Annotation Framework (GrAF) (Ide and Suderman, 2007) and the data formats utilized by two general-purpose annotation systems: the General Architecture for Text Engineering (GATE) (Cunningham et al., 2002) and the Unstructured Information Management Architecture (UIMA) (Ferrucci and Lally in Nat Lang Eng 10(3–4):327–348, 2004). GrAF is intended to serve as a “pivot” to enable interoperability among different formats, and both GATE and UIMA are at least implicitly designed with an eye toward interoperability with other formats and tools. We describe the steps required to perform a round-trip rendering from GrAF to GATE and GrAF to UIMA CAS and back again, and outline the commonalities as well as the differences and gaps that came to light in the process.","Language Resources and Evaluation",2012,"No","linguist annot standard languag resourc annot process softwar bridg gap interoper languag engin architectur graf paper explor interoper data repres graph annot framework graf ide suderman data format util general purpos annot system general architectur text engin gate cunningham al unstructur inform manag architectur uima ferrucci lalli nat lang eng graf intend serv pivot enabl interoper format gate uima implicit design eye interoper format tool describ step requir perform round trip render graf gate graf uima cas back outlin common differ gap light process",0
"KeywordsKurdish BLARK Language tools Computational linguistics Natural language processing ","blark for multi dialect languages towards the kurdish blark","In this paper we introduce the Kurdish BLARK (Basic Language Resource Kit). The original BLARK has not considered multi-dialect characteristics and generally has targeted reasonably well-resourced languages. To consider these two features, we extended BLARK and applied the proposed extension to Kurdish. Kurdish language not only faces a paucity in resources, but also embraces several dialects within a complex linguistic context. This paper presents the Kurdish BLARK and shows that from Natural language processing and computational linguistics perspectives the revised BLARK provides a more applicable view of languages with similar characteristics to Kurdish.","Language Resources and Evaluation",2018,"No","kurdish blark languag tool comput linguist natur languag process blark multi dialect languag kurdish blark paper introduc kurdish blark basic languag resourc kit origin blark consid multi dialect characterist general target resourc languag featur extend blark appli propos extens kurdish kurdish languag face pauciti resourc embrac dialect complex linguist context paper present kurdish blark show natur languag process comput linguist perspect revis blark applic view languag similar characterist kurdish",0
"KeywordsLinguistic annotation Standards Language resources Interoperability ","the linguistic annotation framework a standard for annotation interchange and merging","This paper overviews the International Standards Organization–Linguistic Annotation Framework (ISO–LAF) developed in ISO TC37 SC4. We describe the XML serialization of ISO–LAF, the Graph Annotation Format (GrAF) and discuss the rationale behind the various decisions that were made in determining the standard. We describe the structure of the GrAF headers in detail and provide multiple examples of GrAF representation for text and multi-media. Finally, we discuss the next steps for standardization of interchange formats for linguistic annotations.","Language Resources and Evaluation",2014,"No","linguist annot standard languag resourc interoper linguist annot framework standard annot interchang merg paper overview intern standard organ linguist annot framework iso laf develop iso tc sc describ xml serial iso laf graph annot format graf discuss rational decis made determin standard describ structur graf header detail provid multipl exampl graf represent text multi media final discuss step standard interchang format linguist annot",0
"KeywordsKeyphrase extraction Scientific document processing SemEval-2010 Shared task ","automatic keyphrase extraction from scientific articles","This paper describes the organization and results of the automatic keyphrase extraction task held at the Workshop on Semantic Evaluation 2010 (SemEval-2010). The keyphrase extraction task was specifically geared towards scientific articles. Systems were automatically evaluated by matching their extracted keyphrases against those assigned by the authors as well as the readers to the same documents. We outline the task, present the overall ranking of the submitted systems, and discuss the improvements to the state-of-the-art in keyphrase extraction.","Language Resources and Evaluation",2013,"No","keyphras extract scientif document process semev share task automat keyphras extract scientif articl paper describ organ result automat keyphras extract task held workshop semant evalu semev keyphras extract task specif gear scientif articl system automat evalu match extract keyphras assign author reader document outlin task present rank submit system discuss improv state art keyphras extract",0
"KeywordsStatistical machine translation Domain adaptation  Web crawling Optimisation ","domain adaptation of statistical machine translation with domain focused web crawling","In this paper, we tackle the problem of domain adaptation of statistical machine translation (SMT) by exploiting domain-specific data acquired by domain-focused crawling of text from the World Wide Web. We design and empirically evaluate a procedure for automatic acquisition of monolingual and parallel text and their exploitation for system training, tuning, and testing in a phrase-based SMT framework. We present a strategy for using such resources depending on their availability and quantity supported by results of a large-scale evaluation carried out for the domains of environment and labour legislation, two language pairs (English–French and English–Greek) and in both directions: into and from English. In general, machine translation systems trained and tuned on a general domain perform poorly on specific domains and we show that such systems can be adapted successfully by retuning model parameters using small amounts of parallel in-domain data, and may be further improved by using additional monolingual and parallel training data for adaptation of language and translation models. The average observed improvement in BLEU achieved is substantial at 15.30 points absolute.","Language Resources and Evaluation",2015,"No","statist machin translat domain adapt web crawl optimis domain adapt statist machin translat domain focus web crawl paper tackl problem domain adapt statist machin translat smt exploit domain specif data acquir domain focus crawl text world wide web design empir evalu procedur automat acquisit monolingu parallel text exploit system train tune test phrase base smt framework present strategi resourc depend avail quantiti support result larg scale evalu carri domain environ labour legisl languag pair english french english greek direct english general machin translat system train tune general domain perform poor specif domain show system adapt success retun model paramet small amount parallel domain data improv addit monolingu parallel train data adapt languag translat model averag observ improv bleu achiev substanti point absolut",0
"KeywordsSpanish Grammar Deep processing HPSG LKB DELPH-IN ","the spanish delph in grammar","In this article we present a Spanish grammar implemented in the Linguistic Knowledge Builder system and grounded in the theoretical framework of Head-driven Phrase Structure Grammar. The grammar is being developed in an international multilingual context, the DELPH-IN Initiative, contributing to an open-source repository of software and linguistic resources for various Natural Language Processing applications. We will show how we have refined and extended a core grammar, derived from the LinGO Grammar Matrix, to achieve a broad-coverage grammar. The Spanish DELPH-IN grammar is the most comprehensive grammar for Spanish deep processing, and it is being deployed in the construction of a treebank for Spanish of 60,000 sentences based in a technical corpus in the framework of the European project METANET4U (Enhancing the European Linguistic Infrastructure, GA 270893GA; http://www.meta-net.eu/projects/METANET4U/.) and a smaller treebank of about 15,000 sentences based in a corpus from the press.","Language Resources and Evaluation",2013,"No","spanish grammar deep process hpsg lkb delph spanish delph grammar articl present spanish grammar implement linguist knowledg builder system ground theoret framework head driven phrase structur grammar grammar develop intern multilingu context delph initi contribut open sourc repositori softwar linguist resourc natur languag process applic show refin extend core grammar deriv lingo grammar matrix achiev broad coverag grammar spanish delph grammar comprehens grammar spanish deep process deploy construct treebank spanish sentenc base technic corpus framework european project metanetu enhanc european linguist infrastructur ga ga httpwwwmeta neteuprojectsmetanetu smaller treebank sentenc base corpus press",0
"KeywordsFinite state computational morphology Tswana Disjunctive orthography Tokenisation Verb morphology ","tswana finite state tokenisation","Tswana, a Bantu language in the Sotho group, is characterised by an agglutinative morphology and a disjunctive orthography, which mainly affects the verb category. In particular, verbal prefixes are usually written disjunctively, while suffixes follow a conjunctive writing style. Therefore, Tswana tokenisation cannot be based solely on whitespace, as is the case in many alphabetic, segmented languages, including the conjunctively written Nguni group of South African Bantu languages. This paper shows how a combination of two finite state tokeniser transducers and a finite state morphological analyser are combined to solve the Tswana (verb) tokenisation problem. The approach has the important advantage of bringing the processing of Tswana, beyond the morphological analysis level, in line with what is appropriate for the Nguni languages. This means that the challenge of the disjunctive orthography is met at the tokenisation/morphological analysis level and does not in principle propagate to subsequent levels of analysis such as POS tagging and shallow parsing, etc. The tokenisation approach is novel and, when implemented and evaluated, yields an F1-score of 95 % with respect to a hand tokenised gold standard.","Language Resources and Evaluation",2015,"No","finit state comput morpholog tswana disjunct orthographi tokenis verb morpholog tswana finit state tokenis tswana bantu languag sotho group characteris agglutin morpholog disjunct orthographi affect verb categori verbal prefix written disjunct suffix follow conjunct write style tswana tokenis base sole whitespac case alphabet segment languag includ conjunct written nguni group south african bantu languag paper show combin finit state tokenis transduc finit state morpholog analys combin solv tswana verb tokenis problem approach import advantag bring process tswana morpholog analysi level line nguni languag mean challeng disjunct orthographi met tokenisationmorpholog analysi level principl propag subsequ level analysi pos tag shallow pars tokenis approach implement evalu yield f score respect hand tokenis gold standard",0
"KeywordsPlagiarism detection Authorship verification Stylometry One-class classification ","intrinsic plagiarism analysis","Research in automatic text plagiarism detection focuses on algorithms that compare suspicious documents against a collection of reference documents. Recent approaches perform well in identifying copied or modified foreign sections, but they assume a closed world where a reference collection is given. This article investigates the question whether plagiarism can be detected by a computer program if no reference can be provided, e.g., if the foreign sections stem from a book that is not available in digital form. We call this problem class intrinsic plagiarism analysis; it is closely related to the problem of authorship verification. Our contributions are threefold. (1) We organize the algorithmic building blocks for intrinsic plagiarism analysis and authorship verification and survey the state of the art. (2) We show how the meta learning approach of Koppel and Schler, termed “unmasking”, can be employed to post-process unreliable stylometric analysis results. (3) We operationalize and evaluate an analysis chain that combines document chunking, style model computation, one-class classification, and meta learning.","Language Resources and Evaluation",2011,"No","plagiar detect authorship verif stylometri class classif intrins plagiar analysi research automat text plagiar detect focus algorithm compar suspici document collect refer document recent approach perform identifi copi modifi foreign section assum close world refer collect articl investig question plagiar detect comput program refer provid foreign section stem book digit form call problem class intrins plagiar analysi close relat problem authorship verif contribut threefold organ algorithm build block intrins plagiar analysi authorship verif survey state art show meta learn approach koppel schler term unmask employ post process unreli stylometr analysi result operation evalu analysi chain combin document chunk style model comput class classif meta learn",0
"KeywordsPolarity classification Sentiment analysis Bootstrapping methods Feature engineering Text classification ","bootstrapping polarity classifiers with rule based classification","In this article, we examine the effectiveness of bootstrapping supervised machine-learning polarity classifiers with the help of a domain-independent rule-based classifier that relies on a lexical resource, i.e., a polarity lexicon and a set of linguistic rules. The benefit of this method is that though no labeled training data are required, it allows a classifier to capture in-domain knowledge by training a supervised classifier with in-domain features, such as bag of words, on instances labeled by a rule-based classifier. Thus, this approach can be considered as a simple and effective method for domain adaptation. Among the list of components of this approach, we investigate how important the quality of the rule-based classifier is and what features are useful for the supervised classifier. In particular, the former addresses the issue in how far linguistic modeling is relevant for this task. We not only examine how this method performs under more difficult settings in which classes are not balanced and mixed reviews are included in the data set but also compare how this linguistically-driven method relates to state-of-the-art statistical domain adaptation.","Language Resources and Evaluation",2013,"No","polar classif sentiment analysi bootstrap method featur engin text classif bootstrap polar classifi rule base classif articl examin effect bootstrap supervis machin learn polar classifi domain independ rule base classifi reli lexic resourc polar lexicon set linguist rule benefit method label train data requir classifi captur domain knowledg train supervis classifi domain featur bag word instanc label rule base classifi approach consid simpl effect method domain adapt list compon approach investig import qualiti rule base classifi featur supervis classifi address issu linguist model relev task examin method perform difficult set class balanc mix review includ data set compar linguist driven method relat state art statist domain adapt",0
"KeywordsTreebank Finnish Parsing Morphology ","building the essential resources for finnish the turku dependency treebank","In this paper, we present the final version of a publicly available treebank of Finnish, the Turku Dependency Treebank. The treebank contains 204,399 tokens (15,126 sentences) from 10 different text sources and has been manually annotated in a Finnish-specific version of the well-known Stanford Dependency scheme. The morphological analyses of the treebank have been assigned using a novel machine learning method to disambiguate readings given by an existing tool. As the second main contribution, we present the first open source Finnish dependency parser, trained on the newly introduced treebank. The parser achieves a labeled attachment score of 81 %. The treebank data as well as the parsing pipeline are available under an open license at http://bionlp.utu.fi/.","Language Resources and Evaluation",2014,"No","treebank finnish pars morpholog build essenti resourc finnish turku depend treebank paper present final version public treebank finnish turku depend treebank treebank token sentenc text sourc manual annot finnish specif version stanford depend scheme morpholog analys treebank assign machin learn method disambigu read exist tool main contribut present open sourc finnish depend parser train newli introduc treebank parser achiev label attach score treebank data pars pipelin open licens httpbionlputufi",0
NA,"contents of volume 35",NA,"Computers and the Humanities",2001,"No","content volum na",0
NA,"contents of volume 36",NA,"Computers and the Humanities",2002,"No","content volum na",0
"KeywordsComputational Linguistic Progress Report ","the dictionary of old english a progress report",NA,"Computers and the Humanities",1971,"No","comput linguist progress report dictionari english progress report na",0
NA,"laboratoire danalyse relationnelle des textes prsentation de travaux",NA,"Computers and the Humanities",1986,"No","laboratoir danalys relationnell des text prsentat de travaux na",0
"Key Wordstextual analysis church history Pope John XXIII indexes Italian language ","church history and the computer","Ever since the pioneering work of Roberto Busa, computers have had an important work in humanistic research. The Istituto per le Scienze Religiose has been involved in such research since the 1970s. This article describes the Pope John XXIII project whose aim is to produce a computer-aided index of all the Pope's writings in both Italian and Latin; as well as future projects currently under consideration.","Computers and the Humanities",1990,"No","key wordstextu analysi church histori pope john xxiii index italian languag church histori comput pioneer work roberto busa comput import work humanist research istituto le scienz religios involv research s articl describ pope john xxiii project aim produc comput aid index pope write italian latin futur project consider",0
"KeywordsComputer Study Stylistic Analysis Entire Corpus Edited Text Middle High German ","computer study of medieval german poetry a conference report",NA,"Computers and the Humanities",1967,"No","comput studi stylist analysi entir corpus edit text middl high german comput studi mediev german poetri confer report na",0
"KeywordsComputational Linguistic ","dictionary driven semantic look up",NA,"Computers and the Humanities",2000,"No","comput linguist dictionari driven semant na",0
"KeywordsComputational Linguistic ","letter from cambridge",NA,"Computers and the Humanities",1979,"No","comput linguist letter cambridg na",0
"KeywordsComputational Linguistic ","letter from ljubljana",NA,"Computers and the Humanities",1981,"No","comput linguist letter ljubljana na",0
"KeywordsComputational Linguistic ","computers and the classics",NA,"Computers and the Humanities",1968,"No","comput linguist comput classic na",0
"KeywordsTraditional Method Computational Linguistic Central Problem Language Study Linguistic Data ","computers and medieval english lexicography","Dictionaries and related language reference works constitute a rich but under-exploited resource for the history of languages and of language study in the Middle Ages. Unfortunately, the size and complexity of typical medieval dictionaries make editions and analyses by traditional methods prohibitively expensive in time and money. Using as an example the Latin-Middle English dictionaryMedulla grammatice, the paper describes some central problems in the study of medieval English lexicography and the solutions provided by computers, which, with their immense speed, profound memory, and perfect accuracy can help scholars analyze, edit, and promulgate medieval documents and the linguistic data they contain.","Computers and the Humanities",1978,"No","tradit method comput linguist central problem languag studi linguist data comput mediev english lexicographi dictionari relat languag refer work constitut rich exploit resourc histori languag languag studi middl age size complex typic mediev dictionari make edit analys tradit method prohibit expens time money latin middl english dictionarymedulla grammatic paper describ central problem studi mediev english lexicographi solut provid comput immens speed profound memori perfect accuraci scholar analyz edit promulg mediev document linguist data",0
"Key Wordsreading Old English information theory entropy redundancy ","an information theoretic approach to the written transmission of old english","Information theory offers a means for analyzing some constraints on the reading and copying process in Old English. Entropy for strings of various lengths offers a baseline measure of the uncertainty involved in transmission of Old English texts, while avoiding the pitfalls of applying models of modern reading to early medieval practice. Analysis of lengthy prose and verse texts in Old English revealed uniformly high values for entropy at all string lengths. High entropies may be the result of the language's irregular orthography, poetic koiné, and several dialects and imply that the language may have been easy to write but difficult to read. The low redundancy of the language which its high entropy values indicate suggests that the reader of Old English played an enhanced role in “decoding” a text and may provide an explanation for the high variability in the transmission of Old English verse.","Computers and the Humanities",1989,"No","key wordsread english inform theori entropi redund inform theoret approach written transmiss english inform theori offer mean analyz constraint read copi process english entropi string length offer baselin measur uncertainti involv transmiss english text avoid pitfal appli model modern read earli mediev practic analysi lengthi prose vers text english reveal uniform high valu entropi string length high entropi result languag irregular orthographi poetic koin dialect impli languag easi write difficult read low redund languag high entropi valu suggest reader english play enhanc role decod text provid explan high variabl transmiss english vers",0
"Key Wordshypertext hypermedia Intermedia educational computing student-directed learning collaborative learning literary theory interdisciplinary graphics ","hypertext in literary education criticism and scholarship","After describing the English course and the particular hypertext system that supports it at Brown University, the essay surveys the materials on Context32, that part of the system devoted to literature courses, and narrates how a student uses the system during a typical session in our electronic laboratory/classroom. Next, it presents evidence of the effects of such information technology on student performance, after which it examines the relation of hypertext to contemporary literary theory, in particular to the ideas of decentering, intertextuality, and anti-hierarchical texts. Finally, it explains the continuing developments of Intermedia.","Computers and the Humanities",1989,"No","key wordshypertext hypermedia intermedia educ comput student direct learn collabor learn literari theori interdisciplinari graphic hypertext literari educ critic scholarship describ english hypertext system support brown univers essay survey materi context part system devot literatur cours narrat student system typic session electron laboratoryclassroom present evid effect inform technolog student perform examin relat hypertext contemporari literari theori idea decent intertextu anti hierarch text final explain continu develop intermedia",0
"KeywordsComputational Linguistic ","networking in the humanities lessons from ansaxnet",NA,"Computers and the Humanities",1992,"No","comput linguist network human lesson ansaxnet na",0
"Key Wordsdiscourse analysis expert systems LEX project law natural language processing information retrieval ","discourse analysis for a legal expert system","This paper deals with discourse analysis, with specific reference to the Linguistic and Logic Based Legal Expert System, LEX. In the LEX project we concentrated on a few arbitrarily selected court decisions, extracted the case descriptions, and then added the necessary background knowledge to our prototype expert system to analyze the case descriptions and to deduce the answers to some juridical questions. In this paper we present and comment on a typical discourse representation structure for an accident description in the corpus we studied.","Computers and the Humanities",1991,"No","key wordsdiscours analysi expert system lex project law natur languag process inform retriev discours analysi legal expert system paper deal discours analysi specif refer linguist logic base legal expert system lex lex project concentr arbitrarili select court decis extract case descript ad background knowledg prototyp expert system analyz case descript deduc answer jurid question paper present comment typic discours represent structur accid descript corpus studi",0
"Key Wordsoptical character recognition scanning off-shore keyboarding efficiency ARTFL cost analysis ","optical character scanning a discussion of efficiency and politics","Optical Character Recognition is shown to be significantly more expensive than keyboarding, using off-shore contractors, for entry of large amounts of text where high accuracy is required. Using large test samples in French and English, the paper indicates that OCR applications which require significant post-scan editing are labor intensive projects that can be accomplished more efficiently by keyboarding. Most OCR systems are still not capable of entering large amounts of text accurately enough to avoid an expensive editing step.","Computers and the Humanities",1993,"No","key wordsopt charact recognit scan shore keyboard effici artfl cost analysi optic charact scan discuss effici polit optic charact recognit shown signific expens keyboard shore contractor entri larg amount text high accuraci requir larg test sampl french english paper ocr applic requir signific post scan edit labor intens project accomplish effici keyboard ocr system capabl enter larg amount text accur avoid expens edit step",0
"Key wordshypertext hypermedia WWW hypertext authoring literature instruction ","current uses of hypertext in teaching literature","Literature instructors are using hypertext to enhance their teaching in a broad variety of ways that includes putting course materials on the WWW; creating online tutorials; using annotated hypertexts in addition to or in lieu of print texts; having students write hypertexts; examining the medium of hypertext as a literary and cultural theme; and studying hypertext fiction in the context of traditional literature classes. The article describes examples of each of these uses of hypertext in teaching literature and provides sources of further examples of and information on using hypertext as a teaching tool in literature classes.","Computers and the Humanities",1996,"No","key wordshypertext hypermedia www hypertext author literatur instruct current hypertext teach literatur literatur instructor hypertext enhanc teach broad varieti way includ put materi www creat onlin tutori annot hypertext addit lieu print text student write hypertext examin medium hypertext literari cultur theme studi hypertext fiction context tradit literatur class articl describ exampl hypertext teach literatur sourc exampl inform hypertext teach tool literatur class",0
"KeywordsSentiment ambiguous adjectives Sentiment analysis Word sense disambiguation SemEval ","semeval 2010 task 18 disambiguating sentiment ambiguous adjectives","Sentiment ambiguous adjectives, which have been neglected by most previous researches, pose a challenging task in sentiment analysis. We present an evaluation task at SemEval-2010, designed to provide a framework for comparing different approaches on this problem. The task focuses on 14 Chinese sentiment ambiguous adjectives, and provides manually labeled test data. There are 8 teams submitting 16 systems in this task. In this paper, we define the task, describe the data creation, list the participating systems, and discuss different approaches.","Language Resources and Evaluation",2013,"No","sentiment ambigu adject sentiment analysi word sens disambigu semev semev task disambigu sentiment ambigu adject sentiment ambigu adject neglect previous research pose challeng task sentiment analysi present evalu task semev design provid framework compar approach problem task focus chines sentiment ambigu adject manual label test data team submit system task paper defin task describ data creation list particip system discuss approach",0
"KeywordsQuality Assessment Validation Evaluation Language resources Spoken language resources ","validation of spoken language resources an overview of basic aspects","Spoken language resources (SLRs) are essential for both research and application development. In this article we clarify the concept of SLR validation. We define validation and how it differs from evaluation. Further, relevant principles of SLR validation are outlined. We argue that the best way to validate SLRs is to implement validation throughout SLR production and have it carried out by an external and experienced institute. We address which tasks should be carried out by the validation institute, and which not. Further, we list the basic issues that validation criteria for SLR should address. A standard validation protocol is shown, illustrating how validation can prove its value throughout the production phase in terms of pre-validation, full validation and pre-release validation.","Language Resources and Evaluation",2008,"No","qualiti assess valid evalu languag resourc spoken languag resourc valid spoken languag resourc overview basic aspect spoken languag resourc slrs essenti research applic develop articl clarifi concept slr valid defin valid differ evalu relev principl slr valid outlin argu valid slrs implement valid slr product carri extern experienc institut address task carri valid institut list basic issu valid criteria slr address standard valid protocol shown illustr valid prove product phase term pre valid full valid pre releas valid",0
"KeywordsComputational Linguistic Temporal Closure Annotation Environment ","temporal closure in an annotation environment",NA,"Language Resources and Evaluation",2005,"No","comput linguist tempor closur annot environ tempor closur annot environ na",0
"KeywordsSentiment analysis Opinion mining Sentiment lexicon Polarity detection Emotion classification Semi-automatic translation ","feel a french expanded emotion lexicon","Sentiment analysis allows the semantic evaluation of pieces of text according to the expressed sentiments and opinions. While considerable attention has been given to the polarity (positive, negative) of English words, only few studies were interested in the conveyed emotions (joy, anger, surprise, sadness, etc.) especially in other languages. In this paper, we present the elaboration and the evaluation of a new French lexicon considering both polarity and emotion. The elaboration method is based on the semi-automatic translation and expansion to synonyms of the English NRC Word Emotion Association Lexicon (NRC-EmoLex). First, online translators have been automatically queried in order to create a first version of our new French Expanded Emotion Lexicon (FEEL). Then, a human professional translator manually validated the automatically obtained entries and the associated emotions. She agreed with more than 94 % of the pre-validated entries (those found by a majority of translators) and less than 18 % of the remaining entries (those found by very few translators). This result highlights that online tools can be used to get high quality resources with low cost. Annotating a subset of terms by three different annotators shows that the associated sentiments and emotions are consistent. Finally, extensive experiments have been conducted to compare the final version of FEEL with other existing French lexicons. Various French benchmarks for polarity and emotion classifications have been used in these evaluations. Experiments have shown that FEEL obtains competitive results for polarity, and significantly better results for basic emotions.","Language Resources and Evaluation",2017,"No","sentiment analysi opinion mine sentiment lexicon polar detect emot classif semi automat translat feel french expand emot lexicon sentiment analysi semant evalu piec text express sentiment opinion consider attent polar posit negat english word studi interest convey emot joy anger surpris sad languag paper present elabor evalu french lexicon polar emot elabor method base semi automat translat expans synonym english nrc word emot associ lexicon nrc emolex onlin translat automat queri order creat version french expand emot lexicon feel human profession translat manual valid automat obtain entri emot agre pre valid entri found major translat remain entri found translat result highlight onlin tool high qualiti resourc low cost annot subset term annot show sentiment emot consist final extens experi conduct compar final version feel exist french lexicon french benchmark polar emot classif evalu experi shown feel obtain competit result polar signific result basic emot",0
"KeywordsMultimodal database Theatrical improvisations Motion capture system Continuous emotion ","the usc creativeit database of multimodal dyadic interactions from speech and full body motion capture to continuous emotional annotations","Improvised acting is a viable technique to study expressive human communication and to shed light into actors’ creativity. The USC CreativeIT database provides a novel, freely-available multimodal resource for the study of theatrical improvisation and rich expressive human behavior (speech and body language) in dyadic interactions. The theoretical design of the database is based on the well-established improvisation technique of Active Analysis in order to provide naturally induced affective and expressive, goal-driven interactions. This database contains dyadic theatrical improvisations performed by 16 actors, providing detailed full body motion capture data and audio data of each participant in an interaction. The carefully engineered data collection, the improvisation design to elicit natural emotions and expressive speech and body language, as well as the well-developed annotation processes provide a gateway to study and model various aspects of theatrical performance, expressive behaviors and human communication and interaction.","Language Resources and Evaluation",2016,"No","multimod databas theatric improvis motion captur system continu emot usc creativeit databas multimod dyadic interact speech full bodi motion captur continu emot annot improvis act viabl techniqu studi express human communic shed light actor creativ usc creativeit databas freeli multimod resourc studi theatric improvis rich express human behavior speech bodi languag dyadic interact theoret design databas base establish improvis techniqu activ analysi order provid natur induc affect express goal driven interact databas dyadic theatric improvis perform actor provid detail full bodi motion captur data audio data particip interact care engin data collect improvis design elicit natur emot express speech bodi languag develop annot process provid gateway studi model aspect theatric perform express behavior human communic interact",0
"Keywordsquestion answering temporal ordering annotation events modality temporal expressions ","temporal and event information in natural language text","In this paper, we discuss the role that temporal information plays in natural language text, specifically in the context of question answering systems. We define a descriptive framework with which we can examine the temporally sensitive aspects of natural language queries. We then investigate broadly what properties a general specification language would need, in order to mark up temporal and event information in text. We present a language, TimeML, which attempts to capture the richness of temporal and event related information in language, while demonstrating how it can play an important part in the development of more robust question answering systems.","Language Resources and Evaluation",2005,"No","question answer tempor order annot event modal tempor express tempor event inform natur languag text paper discuss role tempor inform play natur languag text specif context question answer system defin descript framework examin tempor sensit aspect natur languag queri investig broad properti general specif languag order mark tempor event inform text present languag timeml attempt captur rich tempor event relat inform languag demonstr play import part develop robust question answer system",0
"KeywordsQuestion answering Evaluation CLEF ","question answering at the cross language evaluation forum 20032010","The paper offers an overview of the key issues raised during the 8 years’ activity of the Multilingual Question Answering Track at the Cross Language Evaluation Forum (CLEF). The general aim of the track has been to test both monolingual and cross-language Question Answering (QA) systems that process queries and documents in several European languages, also drawing attention to a number of challenging issues for research in multilingual QA. The paper gives a brief description of how the task has evolved over the years and of the way in which the data sets have been created, presenting also a short summary of the different types of questions developed. The document collections adopted in the competitions are outlined as well, and data about participation is provided. Moreover, the main measures used to evaluate system performances are explained and an overall analysis of the results achieved is presented.","Language Resources and Evaluation",2012,"No","question answer evalu clef question answer cross languag evalu forum paper offer overview key issu rais year activ multilingu question answer track cross languag evalu forum clef general aim track test monolingu cross languag question answer qa system process queri document european languag draw attent number challeng issu research multilingu qa paper descript task evolv year data set creat present short summari type question develop document collect adopt competit outlin data particip provid main measur evalu system perform explain analysi result achiev present",0
"KeywordsSentiment analysis Twitter SemEval ","developing a successful semeval task in sentiment analysis of twitter and other social media texts","We present the development and evaluation of a semantic analysis task that lies at the intersection of two very trendy lines of research in contemporary computational linguistics: (1) sentiment analysis, and (2) natural language processing of social media text. The task was part of SemEval, the International Workshop on Semantic Evaluation, a semantic evaluation forum previously known as SensEval. The task ran in 2013 and 2014, attracting the highest number of participating teams at SemEval in both years, and there is an ongoing edition in 2015. The task included the creation of a large contextual and message-level polarity corpus consisting of tweets, SMS messages, LiveJournal messages, and a special test set of sarcastic tweets. The evaluation attracted 44 teams in 2013 and 46 in 2014, who used a variety of approaches. The best teams were able to outperform several baselines by sizable margins with improvement across the 2 years the task has been run. We hope that the long-lasting role of this task and the accompanying datasets will be to serve as a test bed for comparing different approaches, thus facilitating research.","Language Resources and Evaluation",2016,"No","sentiment analysi twitter semev develop success semev task sentiment analysi twitter social media text present develop evalu semant analysi task lie intersect trendi line research contemporari comput linguist sentiment analysi natur languag process social media text task part semev intern workshop semant evalu semant evalu forum previous sensev task ran attract highest number particip team semev year ongo edit task includ creation larg contextu messag level polar corpus consist tweet sms messag livejourn messag special test set sarcast tweet evalu attract team varieti approach team outperform baselin sizabl margin improv year task run hope long last role task accompani dataset serv test bed compar approach facilit research",0
"KeywordsLinguistic annotation Semantic role labelling Frame semantics Semi-automatic annotation ","is it worth the effort assessing the benefits of partial automatic pre labeling for frame semantic annotation","Corpora with high-quality linguistic annotations are an essential component in many NLP applications and a valuable resource for linguistic research. For obtaining these annotations, a large amount of manual effort is needed, making the creation of these resources time-consuming and costly. One attempt to speed up the annotation process is to use supervised machine-learning systems to automatically assign (possibly erroneous) labels to the data and ask human annotators to correct them where necessary. However, it is not clear to what extent these automatic pre-annotations are successful in reducing human annotation effort, and what impact they have on the quality of the resulting resource. In this article, we present the results of an experiment in which we assess the usefulness of partial semi-automatic annotation for frame labeling. We investigate the impact of automatic pre-annotation of differing quality on annotation time, consistency and accuracy. While we found no conclusive evidence that it can speed up human annotation, we found that automatic pre-annotation does increase its overall quality.","Language Resources and Evaluation",2012,"No","linguist annot semant role label frame semant semi automat annot worth effort assess benefit partial automat pre label frame semant annot corpora high qualiti linguist annot essenti compon nlp applic valuabl resourc linguist research obtain annot larg amount manual effort need make creation resourc time consum cost attempt speed annot process supervis machin learn system automat assign possibl erron label data human annot correct clear extent automat pre annot success reduc human annot effort impact qualiti result resourc articl present result experi assess use partial semi automat annot frame label investig impact automat pre annot differ qualiti annot time consist accuraci found conclus evid speed human annot found automat pre annot increas qualiti",0
"KeywordsPropBank Finnish SRL ","the finnish proposition bank","We present the Finnish PropBank, a resource for semantic role labeling (SRL) of Finnish based on the Turku Dependency Treebank whose syntax is annotated in the well-known Stanford Dependency (SD) scheme. The contribution of this paper consists of the lexicon of the verbs and their arguments present in the treebank, as well as the predicate-argument annotation of all verb occurrences in the treebank text. We demonstrate that the annotation is of high quality, that the SD scheme is highly compatible with PropBank annotation, and further that the additional dependencies present in the Turku Dependency Treebank are clearly beneficial for PropBank annotation. Further, we also use the PropBank to provide a strong baseline for automated Finnish SRL using a machine learning SRL system developed for the SemEval’14 shared task on broad-coverage semantic dependency parsing. The PropBank as well as the SRL system are available under a free license at http://bionlp.utu.fi/.","Language Resources and Evaluation",2015,"No","propbank finnish srl finnish proposit bank present finnish propbank resourc semant role label srl finnish base turku depend treebank syntax annot stanford depend sd scheme contribut paper consist lexicon verb argument present treebank predic argument annot verb occurr treebank text demonstr annot high qualiti sd scheme high compat propbank annot addit depend present turku depend treebank benefici propbank annot propbank provid strong baselin autom finnish srl machin learn srl system develop semev share task broad coverag semant depend pars propbank srl system free licens httpbionlputufi",0
"KeywordsAutomatic keyphrase extraction Test collections Annotator disagreement ","creation and evaluation of large keyphrase extraction collections with multiple opinions","While several automatic keyphrase extraction (AKE) techniques have been developed and analyzed, there is little consensus on the definition of the task and a lack of overview of the effectiveness of different techniques.
 Proper evaluation of keyphrase extraction requires large test collections with multiple opinions, currently not available for research. In this paper, we (i) present a set of test collections derived from various sources with multiple annotations (which we also refer to as opinions in the remained of the paper) for each document, (ii) systematically evaluate keyphrase extraction using several supervised and unsupervised AKE techniques, (iii) and experimentally analyze the effects of disagreement on AKE evaluation. Our newly created set of test collections spans different types of topical content from general news and magazines, and is annotated with multiple annotations per article by a large annotator panel. Our annotator study shows that for a given document there seems to be a large disagreement on the preferred keyphrases, suggesting the need for multiple opinions per document. A first systematic evaluation of ranking and classification of keyphrases using both unsupervised and supervised AKE techniques on the test collections shows a superior effectiveness of supervised models, even for a low annotation effort and with basic positional and frequency features, and highlights the importance of a suitable keyphrase candidate generation approach. We also study the influence of multiple opinions, training data and document length on evaluation of keyphrase extraction.
 Our new test collection for keyphrase extraction is one of the largest of its kind and will be made available to stimulate future work to improve reliable evaluation of new keyphrase extractors.","Language Resources and Evaluation",2018,"No","automat keyphras extract test collect annot disagr creation evalu larg keyphras extract collect multipl opinion automat keyphras extract ake techniqu develop analyz consensus definit task lack overview effect techniqu proper evalu keyphras extract requir larg test collect multipl opinion research paper present set test collect deriv sourc multipl annot refer opinion remain paper document ii systemat evalu keyphras extract supervis unsupervis ake techniqu iii experiment analyz effect disagr ake evalu newli creat set test collect span type topic content general news magazin annot multipl annot articl larg annot panel annot studi show document larg disagr prefer keyphras suggest multipl opinion document systemat evalu rank classif keyphras unsupervis supervis ake techniqu test collect show superior effect supervis model low annot effort basic posit frequenc featur highlight import suitabl keyphras candid generat approach studi influenc multipl opinion train data document length evalu keyphras extract test collect keyphras extract largest kind made stimul futur work improv reliabl evalu keyphras extractor",0
"Key Wordsproper names lexicon acquisition text understanding news text ","the analysis and acquisition of proper names for the understanding of free text","Proper Names (PNs) present a problem for the automatic processing and understanding of naturally occurring text. Due to their poor coverage in existing lexical resources and the continual appearance of new names, they represent a large body of unknown lexical data. Moreover, the complexity of the constructions in which they can appear and their own internal structure make them difficult to process, even if they are initially known. Yet the successful analysis of names is often crucial to the full understanding of a text. This paper proposes a solution to the problem and describes a natural language processing (NLP) system, FUMES, which makes use of the internal structure of names and the descriptive information that regularly accompanies them to produce lexical and knowledge base entries for unknown PNs. We present some preliminary results showing the viability of this approach for the identification of proper names.","Computers and the Humanities",1992,"No","key wordsprop name lexicon acquisit text understand news text analysi acquisit proper name understand free text proper name pns present problem automat process understand natur occur text due poor coverag exist lexic resourc continu appear name repres larg bodi unknown lexic data complex construct intern structur make difficult process initi success analysi name crucial full understand text paper propos solut problem describ natur languag process nlp system fume make intern structur name descript inform regular accompani produc lexic knowledg base entri unknown pns present preliminari result show viabil approach identif proper name",0
"Chadwyck-Healey electronic publishing English literature SGML ","literature online building a home for english and american literature on the world wide web","Chadwyck-Healey has a long tradition of electronic publishing. Beginning with production of CD-based literary corpora, it has recently moved many of its products to a web-accessible online environment. The article reflects on experiences with both CD and web-based publications.","Computers and the Humanities",1998,"No","chadwyck healey electron publish english literatur sgml literatur onlin build home english american literatur world wide web chadwyck healey long tradit electron publish begin product cd base literari corpora recent move product web access onlin environ articl reflect experi cd web base public",0
"KeywordsRepresentation System Current System Knowledge Representation Computational Linguistic Description Logic ","using the right tools enhancing retrieval from marked up documents","We are experimenting with the representation of a DTD and associated documents (i.e., documents conformant to the DTD) in a knowledge representation (KR) system, in order to provide more sophisticated query and retrieval from TEI documents than current systems provide. We are using CLASSIC, a frame-based representation system developed at AT&T Bell Laboratories. Like many KR systems, CLASSIC enables the definition of structured concepts/frames, their organization into taxonomies, the creation and manipulation of individual instances of such concepts, and inference such as inheritance, relation transitivity, inverses, etc. In addition, CLASSIC provides for the key inferences of subsumption and classification. By representing a document as an individual instance of a hierarchy of concepts derived from the DTD, and by allowing the creation of additional user-defined concepts and relations, sophisticated query and retrieval operations can be performed. This paper describes CLASSIC and the formalism of description logic that underlies it, and demonstrates how it can be used for enhanced retrieval from richly encoded documents.","Computers and the Humanities",1999,"No","represent system current system knowledg represent comput linguist descript logic tool enhanc retriev mark document experi represent dtd document document conform dtd knowledg represent kr system order provid sophist queri retriev tei document current system provid classic frame base represent system develop bell laboratori kr system classic enabl definit structur conceptsfram organ taxonomi creation manipul individu instanc concept infer inherit relat transit invers addit classic key infer subsumpt classif repres document individu instanc hierarchi concept deriv dtd allow creation addit user defin concept relat sophist queri retriev oper perform paper describ classic formal descript logic under demonstr enhanc retriev rich encod document",0
"KeywordsComputational Linguistic ","questions of authorship attribution and beyond a lecture delivered on the occasion of the roberto busa award ach allc 2001 new york",NA,"Computers and the Humanities",2003,"No","comput linguist question authorship attribut lectur deliv occas roberto busa award ach allc york na",0
"XML SGML TEI markup languages ","xml and the tei","Electronic texts are claimed to exhibit features distinct from their more tangible cousins. The Snapshot project aims to observe and capture language usage in an electronic medium by creating an open corpus of World Wide Web documents. These documents are re-encoded using the TEI guidelines to create a flexible, persistent and portable data repository. This report gives an overview of the decisions made with respect to the re-encoding of HTML documents, and with the structuring the overall corpus.","Computers and the Humanities",1999,"No","xml sgml tei markup languag xml tei electron text claim exhibit featur distinct tangibl cousin snapshot project aim observ captur languag usag electron medium creat open corpus world wide web document document encod tei guidelin creat flexibl persist portabl data repositori report overview decis made respect encod html document structur corpus",0
"Key Wordsnatural language generation linguistic modelling attribute grammar applied linguistics language learning ","a system for natural language sentence generation","This paper describes a natural language generation system known as VINCI, which accepts as input a formal description of some subset of a natural language, and generates strings in the language. With the help of an attribute grammar formalism, the system can be used to simulate on a computer components of several current linguistic theories. The program, implemented in C, runs under a variety of operating systems, including UNIX, MS-DOS and VM/CMS. In this paper we consider not only the design of the system, but also some of its applications in linguistic modelling and second language acquisition research.","Computers and the Humanities",1992,"No","key wordsnatur languag generat linguist model attribut grammar appli linguist languag learn system natur languag sentenc generat paper describ natur languag generat system vinci accept input formal descript subset natur languag generat string languag attribut grammar formal system simul comput compon current linguist theori program implement run varieti oper system includ unix ms dos vmcms paper design system applic linguist model languag acquisit research",0
"Key Wordsworkstation κλεlω data models WORM fuzzy structures lemmatization family reconstitution historical microanalysis ","the historical workstation project","Since 1978 research in the development of software dedicated to the specific problems of historical research has been undertaken at the Max-Planck-Institute für Geschichte in Göttingen. From a background of practical experiences during these years, a concept of what an appropriate ‘workstation” for an historian would be has been derived. It stresses the necessity of three components: (a) software, derived from a detailed analysis of what differentiates information contained in historical sources from such present in current material, (b) databases which are as easily available as printed books and (c) knowledge bases which allow software and data bases to draw upon the information contained in historical reference works. A loose network of European research projects, dedicated to the realization of such a setup, is described.","Computers and the Humanities",1991,"No","key wordsworkst data model worm fuzzi structur lemmat famili reconstitut histor microanalysi histor workstat project research develop softwar dedic specif problem histor research undertaken max planck institut geschicht ttingen background practic experi year concept workstat historian deriv stress necess compon softwar deriv detail analysi differenti inform contain histor sourc present current materi databas easili print book knowledg base softwar data base draw inform contain histor refer work loos network european research project dedic realiz setup",0
"KeywordsMultimodal corpora Embodied conversational agents Gesture generation Human–computer interaction ","an annotation scheme for conversational gestures how to economically capture timing and form","The empirical investigation of human gesture stands at the center of multiple research disciplines, and various gesture annotation schemes exist, with varying degrees of precision and required annotation effort. We present a gesture annotation scheme for the specific purpose of automatically generating and animating character-specific hand/arm gestures, but with potential general value. We focus on how to capture temporal structure and locational information with relatively little annotation effort. The scheme is evaluated in terms of how accurately it captures the original gestures by re-creating those gestures on an animated character using the annotated data. This paper presents our scheme in detail and compares it to other approaches.","Language Resources and Evaluation",2007,"No","multimod corpora embodi convers agent gestur generat human comput interact annot scheme convers gestur econom captur time form empir investig human gestur stand center multipl research disciplin gestur annot scheme exist vari degre precis requir annot effort present gestur annot scheme specif purpos automat generat anim charact specif handarm gestur potenti general focus captur tempor structur locat inform annot effort scheme evalu term accur captur origin gestur creat gestur anim charact annot data paper present scheme detail compar approach",0
"KeywordsTarget Word Natural Language Processing Word Sense Disambiguation Collective Intelligence Language Resource ","collective intelligence and language resources introduction to the special issue on collaboratively constructed language resources","In recent years, collective intelligence has become a field of active research due to the rise of Web 2.0 and the availability of Web-based technologies that support distributed collaboration. Malone et al. (2009) define collective intelligence broadly as “groups of individuals acting collectively in ways that seem intelligent.” The applications of this phenomenon are wide-reaching: recent publications (Malone 2004; Howe 2008; Surowiecki 2004; Benkler 2006; Tapscott and Williams 2006), and a compendium of nearly 250 examples of Web-based collective intelligence collected by the MIT Center for Collective Intelligence 1 clearly demonstrate the diversity of ways in which collective intelligence can be applied. The field is now about to consolidate itself and launch its own conference which will be held for the first time in 2012. 2","Language Resources and Evaluation",2013,"No","target word natur languag process word sens disambigu collect intellig languag resourc collect intellig languag resourc introduct special issu collabor construct languag resourc recent year collect intellig field activ research due rise web avail web base technolog support distribut collabor malon al defin collect intellig broad group individu act collect way intellig applic phenomenon wide reach recent public malon howe surowiecki benkler tapscott william compendium exampl web base collect intellig collect mit center collect intellig demonstr divers way collect intellig appli field consolid launch confer held time",0
"KeywordsVerification Problem Plagiarism Detection Information Retrieval Method Multivariate Discriminant Analysis Statistical Machine Learn ","plagiarism and authorship analysis introduction to the special issue","The Internet has facilitated both the dissemination of anonymous texts as well as easy “borrowing” of ideas and words of others. This has raised a number of important questions regarding authorship. Can we identify the anonymous author of a text by comparing the text with the writings of known authors? Can we determine if a text, or parts of it, has been plagiarized? Such questions are clearly of both academic and commercial importance.","Language Resources and Evaluation",2011,"No","verif problem plagiar detect inform retriev method multivari discrimin analysi statist machin learn plagiar authorship analysi introduct special issu internet facilit dissemin anonym text easi borrow idea word rais number import question authorship identifi anonym author text compar text write author determin text part plagiar question academ commerci import",0
"KeywordsInformation retrieval Text summarization Crowdsourcing services Crowdflower Mechanical Turk ","analyzing the capabilities of crowdsourcing services for text summarization","This paper presents a detailed analysis of the use of crowdsourcing services for the Text Summarization task in the context of the tourist domain. In particular, our aim is to retrieve relevant information about a place or an object pictured in an image in order to provide a short summary which will be of great help for a tourist. For tackling this task, we proposed a broad set of experiments using crowdsourcing services that could be useful as a reference for others who want to rely also on crowdsourcing. From the analysis carried out through our experimental setup and the results obtained, we can conclude that although crowdsourcing services were not good to simply gather gold-standard summaries (i.e., from the results obtained for experiments 1, 2 and 4), the encouraging results obtained in the third and sixth experiments motivate us to strongly believe that they can be successfully employed for finding some patterns of behaviour humans have when generating summaries, and for validating and checking other tasks. Furthermore, this analysis serves as a guideline for the types of experiments that might or might not work when using crowdsourcing in the context of text summarization.","Language Resources and Evaluation",2013,"No","inform retriev text summar crowdsourc servic crowdflow mechan turk analyz capabl crowdsourc servic text summar paper present detail analysi crowdsourc servic text summar task context tourist domain aim retriev relev inform place object pictur imag order provid short summari great tourist tackl task propos broad set experi crowdsourc servic refer reli crowdsourc analysi carri experiment setup result obtain conclud crowdsourc servic good simpli gather gold standard summari result obtain experi encourag result obtain sixth experi motiv strong success employ find pattern behaviour human generat summari valid check task analysi serv guidelin type experi work crowdsourc context text summar",0
"KeywordsLexical resources Swedish WordNet Interoperability LMF SALDO ","saldo a touch of yin to wordnets yang","The English-language Princeton WordNet (PWN) and some wordnets for other languages have been extensively used as lexical–semantic knowledge sources in language technology applications, due to their free availability and their size. The ubiquitousness of PWN-type wordnets tends to overshadow the fact that they represent one out of many possible choices for structuring a lexical–semantic resource, and it could be enlightening to look at a differently structured resource both from the point of view of theoretical–methodological considerations and from the point of view of practical text processing requirements. The resource described here—SALDO—is such a lexical–semantic resource, intended primarily for use in language technology applications, and offering an alternative organization to PWN-style wordnets. We present our work on SALDO, compare it with PWN, and discuss some implications of the differences. We also describe an integrated infrastructure for computational lexical resources where SALDO forms the central component.","Language Resources and Evaluation",2013,"No","lexic resourc swedish wordnet interoper lmf saldo saldo touch yin wordnet yang english languag princeton wordnet pwn wordnet languag extens lexic semant knowledg sourc languag technolog applic due free avail size ubiquit pwn type wordnet overshadow fact repres choic structur lexic semant resourc enlighten differ structur resourc point view theoret methodolog consider point view practic text process requir resourc saldo lexic semant resourc intend primarili languag technolog applic offer altern organ pwn style wordnet present work saldo compar pwn discuss implic differ describ integr infrastructur comput lexic resourc saldo form central compon",0
"KeywordsSemantic annotation Software requirements Semantic role labeling ","software requirements as an application domain for natural language processing","Mapping functional requirements first to specifications and then to code is one of the most challenging tasks in software development. Since requirements are commonly written in natural language, they can be prone to ambiguity, incompleteness and inconsistency. Structured semantic representations allow requirements to be translated to formal models, which can be used to detect problems at an early stage of the development process through validation. Storing and querying such models can also facilitate software reuse. Several approaches constrain the input format of requirements to produce specifications, however they usually require considerable human effort in order to adopt domain-specific heuristics and/or controlled languages. We propose a mechanism that automates the mapping of requirements to formal representations using semantic role labeling. We describe the first publicly available dataset for this task, employ a hierarchical framework that allows requirements concepts to be annotated, and discuss how semantic role labeling can be adapted for parsing software requirements.
","Language Resources and Evaluation",2017,"No","semant annot softwar requir semant role label softwar requir applic domain natur languag process map function requir specif code challeng task softwar develop requir common written natur languag prone ambigu incomplet inconsist structur semant represent requir translat formal model detect problem earli stage develop process valid store queri model facilit softwar reus approach constrain input format requir produc specif requir consider human effort order adopt domain specif heurist control languag propos mechan autom map requir formal represent semant role label describ public dataset task employ hierarch framework requir concept annot discuss semant role label adapt pars softwar requir",0
"KeywordsLanguage technology Multilingual technologies Machine translation Language resources META-NET META-SHARE ","the strategic impact of meta net on the regional national and international level","This article provides an overview of the dissemination work carried out in META-NET from 2010 until 2015; we describe its impact on the regional, national and international level, mainly with regard to politics and the funding situation for LT topics. The article documents the initiative’s work throughout Europe in order to boost progress and innovation in our field.","Language Resources and Evaluation",2016,"No","languag technolog multilingu technolog machin translat languag resourc meta net meta share strateg impact meta net region nation intern level articl overview dissemin work carri meta net describ impact region nation intern level regard polit fund situat lt topic articl document initi work europ order boost progress innov field",0
"KeywordsPart–whole relations Meronymy Holonymy German wordnet GermaNet Compounds Compound-internal relations ","using partwhole relations for automatic deduction of compound internal relations in germanet","This paper provides a deduction-based approach for automatically classifying compound-internal relations in GermaNet, the German version of the Princeton WordNet for English. More specifically, meronymic relations between simplex  and compound nouns provide the necessary input to the deduction patterns that involve different types of compound-internal relations. The scope of these deductions extends to all four meronymic relations modeled in version 6.0 of GermaNet: component, member, substance, and portion. This deduction-based approach provides an effective method for automatically enriching the set of semantic relations included in GermaNet.","Language Resources and Evaluation",2013,"No","part relat meronymi holonymi german wordnet germanet compound compound intern relat partwhol relat automat deduct compound intern relat germanet paper deduct base approach automat classifi compound intern relat germanet german version princeton wordnet english specif meronym relat simplex compound noun provid input deduct pattern involv type compound intern relat scope deduct extend meronym relat model version germanet compon member substanc portion deduct base approach effect method automat enrich set semant relat includ germanet",0
"KeywordsComputational Linguistic ","letter to the editor shakespeare concordance",NA,"Computers and the Humanities",1972,"No","comput linguist letter editor shakespear concord na",0
NA,"the ach page",NA,"Computers and the Humanities",2001,"No","ach page na",0
NA,"contents of volume 37",NA,"Computers and the Humanities",2003,"No","content volum na",0
"KeywordsComputational Linguistic ","the annals of computing the greek testament",NA,"Computers and the Humanities",1980,"No","comput linguist annal comput greek testament na",0
"KeywordsComputational Linguistic ","computers and historians past present and future",NA,"Computers and the Humanities",1996,"No","comput linguist comput historian past present futur na",0
"KeywordsHybrid System Specific Task Similar System Computational Linguistic Knowledge Source ","large scale wsd using learning applied to senseval","A word sense disambiguation system which is going to be used aspart of a NLP system needs to be large scale, able to beoptimised towards a specific task and above all accurate. This paperdescribes the knowledge sources used in a disambiguation system able toachieve all three of these criteria. It is a hybrid system combining sub-symbolic, stochastic and rule-based learning. The paper reportsthe results achieved in Senseval and analyses them to show the system'sstrengths and weaknesses relative to other similar systems.","Computers and the Humanities",2000,"No","hybrid system specif task similar system comput linguist knowledg sourc larg scale wsd learn appli sensev word sens disambigu system aspart nlp system larg scale beoptimis specif task accur paperdescrib knowledg sourc disambigu system toachiev criteria hybrid system combin symbol stochast rule base learn paper reportsth result achiev sensev analys show systemsstrength weak relat similar system",0
"KeywordsHumanity Program Computational Linguistic ","humanities programs available",NA,"Computers and the Humanities",1973,"No","human program comput linguist human program na",0
"Key Wordshumanities computing concordances databases literary criticism social history musicology art history lexicography ","humanities computing 25 years later","This paper attempts to provide an overview of the development of humanities computing during the past twenty-five years. Mention is made of the major applications of the computer to humanities disciplines, and of the most important and representative projects across the world.","Computers and the Humanities",1991,"No","key wordshuman comput concord databas literari critic social histori musicolog art histori lexicographi human comput year paper attempt provid overview develop human comput past twenti year mention made major applic comput human disciplin import repres project world",0
"KeywordsComputational Linguistic Oxford English Dictionary ","creating the electronic new oxford english dictionary",NA,"Computers and the Humanities",1986,"No","comput linguist oxford english dictionari creat electron oxford english dictionari na",0
"Key wordstext as vision space dimensions correspondence analysis concordances contexts Kierkegaard ","the multi dimensional concordance a new tool for literary research","This paper describes the use of correspondence analysis to create the “space” of a book, constructs that of Kierkegaard'sFear and Trembling as an illustration, and distinguishes three separate contexts of some of its most important words: thespatial context (where the search word lies in that named and ordered space); theoverall context (the x words closest to the search word in multi-dimensional space); and the “role/sense” context (the words associated with the search word in each of its most important roles, some of which may represent new senses.) It describes the identification of these contexts, discusses their importance and concludes by noting certain respects in which the procedure might perhaps be improved.","Computers and the Humanities",1993,"No","key wordstext vision space dimens correspond analysi concord context kierkegaard multi dimension concord tool literari research paper describ correspond analysi creat space book construct kierkegaardsfear trembl illustr distinguish separ context import word thespati context search word lie name order space theoveral context word closest search word multi dimension space rolesens context word search word import role repres sens describ identif context discuss import conclud note respect procedur improv",0
"electronic texts literary criticism reception theory versioning ","computer mediated texts and textuality theory and practice","The majority of humanities computingprojects within the discipline of literaturehave been conceived more as digital librariesthan monographs which utilise the medium as asite of interpretation. The impetus to conceiveelectronic research in this way comes from theunderlying philosophy of texts and textualityimplicit in SGML and its instantiation for thehumanities, the TEI, which was conceived as ``amarkup system intended for representing alreadyexisting literary texts''. This article exploresthe most common theories used to conceiveelectronic research in literature, such ashypertext theory, OCHO (Ordered Hierarchy ofContent Objects), and Jerome J. McGann's``noninformational'' forms of textuality. It alsoargues that as our understanding of electronictexts and textuality deepens, and as advancesin technology progresses, other theories, suchas Reception Theory and Versioning, may well beadapted to serve as a theoretical basis forconceiving research more akin to an electronicmonograph than a digital library.","Computers and the Humanities",2002,"No","electron text literari critic recept theori version comput mediat text textual theori practic major human computingproject disciplin literaturehav conceiv digit librariesthan monograph utilis medium asit interpret impetus conceiveelectron research theunder philosophi text textualityimplicit sgml instanti thehuman tei conceiv amarkup system intend repres alreadyexist literari text articl exploresth common theori conceiveelectron research literatur ashypertext theori ocho order hierarchi ofcont object jerom mcgannnoninform form textual alsoargu understand electronictext textual deepen advancesin technolog progress theori sucha recept theori version beadapt serv theoret basi forconceiv research akin electronicmonograph digit librari",0
"Key WordsCD-ROM TLG Project Thesaurus Linguae Graecae Isocrates Project IRIS Ibycus Scholarly Computer Perseus Project Septuagint Studies Project Packard Humanities Foundation ","cd rom and scholarly research in the humanities","The conversion of all classical literature for the period of Homer in the 8th century B.C. through the 6th century A.D. into machine-readable format — designated the Thesaurus Linguae Graecae project — was the impetus behind the use of classical literature in a variety of electronic research environments. Initially targeted for mainframe storage and retrieval, the data is now also being published and distributed on CD-ROM for use with microcomputers. Two such projects, the TLG Project at the University of California-Trvine and the Isocrates Project at Brown University's IRIS Center are described as well as other CD-ROM projects for the storage and dissemination of literature in the humanities and classical research. Various CD-ROM systems are also described, including the Ibycus Scholarly Computer.","Computers and the Humanities",1988,"No","key wordscd rom tlg project thesaurus lingua graeca isocr project iri ibycus scholar comput perseus project septuagint studi project packard human foundat cd rom scholar research human convers classic literatur period homer th centuri th centuri machin readabl format design thesaurus lingua graeca project impetus classic literatur varieti electron research environ initi target mainfram storag retriev data publish distribut cd rom microcomput project tlg project univers california trvine isocr project brown univers iri center cd rom project storag dissemin literatur human classic research cd rom system includ ibycus scholar comput",0
"KeywordsComputational Linguistic ","some notions of similarity among lines of text",NA,"Computers and the Humanities",1977,"No","comput linguist notion similar line text na",0
"Key WordsCéline computer-aided literary analysis concordances ","computerizing cline","This article uses recent work on the computer-aided analysis of texts by the French writer Céline as a framework to discuss Olsen's paper on the current state of computer-aided literary analysis. Drawing on analysis of syntactic structures, lexical creativity and use of proper names, it makes two points: (1) given a rich theoretical framework and sufficiently precise models, even simple computer tools such as text editors and concordances can make a valuable contribution to literary scholarship; (2) it is important to view the computer not as a device for finding what we as readers have failed to notice, but rather as a means of focussing more closely on what we have already felt as readers, and of verifying hypotheses we have produced as researchers.","Computers and the Humanities",1993,"No","key wordsc line comput aid literari analysi concord computer cline articl recent work comput aid analysi text french writer line framework discuss olsen paper current state comput aid literari analysi draw analysi syntact structur lexic creativ proper name make point rich theoret framework suffici precis model simpl comput tool text editor concord make valuabl contribut literari scholarship import view comput devic find reader fail notic mean focuss close felt reader verifi hypothes produc research",0
"Key WordsMallarmé object on-line poetry semantics semiotics semiology sign text analysis truth ","texts on line","The study of signs is divided between those scholars who use the Saussurian binary sign (semiology) and those who prefer Charles Peirce's tripartite sign (semiotics). The common view of the opposition between the two types of signs does not take into consideration the methodological conditions of applicability of these two types of signs. This is particularly important in the field of literary studies and hence for the preparation of electronic programs for text analysis. The Peircian sign explicitly entails the discovery of a truth of meaning that claims to be universal and not reducible to a collection of opinions based on fragmented information; it also imposes the task of elucidating a transhistorical and universal significantion encoded in a text. Contrary to Peirce's view of the sign, our use of computer programs for text analysis, however, demonstrates that we implicitly treat every literary text as a set of linguistic data (letters, phonemes, syntagmatic segments, etc.) which are reducible to units that can be treated separately. A brief comparison of the results obtained from computer analyses of the French poet Stéphane Mallarmé's text, “Le Cygne,” with those obtained from two Peircian analyses (by Riffaterre and Champigny) of the same text demonstrates that our current methods of computer textual analysis are based on a Saussurian semiology, which is unidimensional and limited, and that these methods are still quite unable to produce a semiotic interpretation based on a totalizing hierarchy of the text's various discursive components.","Computers and the Humanities",1993,"No","key wordsmallarm object line poetri semant semiot semiolog sign text analysi truth text line studi sign divid scholar saussurian binari sign semiolog prefer charl peirc tripartit sign semiot common view opposit type sign consider methodolog condit applic type sign import field literari studi prepar electron program text analysi peircian sign explicit entail discoveri truth mean claim univers reduc collect opinion base fragment inform impos task elucid transhistor univers significant encod text contrari peirc view sign comput program text analysi demonstr implicit treat literari text set linguist data letter phonem syntagmat segment reduc unit treat separ comparison result obtain comput analys french poet st phane mallarm text le cygn obtain peircian analys riffaterr champigni text demonstr current method comput textual analysi base saussurian semiolog unidimension limit method unabl produc semiot interpret base total hierarchi text discurs compon",0
"KeywordsComputational Linguistic Phonetic Transcription ","narrow phonetic transcription on the computer taking the phone off the hook",NA,"Computers and the Humanities",1974,"No","comput linguist phonet transcript narrow phonet transcript comput take phone hook na",0
"KeywordsComputational Linguistic Essay Review Computational Stylistic ","respicefinem and thetantum quantum an essay review of computational stylistics for 19671968",NA,"Computers and the Humanities",1968,"No","comput linguist essay review comput stylist respicefinem thetantum quantum essay review comput stylist na",0
"archaeological typology ceramics knowledge acquisition machine learning Sudan ","plata an application of legal a machine learning based system to a typology of archaeological ceramics","The authors here show that machine learning techniques can be used for designing an archaeological typology, at an early stage when the classes are not yet well defined. The program (LEGAL, LEarning with GAlois Lattice) is a machine learning system which uses a set of examples and counter-examples in order to discriminate between classes. Results show a good compatibility between the classes such as the yare defined by the system and the archaeological hypotheses.","Computers and the Humanities",1997,"No","archaeolog typolog ceram knowledg acquisit machin learn sudan plata applic legal machin learn base system typolog archaeolog ceram author show machin learn techniqu design archaeolog typolog earli stage class defin program legal learn galoi lattic machin learn system set exampl counter exampl order discrimin class result show good compat class yare defin system archaeolog hypothes",0
"CCG KCCG Korean Morpho-Syntactic modeling parsing ","korean combinatory categorial grammar and statistical parsing","Korean Combinatory Categorial Grammar (KCCG) is an extendedcombinatory categorial grammar formalism to capture thesyntax and interpretation of a relative freess word order, longdistance scrambling, and other specific characteristics of Korean.KCCG formalism can uniformly handle word order variations amongarguments and adjuncts within a clause, as well as in complexclauses and across clause boundaries, i.e. long distancescrambling. The approach we develop takes advantage of the ability of CCGfor type raising and composition along with the ability of variablecategories and unordered argument modeling for relatively freeword order treatment (Lee et al., 1994; Lee et al., 1997).We apply a probability model and heuristics using Koreancharacteristics to our KCCG parser.Results of the experiments on varioustext genre show that the KCCG parser performsat 87.67/87.03% constituent precision/recall.","Computers and the Humanities",2002,"No","ccg kccg korean morpho syntact model pars korean combinatori categori grammar statist pars korean combinatori categori grammar kccg extendedcombinatori categori grammar formal captur thesyntax interpret relat freess word order longdist scrambl specif characterist koreankccg formal uniform handl word order variat amongargu adjunct claus complexclaus claus boundari long distancescrambl approach develop take advantag abil ccgfor type rais composit abil variablecategori unord argument model freeword order treatment lee al lee al appli probabl model heurist koreancharacterist kccg parserresult experi varioustext genr show kccg parser performsat constitu precisionrecal",0
NA,"quelques rflexions sur le statut epistmologique du texte electronique",NA,"Computers and the Humanities",1985,"No","quelqu rflexion sur le statut epistmologiqu du text electroniqu na",0
"KeywordsGood Predictor Economic Factor Computational Linguistic Important Descriptor Emotional Variable ","the times and the man as predictors of emotion and style in the inaugural addresses of us presidents","Intercorrelations among stylistic and emotional variables and constructvalidity deduced from relationships to other ratings of U.S. presidentssuggest that power language (language that is linguistically simple,emotionally evocative, highly imaged, and rich in references to Americanvalues) is an important descriptor of inaugural addresses. Attempts topredict the use of power language in inaugural addresses from variablesrepresenting the times (year, media, economic factors) and the man(presidential personality) lead to the conclusion that time-basedfactors are the best predictors of the use of such language (81%prediction of variance in the criterion) while presidential personalityadds at most a small amount of prediction to the model. Changes in powerlanguage are discussed as the outcome of a tendency to opt for breadthof communication over depth.","Computers and the Humanities",2001,"No","good predictor econom factor comput linguist import descriptor emot variabl time man predictor emot style inaugur address presid intercorrel stylist emot variabl constructvalid deduc relationship rate presidentssuggest power languag languag linguist simpleemot evoc high imag rich refer americanvalu import descriptor inaugur address attempt topredict power languag inaugur address variablesrepres time year media econom factor manpresidenti person lead conclus time basedfactor predictor languag predict varianc criterion presidenti personalityadd small amount predict model powerlanguag discuss outcom tendenc opt breadthof communic depth",0
"KeywordsMachine Translation Single Authorship Electronic Edition Humanity Computing Attribution Study ","introduction quo vadimus",NA,"Computers and the Humanities",1997,"No","machin translat singl authorship electron edit human comput attribut studi introduct quo vadimus na",0
"KeywordsComputational Linguistic Spanish Dictionary ","computers and the old spanish dictionary",NA,"Computers and the Humanities",1978,"No","comput linguist spanish dictionari comput spanish dictionari na",0
"KeywordsComputational Linguistic Annual Bibliography ","annual bibliography for 1974 and supplement to preceding years",NA,"Computers and the Humanities",1975,"No","comput linguist annual bibliographi annual bibliographi supplement preced year na",0
"optical music recognition musical data acquisition document image analysis pattern recognition ","the challenge of optical music recognition","This article describes the challenges posed by optical musicrecognition – a topic in computer science that aims to convert scannedpages of music into an on-line format. First, the problem is described;then a generalised framework for software is presented that emphasises keystages that must be solved: staff line identification, musical objectlocation, musical feature classification, and musical semantics. Next,significant research projects in the area are reviewed, showing how eachfits the generalised framework. The article concludes by discussingperhaps the most open question in the field: how to compare the accuracy and success of rival systems, highlighting certain steps thathelp ease the task.","Computers and the Humanities",2001,"No","optic music recognit music data acquisit document imag analysi pattern recognit challeng optic music recognit articl describ challeng pose optic musicrecognit topic comput scienc aim convert scannedpag music line format problem generalis framework softwar present emphasis keystag solv staff line identif music objectloc music featur classif music semant signific research project area review show eachfit generalis framework articl conclud discussingperhap open question field compar accuraci success rival system highlight step thathelp eas task",0
"Key Wordsco-occurrence key words library science plus and minus words stylistics statistics ","about the statistical analysis of co occurrence","Various objections are raised against current practice in co-occurrence analysis. The use of Yule's coefficient Y is then advocated.","Computers and the Humanities",1992,"No","key wordsco occurr key word librari scienc minus word stylist statist statist analysi occurr object rais current practic occurr analysi yule coeffici advoc",0
"Key wordsprincipal components analysis Marlowe Shakespeare stylometry style contextual words ","tamburlaine stalks in henry vi","Starting from the accepted premise that Marlowe influenced the young Shakespeare, a selection of strongly contextual words characteristic of Tamburlaine are shown to be reflected in Shakespeare's early history plays. Principal components analysis confirms this. A very similar configuration, however, results when the selected Marlowe-preferred words are non-contextual common words. This feature can not be explained by influence in its conventional sense, particularly when the Shakespeare plays closest to Marlowe are those that share with Marlowe a dearth of selected Shakespeare-preferred common words.","Computers and the Humanities",1996,"No","key wordsprincip compon analysi marlow shakespear stylometri style contextu word tamburlain stalk henri vi start accept premis marlow influenc young shakespear select strong contextu word characterist tamburlain shown reflect shakespear earli histori play princip compon analysi confirm similar configur result select marlow prefer word contextu common word featur explain influenc convent sens shakespear play closest marlow share marlow dearth select shakespear prefer common word",0
"betagraphy codicology database image watermarked paper ","profil an iconographic database for modern watermarked papers","The database Profil has been set up tooffer readers studying modern literarymanuscripts a reference tool to identifywatermarked papers. In the study of writers'drafts as in artists' sketches, the differentkinds of papers used provide valuableinformation on the genesis of a work of art andwatermarks, when they exist, are the bestvisible hint allowing us to identify paper. Amultimedia database, with digitized images moreprecise than usual traced design, seems to beappropriate to register, visualize, and comparemodern watermarked papers. Besides itsusefulness for specialists, such a databasebearing on modern manuscripts should also beconceived in a didactic perspective, as it isoriented towards literary scholars who are notparticularly familiar with the history of modern paper. In this paper we present the database Profilwhich includes a set of digitized images from acollection of betagraphies made by thereproduction service of the National FrenchLibrary. Then we explain problems of databasenormalization when human sciences areinvolved.","Computers and the Humanities",2002,"No","betagraphi codicolog databas imag watermark paper profil iconograph databas modern watermark paper databas profil set tooffer reader studi modern literarymanuscript refer tool identifywatermark paper studi writersdraft artist sketch differentkind paper provid valuableinform genesi work art andwatermark exist bestvis hint allow identifi paper amultimedia databas digit imag moreprecis usual trace design beappropri regist visual comparemodern watermark paper itsus specialist databasebear modern manuscript beconceiv didact perspect isori literari scholar notparticular familiar histori modern paper paper present databas profilwhich includ set digit imag acollect betagraphi made thereproduct servic nation frenchlibrari explain problem databasenorm human scienc areinvolv",0
"KeywordsComputational Linguistic English Drama ","records of early english drama and the computer",NA,"Computers and the Humanities",1978,"No","comput linguist english drama record earli english drama comput na",0
"KeywordsComputational Linguistic ","concordances and indices to middle high german",NA,"Computers and the Humanities",1980,"No","comput linguist concord indic middl high german na",0
"cognates dialects features phonetic alignment phonetic similarity ","phonetic alignment and similarity","The computation of the optimal phonetic alignment andthe phonetic similarity between wordsis an important step in many applications in computational phonology,including dialectometry.After discussing several related algorithms,I present a novel approach to the problem that employsa scoring scheme for computing phonetic similarity between phonetic segmentson the basis of multivalued articulatory phonetic features.The scheme incorporates the key concept of feature salience,which is necessary to properly balance the importance of various features.The new algorithm combines several techniquesdeveloped for sequence comparison:an extended set of edit operations,local and semiglobal modes of alignment,and the capability of retrieving a set of near-optimal alignments.On a set of 82 cognate pairs,it performs better than comparable algorithms reported in the literature.","Computers and the Humanities",2003,"No","cognat dialect featur phonet align phonet similar phonet align similar comput optim phonet align andth phonet similar wordsi import step applic comput phonologyinclud dialectometri discuss relat algorithm present approach problem employsa score scheme comput phonet similar phonet segmentson basi multivalu articulatori phonet featur scheme incorpor key concept featur salienc proper balanc import featur algorithm combin techniquesdevelop sequenc comparison extend set edit operationsloc semiglob mode align capabl retriev set optim align set cognat pair perform compar algorithm report literatur",0
"KeywordsAuthorship attribution Open candidate set Randomized feature set ","authorship attribution in the wild","Most previous work on authorship attribution has focused on the case in which we need to attribute an anonymous document to one of a small set of candidate authors. In this paper, we consider authorship attribution as found in the wild: the set of known candidates is extremely large (possibly many thousands) and might not even include the actual author. Moreover, the known texts and the anonymous texts might be of limited length. We show that even in these difficult cases, we can use similarity-based methods along with multiple randomized feature sets to achieve high precision. Moreover, we show the precise relationship between attribution precision and four parameters: the size of the candidate set, the quantity of known-text by the candidates, the length of the anonymous text and a certain robustness score associated with a attribution.","Language Resources and Evaluation",2011,"No","authorship attribut open candid set random featur set authorship attribut wild previous work authorship attribut focus case attribut anonym document small set candid author paper authorship attribut found wild set candid extrem larg possibl thousand includ actual author text anonym text limit length show difficult case similar base method multipl random featur set achiev high precis show precis relationship attribut precis paramet size candid set quantiti text candid length anonym text robust score attribut",0
"Keywordsdialogue systems evaluation machine learning ","can we talk methods for evaluation and training of spoken dialogue systems","There is a strong relationship between evaluation and methods for automatically training language processing systems, where generally the same resource and metrics are used both to train system components and to evaluate them. To date, in dialogue systems research, this general methodology is not typically applied to the dialogue manager and spoken language generator. However, any metric for evaluating system performance can be used as a feedback function for automatically training the system. This approach is motivated with examples of the application of reinforcement learning to dialogue manager optimization, and the use of boosting to train the spoken language generator.","Language Resources and Evaluation",2005,"No","dialogu system evalu machin learn talk method evalu train spoken dialogu system strong relationship evalu method automat train languag process system general resourc metric train system compon evalu date dialogu system research general methodolog typic appli dialogu manag spoken languag generat metric evalu system perform feedback function automat train system approach motiv exampl applic reinforc learn dialogu manag optim boost train spoken languag generat",0
"KeywordsLinguistic annotation Annotation tools Inter-operability ","inter operability and reusability the science of annotation","Annotating linguistic data has become a major field of interest, both for supplying the necessary data for machine learning approaches to NLP applications, and as a research issue in its own right. This comprises issues of technical formats, tools, and methodologies of annotation. We provide a brief overview of these notions and then introduce the papers assembled in this special issue.","Language Resources and Evaluation",2012,"No","linguist annot annot tool inter oper inter oper reusabl scienc annot annot linguist data major field interest suppli data machin learn approach nlp applic research issu compris issu technic format tool methodolog annot provid overview notion introduc paper assembl special issu",0
"KeywordsFeature selection MDL Clustering Word senses Text processing ","word sense learning based on feature selection and mdl principle","In this paper, we propose a word sense learning algorithm which is capable of unsupervised feature selection and cluster number identification. Feature selection for word sense learning is built on an entropy-based filter and formalized as a constraint optimization problem, the output of which is a set of important features. Cluster number identification is built on a Gaussian mixture model with a MDL-based criterion, and the optimal model order is inferred by minimizing the criterion. To evaluate closeness between the learned sense clusters with the ground-truth classes, we introduce a kind of weighted F-measure to model the effort needed to reconstruct the classes from the clusters. Experiments show that the algorithm can retrieve important features, roughly estimate the class numbers automatically and outperforms other algorithms in terms of the weighted F-measure. In addition, we also try to apply the algorithm to a specific task of adding new words into a Chinese thesaurus.","Language Resources and Evaluation",2006,"No","featur select mdl cluster word sens text process word sens learn base featur select mdl principl paper propos word sens learn algorithm capabl unsupervis featur select cluster number identif featur select word sens learn built entropi base filter formal constraint optim problem output set import featur cluster number identif built gaussian mixtur model mdl base criterion optim model order infer minim criterion evalu close learn sens cluster ground truth class introduc kind weight measur model effort need reconstruct class cluster experi show algorithm retriev import featur rough estim class number automat outperform algorithm term weight measur addit appli algorithm specif task ad word chines thesaurus",0
"Key wordsTEI text encoding encoding schemes electronic text markup language tagging SGML ","the design of the tei encoding scheme","This paper discusses the basic design of the encoding scheme described by the Text Encoding Initiative'sGuidelines for Electronic Text Encoding and Interchange (TEI document number TEI P3, hereafter simplyP3 orthe Guidelines). It first reviews the basic design goals of the TEI project and their development during the course of the project. Next, it outlines some basic notions relevant for the design of any markup language and uses those notions to describe the basic structure of the TEI encoding scheme. It also describes briefly the “core” tag set defined in chapter 6 of P3, and the “default text structure” defined in chapter 7 of that work. The final section of the paper attempts an evaluation of P3 in the light of its original design goals, and outlines areas in which further work is still needed.","Computers and the Humanities",1995,"No","key wordstei text encod encod scheme electron text markup languag tag sgml design tei encod scheme paper discuss basic design encod scheme text encod initiativesguidelin electron text encod interchang tei document number tei p simplyp orth guidelin review basic design goal tei project develop project outlin basic notion relev design markup languag notion describ basic structur tei encod scheme describ briefli core tag set defin chapter p default text structur defin chapter work final section paper attempt evalu p light origin design goal outlin area work need",0
"word sense disambiguation decision lists supervised machine learning lexical ambiguity resolution SENSEVAL ","hierarchical decision lists for word sense disambiguation","This paper describes a supervised algorithm for word sensedisambiguation based on hierarchies of decision lists. This algorithmsupports a useful degree of conditional branching while minimizing thetraining data fragmentation typical of decision trees. Classificationsare based on a rich set of collocational, morphological and syntacticcontextual features, extracted automatically from training data andweighted sensitive to the nature of the feature and feature class. Thealgorithm is evaluated comprehensively in the SENSEVAL framework,achieving the top performance of all participating supervised systems onthe 36 test words where training data is available.","Computers and the Humanities",2000,"No","word sens disambigu decis list supervis machin learn lexic ambigu resolut sensev hierarch decis list word sens disambigu paper describ supervis algorithm word sensedisambigu base hierarchi decis list algorithmsupport degre condit branch minim thetrain data fragment typic decis tree classificationsar base rich set colloc morpholog syntacticcontextu featur extract automat train data andweight sensit natur featur featur class thealgorithm evalu comprehens sensev frameworkachiev top perform particip supervis system onth test word train data",0
"KeywordsComputational Linguistic Winged Word ","winged words varieties of computer application to literature",NA,"Computers and the Humanities",1967,"No","comput linguist wing word wing word varieti comput applic literatur na",0
"KeywordsCorrection ranking Soundex Shapex Spelling error correction Urdu ","a novel approach for ranking spelling error corrections for urdu","This paper presents a scheme for ranking of spelling error corrections for Urdu. Conventionally spell-checking techniques do not provide any explicit ranking mechanism. Ranking is either implicit in the correction algorithm or corrections are not ranked at all. The research presented in this paper shows that for Urdu, phonetic similarity between the corrections and the erroneous word can serve as a useful parameter for ranking the corrections. This combined with a new technique Shapex that uses visual similarity of characters for ranking gives an improvement of 23% in the accuracy of the one-best match compared to the result obtained when the ranking is done on the basis of word frequencies only.","Language Resources and Evaluation",2007,"No","correct rank soundex shapex spell error correct urdu approach rank spell error correct urdu paper present scheme rank spell error correct urdu convent spell check techniqu provid explicit rank mechan rank implicit correct algorithm correct rank research present paper show urdu phonet similar correct erron word serv paramet rank correct combin techniqu shapex visual similar charact rank improv accuraci match compar result obtain rank basi word frequenc",0
"KeywordsSwahili Multilinguality Information extraction Named entity recognition and classification Geo-tagging Quotation recognition Date recognition Subject domain classification News analysis Media monitoring ","expanding a multilingual media monitoring and information extraction tool to a new language swahili","The Europe Media Monitor (EMM) family of applications is a set of multilingual tools that gather, cluster and classify news in currently fifty languages and that extract named entities and quotations (reported speech) from twenty languages. In this paper, we describe the recent effort of adding the African Bantu language Swahili to EMM. EMM is designed in an entirely modular way, allowing plugging in a new language by providing the language-specific resources for that language. We thus describe the type of language-specific resources needed, the effort involved, and ways of boot-strapping the generation of these resources in order to keep the effort of adding a new language to a minimum. The text analysis applications pursued in our efforts include clustering, classification, recognition and disambiguation of named entities (persons, organisations and locations), recognition and normalisation of date expressions, as well as the identification of reported speech quotations by and about people.","Language Resources and Evaluation",2011,"No","swahili multilingu inform extract name entiti recognit classif geo tag quotat recognit date recognit subject domain classif news analysi media monitor expand multilingu media monitor inform extract tool languag swahili europ media monitor emm famili applic set multilingu tool gather cluster classifi news fifti languag extract name entiti quotat report speech twenti languag paper describ recent effort ad african bantu languag swahili emm emm design modular allow plug languag provid languag specif resourc languag describ type languag specif resourc need effort involv way boot strap generat resourc order effort ad languag minimum text analysi applic pursu effort includ cluster classif recognit disambigu name entiti person organis locat recognit normalis date express identif report speech quotat peopl",0
"KeywordsMachine translation Human evaluation Error analysis ","involving language professionals in the evaluation of machine translation","
Significant breakthroughs in machine translation (MT) only seem possible if human translators are taken into the loop. While automatic evaluation and scoring mechanisms such as BLEU have enabled the fast development of systems, it is not clear how systems can meet real-world (quality) requirements in industrial translation scenarios today. The taraXŰ project has paved the way for wide usage of multiple MT outputs through various feedback loops in system development. The project has integrated human translators into the development process thus collecting feedback for possible improvements. This paper describes results from detailed human evaluation. Performance of different types of translation systems has been compared and analysed via ranking, error analysis and post-editing.","Language Resources and Evaluation",2014,"No","machin translat human evalu error analysi involv languag profession evalu machin translat signific breakthrough machin translat mt human translat loop automat evalu score mechan bleu enabl fast develop system clear system meet real world qualiti requir industri translat scenario today tarax project pave wide usag multipl mt output feedback loop system develop project integr human translat develop process collect feedback improv paper describ result detail human evalu perform type translat system compar analys rank error analysi post edit",0
"KeywordsMultimodal annotation Feedback Hand and facial gestures ","the mumin coding scheme for the annotation of feedback turn management and sequencing phenomena","This paper deals with a multimodal annotation scheme dedicated to the study of gestures in interpersonal communication, with particular regard to the role played by multimodal expressions for feedback, turn management and sequencing. The scheme has been developed under the framework of the MUMIN network and tested on the analysis of multimodal behaviour in short video clips in Swedish, Finnish and Danish. The preliminary results obtained in these studies show that the reliability of the categories defined in the scheme is acceptable, and that the scheme as a whole constitutes a versatile analysis tool for the study of multimodal communication behaviour.","Language Resources and Evaluation",2007,"No","multimod annot feedback hand facial gestur mumin code scheme annot feedback turn manag sequenc phenomena paper deal multimod annot scheme dedic studi gestur interperson communic regard role play multimod express feedback turn manag sequenc scheme develop framework mumin network test analysi multimod behaviour short video clip swedish finnish danish preliminari result obtain studi show reliabl categori defin scheme accept scheme constitut versatil analysi tool studi multimod communic behaviour",0
"KeywordsComputational Linguistic Computer Study ACLS Program ","the acls program for computer studies in the humanities notes on computers and the humanities",NA,"Computers and the Humanities",1966,"No","comput linguist comput studi acl program acl program comput studi human note comput human na",0
"KeywordsComputational Linguistic Literary Scholarship ","computers and literary scholarship",NA,"Computers and the Humanities",1971,"No","comput linguist literari scholarship comput literari scholarship na",0
"KeywordsComputational Linguistic ","style and structure in the middle english poemcleanness",NA,"Computers and the Humanities",1987,"No","comput linguist style structur middl english poemclean na",0
"Key wordsTEI SGML linguistic tagging feature structure features lexical tagging interlinear text analysis ","a rationale for the tei recommendations for feature structure markup","In this paper, we concentrate on justifying the decisions we made in developing the TEI recommendations for feature structure markup. The first four sections of this paper present the justification for the recommended treatment of feature structures, of features and their values, and of combinations of features or values and of alternations and negations of features and their values. Section 5 departs briefly from the linguistic focus to argue that the markup scheme developed for feature structures is in fact a general-purpose mechanism that can be used for a wide range of applications. Section 6 describes an auxiliary document called a “feature system declaration” that is used to document and validate a system of feature-structure markup. The seventh and final section illustrates the use of the recommended markup scheme with two examples, lexical tagging and interlinear text analysis.","Computers and the Humanities",1995,"No","key wordstei sgml linguist tag featur structur featur lexic tag interlinear text analysi rational tei recommend featur structur markup paper concentr justifi decis made develop tei recommend featur structur markup section paper present justif recommend treatment featur structur featur valu combin featur valu altern negat featur valu section depart briefli linguist focus argu markup scheme develop featur structur fact general purpos mechan wide rang applic section describ auxiliari document call featur system declar document valid system featur structur markup seventh final section illustr recommend markup scheme exampl lexic tag interlinear text analysi",0
"digital library letter publishing literature ","putting the dialogue back together re creating structure in letter publishing","In this paper, we will present a publication system in which selectedmaterial from letter collections is presented as dialogues between twopersons.","Computers and the Humanities",2003,"No","digit librari letter publish literatur put dialogu back creat structur letter publish paper present public system selectedmateri letter collect present dialogu twoperson",0
"Key wordspoetry meter rhythm linguistics alexandrine Cornulier Molière Racine Corneille ","toward a theory of rhythm in french poetry computer assisted recognition of rhythmic groups in traditional isometrical alexandrines","Benoît de Cornulier's writings on French poetry concentrate on metrical boundaries, or caesura; however, the the criteria upon which he bases his analyses are useful in studying rhythm, or the relationship between syllables within the alexandrine's twohémistiches. This study focuses on three aspects of rhythm in French poetry: the definition of rhythm following Cornulier; the development of a method using the computer to detect rhythmic patterns in traditional isometrical alexandrines; the results of such a study when applied to three classical seventeenth-century plays which are composed of isometrical alexandrines (Corneille'sPolyeucte, Racine'sPhèdre, and Molière'sLe Tartuffe).","Computers and the Humanities",1993,"No","key wordspoetri meter rhythm linguist alexandrin cornuli moli racin corneill theori rhythm french poetri comput assist recognit rhythmic group tradit isometr alexandrin beno de cornuli write french poetri concentr metric boundari caesura criteria base analys studi rhythm relationship syllabl alexandrin twoh mistich studi focus aspect rhythm french poetri definit rhythm cornuli develop method comput detect rhythmic pattern tradit isometr alexandrin result studi appli classic seventeenth centuri play compos isometr alexandrin corneillespolyeuct racinesph dre moli sle tartuff",0
"Key Wordstroubadours seriation chronology ","musical chronology by seriation","The statistical process of seriation arranges a set of objects along a one-dimensional line so that the distance between each pair of objects reflects the dissimilarity between them. The process has recently been cast into an algorithm which makes it feasible to seriate a few tens of objects efficiently on a personal computer. This algorithm is now used for establishing a chronology of the Troubadours according to certain melodic features in their works. Other musicological uses for seriation are proposed.","Computers and the Humanities",1994,"No","key wordstroubadour seriat chronolog music chronolog seriat statist process seriat arrang set object dimension line distanc pair object reflect dissimilar process recent cast algorithm make feasibl seriat ten object effici person comput algorithm establish chronolog troubadour melod featur work musicolog seriat propos",0
NA,"informatique et histoire mdivale linstitut de recherche et dhistoire de textes","In order to handle its own documentation as well as in order to come to the aid of isolated researchers who have no access to computers, the Institute of Research and of the History of Texts is attempting to put together a certain number of programs which can satisfy the needs of various types of medieval historical documents.","Computers and the Humanities",1978,"No","informatiqu histoir mdival linstitut de recherch dhistoir de text order handl document order aid isol research access comput institut research histori text attempt put number program satisfi type mediev histor document",0
"KeywordsComputational Linguistic Early Sixty Early Center Word Index ","automated concordances and word indexes the early sixties and the early centers",NA,"Computers and the Humanities",1981,"No","comput linguist earli sixti earli center word index autom concord word index earli sixti earli center na",0
"KeywordsWord sense disambiguation Idiom detection Linguistic knowledge ","detecting japanese idioms with a linguistically rich dictionary","Detecting idioms in a sentence is important to sentence understanding. This paper discusses the linguistic knowledge for idiom detection. The challenges are that idioms can be ambiguous between literal and idiomatic meanings, and that they can be “transformed” when expressed in a sentence. However, there has been little research on Japanese idiom detection with its ambiguity and transformations taken into account. We propose a set of linguistic knowledge for idiom detection that is implemented in an idiom dictionary. We evaluated the linguistic knowledge by measuring the performance of an idiom detector that exploits the dictionary. As a result, more than 90% of the idioms are detected with 90% accuracy.","Language Resources and Evaluation",2006,"No","word sens disambigu idiom detect linguist knowledg detect japanes idiom linguist rich dictionari detect idiom sentenc import sentenc understand paper discuss linguist knowledg idiom detect challeng idiom ambigu liter idiomat mean transform express sentenc research japanes idiom detect ambigu transform account propos set linguist knowledg idiom detect implement idiom dictionari evalu linguist knowledg measur perform idiom detector exploit dictionari result idiom detect accuraci",0
"KeywordsAligned wordnets BalkaNet EuroWordNet Interlingual mapping Lexical ontology Ontology projection Princeton WordNet Romanian ","the romanian wordnet in a nutshell","The project on the Romanian wordnet has been under continuous development for more than 10 years now. It has been in constant use in many projects and applications which determined, to a large extent, the content and coverage of various lexical domains. The article presents the most recent developments of the Romanian wordnet and offers quantitative data for its current version.","Language Resources and Evaluation",2013,"No","align wordnet balkanet eurowordnet interlingu map lexic ontolog ontolog project princeton wordnet romanian romanian wordnet nutshel project romanian wordnet continu develop year constant project applic determin larg extent content coverag lexic domain articl present recent develop romanian wordnet offer quantit data current version",0
"KeywordsText classification Natural language processing Knowledge representation Semantic enrichment Use case specification ","using semantic roles to improve text classification in the requirements domain","Engineering activities often produce considerable documentation as a by-product of the development process. Due to their complexity, technical analysts can benefit from text processing techniques able to identify concepts of interest and analyze deficiencies of the documents in an automated fashion. In practice, text sentences from the documentation are usually transformed to a vector space model, which is suitable for traditional machine learning classifiers. However, such transformations suffer from problems of synonyms and ambiguity that cause classification mistakes. For alleviating these problems, there has been a growing interest in the semantic enrichment of text. Unfortunately, using general-purpose thesaurus and encyclopedias to enrich technical documents belonging to a given domain (e.g. requirements engineering) often introduces noise and does not improve classification. In this work, we aim at boosting text classification by exploiting information about semantic roles. We have explored this approach when building a multi-label classifier for identifying special concepts, called domain actions, in textual software requirements. After evaluating various combinations of semantic roles and text classification algorithms, we found that this kind of semantically-enriched data leads to improvements of up to 18% in both precision and recall, when compared to non-enriched data. Our enrichment strategy based on semantic roles also allowed classifiers to reach acceptable accuracy levels with small training sets. Moreover, semantic roles outperformed Wikipedia- and WordNET-based enrichments, which failed to boost requirements classification with several techniques. These results drove the development of two requirements tools, which we successfully applied in the processing of textual use cases.","Language Resources and Evaluation",2018,"No","text classif natur languag process knowledg represent semant enrich case specif semant role improv text classif requir domain engin activ produc consider document product develop process due complex technic analyst benefit text process techniqu identifi concept interest analyz defici document autom fashion practic text sentenc document transform vector space model suitabl tradit machin learn classifi transform suffer problem synonym ambigu classif mistak allevi problem grow interest semant enrich text general purpos thesaurus encyclopedia enrich technic document belong domain requir engin introduc nois improv classif work aim boost text classif exploit inform semant role explor approach build multi label classifi identifi special concept call domain action textual softwar requir evalu combin semant role text classif algorithm found kind semant enrich data lead improv precis recal compar enrich data enrich strategi base semant role allow classifi reach accept accuraci level small train set semant role outperform wikipedia wordnet base enrich fail boost requir classif techniqu result drove develop requir tool success appli process textual case",0
"KeywordsTone Algorithm Sesotho Bantu languages Text-to-Speech systems ","improving a tone labeling algorithm for sesotho","
We report on a study that aimed to improve an existing tone label prediction algorithm for Sesotho, an official language of South Africa. Tone is an important prosodic feature of Sesotho, since speakers use tone to distinguish meaning. In order to implement tone in a Text-to-Speech system for Sesotho, a tone modeling algorithm must receive as input the tone labels of the syllables of each word. Then it can predict the appropriate intonation of the word. Since Sesotho does not mark tone labels in orthography, the labels have to be predicted according to the tonal rules of the language. The existing tone label prediction algorithm has two drawbacks, namely it implements three tonal rules only and is restricted to the clitic phrase domain. In our study, we developed an algorithm that implements four additional tonal rules and addresses all parts of speech. The results show that the latter algorithm significantly improves the existing one by increasing the number of matched tone labels.
","Language Resources and Evaluation",2015,"No","tone algorithm sesotho bantu languag text speech system improv tone label algorithm sesotho report studi aim improv exist tone label predict algorithm sesotho offici languag south africa tone import prosod featur sesotho speaker tone distinguish mean order implement tone text speech system sesotho tone model algorithm receiv input tone label syllabl word predict inton word sesotho mark tone label orthographi label predict tonal rule languag exist tone label predict algorithm drawback implement tonal rule restrict clitic phrase domain studi develop algorithm implement addit tonal rule address part speech result show algorithm signific improv exist increas number match tone label",0
"KeywordsAutomated gesture analysis Gesture studies Type of gestures Gestures phases Gesture recognition Gesture segmentation ","studies in automated hand gesture analysis an overview of functional types and gesture phases","
This paper presents an overview of studies on automated hand gesture analysis, which is mainly concerned with recognition and segmentation issues related to functional types and gesture phases. The issues selected for discussion have been arranged in a way that takes account of problems within the Theory of Gestures that each study seeks to address. Their principal computational factors that were involved in conducting the analysis of automated hand gesture have been examined, and an analysis of open research issues has been carried out for each application dealt with in the studies.
","Language Resources and Evaluation",2017,"No","autom gestur analysi gestur studi type gestur gestur phase gestur recognit gestur segment studi autom hand gestur analysi overview function type gestur phase paper present overview studi autom hand gestur analysi concern recognit segment issu relat function type gestur phase issu select discuss arrang take account problem theori gestur studi seek address princip comput factor involv conduct analysi autom hand gestur examin analysi open research issu carri applic dealt studi",0
"KeywordsKnowledge representation Image selection Image annotation EcoLexicon ","image selection and annotation for an environmental knowledge base","Images play an important role in the representation and acquisition of specialized knowledge. Not surprisingly, terminological knowledge bases (TKBs) often include images as a way to enhance the information in concept entries. However, the selection of these images should not be random, but rather based on specific guidelines that take into account the type and nature of the concept being described. This paper presents a proposal on how to combine the features of images with the conceptual propositions in EcoLexicon, a multilingual TKB on the environment. This proposal is based on the following: (1) the combinatory possibilities of concept types; (2) image types, such as photographs, drawings and flow charts; (3) morphological features or visual knowledge patterns (VKPs), such as labels, colours, arrows, and their effect on the functional nature of each image type. Currently, images are stored in association with concept entries according to the semantic content of their definitions, but they are not described or annotated according to the parameters that guided their selection, which would undoubtedly contribute to the systematization and automatization of the process. First, the images included in EcoLexicon were analyzed in terms of their adequateness, the semantic relations expressed, the concept types and their VKPs. Then, with these data, guidelines for image selection and annotation were created. The final aim is twofold: (1) to systematize the selection of images and (2) to start annotating old and new images so that the system can automatically allocate them in different concept entries based on shared conceptual propositions.","Language Resources and Evaluation",2016,"No","knowledg represent imag select imag annot ecolexicon imag select annot environment knowledg base imag play import role represent acquisit special knowledg surpris terminolog knowledg base tkbs includ imag enhanc inform concept entri select imag random base specif guidelin account type natur concept paper present propos combin featur imag conceptu proposit ecolexicon multilingu tkb environ propos base combinatori possibl concept type imag type photograph draw flow chart morpholog featur visual knowledg pattern vkps label colour arrow effect function natur imag type imag store associ concept entri semant content definit annot paramet guid select undoubt contribut systemat automat process imag includ ecolexicon analyz term adequ semant relat express concept type vkps data guidelin imag select annot creat final aim twofold systemat select imag start annot imag system automat alloc concept entri base share conceptu proposit",0
"KeywordsLexical classification Lexical resources Computational linguistics ","a large scale classification of english verbs","Lexical classifications have proved useful in supporting various natural language processing (NLP) tasks. The largest verb classification for English is Levin’s (1993) work which defines groupings of verbs based on syntactic and semantic properties. VerbNet (VN) (Kipper et al. 2000; Kipper-Schuler 2005)—an extensive computational verb lexicon for English—provides detailed syntactic-semantic descriptions of Levin classes. While the classes included are extensive enough for some NLP use, they are not comprehensive. Korhonen and Briscoe (2004) have proposed a significant extension of Levin’s classification which incorporates 57 novel classes for verbs not covered (comprehensively) by Levin. Korhonen and Ryant (unpublished) have recently proposed another extension including 53 additional classes. This article describes the integration of these two extensions into VN. The result is a comprehensive Levin-style classification for English verbs providing over 90% token coverage of the Proposition Bank data (Palmer et al. 2005) and thus can be highly useful for practical applications.","Language Resources and Evaluation",2008,"No","lexic classif lexic resourc comput linguist larg scale classif english verb lexic classif prove support natur languag process nlp task largest verb classif english levin work defin group verb base syntact semant properti verbnet vn kipper al kipper schuler extens comput verb lexicon english detail syntact semant descript levin class class includ extens nlp comprehens korhonen brisco propos signific extens levin classif incorpor class verb cover comprehens levin korhonen ryant unpublish recent propos extens includ addit class articl describ integr extens vn result comprehens levin style classif english verb provid token coverag proposit bank data palmer al high practic applic",0
"KeywordsControlled hybrid language Controlled visual language Controlled natural language Electronic navigational charts Maritime navigation ","a hybrid visualnatural controlled language","We define the notion of controlled hybrid language that allows information share and interaction between a controlled natural language (specified by a context-free grammar) and a controlled visual language (specified by a Symbol-Relation grammar). We present the controlled hybrid language INAUT, used to represent nautical charts of the French Naval and Hydrographic Service (SHOM) and their companion texts (Instructions nautiques).","Language Resources and Evaluation",2017,"No","control hybrid languag control visual languag control natur languag electron navig chart maritim navig hybrid visualnatur control languag defin notion control hybrid languag inform share interact control natur languag context free grammar control visual languag symbol relat grammar present control hybrid languag inaut repres nautic chart french naval hydrograph servic shom companion text instruct nautiqu",0
"KeywordsAutomatic treebank conversion Feature-based approach Part of speech Constituency syntactic structure ","a feature based approach to better automatic treebank conversion","In the field of constituency parsing, there exist multiple human-labeled treebanks which are built on non-overlapping text samples and follow different annotation standards. Due to the extreme cost of annotating parse trees by human, it is desirable to automatically convert one treebank (called source treebank) to the standard of another treebank (called target treebank) which we are interested in. Conversion results can be manually corrected to obtain higher-quality annotations or can be directly used as additional training data for building syntactic parsers. To perform automatic treebank conversion, we divide constituency parses into two separate levels: the part-of-speech (POS) and syntactic structure (bracketing structures and constituent labels), and conduct conversion on these two levels respectively with a feature-based approach. The basic idea of the approach is to encode original annotations in a source treebank as guide features during the conversion process. Experiments on two Chinese treebanks show that our approach can convert POS tags and syntactic structures with the accuracy of 96.6 and 84.8 %, respectively, which are the best reported results on this task.","Language Resources and Evaluation",2013,"No","automat treebank convers featur base approach part speech constitu syntact structur featur base approach automat treebank convers field constitu pars exist multipl human label treebank built overlap text sampl follow annot standard due extrem cost annot pars tree human desir automat convert treebank call sourc treebank standard treebank call target treebank interest convers result manual correct obtain higher qualiti annot direct addit train data build syntact parser perform automat treebank convers divid constitu pars separ level part speech pos syntact structur bracket structur constitu label conduct convers level featur base approach basic idea approach encod origin annot sourc treebank guid featur convers process experi chines treebank show approach convert pos tag syntact structur accuraci report result task",0
"KeywordsMorphological tagging Data-driven lemmatization Averaged perceptron Finnish Open-source ","finnpos an open source morphological tagging and lemmatization toolkit for finnish","This paper describes FinnPos, an open-source morphological tagging and lemmatization toolkit for Finnish. The morphological tagging model is based on the averaged structured perceptron classifier. Given training data, new taggers are estimated in a computationally efficient manner using a combination of beam search and model cascade. The lemmatization is performed employing a combination of a rule-based morphological analyzer, OMorFi, and a data-driven lemmatization model. The toolkit is readily applicable for tagging and lemmatization of running text with models learned from the recently published Finnish Turku Dependency Treebank and FinnTreeBank. Empirical evaluation on these corpora shows that FinnPos performs favorably compared to reference systems in terms of tagging and lemmatization accuracy. In addition, we demonstrate that our system is highly competitive with regard to computational efficiency of learning new models and assigning analyses to novel sentences.","Language Resources and Evaluation",2016,"No","morpholog tag data driven lemmat averag perceptron finnish open sourc finnpo open sourc morpholog tag lemmat toolkit finnish paper describ finnpo open sourc morpholog tag lemmat toolkit finnish morpholog tag model base averag structur perceptron classifi train data tagger estim comput effici manner combin beam search model cascad lemmat perform employ combin rule base morpholog analyz omorfi data driven lemmat model toolkit readili applic tag lemmat run text model learn recent publish finnish turku depend treebank finntreebank empir evalu corpora show finnpo perform favor compar refer system term tag lemmat accuraci addit demonstr system high competit regard comput effici learn model assign analys sentenc",0
"KeywordsComputational Linguistic Technical Review ","technical reviews",NA,"Computers and the Humanities",1994,"No","comput linguist technic review technic review na",0
"TEI10 SGML TEI markup conference research communities ","the text encoding initiative at 10 not just an interchange format anymore but a new research community","Mylonas and Renear introduce a volume of selected papers from The Text Encoding Initiative 10th Anniversary Conference, held at Brown University in November 1997. The Text Encoding Initiative (TEI), was launched in 1987 and sponsored by the Association for Computers and the Humanities, the Association for Literary and Linguistic Computing, and the Association for Computational Linguistics. It had as its original objective the development of an interchange language for textual data. This effort was completely successful and the TEI Guidelines are now widely accepted as the standard interchange format for textual data. Mylonas and Renear also note that the TEI has accomplished two other major achievements: it has produced a powerful new data description language (which is influencing the development of new WWW standards); and, most importantly, it has motivated the development of an entirely new research community, focused on understanding the role of text structure and markup in the use of emerging information technologies in culture, scholarship, and communication.","Computers and the Humanities",1999,"No","tei sgml tei markup confer research communiti text encod initi interchang format anymor research communiti mylona renear introduc volum select paper text encod initi th anniversari confer held brown univers novemb text encod initi tei launch sponsor associ comput human associ literari linguist comput associ comput linguist origin object develop interchang languag textual data effort complet success tei guidelin wide accept standard interchang format textual data mylona renear note tei accomplish major achiev produc power data descript languag influenc develop www standard import motiv develop research communiti focus understand role text structur markup emerg inform technolog cultur scholarship communic",0
"cross-language information retrieval information access Japanese-English machine translation probabilistic retrieval ","a framework for cross language information access application to english and japanese","Internet search engines allow access to online information from all over the world. However, there is currently a general assumption that users are fluent in the languages of all documentsthat they might search for. This has for historical reasons usually been a choice between English and the locally supported language. Given the rapidly growing size of the Internet, it is likely that future users will need to access information in languages in which they are not fluent or have no knowledge of at all. This papershows how information retrieval and machine translation can becombined in a cross-language information access frameworkto help overcome the language barrier. We presentencouraging preliminary experimental results using English queries toretrieve documents from the standard Japanese language BMIR-J2retrieval test collection. We outline the scope and purpose ofcross-language information access and provide an example applicationto suggest that technology already exists to provide effective andpotentially useful applications.","Computers and the Humanities",2001,"No","cross languag inform retriev inform access japanes english machin translat probabilist retriev framework cross languag inform access applic english japanes internet search engin access onlin inform world general assumpt user fluent languag documentsthat search histor reason choic english local support languag rapid grow size internet futur user access inform languag fluent knowledg papershow inform retriev machin translat becombin cross languag inform access frameworkto overcom languag barrier presentencourag preliminari experiment result english queri toretriev document standard japanes languag bmir jretriev test collect outlin scope purpos ofcross languag inform access provid applicationto suggest technolog exist provid effect andpotenti applic",0
NA,"j pittermann a pittermann and w minker handling emotions in humancomputer dialogues","“Handling emotions in human–computer dialogues”, written by Pittermann, Pittermann and Minker, is a complete and interesting book about affective computing in spoken dialogue systems. Dialogue systems are an integrated part of our daily life. They generally mean simplicity, time saving and safety. For example, when driving, hand-free operations are necessary and therefore the possibility of giving commands through speech is a necessity. However, to implement more flexible dialogue systems, it is important that these systems can adapt to the speaker. In particular, it seems very important that dialogue systems should be able to recognize and cope with our emotions. To human, the emotions recognition process occurs in an automatic, unconscious, and effortless fashion. Emotions explicitly affect our autonomic nervous system (e.g., cardiovascular and skin conductance changes) and our somatic nervous system (motor expression in face, voice and body). We usually don’t have many...","Language Resources and Evaluation",2011,"No","pittermann pittermann minker handl emot humancomput dialogu handl emot human comput dialogu written pittermann pittermann minker complet interest book affect comput spoken dialogu system dialogu system integr part daili life general simplic time save safeti drive hand free oper possibl give command speech necess implement flexibl dialogu system import system adapt speaker import dialogu system recogn cope emot human emot recognit process occur automat unconsci effortless fashion emot explicit affect autonom nervous system cardiovascular skin conduct somat nervous system motor express face voic bodi don",0
NA,"steven bird ewan klein and edward loper natural language processing with python analyzing text with the natural language toolkit","Natural Language Processing (NLP) is experiencing rapid growth as its theories and methods are more and more deployed in a wide range of different fields. In the humanities, the work on corpora is gaining increasing prominence. Within industry, people need NLP for market analysis, web software development to name a few examples. For this reason it is important for many people to have some working knowledge of NLP. The book “Natural Language Processing with Python” by Steven Bird, Ewan Klein and Edward Loper is a recent contribution to cover this demand. It introduces the freely available Natural Language Toolkit (NLTK) 1—a project by the same authors—that was designed with the following goals: simplicity, consistency, extensibility and modularity.","Language Resources and Evaluation",2010,"No","steven bird ewan klein edward loper natur languag process python analyz text natur languag toolkit natur languag process nlp experienc rapid growth theori method deploy wide rang field human work corpora gain increas promin industri peopl nlp market analysi web softwar develop exampl reason import peopl work knowledg nlp book natur languag process python steven bird ewan klein edward loper recent contribut cover demand introduc freeli natur languag toolkit nltk project author design goal simplic consist extens modular",0
"KeywordsTerm extraction Term weighting Maximal marginal relevance Document re-ranking Information retrieval ","chinese document re ranking based on automatically acquired term resource","In this paper, we address the problem of document re-ranking in information retrieval, which is usually conducted after initial retrieval to improve rankings of relevant documents. To deal with this problem, we propose a method which automatically constructs a term resource specific to the document collection and then applies the resource to document re-ranking. The term resource includes a list of terms extracted from the documents as well as their weighting and correlations computed after initial retrieval. The term weighting based on local and global distribution ensures the re-ranking not sensitive to different choices of pseudo relevance, while the term correlation helps avoid any bias to certain specific concept embedded in queries. Experiments with NTCIR3 data show that the approach can not only improve performance of initial retrieval, but also make significant contribution to standard query expansion.","Language Resources and Evaluation",2009,"No","term extract term weight maxim margin relev document rank inform retriev chines document rank base automat acquir term resourc paper address problem document rank inform retriev conduct initi retriev improv rank relev document deal problem propos method automat construct term resourc specif document collect appli resourc document rank term resourc includ list term extract document weight correl comput initi retriev term weight base local global distribut ensur rank sensit choic pseudo relev term correl help avoid bias specif concept embed queri experi ntcir data show approach improv perform initi retriev make signific contribut standard queri expans",0
"KeywordsArabic WordNet Hyponymy extraction Maximal frequent sequence WordNet-based application ","on the evaluation and improvement of arabic wordnet coverage and usability","Built on the basis of the methods developed for Princeton WordNet and EuroWordNet, Arabic WordNet (AWN) has been an interesting project which combines WordNet structure compliance with Arabic particularities. In this paper, some AWN shortcomings related to coverage and usability are addressed. The use of AWN in question/answering (Q/A) helped us to deeply evaluate the resource from an experience-based perspective. Accordingly, an enrichment of AWN was built by semi-automatically extending its content. Indeed, existing approaches and/or resources developed for other languages were adapted and used for AWN. The experiments conducted in Arabic Q/A have shown an improvement of both AWN coverage as well as usability. Concerning coverage, a great amount of named entities extracted from YAGO were connected with corresponding AWN synsets. Also, a significant number of new verbs and nouns (including Broken Plural forms) were added. In terms of usability, thanks to the use of AWN, the performance for the AWN-based Q/A application registered an overall improvement with respect to the following three measures: accuracy (+9.27 % improvement), mean reciprocal rank (+3.6 improvement) and number of answered questions (+12.79 % improvement).","Language Resources and Evaluation",2013,"No","arab wordnet hyponymi extract maxim frequent sequenc wordnet base applic evalu improv arab wordnet coverag usabl built basi method develop princeton wordnet eurowordnet arab wordnet awn interest project combin wordnet structur complianc arab particular paper awn shortcom relat coverag usabl address awn questionansw help deepli evalu resourc experi base perspect enrich awn built semi automat extend content exist approach resourc develop languag adapt awn experi conduct arab shown improv awn coverag usabl coverag great amount name entiti extract yago connect awn synset signific number verb noun includ broken plural form ad term usabl awn perform awn base applic regist improv respect measur accuraci improv reciproc rank improv number answer question improv",0
NA,"technical review",NA,"Computers and the Humanities",1992,"No","technic review na",0
NA,"the problem of a statistical approach",NA,"Computers and the Humanities",1991,"No","problem statist approach na",0
"Key Wordslexicography historical dictionaries Italian language philology relational databases vocabulary ","the italian vocabulary center","The Opera del Vocabolario Italiano was given a mandate in 1964 to create a Historical Dictionary of the Italian Language. The main objective was to provide a tool which would give vital information on the development of the Italian language from its origins to the present day. In 1986 the Center incorporated modern computer technology into the project and this led to a series of decisions which affected the nature and the outcome of the project. This article traces the development of the project, and describes both hardware and software systems used, as well as the nature of the relational database being created and its linguistic applications.","Computers and the Humanities",1990,"No","key wordslexicographi histor dictionari italian languag philolog relat databas vocabulari italian vocabulari center opera del vocabolario italiano mandat creat histor dictionari italian languag main object provid tool give vital inform develop italian languag origin present day center incorpor modern comput technolog project led seri decis affect natur outcom project articl trace develop project describ hardwar softwar system natur relat databas creat linguist applic",0
NA,"mthode danalyses statistiques informatises des microtoponymes franc comtois",NA,"Computers and the Humanities",1986,"No","mthode danalys statistiqu informatis des microtoponym franc comtoi na",0
"KeywordsAnnual Meeting Computational Linguistic ","the annual meeting of the acl",NA,"Computers and the Humanities",1973,"No","annual meet comput linguist annual meet acl na",0
"KeywordsComputational Linguistic Dialectal Variation ","computer produced mapping of dialectal variation",NA,"Computers and the Humanities",1970,"No","comput linguist dialect variat comput produc map dialect variat na",0
"KeywordsResearch Centre Computational Linguistic Automatic Treatment Classical Archaeology ","the research centre for automatic treatments in classical archaeology",NA,"Computers and the Humanities",1986,"No","research centr comput linguist automat treatment classic archaeolog research centr automat treatment classic archaeolog na",0
"Classification Information Model classification information word sense disambiguation ","word sense disambiguation using the classification information model","A Classification Information Model is a pattern classification model.The model decides the proper class of an input instance by integrating individual decisions, each of which is made with each feature in the pattern.Each individual decision is weighted according to the distributional property of the feature deriving the decision. An individual decision and its weight are represented as classification information which is extracted from the training instances.In the word sense disambiguation based on the model, the proper sense of an input instance is determined by the weighted sum of whole individual decisions derived from the features contained in the instance.","Computers and the Humanities",2000,"No","classif inform model classif inform word sens disambigu word sens disambigu classif inform model classif inform model pattern classif model model decid proper class input instanc integr individu decis made featur pattern individu decis weight distribut properti featur deriv decis individu decis weight repres classif inform extract train instanc word sens disambigu base model proper sens input instanc determin weight sum individu decis deriv featur contain instanc",0
"KeywordsHistory Data Automatic Processing Computational Linguistic ","the first international conference on automatic processing of art history data and documents a report",NA,"Computers and the Humanities",1980,"No","histori data automat process comput linguist intern confer automat process art histori data document report na",0
"KeywordsComputational Linguistic ","statistical analysis of dialectal boundaries",NA,"Computers and the Humanities",1974,"No","comput linguist statist analysi dialect boundari na",0
"KeywordsPresent Situation Computational Linguistic ","music and computing the present situation",NA,"Computers and the Humanities",1967,"No","present situat comput linguist music comput present situat na",0
"KeywordsComputational Linguistic Progress Report ","the study of old italian at utrecht a progress report",NA,"Computers and the Humanities",1969,"No","comput linguist progress report studi italian utrecht progress report na",0
"KeywordsComputational Linguistic Classical Literature ","computers and classical literature 19701971",NA,"Computers and the Humanities",1971,"No","comput linguist classic literatur comput classic literatur na",0
"KeywordsLiterary Research Computational Linguistic Conference Report ","symposium on the uses of the computer in literary research a conference report",NA,"Computers and the Humanities",1970,"No","literari research comput linguist confer report symposium comput literari research confer report na",0
"KeywordsStatistical Operation Automatic Processing Computational Linguistic Automatic Composition Latin Work ","computers and the classics a supplement","As an addition to the report by McDonough in CHum,II (1967), 37–40, Delatte lists the operations of the Laboratoire d'Analyse statistique des Langues Anciennes at Liège. He describes eight programs for automatic processing, automatic composition and printing of concordances, and various statistical operations. Forty-three Latin works and five Greek, totaling approximately 600,000 words, are catalogued.","Computers and the Humanities",1968,"No","statist oper automat process comput linguist automat composit latin work comput classic supplement addit report mcdonough chumii delatt list oper laboratoir analys statistiqu des langu ancienn li ge describ program automat process automat composit print concord statist oper forti latin work greek total approxim word catalogu",0
"KeywordsComputational Linguistic Philosophical Text ","a computer assisted study of a philosophical text",NA,"Computers and the Humanities",1969,"No","comput linguist philosoph text comput assist studi philosoph text na",0
NA,"the awk programming language",NA,"Computers and the Humanities",1992,"No","awk program languag na",0
"KeywordsComputational Linguistic ","humanities computing activities in italy",NA,"Computers and the Humanities",1968,"No","comput linguist human comput activ itali na",0
"KeywordsComputational Linguistic Egypt Literature ","indexes of citations from ancient egypt literature",NA,"Computers and the Humanities",1986,"No","comput linguist egypt literatur index citat ancient egypt literatur na",0
"KeywordsComputational Linguistic ","letter from boulder",NA,"Computers and the Humanities",1981,"No","comput linguist letter boulder na",0
"KeywordsComputational Linguistic Project Report ","project reports",NA,"Computers and the Humanities",1978,"No","comput linguist project report project report na",0
"KeywordsData Base Computational Linguistic Vocabulary Data ","on a vocabulary data base",NA,"Computers and the Humanities",1980,"No","data base comput linguist vocabulari data vocabulari data base na",0
"browsing and navigation in large hypermedia image-based humanities computing TEI text/image coupling text encoding and rendering transcription/editing tools XML ","text image coupling for editing literary sources","Users need more sophisticatedtools to handle the growing numberof image-based documents availablein databases. In this paper, wepresent a system devoted to theediting and browsing of complexliterary hypermedia includingoriginal manuscript documents andother handwritten sources. Editingcapabilities allow the user totranscribe manuscript images in aninteractive way and to encode theresulting textual representationby means of a logical markuplanguage (based on the XML/TEIspecification). Bothrepresentations (image andstructured text) are tightlylinked to facilitate the readingand the interpretation ofdocuments. This text/imagecoupling scheme is an attempt tounify several layers ofinformation in order to providethe user with a global vision ofthe work. Our system also suppliestools capable of processing andrelating information stored bothin images and structured texts.Finally, application-specificvisualization techniques have beendeveloped in order to provideusers with a way to identifyrelationships between sourcedocuments and help them tonavigate.","Computers and the Humanities",2002,"No","brows navig larg hypermedia imag base human comput tei textimag coupl text encod render transcriptionedit tool xml text imag coupl edit literari sourc user sophisticatedtool handl grow numberof imag base document availablein databas paper wepres system devot theedit brows complexliterari hypermedia includingorigin manuscript document andoth handwritten sourc editingcap user totranscrib manuscript imag aninteract encod theresult textual representationbi mean logic markuplanguag base xmlteispecif bothrepresent imag andstructur text tightlylink facilit readingand interpret ofdocu textimagecoupl scheme attempt tounifi layer ofinform order provideth user global vision ofth work system suppliestool capabl process andrel inform store bothin imag structur textsfin applic specificvisu techniqu beendevelop order provideus identifyrelationship sourcedocu tonavig",0
"authorship attribution lexical statistics stylistics vocabulary richness ","another perspective on vocabulary richness","This article examines the usefulness ofvocabulary richness for authorship attributionand tests the assumption that appropriatemeasures of vocabulary richness can capture anauthor's distinctive style or identity. Afterbriefly discussing perceived and actualvocabulary richness, I show that doubling andcombining texts affects some measures incomputationally predictable but conceptuallysurprising ways. I discuss some theoretical andempirical problems with some measures anddevelop simple methods to test how wellvocabulary richness distinguishes texts bydifferent authors. These methods show thatvocabulary richness is ineffective for largegroups of texts because of the extremevariability within and among them. I concludethat vocabulary richness is of marginal valuein stylistic and authorship studies because thebasic assumption that it constitutes awordprint for authors is false.","Computers and the Humanities",2003,"No","authorship attribut lexic statist stylist vocabulari rich perspect vocabulari rich articl examin use ofvocabulari rich authorship attributionand test assumpt appropriatemeasur vocabulari rich captur anauthor distinct style ident afterbriefli discuss perceiv actualvocabulari rich show doubl andcombin text affect measur incomput predict conceptuallysurpris way discuss theoret andempir problem measur anddevelop simpl method test wellvocabulari rich distinguish text bydiffer author method show thatvocabulari rich ineffect largegroup text extremevari concludethat vocabulari rich margin valuein stylist authorship studi thebas assumpt constitut awordprint author fals",0
"KeywordsComputational Linguistic ","introductions telecommunications and the scholar",NA,"Computers and the Humanities",1992,"No","comput linguist introduct telecommun scholar na",0
"Canterbury Tales Chaucer gene order phylogenetic analysis stemmatology ","analyzing the order of items in manuscripts of the canterbury tales","Chaucer's CanterburyTales consists of loosely-connected stories,appearing in many different orders in extantmanuscripts. Differences in order result fromrearrangements by scribes during copying, andmay reveal relationships among manuscripts. Identifying these relationships is analogous todetermining evolutionary relationships amongorganisms from the order of genes on a genome. We use gene order analysis to construct astemma for the Canterbury Tales. Thisstemma shows relationships predicted by earlierscholars, reveals new relationships, and sharesfeatures with a word variation stemma. Ourresults support the idea that there was noestablished order when the first manuscriptswere written.","Computers and the Humanities",2003,"No","canterburi tale chaucer gene order phylogenet analysi stemmatolog analyz order item manuscript canterburi tale chaucer canterburytal consist loos connect storiesappear order extantmanuscript differ order result fromrearrang scribe copi andmay reveal relationship manuscript identifi relationship analog todetermin evolutionari relationship amongorgan order gene genom gene order analysi construct astemma canterburi tale thisstemma show relationship predict earlierscholar reveal relationship sharesfeatur word variat stemma ourresult support idea noestablish order manuscriptswer written",0
"conflation algorithm Hartlib Papers Collection Latin Patrologia Latina stemming text databases ","retrieval of morphological variants in searches of latin text databases","This paper reports a detailed evaluation of the effectiveness of a system that has been developed for the identification and retrieval of morphological variants in searches of Latin text databases. A user of the retrieval system enters the principal parts of the search term (two parts for a noun or adjective, three parts for a deponent verb, and four parts for other verbs), this enabling the identification of the type of word that is to be processed and of the rules that are to be followed in determining the morphological variants that should be retrieved. Two different search algorithms are described. The algorithms are applied to the Latin portion of the Hartlib Papers Collection and to a range of classical, vulgar and medieval Latin texts drawn from the Patrologia Latina and from the PHI Disk 5.3 datasets. The effectiveness of these searches demonstrates the effectiveness of our procedures in providing access to the full range of classical and post-classical Latin text databases.","Computers and the Humanities",1997,"No","conflat algorithm hartlib paper collect latin patrologia latina stem text databas retriev morpholog variant search latin text databas paper report detail evalu effect system develop identif retriev morpholog variant search latin text databas user retriev system enter princip part search term part noun adject part depon verb part verb enabl identif type word process rule determin morpholog variant retriev search algorithm algorithm appli latin portion hartlib paper collect rang classic vulgar mediev latin text drawn patrologia latina phi disk dataset effect search demonstr effect procedur provid access full rang classic post classic latin text databas",0
"KeywordsComputational Linguistic Medieval Text Automatic Collation ","automatic collation a technique for medieval texts",NA,"Computers and the Humanities",1973,"No","comput linguist mediev text automat collat automat collat techniqu mediev text na",0
"KeywordsInitial Data Natural Language Internal Representation Specific Structuration Specific Information ","limited context semantic translation from a single knowledge base for a natural language and structuring metarules","The article introduces an experimental system which produces multilingual semantic translations from relatively short texts from a given context.","Computers and the Humanities",1986,"No","initi data natur languag intern represent specif structur specif inform limit context semant translat singl knowledg base natur languag structur metarul articl introduc experiment system produc multilingu semant translat short text context",0
"KeywordsComputational Linguistic Automatic Expansion ","automatic expansion of abbreviations an experiment with old icelandic",NA,"Computers and the Humanities",1982,"No","comput linguist automat expans automat expans abbrevi experi iceland na",0
"KeywordsArtificial Intelligence Knowledge Representation Computational Linguistic ","artificial intelligence history and knowledge representation",NA,"Computers and the Humanities",1982,"No","artifici intellig knowledg represent comput linguist artifici intellig histori knowledg represent na",0
"KeywordsComputational Linguistic Annual Bibliography ","annual bibliography for 1972 and supplement to preceding years",NA,"Computers and the Humanities",1973,"No","comput linguist annual bibliographi annual bibliographi supplement preced year na",0
"KeywordsComputational Linguistic Naturalistic Inquiry ","evaluating evolution naturalistic inquiry and the perseus project",NA,"Computers and the Humanities",1991,"No","comput linguist naturalist inquiri evalu evolut naturalist inquiri perseus project na",0
"KeywordsData Base Knowledge Representation Computational Linguistic Linguistic Study ","data bases and knowledge representation for literary and linguistic studies",NA,"Computers and the Humanities",1983,"No","data base knowledg represent comput linguist linguist studi data base knowledg represent literari linguist studi na",0
"Key wordsSGML TEI text encoding history historical documents Sasines ","speaking with one voice encoding standards and the prospects for an integrated approach to computing in history","This paper focusses on the types of questions that are raised in the encoding of historical documents. Using the example of a 17th century Scottish Sasine, the authors show how TEI-based encoding can produce a text which will be of major value to a variety of future historical researchers. Firstly, they show how to produce a machine-readable transcription which would be comprehensible to a word-processor as a text stream filled with print and formatting instructions; to a text analysis package as compilation of named text segments of some known structure; and to a statistical package as a set of observations each of which comprises a number of defined and named variables. Secondly, they make provision for a machine-readable transcription where the encoder's research agenda and assumptions are reversible or alterable by secondary analysts who will have access to a maximum amount of information contained in the original source.","Computers and the Humanities",1995,"No","key wordssgml tei text encod histori histor document sasin speak voic encod standard prospect integr approach comput histori paper focuss type question rais encod histor document th centuri scottish sasin author show tei base encod produc text major varieti futur histor research first show produc machin readabl transcript comprehens word processor text stream fill print format instruct text analysi packag compil name text segment structur statist packag set observ compris number defin name variabl make provis machin readabl transcript encod research agenda assumpt revers alter secondari analyst access maximum amount inform contain origin sourc",0
"KeywordsClassroom Instruction Heuristic Procedure Tutorial Group College Freshman Human Tutor ","using computer technology to teach and evaluate prewriting","The purpose of this research was to determine whether computer-aided instruction may be effectively utilized in stimulating prewriting composition when the CAI is based upon (1) conceptual (cognitive) strategies, (2) “data-driven” guidance (resulting from CAE techniques), and (3) recent findings in tutorial strategies research. If this specifically designed CAI is as good a means of prewriting instruction as personal tutoring and a better means than classroom instruction, then the practical and economical implications may be weighed in a decision to use such techniques. Forty-three college freshmen in three basic writing classes participated in this study. One class was exposed to a CAI medium, the other two either to a human tutor or to classroom instruction. A computer-aided evaluation of previous essays provided focus, and other intellectual processing cues provided information on an expository topic; this “database” was then used to construct a CAI program to encourage “specificity” and “depth of intellectual processing“ in students' prewriting composition. The program also possessed and was designed to provide “conceptual guidance” through the use of five heuristic procedures; thus it contained two key elements that a human tutor would possess in working with a topic—knowledge of the topic, and a means for eliciting that knowledge from the tutee. The second treatment method used consisted of instruction by human tutors, utilizing the same methodology. The control for the study consisted of a classroom instruction group. Results showed the CAI group demonstrating gains in every category of measurement utilized in this study, and its performances was significantly better than both the tutorial group on two of the post-test measures. The CAI group was superior, through not significantly, on post-test performances in every category used in the study except fluency.","Computers and the Humanities",1987,"No","classroom instruct heurist procedur tutori group colleg freshman human tutor comput technolog teach evalu prewrit purpos research determin comput aid instruct effect util stimul prewrit composit cai base conceptu cognit strategi data driven guidanc result cae techniqu recent find tutori strategi research specif design cai good mean prewrit instruct person tutor mean classroom instruct practic econom implic weigh decis techniqu forti colleg freshmen basic write class particip studi class expos cai medium human tutor classroom instruct comput aid evalu previous essay provid focus intellectu process cue provid inform expositori topic databas construct cai program encourag specif depth intellectu process student prewrit composit program possess design provid conceptu guidanc heurist procedur contain key element human tutor possess work topic knowledg topic mean elicit knowledg tute treatment method consist instruct human tutor util methodolog control studi consist classroom instruct group result show cai group demonstr gain categori measur util studi perform signific tutori group post test measur cai group superior signific post test perform categori studi fluenci",0
NA,"editors introduction to the special issue papers from lrec 2014","This special issue of Language Resources and Evaluation includes a selection of extended papers from the Ninth Language Resources and Evaluation Conference, held in Reykjavik, Iceland, in May 2014. The conference drew a record number of participants, reflecting the steadily increasing interest and activity in the field of language resource creation, enhancement, management, and evaluation since the first LREC was held in 1998. The selection of articles for this special issue was made on the basis of recommendations from LREC reviewers, who were asked to indicate suitability for publication of an extended version in LRE for each paper they reviewed.","Language Resources and Evaluation",2016,"No","editor introduct special issu paper lrec special issu languag resourc evalu includ select extend paper ninth languag resourc evalu confer held reykjavik iceland confer drew record number particip reflect steadili increas interest activ field languag resourc creation enhanc manag evalu lrec held select articl special issu made basi recommend lrec review ask suitabl public extend version lre paper review",0
NA,"annual bibliography for 1968",NA,"Computers and the Humanities",1969,"No","annual bibliographi na",0
"authorship attribution statistics stylistics ","the state of authorship attribution studies some problems and solutions","The statement, ’’Results of most non-traditional authorship attribution studies are not universally accepted as definitive,'' is explicated. A variety of problems in these studies are listed and discussed: studies governed by expediency; a lack of competent research; flawed statistical techniques; corrupted primary data; lack of expertise in allied fields; a dilettantish approach; inadequate treatment of errors. Various solutions are suggested: construct a correct and complete experimental design; educate the practitioners; study style in its totality; identify and educate the gatekeepers; develop a complete theoretical framework; form an association of practitioners.","Computers and the Humanities",1997,"No","authorship attribut statist stylist state authorship attribut studi problem solut statement result tradit authorship attribut studi univers accept definit explic varieti problem studi list discuss studi govern expedi lack compet research flaw statist techniqu corrupt primari data lack expertis alli field dilettantish approach inadequ treatment error solut suggest construct correct complet experiment design educ practition studi style total identifi educ gatekeep develop complet theoret framework form associ practition",0
"Key WordsEddington humanistic speculation letter frequencies Etaoin Shrdlu cryptology digrams trigrams tries data structures ","an infinite order solution to the eddington problem or getting monkeys to type shakespeare","Could a troupe of monkeys really produce Shakespeare if allowed to bang away at the word processor long enough? This age old question, commonly referred to as the Eddington problem, relates to fundamental issues of probability, and is examined in this article in a new light. Based on earlier research by the physicist William Bennett, Jr., the author describes a data structure which enables the computer to simulate the hypothetical monkeys. Exploiting principles of cryptology, the computer leads the simulated monkeys closer to their goal. Though the intent of the article is to encourage humanistic speculation, the final result proves to be quite practical and may come as a surprise to computer scientists and humanists alike.","Computers and the Humanities",1993,"No","key wordseddington humanist specul letter frequenc etaoin shrdlu cryptolog digram trigram data structur infinit order solut eddington problem monkey type shakespear troup monkey produc shakespear allow bang word processor long age question common refer eddington problem relat fundament issu probabl examin articl light base earlier research physicist william bennett jr author describ data structur enabl comput simul hypothet monkey exploit principl cryptolog comput lead simul monkey closer goal intent articl encourag humanist specul final result prove practic surpris comput scientist humanist alik",0
"cluster analysis dialectometry Finnish dialects idiolectal variation transitional dialects ","neighbours or enemies competing variants causing differences in transitional dialects","The aim of this study is to show how clusteranalysis can shed light on very complexvariation in a transitional dialect zone ineastern Finland. In the course of history thisarea has been on the border between Sweden andRussia and the population has clearly been oftwo kinds: the Savo people and the Karelians.It is a well-known fact that there is variationamong these dialects, but the spread and extentof the variation has not been demonstrated previously.The idiolects of the area were studied in thelight of ten phonological and morphologicalfeatures. The material consisted of recordingsof 198 idiolects, totalling around 195 hoursand representing 19 parishes. The variation wasanalysed using hierarchical cluster analysis.While the analysis showed the extent of thevariation between idiolects and parishes, italso demonstrated how the effects of the oldparishes, borders and settlements are stillvisible in the dialects. On the parish level,the data formed clear clusters that correspondwith the main dialects in the area and itssurroundings. On the idiolect level, however,the speakers from the surrounding areas formedfairly homogenous clusters but the idiolectsfrom the Savonlinna area were spread acrossalmost all clusters.","Computers and the Humanities",2003,"No","cluster analysi dialectometri finnish dialect idiolect variat transit dialect neighbour enemi compet variant caus differ transit dialect aim studi show clusteranalysi shed light complexvari transit dialect zone ineastern finland histori thisarea border sweden andrussia popul oftwo kind savo peopl karelian fact variationamong dialect spread extentof variat demonstr previous idiolect area studi thelight ten phonolog morphologicalfeatur materi consist recordingsof idiolect total hoursand repres parish variat wasanalys hierarch cluster analysi analysi show extent thevari idiolect parish italso demonstr effect oldparish border settlement stillvis dialect parish level data form clear cluster correspondwith main dialect area itssurround idiolect level speaker surround area formedfair homogen cluster idiolectsfrom savonlinna area spread acrossalmost cluster",0
"KeywordsComputational Linguistic British Library ","the reappearances of st basil the great in british library ms cotton otho b x",NA,"Computers and the Humanities",2002,"No","comput linguist british librari reappear st basil great british librari ms cotton otho na",0
"KeywordsComputational Linguistic Chinese Character ","the morphology of chinese characters a survey of models and applications","Preprints for seminar on Input/Output Systems for Japanese and Chinese Characters, Tokyo, 1971. U.S.-Japan Committee on scientific cooperation","Computers and the Humanities",1975,"No","comput linguist chines charact morpholog chines charact survey model applic preprint seminar inputoutput system japanes chines charact tokyo japan committe scientif cooper",0
NA,"annual bibliography for 1975 and supplement to preceding years",NA,"Computers and the Humanities",1976,"No","annual bibliographi supplement preced year na",0
NA,"i mani and j pustejovsky interpreting motion grounded representations for spatial language","The book under review is the fifth volume of the series Exploration in Language and Space of OUP. It is organized into six chapters: the odd-numbered chapters and part of Chapter 6 are written by Mani, whereas the even-numbered chapters by Pustejovsky.","Language Resources and Evaluation",2014,"No","mani pustejovski interpret motion ground represent spatial languag book review volum seri explor languag space oup organ chapter odd number chapter part chapter written mani number chapter pustejovski",0
"KeywordsAnnotation tool Treebank Minimal human intervention Parsing ","a segment based annotation tool for korean treebanks with minimal human intervention","In this paper, we propose a segment-based annotation tool providing appropriate interactivity between a human annotator and an automatic parser. The proposed annotation tool provides the preview of a complete sentence structure suggested by the parser, and updates the preview whenever the annotator cancels or selects each segmentation point. Thus, the annotator can select the proper sentence segments maximizing parsing accuracy and minimizing human intervention. Experimental results show that the proposed tool allows the annotator to be able to reduce human intervention by approximately 39% compared with manual annotation. Sejong Korean treebank, one of the large scale treebanks, was constructed with the proposed annotation tool.","Language Resources and Evaluation",2006,"No","annot tool treebank minim human intervent pars segment base annot tool korean treebank minim human intervent paper propos segment base annot tool provid interact human annot automat parser propos annot tool preview complet sentenc structur suggest parser updat preview annot cancel select segment point annot select proper sentenc segment maxim pars accuraci minim human intervent experiment result show propos tool annot reduc human intervent approxim compar manual annot sejong korean treebank larg scale treebank construct propos annot tool",0
"KeywordsAutomatic term recognition Terminology extraction Open source software ","atr4s toolkit with state of the art automatic terms recognition methods in scala","Automatically recognized terminology is widely used for various domain-specific texts processing tasks, such as machine translation, information retrieval or ontology construction. However, there is still no agreement on which methods are best suited for particular settings and, moreover, there is no reliable comparison of already developed methods. We believe that one of the main reasons is the lack of state-of-the-art method implementations, which are usually non-trivial to recreate—mostly, in terms of software engineering efforts. In order to address these issues, we present ATR4S, an open-source software written in Scala that comprises 13 state-of-the-art methods for automatic terminology recognition (ATR) and implements the whole pipeline from text document preprocessing, to term candidates collection, term candidate scoring, and finally, term candidate ranking. It is highly scalable, modular and configurable tool with support of automatic caching. We also compare 13 state-of-the-art methods on 7 open datasets by average precision and processing time. Experimental comparison reveals that no single method demonstrates best average precision for all datasets and that other available tools for ATR do not contain the best methods.","Language Resources and Evaluation",2018,"No","automat term recognit terminolog extract open sourc softwar atr toolkit state art automat term recognit method scala automat recogn terminolog wide domain specif text process task machin translat inform retriev ontolog construct agreement method suit set reliabl comparison develop method main reason lack state art method implement trivial recreat term softwar engin effort order address issu present atr open sourc softwar written scala compris state art method automat terminolog recognit atr implement pipelin text document preprocess term candid collect term candid score final term candid rank high scalabl modular configur tool support automat cach compar state art method open dataset averag precis process time experiment comparison reveal singl method demonstr averag precis dataset tool atr method",0
"Keywordsafst uima Annotation-based analytics development Pattern matching over annotations Annotation lattices High density annotation repositories Finite-state transduction Corpus analysis ","a framework for traversing dense annotation lattices","Pattern matching, or querying, over annotations is a general purpose paradigm for inspecting, navigating, mining, and transforming annotation repositories—the common representation basis for modern pipelined text processing architectures. The open-ended nature of these architectures and expressiveness of feature structure-based annotation schemes account for the natural tendency of such annotation repositories to become very dense, as multiple levels of analysis get encoded as layered annotations. This particular characteristic presents challenges for the design of a pattern matching framework capable of interpreting ‘flat’ patterns over arbitrarily dense annotation lattices. We present an approach where a finite state device applies (compiled) pattern grammars over what is, in effect, a linearized ‘projection’ of a particular route through the lattice. The route is derived by a mix of static grammar analysis and runtime interpretation of navigational directives within an extended grammar formalism; it selects just the annotations sequence appropriate for the patterns at hand. For expressive and efficient pattern matching in dense annotations stores, our implemented approach achieves a mix of lattice traversal and finite state scanning by exposing a language which, to its user, provides constructs for specifying sequential, structural, and configurational constraints among annotations.","Language Resources and Evaluation",2010,"No","afst uima annot base analyt develop pattern match annot annot lattic high densiti annot repositori finit state transduct corpus analysi framework travers dens annot lattic pattern match queri annot general purpos paradigm inspect navig mine transform annot repositori common represent basi modern pipelin text process architectur open end natur architectur express featur structur base annot scheme account natur tendenc annot repositori dens multipl level analysi encod layer annot characterist present challeng design pattern match framework capabl interpret flat pattern arbitrarili dens annot lattic present approach finit state devic appli compil pattern grammar effect linear project rout lattic rout deriv mix static grammar analysi runtim interpret navig direct extend grammar formal select annot sequenc pattern hand express effici pattern match dens annot store implement approach achiev mix lattic travers finit state scan expos languag user construct sequenti structur configur constraint annot",0
"KeywordsReferential translation machine RTM Semantic similarity Machine translation Performance prediction Machine translation performance prediction ","referential translation machines for predicting semantic similarity","Referential translation machines (RTMs) are a computational model effective at judging monolingual and bilingual similarity while identifying translation acts between any two data sets with respect to interpretants, data close to the task instances. RTMs pioneer a language-independent approach to all similarity tasks and remove the need to access any task- or domain-specific information or resource. We use RTMs for predicting the semantic similarity of text and present state-of-the-art results showing that RTMs can achieve better results on the test set than on the training set. Interpretants are used to derive features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and the presence of the acts of translation, which may ubiquitously be observed in communication. RTMs can achieve top performance at SemEval in various semantic similarity prediction tasks as well as similarity prediction tasks in bilingual settings. We obtain rankings of various prediction tasks using the performance of RTM and relative evaluation metrics, which can help identify which tasks and subtasks require more work by design.","Language Resources and Evaluation",2016,"No","referenti translat machin rtm semant similar machin translat perform predict machin translat perform predict referenti translat machin predict semant similar referenti translat machin rtms comput model effect judg monolingu bilingu similar identifi translat act data set respect interpret data close task instanc rtms pioneer languag independ approach similar task remov access task domain specif inform resourc rtms predict semant similar text present state art result show rtms achiev result test set train set interpret deriv featur measur close test sentenc train data difficulti translat presenc act translat ubiquit observ communic rtms achiev top perform semev semant similar predict task similar predict task bilingu set obtain rank predict task perform rtm relat evalu metric identifi task subtask requir work design",0
"KeywordsHyponym and hypernym learning Text mining Ontology induction Wordnet evaluation ","tailoring the automated construction of large scale taxonomies using the web","It has long been a dream to have available a single, centralized, semantic thesaurus or terminology taxonomy to support research in a variety of fields. Much human and computational effort has gone into constructing such resources, including the original WordNet and subsequent wordnets in various languages. To produce such resources one has to overcome well-known problems in achieving both wide coverage and internal consistency within a single wordnet and across many wordnets. In particular, one has to ensure that alternative valid taxonomizations covering the same basic terms are recognized and treated appropriately. In this paper we describe a pipeline of new, powerful, minimally supervised, automated algorithms that can be used to construct terminology taxonomies and wordnets, in various languages, by harvesting large amounts of online domain-specific or general text. We illustrate the effectiveness of the algorithms both to build localized, domain-specific wordnets and to highlight and investigate certain deeper ontological problems such as parallel generalization hierarchies. We show shortcomings and gaps in the manually-constructed English WordNet in various domains.","Language Resources and Evaluation",2013,"No","hyponym hypernym learn text mine ontolog induct wordnet evalu tailor autom construct larg scale taxonomi web long dream singl central semant thesaurus terminolog taxonomi support research varieti field human comput effort construct resourc includ origin wordnet subsequ wordnet languag produc resourc overcom problem achiev wide coverag intern consist singl wordnet wordnet ensur altern valid taxonom cover basic term recogn treat appropri paper describ pipelin power minim supervis autom algorithm construct terminolog taxonomi wordnet languag harvest larg amount onlin domain specif general text illustr effect algorithm build local domain specif wordnet highlight investig deeper ontolog problem parallel general hierarchi show shortcom gap manual construct english wordnet domain",0
"KeywordsInformation extraction Lexical ontology Wordnet Clustering Semantic relations ","eco and ontopt a flexible approach for creating a portuguese wordnet automatically","A wordnet is an important tool for developing natural language processing applications for a language. However, most wordnets are handcrafted by experts, which limits their growth. In this article, we propose an automatic approach to create wordnets by exploiting textual resources, dubbed ECO. After extracting semantic relation instances, identified by discriminating textual patterns, ECO discovers synonymy clusters, used as synsets, and attaches the remaining relations to suitable synsets. Besides introducing each step of ECO, we report on how it was implemented to create Onto.PT, a public lexical ontology for Portuguese. Onto.PT is the result of the automatic exploitation of Portuguese dictionaries and thesauri, and it aims to minimise the main limitations of existing Portuguese lexical knowledge bases.","Language Resources and Evaluation",2014,"No","inform extract lexic ontolog wordnet cluster semant relat eco ontopt flexibl approach creat portugues wordnet automat wordnet import tool develop natur languag process applic languag wordnet handcraft expert limit growth articl propos automat approach creat wordnet exploit textual resourc dub eco extract semant relat instanc identifi discrimin textual pattern eco discov synonymi cluster synset attach remain relat suitabl synset introduc step eco report implement creat pt public lexic ontolog portugues pt result automat exploit portugues dictionari thesauri aim minimis main limit exist portugues lexic knowledg base",0
"agglutinative languages authorship attribution statistical analysis stylochoronometry stylometry Turkish ","change of writing style with time","This study investigates the writing stylechange of two Turkish authors, Çetin Altanand Yaşar Kemal, in their old and newworks using respectively their newspapercolumns and novels. The style markers are thefrequencies of word lengths in both text andvocabulary, and the rate of usage of mostfrequent words. For both authors, t-tests andlogistic regressions show that the length ofthe words in new works is significantly longerthan that of the old. The principal componentanalyses graphically illustrate the separationbetween old and new texts. The works arecorrectly categorized as old or new with 75 to100% accuracy and 92% average accuracy usingdiscriminant analysis-based cross validation. The results imply higher time gap may havepositive impact in separation andcategorization. For Altan a regressionanalysis demonstrates a decrease in averageword length as the age of his column increases. One interesting observation is that for oneword each author has similar preference changesover time.","Computers and the Humanities",2004,"No","agglutin languag authorship attribut statist analysi stylochoronometri stylometri turkish chang write style time studi investig write stylechang turkish author etin altanand ya ar kemal newwork newspapercolumn novel style marker thefrequ word length text andvocabulari rate usag mostfrequ word author test andlogist regress show length ofth word work signific longerthan princip componentanalys graphic illustr separationbetween text work arecorrect categor to accuraci averag accuraci usingdiscrimin analysi base cross valid result impli higher time gap haveposit impact separ andcategor altan regressionanalysi demonstr decreas averageword length age column increas interest observ oneword author similar prefer changesov time",0
"Key wordswordlist lexical analysis semantic fields collocate display distribution display index display KWIC ","literary studies a computer assisted teaching methodology","We used TACT computer software to teach Joseph Conrad's novel Heart of Darkness to BA (Hops) students at the University of Luton in England. Conrad's novel is one of the texts used in the ‘Language and New Literatures’ modules (units). In these modules we combine analytical approaches to literary texts with linguistic methods. We used TACT to reinforce the understanding of the text of Heart of Darkness achieved through such a combination of methods. An exposure to the computer-based approaches to the text described in this article made the students' interaction with the text a more complex and rewarding experience.","Computers and the Humanities",1996,"No","key wordswordlist lexic analysi semant field colloc display distribut display index display kwic literari studi comput assist teach methodolog tact comput softwar teach joseph conrad heart dark ba hop student univers luton england conrad text languag literatur modul unit modul combin analyt approach literari text linguist method tact reinforc understand text heart dark achiev combin method exposur comput base approach text articl made student interact text complex reward experi",0
"KeywordsAntonio Zampolli language resources and evaluation ","introduction to the special inaugural issue","This first issue of Language Resources and Evaluation is dedicated to the memory of Antonio Zampolli, whom few would dispute is the one person who has led the way in promoting and establishing the development of language resources (LR) of all kinds for the past four decades. In this inaugural issue, we have attempted to bring together articles by major figures in the field in order to provide an overview of the history, state of the art, and the future of the creation, annotation, exploitation, evaluation, and distribution of LR. Hopefully, this collection of articles will serve not only as a tribute to Antonio, but also as a framework out of which this journal – which almost certainly would not have existed were it not for him – can grow.","Language Resources and Evaluation",2005,"No","antonio zampolli languag resourc evalu introduct special inaugur issu issu languag resourc evalu dedic memori antonio zampolli disput person led promot establish develop languag resourc lr kind past decad inaugur issu attempt bring articl major figur field order provid overview histori state art futur creation annot exploit evalu distribut lr collect articl serv tribut antonio framework journal exist grow",0
"KeywordsPhonetic transcription dictionary Pronunciation dictionary Grapheme to phoneme (G2P) conversion Letter-to-sound mapping Letter-to-phoneme (L2P) transcription Romanian language ","romanian phonetic transcription dictionary for speeding up language technology development","This paper intends to present a machine readable Romanian language pronunciation dictionary called NaviRo. The dictionary contains 138,500 unique words from the DexOnline dictionary together with their phonetic transcriptions in speech assessment method phonetic alphabet. The development of the pronunciation dictionary and the performed validation tests are also described in the paper. NaviRo pronunciation dictionary is freely available on the project website (http://users.utcluj.ro/~jdomokos/naviro) in plain text, Hidden Markov Model Toolkit and Festival speech synthesis system dictionary format. There are also available for download the used grapheme and phoneme sets and the audio samples for the used phonemes. The use of these resources is completely unrestricted for any research purposes in order to speed up Romanian language speech technology research.","Language Resources and Evaluation",2015,"No","phonet transcript dictionari pronunci dictionari graphem phonem gp convers letter sound map letter phonem lp transcript romanian languag romanian phonet transcript dictionari speed languag technolog develop paper intend present machin readabl romanian languag pronunci dictionari call naviro dictionari uniqu word dexonlin dictionari phonet transcript speech assess method phonet alphabet develop pronunci dictionari perform valid test paper naviro pronunci dictionari freeli project websit httpusersutclujrojdomokosnaviro plain text hidden markov model toolkit festiv speech synthesi system dictionari format download graphem phonem set audio sampl phonem resourc complet unrestrict research purpos order speed romanian languag speech technolog research",0
"KeywordsCollaboration Computer-mediated human interaction Interactive explanation Gaze Gesture Multimodal communication ","the importance of gaze and gesture in interactive multimodal explanation","The objective of this research is twofold. Firstly, we argue that gaze and gesture play an essential part in interactive explanation and that it is thus a multimodal phenomenon. Two corpora are analyzed: (1) a group of teacher novices and experts and (2) a student teacher dyad, both of whom construct explanations of students’ reasoning after viewing videos of student dyads who are solving physics problems. We illustrate roles of gaze in explanations constructed within a group and roles of gesture in explanation constructed within a dyad. Secondly, we show how the analysis of such knowledge-rich empirical data pinpoints particular difficulties in designing human–computer interfaces that can support explanation between humans, or a fortiori, that can support explanation between a human and a computer.","Language Resources and Evaluation",2007,"No","collabor comput mediat human interact interact explan gaze gestur multimod communic import gaze gestur interact multimod explan object research twofold first argu gaze gestur play essenti part interact explan multimod phenomenon corpora analyz group teacher novic expert student teacher dyad construct explan student reason view video student dyad solv physic problem illustr role gaze explan construct group role gestur explan construct dyad show analysi knowledg rich empir data pinpoint difficulti design human comput interfac support explan human fortiori support explan human comput",0
"KeywordsCombinatory categorial grammar CCG Treebank Hindi Non-projective dependencies ","hindi ccgbank a ccg treebank from the hindi dependency treebank","In this paper, we present an approach for automatically creating a combinatory categorial grammar (CCG) treebank from a dependency treebank for the subject–object–verb language Hindi. Rather than a direct conversion from dependency trees to CCG trees, we propose a two stage approach: a language independent generic algorithm first extracts a CCG lexicon from the dependency treebank. An exhaustive CCG parser then creates a treebank of CCG derivations. We also discuss special cases of this generic algorithm to handle linguistic phenomena specific to Hindi. In doing so we extract different constructions with long-range dependencies like coordinate constructions and non-projective dependencies resulting from constructions like relative clauses, noun elaboration and verbal modifiers.","Language Resources and Evaluation",2018,"No","combinatori categori grammar ccg treebank hindi project depend hindi ccgbank ccg treebank hindi depend treebank paper present approach automat creat combinatori categori grammar ccg treebank depend treebank subject object verb languag hindi direct convers depend tree ccg tree propos stage approach languag independ generic algorithm extract ccg lexicon depend treebank exhaust ccg parser creat treebank ccg deriv discuss special case generic algorithm handl linguist phenomena specif hindi extract construct long rang depend coordin construct project depend result construct relat claus noun elabor verbal modifi",0
"KeywordsComputational Linguistic ","fortiers accusations a reply",NA,"Computers and the Humanities",1989,"No","comput linguist fortier accus repli na",0
"Key wordshistory and computing historiography ","bringing bacon home the divergent progress of computer aided historical research in europe and the united states","This historiographical article surveys the different developmental trajectories of computer-aided historical research and teaching in Western Europe and in the United States, and seeks synergies which promise to enhance the discipline.","Computers and the Humanities",1996,"No","key wordshistori comput historiographi bring bacon home diverg progress comput aid histor research europ unit state historiograph articl survey development trajectori comput aid histor research teach western europ unit state seek synergi promis enhanc disciplin",0
"Word Sense Disambiguation lexical tuning part of speech tagging lexical rules vagueness ","is word sense disambiguation just one more nlp task","The paper examines the task of Word Sense Disambiguation (WSD) criticallyand compares it with Part of Speech (POS) tagging, arguing that the abilityof a writer to create new senses distinguishes the tasks and makes it moreproblematic to test WSD by the mark-up-and-model paradigm, because newsenses cannot be marked up against dictionaries. This serves to set WSDapart and puts limits on its effectiveness as an independent NLP task.Moreover, it is argued that current WSD methods based on very small wordsamples are also potentially misleading because they may or may not scaleup. Since all-word WSD methods are now available and are producing figurescomparable to the smaller scale tasks, it is argued that we shouldconcentrate on the former and find ways of bootstrapping test materialsfor such tests in the future.","Computers and the Humanities",2000,"No","word sens disambigu lexic tune part speech tag lexic rule vagu word sens disambigu nlp task paper examin task word sens disambigu wsd criticallyand compar part speech pos tag argu abilityof writer creat sens distinguish task make moreproblemat test wsd mark model paradigm newsens mark dictionari serv set wsdapart put limit effect independ nlp task argu current wsd method base small wordsampl potenti mislead scaleup word wsd method produc figurescompar smaller scale task argu shouldconcentr find way bootstrap test materialsfor test futur",0
"KeywordsSentence and clause structure Dependency and coordination Annotation ","annotation of sentence structure","The focus of this article is on the creation of a collection of sentences manually annotated with respect to their sentence structure. We show that the concept of linear segments—linguistically motivated units, which may be easily detected automatically—serves as a good basis for the identification of clauses in Czech. The segment annotation captures such relationships as subordination, coordination, apposition and parenthesis; based on segmentation charts, individual clauses forming a complex sentence are identified. The annotation of a sentence structure enriches a dependency-based framework with explicit syntactic information on relations among complex units like clauses. We have gathered a collection of 3,444 sentences from the Prague Dependency Treebank, which were annotated with respect to their sentence structure (these sentences comprise 10,746 segments forming 6,341 clauses). The main purpose of the project is to gain a development data—promising results for Czech NLP tools (as a dependency parser or a machine translation system for related languages) that adopt an idea of clause segmentation have been already reported. The collection of sentences with annotated sentence structure provides the possibility of further improvement of such tools.","Language Resources and Evaluation",2012,"No","sentenc claus structur depend coordin annot annot sentenc structur focus articl creation collect sentenc manual annot respect sentenc structur show concept linear segment linguist motiv unit easili detect automat serv good basi identif claus czech segment annot captur relationship subordin coordin apposit parenthesi base segment chart individu claus form complex sentenc identifi annot sentenc structur enrich depend base framework explicit syntact inform relat complex unit claus gather collect sentenc pragu depend treebank annot respect sentenc structur sentenc compris segment form claus main purpos project gain develop data promis result czech nlp tool depend parser machin translat system relat languag adopt idea claus segment report collect sentenc annot sentenc structur possibl improv tool",0
"KeywordsThesauri Controlled vocabularies Manual translation process ","a cost effective lexical acquisition process for large scale thesaurus translation","Thesauri and controlled vocabularies facilitate access to digital collections by explicitly representing the underlying principles of organization. Translation of such resources into multiple languages is an important component for providing multilingual access. However, the specificity of vocabulary terms in most thesauri precludes fully-automatic translation using general-domain lexical resources. In this paper, we present an efficient process for leveraging human translations to construct domain-specific lexical resources. This process is illustrated on a thesaurus of 56,000 concepts used to catalog a large archive of oral histories. We elicited human translations on a small subset of concepts, induced a probabilistic phrase dictionary from these translations, and used the resulting resource to automatically translate the rest of the thesaurus. Two separate evaluations demonstrate the acceptability of the automatic translations and the cost-effectiveness of our approach.","Language Resources and Evaluation",2009,"No","thesauri control vocabulari manual translat process cost effect lexic acquisit process larg scale thesaurus translat thesauri control vocabulari facilit access digit collect explicit repres under principl organ translat resourc multipl languag import compon provid multilingu access specif vocabulari term thesauri preclud fulli automat translat general domain lexic resourc paper present effici process leverag human translat construct domain specif lexic resourc process illustr thesaurus concept catalog larg archiv oral histori elicit human translat small subset concept induc probabilist phrase dictionari translat result resourc automat translat rest thesaurus separ evalu demonstr accept automat translat cost effect approach",0
"KeywordsMultiword expressions Treebanks Annotation Inter-annotator agreement Named entities ","annotation of multiword expressions in the prague dependency treebank","We describe annotation of multiword expressions (MWEs) in the Prague dependency treebank, using several automatic pre-annotation steps. We use subtrees of the tectogrammatical tree structures of the Prague dependency treebank to store representations of the MWEs in the dictionary and pre-annotate following occurrences automatically. We also show a way to measure reliability of this type of annotation.","Language Resources and Evaluation",2010,"No","multiword express treebank annot inter annot agreement name entiti annot multiword express pragu depend treebank describ annot multiword express mwes pragu depend treebank automat pre annot step subtre tectogrammat tree structur pragu depend treebank store represent mwes dictionari pre annot occurr automat show measur reliabl type annot",0
"KeywordsParsing Textual entailments ","parser evaluation using textual entailments","Parser Evaluation using Textual Entailments (PETE) is a shared task in the SemEval-2010 Evaluation Exercises on Semantic Evaluation. The task involves recognizing textual entailments based on syntactic information alone. PETE introduces a new parser evaluation scheme that is formalism independent, less prone to annotation error, and focused on semantically relevant distinctions. This paper describes the PETE task, gives an error analysis of the top-performing Cambridge system, and introduces a standard entailment module that can be used with any parser that outputs Stanford typed dependencies.","Language Resources and Evaluation",2013,"No","pars textual entail parser evalu textual entail parser evalu textual entail pete share task semev evalu exercis semant evalu task involv recogn textual entail base syntact inform pete introduc parser evalu scheme formal independ prone annot error focus semant relev distinct paper describ pete task error analysi top perform cambridg system introduc standard entail modul parser output stanford type depend",0
"KeywordsUrdu Deep grammars Grammer engineering Parallel grammar development LFG ","urdu in a parallel grammar development environment","In this paper, we report on the role of the Urdu grammar in the Parallel Grammar (ParGram) project (Butt, M., King, T. H., Niño, M.-E., & Segond, F. (1999). A grammar writer’s cookbook. CSLI Publications; Butt, M., Dyvik, H., King, T. H., Masuichi, H., & Rohrer, C. (2002). ‘The parallel grammar project’. In: Proceedings of COLING 2002, Workshop on grammar engineering and evaluation, pp. 1–7). The Urdu grammar was able to take advantage of standards in analyses set by the original grammars in order to speed development. However, novel constructions, such as correlatives and extensive complex predicates, resulted in expansions of the analysis feature space as well as extensions to the underlying parsing platform. These improvements are now available to all the project grammars.","Language Resources and Evaluation",2007,"No","urdu deep grammar grammer engin parallel grammar develop lfg urdu parallel grammar develop environ paper report role urdu grammar parallel grammar pargram project butt king ni segond grammar writer cookbook csli public butt dyvik king masuichi rohrer parallel grammar project proceed cole workshop grammar engin evalu pp urdu grammar advantag standard analys set origin grammar order speed develop construct correl extens complex predic result expans analysi featur space extens under pars platform improv project grammar",0
NA,"instructions for authors",NA,"Computers and the Humanities",2003,"No","instruct author na",0
"KeywordsInfectious disease surveillance Multilingual ontology Text mining ","a multilingual ontology for infectious disease surveillance rationale design and challenges","A lack of surveillance system infrastructure in the Asia-Pacific region is seen as hindering the global control of rapidly spreading infectious diseases such as the recent avian H5N1 epidemic. As part of improving surveillance in the region, the BioCaster project aims to develop a system based on text mining for automatically monitoring Internet news and other online sources in several regional languages. At the heart of the system is an application ontology which serves the dual purpose of enabling advanced searches on the mined facts and of allowing the system to make intelligent inferences for assessing the priority of events. However, it became clear early on in the project that existing classification schemes did not have the necessary language coverage or semantic specificity for our needs. In this article we present an overview of our needs and explore in detail the rationale and methods for developing a new conceptual structure and multilingual terminological resource that focusses on priority pathogens and the diseases they cause. The ontology is made freely available as an online database and downloadable OWL file.","Language Resources and Evaluation",2007,"No","infecti diseas surveil multilingu ontolog text mine multilingu ontolog infecti diseas surveil rational design challeng lack surveil system infrastructur asia pacif region hinder global control rapid spread infecti diseas recent avian hn epidem part improv surveil region biocast project aim develop system base text mine automat monitor internet news onlin sourc region languag heart system applic ontolog serv dual purpos enabl advanc search mine fact allow system make intellig infer assess prioriti event clear earli project exist classif scheme languag coverag semant specif articl present overview explor detail rational method develop conceptu structur multilingu terminolog resourc focuss prioriti pathogen diseas ontolog made freeli onlin databas download owl file",0
"Keywordsauthorship attribution Herdan’s Vm lexical statistics vocabulary repetition vocabulary richness Yule’s K ","yules characteristic k revisited","The measure of lexical repetition constitutes one of the variables used to determine the lexical richness of literary texts, a value further employed in authorship attribution studies. Although most of the constants for lexical richness actually depend on text length, Yule’s characteristic is considered to be highly reliable for being text length independent. It is not the aim of this paper questioning the validity of K to measure the lexical repeat-rate, nor to evaluate its usefulness in authorship studies, but to review the most accurate procedure to calculate its value in the light of the lack of standardization found in the specific literature. At the same time, the peculiar calculation of Yule’s K by TACT is explained. Our study suggests that standardization will certainly help improve the studies where K is employed.","Language Resources and Evaluation",2005,"No","authorship attribut herdan vm lexic statist vocabulari repetit vocabulari rich yule yule characterist revisit measur lexic repetit constitut variabl determin lexic rich literari text employ authorship attribut studi constant lexic rich depend text length yule characterist consid high reliabl text length independ aim paper question valid measur lexic repeat rate evalu use authorship studi review accur procedur calcul light lack standard found specif literatur time peculiar calcul yule tact explain studi suggest standard improv studi employ",0
"KeywordsWordnet WordNet Synset Lexical unit plWordNet Wordnet relations Constitutive relations Register Aspect ","the chicken and egg problem in wordnet design synonymy synsets and constitutive relations","Wordnets are built of synsets, not of words. A synset consists of words. Synonymy is a relation between words. Words go into a synset because they are synonyms. Later, a wordnet treats words as synonymous because they belong in the same synset\(\ldots\) Such circularity, a well-known problem, poses a practical difficulty in wordnet construction, notably when it comes to maintaining consistency. We propose to make a wordnet a net of words or, to be more precise, lexical units. We discuss our assumptions and present their implementation in a steadily growing Polish wordnet. A small set of constitutive relations allows us to construct synsets automatically out of groups of lexical units with the same connectivity. Our analysis includes a thorough comparative overview of systems of relations in several influential wordnets. The additional synset-forming mechanisms include stylistic registers and verb aspect.","Language Resources and Evaluation",2013,"No","wordnet wordnet synset lexic unit plwordnet wordnet relat constitut relat regist aspect chicken egg problem wordnet design synonymi synset constitut relat wordnet built synset word synset consist word synonymi relat word word synset synonym wordnet treat word synonym belong synsetldot circular problem pose practic difficulti wordnet construct notabl maintain consist propos make wordnet net word precis lexic unit discuss assumpt present implement steadili grow polish wordnet small set constitut relat construct synset automat group lexic unit connect analysi includ compar overview system relat influenti wordnet addit synset form mechan includ stylist regist verb aspect",0
"KeywordsDependency treebank Annotation scheme Harmonization ","hamledt harmonized multi language dependency treebank","We present HamleDT—a HArmonized Multi-LanguagE Dependency Treebank. HamleDT is a compilation 
of existing dependency treebanks (or dependency conversions of other treebanks), transformed so that they all conform to the same annotation style. In the present article, we provide a thorough investigation and discussion of a number of phenomena that are comparable across languages, though their annotation in treebanks often differs. We claim that transformation procedures can be designed to automatically identify most such phenomena and convert them to a unified annotation style. This unification is beneficial both to comparative corpus linguistics and to machine learning of syntactic parsing.","Language Resources and Evaluation",2014,"No","depend treebank annot scheme harmon hamledt harmon multi languag depend treebank present hamledt harmon multi languag depend treebank hamledt compil exist depend treebank depend convers treebank transform conform annot style present articl provid investig discuss number phenomena compar languag annot treebank differ claim transform procedur design automat identifi phenomena convert unifi annot style unif benefici compar corpus linguist machin learn syntact pars",0
"KeywordsComputational Linguistic Word Index ","automated concordances and word indexes the process the programs and the products",NA,"Computers and the Humanities",1981,"No","comput linguist word index autom concord word index process program product na",0
"KeywordsComputational Linguistic Future History ","machine readable archives and future history",NA,"Computers and the Humanities",1976,"No","comput linguist futur histori machin readabl archiv futur histori na",0
"KeywordsComputational Linguistic Language Curriculum ","the role of computer assisted learning in a proficiency based language curriculum",NA,"Computers and the Humanities",1990,"No","comput linguist languag curriculum role comput assist learn profici base languag curriculum na",0
"Canterbury Tales Chaucer critical editions electronic publishing SGML ","publishing an electronic textual edition the case of the wife of baths prologue on cd rom","The article reports on one of the more sophisticated critical editions ever to be published in electronic format. The Wife of Bath is richly encoded, provides access to literally thousands of manuscript images, and enables users to assess the relationships between the numerous extant manuscript editions. The authors assess the methods used in the edition's development and the lessons learned through its production.","Computers and the Humanities",1998,"No","canterburi tale chaucer critic edit electron publish sgml publish electron textual edit case wife bath prologu cd rom articl report sophist critic edit publish electron format wife bath rich encod access liter thousand manuscript imag enabl user assess relationship numer extant manuscript edit author assess method edit develop lesson learn product",0
"KeywordsComputational Linguistic ","the making of a masterpiece stephan cranes the red badge of courage",NA,"Computers and the Humanities",1980,"No","comput linguist make masterpiec stephan crane red badg courag na",0
"Key Wordsstyle/stylistics quantitative studies of literature ambiguity/disambiguation/figurative language textuality literary theory relevance parody reader response (theory/research) ","quantitative studies of literature a critique and an outlook","The present paper is a critique of quantitative studies of literature. It is argued that such studies are involved in an act of reification, in which, moreover, fundamental ingredients of the texts, e.g. their (highly important) range of figurative meanings, are eliminated from the analysis. Instead a concentration on lower levels of linguistic organization, such as grammar and lexis, may be observed, in spite of the fact that these are often the least relevant aspects of the text. In doing so, quantitative studies of literature significantly reduce not only the cultural value of texts, but also the generalizability of its own findings. What is needed, therefore, is an awareness and readiness to relate to matters of textuality as an organizing principle underlying the cultural functioning of literary works of art.","Computers and the Humanities",1989,"No","key wordsstylestylist quantit studi literatur ambiguitydisambiguationfigur languag textual literari theori relev parodi reader respons theoryresearch quantit studi literatur critiqu outlook present paper critiqu quantit studi literatur argu studi involv act reific fundament ingredi text high import rang figur mean elimin analysi concentr lower level linguist organ grammar lexi observ spite fact relev aspect text quantit studi literatur signific reduc cultur text generaliz find need awar readi relat matter textual organ principl under cultur function literari work art",0
"KeywordsComputational Linguistic ","some considerations concerning encoding and concording texts",NA,"Computers and the Humanities",1978,"No","comput linguist consider encod concord text na",0
"KeywordsAmbiguous Word Basic Word Lexical Ambiguity Punctuation Mark Word Combination ","the contextological dictionary use in programmed language teaching",NA,"Computers and the Humanities",1979,"No","ambigu word basic word lexic ambigu punctuat mark word combin contextolog dictionari program languag teach na",0
"KeywordsComputational Linguistic Humanity Computing Select Resource ","select resources for image based humanities computing",NA,"Computers and the Humanities",2002,"No","comput linguist human comput select resourc select resourc imag base human comput na",0
"Key Wordscomputer writing aids grammar checkers style checkers computers and writing CorrecText Correct Grammar ","a new grammar checker","CorrecText, from Houghton-Mifflin, is a significant advance in grammar checkers, because it uses a full parse of sentences in its analysis. Though limited by the fact that many English sentences are syntactically ambiguous, the program can find many errors in grammar, style, and usage. Still, questions remain about where and how it can be useful. It is not terribly useful on final versions of good prose; the makers assert that it is more useful on unedited prose, where there are many inadvertent errors. An empirical study of much unedited prose would not only verify this assertion, but would help improve the program.","Computers and the Humanities",1990,"No","key wordscomput write aid grammar checker style checker comput write correctext correct grammar grammar checker correctext houghton mifflin signific advanc grammar checker full pars sentenc analysi limit fact english sentenc syntact ambigu program find error grammar style usag question remain terribl final version good prose maker assert unedit prose inadvert error empir studi unedit prose verifi assert improv program",0
"KeywordsComputational Linguistic Interactive Program Dialect Dictionary ","use of an interactive program in analyzing data for a dialect dictionary",NA,"Computers and the Humanities",1975,"No","comput linguist interact program dialect dictionari interact program analyz data dialect dictionari na",0
NA,"du texte latin la concordance imprime","The development of the technique of laser printing opens new ways for the writing of concordances to Latin texts, by means of electronic data processing. It is indeed possible to obtain as good a quality of printing as with traditional typography, even for a very important number of characters, which is always the case with concordances. A first step to obtaining perfection in the format of the final document consists in collecting data carefully and programming the software so as to provide scholars with a tool that will really facilitate their researches. Although the cutting out of the “useful context” is a very difficult problem to solve when using computers, as many data (punctuation, capital letters, diacritic signs...) as possible can be collected from the very beginning. These data will then make the text provided for each keyword more comprehensible. The programming must obviously be improved to take all the collected data into account. The reader of a concordance produced in such a way will have at his disposal a text easy to read not only because of its typographic quality, but also because of all its information on reference, punctuation, lemma, manuscript tradition; also he will be able to find in appendices pertinent frequency lists, lists of names and all sorts of lexical, morphological, syntactical information according to the work involved in the data collecting and programming.","Computers and the Humanities",1986,"No","du text latin la concord imprim develop techniqu laser print open way write concord latin text mean electron data process obtain good qualiti print tradit typographi import number charact case concord step obtain perfect format final document consist collect data care program softwar provid scholar tool facilit research cut context difficult problem solv comput data punctuat capit letter diacrit sign collect begin data make text provid keyword comprehens program improv collect data account reader concord produc dispos text easi read typograph qualiti inform refer punctuat lemma manuscript tradit find appendic pertin frequenc list list name sort lexic morpholog syntact inform work involv data collect program",0
"KeywordsNatural Language Computational Linguistic Language Text Natural Language Text Computer Detection ","computer detection of errors in natural language texts some research on pattern matching",NA,"Computers and the Humanities",1987,"No","natur languag comput linguist languag text natur languag text comput detect comput detect error natur languag text research pattern match na",0
"KeywordsComputational Linguistic ","prolegomena to pictorial concordances",NA,"Computers and the Humanities",1981,"No","comput linguist prolegomena pictori concord na",0
"Key Wordstree model tree topology fitting tree algorithm textual analysis lexical connexion Victor Hugo Emile Zola semantic memory ","using a tree model in textual analysis","The purpose of this paper is to show the utility of the application of a non-ultrametric tree-model to textual data. The first part introduces a basic topological property of the tree and the notion of neighbourhood, which reflects the structure of the tree. The second part emphasizes through illustration examples the adequacy of this model for representing different varieties of textual data.","Computers and the Humanities",1989,"No","key wordstre model tree topolog fit tree algorithm textual analysi lexic connexion victor hugo emil zola semant memori tree model textual analysi purpos paper show util applic ultrametr tree model textual data part introduc basic topolog properti tree notion neighbourhood reflect structur tree part emphas illustr exampl adequaci model repres varieti textual data",0
"Key WordsEnglish romantic renaissance drama tragedy computational stylistics function-words multivariate statistics ","lyrical drama and the turbid mountebanks styles of dialogue in romantic and renaissance tragedy","Critics have condemned English Romantic tragedies as a series of poor imitations of Renaissance tragedy. This paper tests such “literary-critical” questions through statistical comparisons of ten plays from each group. The measures chosen give evidence of a strong and consistent difference between the groups, going beyond historical changes in the language. The Romantic tragedies are more expository; the Renaissance ones include more commonplace interactions between characters. The later plays do not show the marked variations in function-word frequencies of their predecessors. Of the Renaissance plays, Shakespeare's show the closest affinity to the Romantic tragedies, and the most telling contrasts.","Computers and the Humanities",1994,"No","key wordsenglish romant renaiss drama tragedi comput stylist function word multivari statist lyric drama turbid mountebank style dialogu romant renaiss tragedi critic condemn english romant tragedi seri poor imit renaiss tragedi paper test literari critic question statist comparison ten play group measur chosen give evid strong consist differ group histor languag romant tragedi expositori renaiss includ commonplac interact charact play show mark variat function word frequenc predecessor renaiss play shakespear show closest affin romant tragedi tell contrast",0
"KeywordsComputational Linguistic Software Review ","software reviews",NA,"Computers and the Humanities",1986,"No","comput linguist softwar review softwar review na",0
"KeywordsComputer Analysis Computational Linguistic Metrical Pattern ","a computer analysis of metrical patterns inbeowulf",NA,"Computers and the Humanities",1978,"No","comput analysi comput linguist metric pattern comput analysi metric pattern inbeowulf na",0
NA,"software review",NA,"Computers and the Humanities",1989,"No","softwar review na",0
"KeywordsHumanity Research Computational Linguistic ","computer aided humanities research at the university of wisconsin",NA,"Computers and the Humanities",1969,"No","human research comput linguist comput aid human research univers wisconsin na",0
"Key Wordseducation human computer interaction artificial intelligence computing humanities ","beauty and the beast new approaches to teaching computing for humanities students at the university of aberdeen","This paper reports on the history and development of a new undergraduate course teaching computing for humanities students at the University of Aberdeen, and assesses some new teaching approaches developed in the course. It is noted that teaching computing to humanities students appears to be viewed with suspicion by some Computer Science and Humanities Departments. The two camps seem to fear, for different reasons, that issues and practices important to their disciplines will be compromised or watered down. This paper describes an attempt to reverse any such attitudes on the part of staff and students and to take undergraduates considerably beyond mere word processing and computer literacy. Various methods and techniques used in the course are presented and their value assessed. The importance of using a consistent computer interface to helping students form a stable conceptual model of computers is considered. We reflect on the value of teaching more about Human Computer Interaction and Artificial Intelligence than is usual in Humanities Computing courses. A number of lessons are drawn from the course.","Computers and the Humanities",1992,"No","key wordseduc human comput interact artifici intellig comput human beauti beast approach teach comput human student univers aberdeen paper report histori develop undergradu teach comput human student univers aberdeen assess teach approach develop note teach comput human student appear view suspicion comput scienc human depart camp fear reason issu practic import disciplin compromis water paper describ attempt revers attitud part staff student undergradu consider mere word process comput literaci method techniqu present assess import consist comput interfac help student form stabl conceptu model comput consid reflect teach human comput interact artifici intellig usual human comput cours number lesson drawn",0
"KeywordsComputational Linguistic Universal Alphabet ","a universal alphabet for experiments in comparative phonology",NA,"Computers and the Humanities",1981,"No","comput linguist univers alphabet univers alphabet experi compar phonolog na",0
NA,"projekt untersuchungen zu altislndischen rechtstexten","This paper gives a brief survey of the Saarbrücken project on Old Icelandic legal texts, sponsored by the German Research Society, within the Special Research Area “Computer linguistics.” The project's main points of interest are (1) producing adequate machine-readable versions and parsed indices of all legal texts in Old Icelandic, (2) graphemic studies of legal manuscripts, and (3) studies of the distribution and valence of the verbs in those texts. A proposal for encoding Old Norse/Old Icelandic demonstrates how texts of different standards (normalized, diplomatic, graphetic) can be encoded as compatibly as possible. The description of a combined normalization-lemmatization process reveals that even little normalization in a diplomatic text will save much manual parsing.","Computers and the Humanities",1978,"No","projekt untersuchungen zu altislndischen rechtstexten paper survey saarbr cken project iceland legal text sponsor german research societi special research area comput linguist project main point interest produc adequ machin readabl version pars indic legal text iceland graphem studi legal manuscript studi distribut valenc verb text propos encod nors iceland demonstr text standard normal diplomat graphet encod compat descript combin normal lemmat process reveal normal diplomat text save manual pars",0
"KeywordsComputational Linguistic Literary Attribution ","literary attribution and likelihood ratio tests the case of the middle englishpearl poems",NA,"Computers and the Humanities",1983,"No","comput linguist literari attribut literari attribut likelihood ratio test case middl englishpearl poem na",0
"Key wordsChrétien de Troyes complex media database electronic archive electronic text hypertext medieval manuscript culture Old French TEI World Wide Web ","the charrette project manipulating text and image in an electronic archive of a medieval manuscript tradition","This paper concerns the Charrette Project, a multimedia electronic archive of a medieval manuscript tradition. In this paper, we argue that the computer's strengths in manipulating complex and varied resources should be an important organizing principle in the conception and construction of electronic text projects. Specifically, we describe the elements of the Charrette archive, its architecture, and its potential for scholarly research and pedagogical applications.","Computers and the Humanities",1996,"No","key wordschr tien de troy complex media databas electron archiv electron text hypertext mediev manuscript cultur french tei world wide web charrett project manipul text imag electron archiv mediev manuscript tradit paper concern charrett project multimedia electron archiv mediev manuscript tradit paper argu comput strength manipul complex vari resourc import organ principl concept construct electron text project specif describ element charrett archiv architectur potenti scholar research pedagog applic",0
"TEI SGML proper nouns text encoding Women Writers Project CELT ","names proper and improper applying the tei to the classification of proper nouns","This paper discusses the encoding of proper names using the TEI Guidelines, describing the practice of the Women Writers Project at Brown University, and the CELT Project at University College, Cork. We argue that such encoding may be necessary to enable historical and literary research, and that the specific approach taken will depend on the needs of the project and the audience to be served. Because the TEI Guidelines provide a fairly flexible system for the encoding of proper names, we conclude that projects may need to collaborate to determine more specific constraints, to ensure consistency of approach and compatibility of data.","Computers and the Humanities",1997,"No","tei sgml proper noun text encod women writer project celt name proper improp appli tei classif proper noun paper discuss encod proper name tei guidelin describ practic women writer project brown univers celt project univers colleg cork argu encod enabl histor literari research specif approach depend project audienc serv tei guidelin provid fair flexibl system encod proper name conclud project collabor determin specif constraint ensur consist approach compat data",0
"Key wordssound symbolism alliteration Japanese poetry Kokinshû poetics Icon programming language database software ","a computer study of systematic sound symbolism in classical japanese verse","The analysis of sound symbolism in poetry is one of the more promising applications of computational methods. This paper proposes using database software with spreadsheet capabilities to give maximum versatility in the examination of consonant alliteration. In this case the database is drawn from a 10th century anthology of classical Japanese verse called theKokinshû. In recent years scholars have pointed out a few obvious examples of sound symbolism inKokinshû poetry. This study attempts to show that with these few notable exceptions, poetry of the period seems to have striven toward a balance in sound, avoiding techniques such as word initial alliteration which might call attention to itself.","Computers and the Humanities",1994,"No","key wordssound symbol alliter japanes poetri kokinsh poetic icon program languag databas softwar comput studi systemat sound symbol classic japanes vers analysi sound symbol poetri promis applic comput method paper propos databas softwar spreadsheet capabl give maximum versatil examin conson alliter case databas drawn th centuri antholog classic japanes vers call thekokinsh recent year scholar point obvious exampl sound symbol inkokinsh poetri studi attempt show notabl except poetri period striven balanc sound avoid techniqu word initi alliter call attent",0
"KeywordsComputational Linguistic Concordance Program ","an experimental concordance program",NA,"Computers and the Humanities",1970,"No","comput linguist concord program experiment concord program na",0
"KeywordsComputational Linguistic ","troubadours and transposition a computer aided study",NA,"Computers and the Humanities",1982,"No","comput linguist troubadour transposit comput aid studi na",0
"lexical statistics Monte Carlo methods vocabulary richness ","how variable may a constant be measures of lexical richness in perspective","A well-known problem in the domain of quantitative linguistics and stylistics concerns the evaluation of the lexical richness of texts. Since the most obvious measure of lexical richness, the vocabulary size (the number of different word types), depends heavily on the text length (measured in word tokens), a variety of alternative measures has been proposed which are claimed to be independent of the text length. This paper has a threefold aim. Firstly, we have investigated to what extent these alternative measures are truly textual constants. We have observed that in practice all measures vary substantially and systematically with the text length. We also show that in theory, only three of these measures are truly constant or nearly constant. Secondly, we have studied the extent to which these measures tap into different aspects of lexical structure. We have found that there are two main families of constants, one measuring lexical richness and one measuring lexical repetition. Thirdly, we have considered to what extent these measures can be used to investigate questions of textual similarity between and within authors. We propose to carry out such comparisons by means of the empirical trajectories of texts in the plane spanned by the dimensions of lexical richness and lexical repetition, and we provide a statistical technique for constructing confidence intervals around the empirical trajectories of texts. Our results suggest that the trajectories tap into a considerable amount of authorial structure without, however, guaranteeing that spatial separation implies a difference in authorship.","Computers and the Humanities",1998,"No","lexic statist mont carlo method vocabulari rich variabl constant measur lexic rich perspect problem domain quantit linguist stylist concern evalu lexic rich text obvious measur lexic rich vocabulari size number word type depend heavili text length measur word token varieti altern measur propos claim independ text length paper threefold aim first investig extent altern measur textual constant observ practic measur vari substanti systemat text length show theori measur constant constant studi extent measur tap aspect lexic structur found main famili constant measur lexic rich measur lexic repetit third consid extent measur investig question textual similar author propos carri comparison mean empir trajectori text plane span dimens lexic rich lexic repetit provid statist techniqu construct confid interv empir trajectori text result suggest trajectori tap consider amount authori structur guarante spatial separ impli differ authorship",0
NA,"standards for encoding data in a natural language",NA,"Computers and the Humanities",1967,"No","standard encod data natur languag na",0
"Key Wordscomputational morphology two-level morphology morphological parsing Koskenniemi KIMMO interlinear text Tagalog TEX ","glossing text with the pc kimmo morphological parser","For those studying languages with rich word structures, a morphological parser is a valuable tool. PC-KIMMO is a parser for small computers that is based on Koskenniemi's two-level model of morphology. Of the many practical uses for a morphological parser such as PC-KIMMO, this article describes one: producing automatically glossed interlinear text.","Computers and the Humanities",1992,"No","key wordscomput morpholog level morpholog morpholog pars koskenniemi kimmo interlinear text tagalog tex gloss text pc kimmo morpholog parser studi languag rich word structur morpholog parser valuabl tool pc kimmo parser small comput base koskenniemi level model morpholog practic morpholog parser pc kimmo articl describ produc automat gloss interlinear text",0
"KeywordsComputational Linguistic Critical Survey ","the computer in archaeology a critical survey",NA,"Computers and the Humanities",1972,"No","comput linguist critic survey comput archaeolog critic survey na",0
"KeywordsPresent State Good Deal Modern Technology Computational Linguistic Scholarly Community ","computer implemented music analysis and the copyright law","Our discussion has shown not only that there is a good deal of uncertainty in the present state of the law concerning the problem that we posed, but also that, even if currently recommended legislation were to make the recently enacted copyright law fully applicable to it, a number of open questions would remain.","Computers and the Humanities",1980,"No","present state good deal modern technolog comput linguist scholar communiti comput implement music analysi copyright law discuss shown good deal uncertainti present state law problem pose recommend legisl make recent enact copyright law fulli applic number open question remain",0
"KeywordsComputational Linguistic Teaching Literature ","technology in teaching literature and culturesome reflections",NA,"Computers and the Humanities",2000,"No","comput linguist teach literatur technolog teach literatur culturesom reflect na",0
"language resources metadata open archives ","extending dublin core metadata to support the description and discovery of language resources","As language data and associatedtechnologies proliferate and as the languageresources community expands, it is becomingincreasingly difficult to locate and reuse existingresources. Are there any lexical resources forsuch-and-such a language? What tool workswith transcripts in this particular format?What is a good format to use for linguisticdata of this type? Questions like these dominate manymailing lists, since web search engines are anunreliable way to find language resources. Thispaper reports on a new digital infrastructurefor discovering language resources beingdeveloped by the Open Language Archives Community(OLAC). At the core of OLAC is its metadataformat, which is designed to facilitatedescription and discovery of all kinds oflanguage resources, including data, tools, oradvice. The paper describes OLAC metadata, itsrelationship to Dublin Core metadata, and itsdissemination using the metadata harvesting protocol of the Open Archives Initiative.","Computers and the Humanities",2003,"No","languag resourc metadata open archiv extend dublin core metadata support descript discoveri languag resourc languag data associatedtechnolog prolifer languageresourc communiti expand becomingincreas difficult locat reus existingresourc lexic resourc forsuch languag tool workswith transcript format good format linguisticdata type question domin manymail list web search engin anunreli find languag resourc thispap report digit infrastructurefor discov languag resourc beingdevelop open languag archiv communityolac core olac metadataformat design facilitatedescript discoveri kind oflanguag resourc includ data tool oradvic paper describ olac metadata itsrelationship dublin core metadata itsdissemin metadata harvest protocol open archiv initi",0
"PhotoCD digitisation World Wide Web medieval manuscript Aberdeen Bestiary ","text and illustration the digitisation of a medieval manuscript","This paper considers the choice of the medieval Aberdeen Bestiaryas the first project in Aberdeen University Library’s digitisationprogramme, and discusses some of the unusual features of themanuscript itself.","Computers and the Humanities",1997,"No","photocd digitis world wide web mediev manuscript aberdeen bestiari text illustr digitis mediev manuscript paper consid choic mediev aberdeen bestiarya project aberdeen univers librari digitisationprogramm discuss unusu featur themanuscript",0
NA,"etat prsent de lutilisation des ordinateurs pour ltude de la littrature franaise","This article examines only published work directly related to the study of French literature; excluded are automatic translation, production of bibliographies or critical editions, and linguistic studies.","Computers and the Humanities",1971,"No","etat prsent de lutilis des ordinateur pour ltude de la littratur franais articl examin publish work direct relat studi french literatur exclud automat translat product bibliographi critic edit linguist studi",0
"KeywordsComputational Linguistic System Overview Tune Index National Tune ","the national tune index a systems overview",NA,"Computers and the Humanities",1981,"No","comput linguist system overview tune index nation tune nation tune index system overview na",0
"KeywordsInternal Structure Computer Analysis Computational Linguistic Transition Area Complex Transition ","computer analysis of a dialectal transition belt","The layout of the isogloss routes suggests a complex transition area with transitions between the East and West Midlands as well as between the North and the South. And so it is. But it is essentially a transition-area between the Core North and the Core South, with the frontiers of minor speech-areas super-imposed on that bilateral division.","Computers and the Humanities",1980,"No","intern structur comput analysi comput linguist transit area complex transit comput analysi dialect transit belt layout isogloss rout suggest complex transit area transit east west midland north south essenti transit area core north core south frontier minor speech area super impos bilater divis",0
"Key WordsRousseau Emile Profession de foi macro/micro-context modal sequence and length function/content words ","lexical and focal preferences in rousseausprofession de foi du vicaire savoyard book iv ofemile","Based on the ARTFL version of theProfession and excerpts fromEmile, high frequency function and content words, as defined by Brunet, are analyzed via Pearson chi square tests. Next, four measures of narrative voice from the same populations are compared using Markovian chains and further chi square tests. In a third analysis the two orders of evidence are juxtaposed. The lexical and narratological preferences of theVicaire and theGouverneur, while not resolving the problematic of chronological composition (Burgelin, 1969), highlight the distinctiveness of each character.","Computers and the Humanities",1989,"No","key wordsrousseau emil profess de foi macromicro context modal sequenc length functioncont word lexic focal prefer rousseausprofess de foi du vicair savoyard book iv ofemil base artfl version theprofess excerpt fromemil high frequenc function content word defin brunet analyz pearson chi squar test measur narrat voic popul compar markovian chain chi squar test analysi order evid juxtapos lexic narratolog prefer thevicair thegouverneur resolv problemat chronolog composit burgelin highlight distinct charact",0
"Key WordsGIRCSE lexicography philology textual analysis lemmatization pre-editing research projects expression signs computing ","the interdisciplinary group for expression signs computing","The following is a description of the facilities, activities and research projects of the GIRCSE at the Università Cattolica del Sacro Cuore. The Group offers courses on computing in the humanities and assists scholars in their computer-based philological research. We discuss the problems of pre-editing of texts, lemmatization and textual analysis programs. We provide a comprehensive list of the Group's independent and collaborative research projects.","Computers and the Humanities",1990,"No","key wordsgircs lexicographi philolog textual analysi lemmat pre edit research project express sign comput interdisciplinari group express sign comput descript facil activ research project gircs universit cattolica del sacro cuor group offer cours comput human assist scholar comput base philolog research discuss problem pre edit text lemmat textual analysi program provid comprehens list group independ collabor research project",0
"KeywordsField Data Statistical Summary Contingency Table Formal Method Careful Research ","computer applications in cultural anthropology","This paper covers important developments in the use of computers for quantitative research in cultural anthropology, particularly in areas which (unlike statistics) are uniquely anthropological. These fall into statistical topics and topics in scaling and measurement. By far the largest single usage of computers by cultural anthropologists is for statistical summaries of field data and for simple statistical tests such as thechi-squared for the analysis of field data or for cross-cultural studies. As the discipline develops this situation will remain the same. In fact, the proportion of people who use the computer primarily for contingency tables, frequency counts, and correlation analysis may very well increase, since there are many potential users who would fall in this category and only a few potential users who would perform other operations such as multi-dimensional scaling or simulation. The few other computer techniques that would be relevant to anthropology, and for which the technology already exists, include linear regression, as practiced by economists, and linear programming (also practiced by economists), both of which could be extremely useful in the study of peasant economy. Careful research with such models could dispel some of the controversy which has been hindering the development of economic anthropology for the last fifteen years. The training of anthropologists who can understand the relevance of such models to their work may be far in the future, since the majority of them are still skeptical of most formal methods and of the computers which make them work.","Computers and the Humanities",1970,"No","field data statist summari conting tabl formal method care research comput applic cultur anthropolog paper cover import develop comput quantit research cultur anthropolog area unlik statist uniqu anthropolog fall statist topic topic scale measur largest singl usag comput cultur anthropologist statist summari field data simpl statist test thechi squar analysi field data cross cultur studi disciplin develop situat remain fact proport peopl comput primarili conting tabl frequenc count correl analysi increas potenti user fall categori potenti user perform oper multi dimension scale simul comput techniqu relev anthropolog technolog exist includ linear regress practic economist linear program practic economist extrem studi peasant economi care research model dispel controversi hinder develop econom anthropolog fifteen year train anthropologist understand relev model work futur major skeptic formal method comput make work",0
NA,"vers un systme expert pour lanalyse des textes de moyen francais","Concerning Medieval French, text-processing has to handle two major problems: first the under-developed state of linguistic knowledge of scholars; secondly the very large variations in forms and spellings occurring throughout the manuscripts. In order to extract and provide lexicographical and grammatical information from many yet undescribed texts, our research group has decided to devise an expert system which would help identify and parse the text-words and would be linked to an open relational database. The design of our expert system is outlined here: we especially outline the initial knowledge base and the nature of the different sets of rules we will make use of. Some examples describe how the system will operate and the types of advantages that are expected from its use. Finally the paper lists the studies and published work which in the last years have provided the groundwork for this project.","Computers and the Humanities",1986,"No","ver systm expert pour lanalys des text de moyen francai mediev french text process handl major problem develop state linguist knowledg scholar larg variat form spell occur manuscript order extract provid lexicograph grammat inform undescrib text research group decid devis expert system identifi pars text word link open relat databas design expert system outlin outlin initi knowledg base natur set rule make exampl describ system oper type advantag expect final paper list studi publish work year provid groundwork project",0
"KeywordsComputational Linguistic Network Requirement ","dancing to the telephone network requirements and opportunities",NA,"Computers and the Humanities",1998,"No","comput linguist network requir danc telephon network requir opportun na",0
"assessment nominal record linkage nominal retrieval ","assessment of systems for nominal retrieval and historical record linkage","Problems in retrieval of names form large data bases and in nominal record linkage are discussed with respect to computational solutions. The quest for robust methods that can handle the typical variability of historical nominal information is discussed, with some emphasis on probabilistic methods. It is argued that comparison and assessment of different systems used on the same data could enhance our understanding of methodological issues.","Computers and the Humanities",1998,"No","assess nomin record linkag nomin retriev assess system nomin retriev histor record linkag problem retriev name form larg data base nomin record linkag discuss respect comput solut quest robust method handl typic variabl histor nomin inform discuss emphasi probabilist method argu comparison assess system data enhanc understand methodolog issu",0
"KeywordsComputational Linguistic Large Text ","estimating changes in collocations of key words across a large text a case study of coleridges notebooks",NA,"Computers and the Humanities",1992,"No","comput linguist larg text estim colloc key word larg text case studi coleridg notebook na",0
"KeywordsComputational Linguistic Computational Consideration Expressive Metaphor Literal Analogy ","computational considerations for the processing of explanatory literal analogies and expressive metaphors",NA,"Computers and the Humanities",1987,"No","comput linguist comput consider express metaphor liter analog comput consider process explanatori liter analog express metaphor na",0
"KeywordsForeign Language Computational Linguistic ","blueprint for a comprehensive foreign language cai curriculum",NA,"Computers and the Humanities",1984,"No","foreign languag comput linguist blueprint comprehens foreign languag cai curriculum na",0
"KeywordsTechnical Development Computational Linguistic Library Science American Library ","automation in american libraries","Despite its concentration on technical developments in library science, this survey is offered here because it is relevant to humanists for two reasons: first, that humanists all use libraries and therefore need to encourage their evolution to higher levels of efficiency, and, second, that the procedures outlined here may be of help to humanists in establishing procedures for their own research.","Computers and the Humanities",1968,"No","technic develop comput linguist librari scienc american librari autom american librari concentr technic develop librari scienc survey offer relev humanist reason humanist librari encourag evolut higher level effici procedur outlin humanist establish procedur research",0
"KeywordsVerse Computational Linguistic German Verse ","phonology and style a computer assisted approach to german verse",NA,"Computers and the Humanities",1981,"No","vers comput linguist german vers phonolog style comput assist approach german vers na",0
"KeywordsComputational Linguistic National Endowment ","funding computer aided research in the division of research grants at the national endowment for the humanities",NA,"Computers and the Humanities",1978,"No","comput linguist nation endow fund comput aid research divis research grant nation endow human na",0
"Key Wordsnovel theory narrative statistics Genett Sartre ","babies bathwater and the study of literature","Although many scholars in literature currently seem mainly interested in theory, the focus on literary texts is what defines literature studies. Computer technology and the statistical methods it fosters are applicable to both the theoretical and to the interpretative issues which scholars of literature habitually address. Genette's distinction between the homodiegetic and the autodiegetic perspective in first-person narrative can be confirmed statistically. Roquentin's loneliness inLa nausée can be shown to be a formal characteristic of the type of novel he narrates, thus validating his commentary on his society. The computer can be used to deal with standard literary questions in a principled fashion, and a new orientation of literature studies on a cultural history model, which Mark Olsen recommends, is not necessary.","Computers and the Humanities",1993,"No","key wordsnovel theori narrat statist genett sartr babi bathwat studi literatur scholar literatur interest theori focus literari text defin literatur studi comput technolog statist method foster applic theoret interpret issu scholar literatur habitu address genett distinct homodieget autodieget perspect person narrat confirm statist roquentin loneli inla naus shown formal characterist type narrat valid commentari societi comput deal standard literari question principl fashion orient literatur studi cultur histori model mark olsen recommend",0
"KeywordsComputational Linguistic ","courseware review",NA,"Computers and the Humanities",1993,"No","comput linguist coursewar review na",0
"KeywordsTextual Feature Computational Linguistic Standard Generalize Intended Purpose Careful Thought ","some problems of tei markup and early printed books","This paper presents two groups of text encodingproblems encountered by the Brown University WomenWriters Project (WWP). The WWP is creating a full-textdatabase of transcriptions of pre-1830 printed bookswritten by women in English. For encoding our texts weuse Standard Generalized Markup Language (SGML),following the Text Encoding Initiative’s Guidelines for Electronic Text Encoding andInterchange. SGML is a powerful text encoding systemfor describing complex textual features, but a fullexpression of these may require very complex encoding,and careful thought about the intended purpose of theencoded text. We present here several possibleapproaches to these encoding problems, and analyze theissues they raise.","Computers and the Humanities",1997,"No","textual featur comput linguist standard general intend purpos care thought problem tei markup earli print book paper present group text encodingproblem encount brown univers womenwrit project wwp wwp creat full textdatabas transcript pre print bookswritten women english encod text weus standard general markup languag sgml text encod initi guidelin electron text encod andinterchang sgml power text encod systemfor describ complex textual featur fullexpress requir complex encod care thought intend purpos theencod text present possibleapproach encod problem analyz theissu rais",0
"KeywordsContent Analysis Computational Linguistic ","a system for text and content analysis",NA,"Computers and the Humanities",1976,"No","content analysi comput linguist system text content analysi na",0
"Key WordsColumbus Las Casas style Spanish literature dual authorship ","the two authors of columbusdiary","Although Columbus'Diary of the first voyage to America as we know it is largely a transcription of the original diary carried out by Bartolomé de las Casas, commentators and readers often treat it as if it were Columbus' work alone. Editions published to date do not separate the explorer's narrative from that of his transcriber or editor. Since style can influence readers' perceptions of a writer's personality, it is important to determine characteristics of writing attributed to Columbus that may pertain instead to his trascriber. This study employs the computer to explore the style of Las Casas and that of Columbus. Differences in the writing of each “author” emerge with computer assistance by isolating Columbus' words from those of his transcriber and analyzing selected features of vocabulary, sentence length, and syntax.1","Computers and the Humanities",1993,"No","key wordscolumbus las casa style spanish literatur dual authorship author columbusdiari columbusdiari voyag america larg transcript origin diari carri bartolom de las casa comment reader treat columbus work edit publish date separ explor narrat transcrib editor style influenc reader percept writer person import determin characterist write attribut columbus pertain trascrib studi employ comput explor style las casa columbus differ write author emerg comput assist isol columbus word transcrib analyz select featur vocabulari sentenc length syntax",0
"KeywordsComputational Linguistic ","culture structure and the new history a critique and an agenda",NA,"Computers and the Humanities",1975,"No","comput linguist cultur structur histori critiqu agenda na",0
"Key wordsneural networks function words authorship attribution The Federalist Papers ","neural network applications in stylometry the federalist papers","Neural Networks have recently been a matter of extensive research and popularity. Their application has increased considerably in areas in which we are presented with a large amount of data and we have to identify an underlying pattern. This paper will look at their application to stylometry. We believe that statistical methods of attributing authorship can be coupled effectively with neural networks to produce a very powerful classification tool. We illustrate this with an example of a famous case of disputed authorship, The Federalist Papers. Our method assigns the disputed papers to Madison, a result which is consistent with previous work on the subject.","Computers and the Humanities",1996,"No","key wordsneur network function word authorship attribut federalist paper neural network applic stylometri federalist paper neural network recent matter extens research popular applic increas consider area present larg amount data identifi under pattern paper applic stylometri statist method attribut authorship coupl effect neural network produc power classif tool illustr famous case disput authorship federalist paper method assign disput paper madison result consist previous work subject",0
"KeywordsComputational Linguistic ","the place of images in a world of text",NA,"Computers and the Humanities",2002,"No","comput linguist place imag world text na",0
"KeywordsComputational Linguistic ","on automatic speech understanding systems",NA,"Computers and the Humanities",1975,"No","comput linguist automat speech understand system na",0
NA,"concordances syntagmatiques et analyse de surface","This paper describes two procedures for the establishment of syntagmatic concordances through pre-coding and automatical analysis for the nominal syntagm. The first is pre-coding of the text, in this instance the poetry of P. Valéry, in accordance with taxonomic grids enumerating the typical patterns followed by the syntagm to be analyzed. From a strictly mathematical point of view, there are too many possible combinations (several thousand), and the number of typical patterns is therefore reduced as much as possible. In these taxonomic grids the analytical structures are poor. The second is the automatic analysis of simultaneous occurrences to left and right of a pivot term set by pre-coding. All the categories known as the “parts of speech” are precoded. In this process of recognition, the categories comprised by the units of the chain and their syntactical functions are also pre-coded.","Computers and the Humanities",1974,"No","concord syntagmatiqu analys de surfac paper describ procedur establish syntagmat concord pre code automat analysi nomin syntagm pre code text instanc poetri val ry accord taxonom grid enumer typic pattern syntagm analyz strict mathemat point view combin thousand number typic pattern reduc taxonom grid analyt structur poor automat analysi simultan occurr left pivot term set pre code categori part speech precod process recognit categori compris unit chain syntact function pre code",0
"KeywordsComputational Linguistic Linguistic Performance ","computational linguistics and the study of linguistic performance",NA,"Computers and the Humanities",1972,"No","comput linguist linguist perform comput linguist studi linguist perform na",0
"KeywordsComputational Linguistic ","computer applications in archaeology",NA,"Computers and the Humanities",1967,"No","comput linguist comput applic archaeolog na",0
"KeywordsComputational Linguistic Computer Center Compleat Computer ","the compleat computer center",NA,"Computers and the Humanities",1969,"No","comput linguist comput center compleat comput compleat comput center na",0
"KeywordsComputational Linguistic ","style precept personality a test case thomas sprat 16351713",NA,"Computers and the Humanities",1971,"No","comput linguist style precept person test case thoma sprat na",0
"KeywordsData Management Computational Linguistic Data Management System Linguistic Data ","ldms a linguistic data management system",NA,"Computers and the Humanities",1983,"No","data manag comput linguist data manag system linguist data ldms linguist data manag system na",0
"KeywordsRemote text-to-speech synthesis evaluation Text-to-speech synthesis modules ECESS consortium ","remote based text to speech modules evaluation framework the res framework","The ECESS consortium (European Center of Excellence in Speech Synthesis) aims to speed up progress in speech synthesis technology, by providing an appropriate evaluation framework. The key element of the evaluation framework is based on the partition of a text-to-speech synthesis system into distributed TTS modules. A text processing, prosody generation, and an acoustic synthesis module have been specified currently. A split into various modules has the advantage that the developers of an institution active in ECESS, can concentrate its efforts on a single module, and test its performance in a complete system using missing modules from the developers of other institutions. In this way, complete TTS systems can be built using high performance modules from different institutions. In order to evaluate the modules and to connect modules efficiently, a remote evaluation platform—the Remote Evaluation System (RES) based on the existing internet infrastructure—has been developed within ECESS. The RES is based on client–server architecture. It consists of RES module servers, which encapsulate the modules of the developers, a RES client, which sends data to and receives data from the RES module servers, and a RES server, which connects the RES module servers, and organizes the flow of information. RES can be used by developers for selecting RES module from the internet, which contains a missing TTS module needed to test and improve the performances of their own modules. Finally, the RES allows for the evaluation of TTS modules running at different institutions worldwide. When using the RES client, the institution performing the evaluation is able to set-up and performs various evaluation tasks by sending test data via the RES client and receiving results from the RES module servers. Currently ELDA www.elda.org is setting-up an evaluation using the RES client, which will then be extended to an evaluation client specializing in the envisaged evaluation tasks.","Language Resources and Evaluation",2010,"No","remot text speech synthesi evalu text speech synthesi modul ecess consortium remot base text speech modul evalu framework res framework ecess consortium european center excel speech synthesi aim speed progress speech synthesi technolog provid evalu framework key element evalu framework base partit text speech synthesi system distribut tts modul text process prosodi generat acoust synthesi modul split modul advantag develop institut activ ecess concentr effort singl modul test perform complet system miss modul develop institut complet tts system built high perform modul institut order evalu modul connect modul effici remot evalu platform remot evalu system res base exist internet infrastructur develop ecess res base client server architectur consist res modul server encapsul modul develop res client send data receiv data res modul server res server connect res modul server organ flow inform res develop select res modul internet miss tts modul need test improv perform modul final res evalu tts modul run institut worldwid res client institut perform evalu set perform evalu task send test data res client receiv result res modul server elda wwweldaorg set evalu res client extend evalu client special envisag evalu task",0
NA,"lre journal cnl introduction","A controlled natural language (CNL) is based on a natural language but includes restrictions on vocabulary, grammar, and/or semantics, in order to reduce or eliminate ambiguity and complexity.
","Language Resources and Evaluation",2017,"No","lre journal cnl introduct control natur languag cnl base natur languag includ restrict vocabulari grammar semant order reduc elimin ambigu complex",0
"KeywordsSummarization Discussion forums Data collection User study Inter-rater agreement Evaluation ","creating a reference data set for the summarization of discussion forum threads","In this paper we address extractive summarization of long threads in online discussion fora. We present an elaborate user evaluation study to determine human preferences in forum summarization and to create a reference data set. We showed long threads to ten different raters and asked them to create a summary by selecting the posts that they considered to be the most important for the thread. We study the agreement between human raters on the summarization task, and we show how multiple reference summaries can be combined to develop a successful model for automatic summarization. We found that although the inter-rater agreement for the summarization task was slight to fair, the automatic summarizer obtained reasonable results in terms of precision, recall, and ROUGE. Moreover, when human raters were asked to choose between the summary created by another human and the summary created by our model in a blind side-by-side comparison, they judged the model’s summary equal to or better than the human summary in over half of the cases. This shows that even for a summarization task with low inter-rater agreement, a model can be trained that generates sensible summaries. In addition, we investigated the potential for personalized summarization. However, the results for the three raters involved in this experiment were inconclusive. We release the reference summaries as a publicly available dataset.","Language Resources and Evaluation",2018,"No","summar discuss forum data collect user studi inter rater agreement evalu creat refer data set summar discuss forum thread paper address extract summar long thread onlin discuss fora present elabor user evalu studi determin human prefer forum summar creat refer data set show long thread ten rater ask creat summari select post consid import thread studi agreement human rater summar task show multipl refer summari combin develop success model automat summar found inter rater agreement summar task slight fair automat summar obtain reason result term precis recal roug human rater ask choos summari creat human summari creat model blind side side comparison judg model summari equal human summari half case show summar task low inter rater agreement model train generat summari addit investig potenti person summar result rater involv experi inconclus releas refer summari public dataset",0
"KeywordsStemming algorithm Hausa language Root word Over-stemming Under-stemming Stemmer accuracy ","stemming hausa text using affix stripping rules and reference look up","Stemming is a process of reducing a derivational or inflectional word to its root or stem by stripping all its affixes. It is been used in applications such as information retrieval, machine translation, and text summarization, as their pre-processing step to increase efficiency. Currently, there are a few stemming algorithms which have been developed for languages such as English, Arabic, Turkish, Malay and Amharic. Unfortunately, no algorithm has been used to stem text in Hausa, a Chadic language spoken in West Africa. To address this need, we propose stemming Hausa text using affix-stripping rules and reference lookup. We stemmed Hausa text, using 78 affix stripping rules applied in 4 steps and a reference look-up consisting of 1500 Hausa root words. The over-stemming index, under-stemming index, stemmer weight, word stemmed factor, correctly stemmed words factor and average words conflation factor were calculated to determine the effect of reference look-up on the strength and accuracy of the stemmer. It was observed that reference look-up aided in reducing both over-stemming and under-stemming errors, increased accuracy and has a tendency to reduce the strength of an affix stripping stemmer. The rationality behind the approach used is discussed and directions for future research are identified.","Language Resources and Evaluation",2016,"No","stem algorithm hausa languag root word stem stem stemmer accuraci stem hausa text affix strip rule refer stem process reduc deriv inflect word root stem strip affix applic inform retriev machin translat text summar pre process step increas effici stem algorithm develop languag english arab turkish malay amhar algorithm stem text hausa chadic languag spoken west africa address propos stem hausa text affix strip rule refer lookup stem hausa text affix strip rule appli step refer consist hausa root word stem index stem index stemmer weight word stem factor correct stem word factor averag word conflat factor calcul determin effect refer strength accuraci stemmer observ refer aid reduc stem stem error increas accuraci tendenc reduc strength affix strip stemmer ration approach discuss direct futur research identifi",0
"KeywordsMachine translation Catalogue of phrases Controlled natural language Text quality Avalanche warning ","fully automatic multi language translation with a catalogue of phrases successful employment for the swiss avalanche bulletin","The Swiss avalanche bulletin is produced twice a day in four languages. Due to the lack of time available for manual translation, a fully automated translation system is employed, based on a catalogue of predefined phrases and predetermined rules of how these phrases can be combined to produce sentences. Because this catalogue of phrases is limited to a small sublanguage, the system is able to automatically translate such sentences from German into the target languages French, Italian and English without subsequent proofreading or correction. Having been operational for two winter seasons, we assess here the quality of the produced texts based on two different surveys where participants rated texts from real avalanche bulletins from both origins, the catalogue of phrases versus manually written and translated texts. With a mean recognition rate of 55 %, users can hardly distinguish between the two types of texts, and give very similar ratings with respect to their language quality. Overall, the output from the catalogue system can be considered virtually equivalent to a text written by avalanche forecasters and then manually translated by professional translators. Furthermore, forecasters declared that all relevant situations were captured by the system with sufficient accuracy. Forecaster’s working load did not change with the introduction of the catalogue: the extra time to find matching sentences is compensated by the fact that they no longer need to double-check manually translated texts. The reduction of daily translation costs is expected to offset the initial development costs within a few years.","Language Resources and Evaluation",2017,"No","machin translat catalogu phrase control natur languag text qualiti avalanch warn fulli automat multi languag translat catalogu phrase success employ swiss avalanch bulletin swiss avalanch bulletin produc day languag due lack time manual translat fulli autom translat system employ base catalogu predefin phrase predetermin rule phrase combin produc sentenc catalogu phrase limit small sublanguag system automat translat sentenc german target languag french italian english subsequ proofread correct oper winter season assess qualiti produc text base survey particip rate text real avalanch bulletin origin catalogu phrase versus manual written translat text recognit rate user distinguish type text give similar rate respect languag qualiti output catalogu system consid virtual equival text written avalanch forecast manual translat profession translat forecast declar relev situat captur system suffici accuraci forecast work load chang introduct catalogu extra time find match sentenc compens fact longer doubl check manual translat text reduct daili translat cost expect offset initi develop cost year",0
"KeywordsTree Adjoining Grammar LTAG-spinal Treebank Dependency parsing ","ltag spinal and the treebank","We introduce LTAG-spinal, a novel variant of traditional Lexicalized Tree Adjoining Grammar (LTAG) with desirable linguistic, computational and statistical properties. Unlike in traditional LTAG, subcategorization frames and the argument–adjunct distinction are left underspecified in LTAG-spinal. LTAG-spinal with adjunction constraints is weakly equivalent to LTAG. The LTAG-spinal formalism is used to extract an LTAG-spinal Treebank from the Penn Treebank with Propbank annotation. Based on Propbank annotation, predicate coordination and LTAG adjunction structures are successfully extracted. The LTAG-spinal Treebank makes explicit semantic relations that are implicit or absent from the original PTB. LTAG-spinal provides a very desirable resource for statistical LTAG parsing, incremental parsing, dependency parsing, and semantic parsing. This treebank has been successfully used to train an incremental LTAG-spinal parser and a bidirectional LTAG dependency parser.","Language Resources and Evaluation",2008,"No","tree adjoin grammar ltag spinal treebank depend pars ltag spinal treebank introduc ltag spinal variant tradit lexic tree adjoin grammar ltag desir linguist comput statist properti unlik tradit ltag subcategor frame argument adjunct distinct left underspecifi ltag spinal ltag spinal adjunct constraint weak equival ltag ltag spinal formal extract ltag spinal treebank penn treebank propbank annot base propbank annot predic coordin ltag adjunct structur success extract ltag spinal treebank make explicit semant relat implicit absent origin ptb ltag spinal desir resourc statist ltag pars increment pars depend pars semant pars treebank success train increment ltag spinal parser bidirect ltag depend parser",0
"KeywordsProbability Model Computational Linguistic Probabilistic Classifier Representational Power Decomposable Model ","selecting decomposable models for word sense disambiguation thegrling sdm system","This paper describes the grling-sdm system, which is asupervised probabilistic classifier that participated in the 1998SENSEVAL competition for word-sense disambiguation. This systemuses model search to select decomposable probability models describingthe dependencies among the feature variables.These types of models have been found to be advantageous in terms ofefficiency and representational power. Performance on the SENSEVALevaluation data is discussed.","Computers and the Humanities",2000,"No","probabl model comput linguist probabilist classifi represent power decompos model select decompos model word sens disambigu thegrl sdm system paper describ grling sdm system asupervis probabilist classifi particip sensev competit word sens disambigu systemus model search select decompos probabl model describingth depend featur variabl type model found advantag term ofeffici represent power perform sensevalevalu data discuss",0
"KeywordsData Entry Computational Linguistic ","hardware review the kurzweil data entry machine",NA,"Computers and the Humanities",1981,"No","data entri comput linguist hardwar review kurzweil data entri machin na",0
"KeywordsHumanity Research Computational Linguistic Round Table House Round ","the grinnell house round table on a center for computer aided humanities research",NA,"Computers and the Humanities",1985,"No","human research comput linguist round tabl hous round grinnel hous round tabl center comput aid human research na",0
"KeywordsWord List Magnetic Tape Optical Character Recognition Text Processing Text Element ","a text processing system for the preparation of critical editions",NA,"Computers and the Humanities",1979,"No","word list magnet tape optic charact recognit text process text element text process system prepar critic edit na",0
"KeywordsControlled natural languages Ontology authoring Semantic web, state of the art ","cnls for the semantic web a state of the art","One of the core challenges for building the semantic web is the creation of ontologies, a process known as ontology authoring. Controlled natural languages (CNLs) propose different frameworks for interfacing and creating ontologies in semantic web systems using restricted natural language. However, in order to engage non-expert users with no background in knowledge engineering, these language interfacing must be reliable, easy to understand and accepted by users. This paper includes the state-of-the-art for CNLs in terms of ontology authoring and the semantic web. In addition, it includes a detailed analysis of user evaluations with respect to each CNL and offers analytic conclusions with respect to the field.","Language Resources and Evaluation",2017,"No","control natur languag ontolog author semant web state art cnls semant web state art core challeng build semant web creation ontolog process ontolog author control natur languag cnls propos framework interfac creat ontolog semant web system restrict natur languag order engag expert user background knowledg engin languag interfac reliabl easi understand accept user paper includ state art cnls term ontolog author semant web addit includ detail analysi user evalu respect cnl offer analyt conclus respect field",0
"KeywordsProcessing Technique Processing Algorithm Computational Linguistic Pattern Processing Processing Problem ","pattern processing in melodic sequences challenges caveats and prospects","In this paper a number of issues relating to theapplication of string processing techniques on musicalsequences are discussed. A brief survey of somemusical string processing algorithms is given and someissues of melodic representation, abstraction,segmentation and categorisation are presented. Thispaper is not intended to provide solutions tostring processing problems but rather tohighlight possible stumbling-block areas andraise awareness of primarily music‐elatedparticularities that can cause problems in matchingapplications.","Computers and the Humanities",2001,"No","process techniqu process algorithm comput linguist pattern process process problem pattern process melod sequenc challeng caveat prospect paper number issu relat theapplic string process techniqu musicalsequ discuss survey somemus string process algorithm someissu melod represent abstractionsegment categoris present thispap intend provid solut tostr process problem tohighlight stumbl block area andrais awar primarili music elatedparticular problem matchingappl",0
"KeywordsConcept concreteness Dictionary definitions Semantic lexicons Polysemy Computational lexicography ","the potentials and limitations of modelling concept concreteness in computational semantic lexicons with dictionary definitions","This paper explores the feasibility of modelling concept concreteness perceived by humans and representing it in computational semantic lexicons, addressing an issue at the crossroads of computational linguistics, lexicography, and psycholinguistics. The inherent distinction between concrete words and abstract words in psychology has relied mostly on subjective human ratings. This practice is hardly scalable and does not consider the effect of polysemy. In view of this, we attempt to obtain a measure of concreteness from dictionary definitions comparable to human judgement, capitalising on conventional lexicographic assumptions and the regularities exhibited in the surface structures of sense definitions. The structural pattern of a definition is analysed and scored on a 7-point scale of concreteness ratings. The definition scores turned out to be quite effective for a dichotomous distinction between concrete and abstract concepts and more consistent with human ratings for the former. Beyond the two-way distinction, however, the results were more variable. The study has thus revealed the potentials and limitations of our approach, suggesting that different defining styles probably reflect the describability of concepts, and describability alone may not be sufficient for differentiating the degree of concreteness. The range of definition patterns has to be reconsidered, in combination with other inseparable factors constituting our perception of concreteness, for better modelling on a finer scale of concreteness distinction to enrich semantic lexicons for natural language processing.","Language Resources and Evaluation",2013,"No","concept concret dictionari definit semant lexicon polysemi comput lexicographi potenti limit model concept concret comput semant lexicon dictionari definit paper explor feasibl model concept concret perceiv human repres comput semant lexicon address issu crossroad comput linguist lexicographi psycholinguist inher distinct concret word abstract word psycholog reli subject human rate practic scalabl effect polysemi view attempt obtain measur concret dictionari definit compar human judgement capitalis convent lexicograph assumpt regular exhibit surfac structur sens definit structur pattern definit analys score point scale concret rate definit score turn effect dichotom distinct concret abstract concept consist human rate distinct result variabl studi reveal potenti limit approach suggest defin style reflect describ concept describ suffici differenti degre concret rang definit pattern reconsid combin insepar factor constitut percept concret model finer scale concret distinct enrich semant lexicon natur languag process",0
"KeywordsMultimodal Corpus annotation Audio ","woz acoustic data collection for interactive tv","This paper describes a multichannel acoustic data collection recorded under the European DICIT project, during Wizard of Oz (WOZ) experiments carried out at FAU and FBK-irst laboratories. The application of interest in DICIT is a distant-talking interface for control of interactive TV working in a typical living room, with many interfering devices. The objective of the experiments was to collect a database supporting efficient development and tuning of acoustic processing algorithms for signal enhancement. In DICIT, techniques for sound source localization, multichannel acoustic echo cancellation, blind source separation, speech activity detection, speaker identification and verification as well as beamforming are combined to achieve a maximum possible reduction of the user speech impairments typical of distant-talking interfaces. The collected database permitted to simulate at preliminary stage a realistic scenario and to tailor the involved algorithms to the observed user behaviors. In order to match the project requirements, the WOZ experiments were recorded in three languages: English, German and Italian. Besides the user inputs, the database also contains non-speech related acoustic events, room impulse response measurements and video data, the latter used to compute three-dimensional positions of each subject. Sessions were manually transcribed and segmented at word level, introducing also specific labels for acoustic events.","Language Resources and Evaluation",2010,"No","multimod corpus annot audio woz acoust data collect interact tv paper describ multichannel acoust data collect record european dicit project wizard oz woz experi carri fau fbk irst laboratori applic interest dicit distant talk interfac control interact tv work typic live room interf devic object experi collect databas support effici develop tune acoust process algorithm signal enhanc dicit techniqu sound sourc local multichannel acoust echo cancel blind sourc separ speech activ detect speaker identif verif beamform combin achiev maximum reduct user speech impair typic distant talk interfac collect databas permit simul preliminari stage realist scenario tailor involv algorithm observ user behavior order match project requir woz experi record languag english german italian user input databas speech relat acoust event room impuls respons measur video data comput dimension posit subject session manual transcrib segment word level introduc specif label acoust event",0
"KeywordsDocumentation Lexical types Linguistic grammar Treebank ","semi automatic documentation of an implemented linguistic grammar augmented with a treebank","We have constructed a large scale and detailed database of lexical types in Japanese from a treebank that includes detailed linguistic information. The database helps treebank annotators and grammar developers to share precise knowledge about the grammatical status of words that constitute the treebank, allowing for consistent large-scale treebanking and grammar development. In addition, it clarifies what lexical types are needed for precise Japanese NLP on the basis of the treebank. In this paper, we report on the motivation and methodology of the database construction.","Language Resources and Evaluation",2008,"No","document lexic type linguist grammar treebank semi automat document implement linguist grammar augment treebank construct larg scale detail databas lexic type japanes treebank includ detail linguist inform databas help treebank annot grammar develop share precis knowledg grammat status word constitut treebank allow consist larg scale treebank grammar develop addit clarifi lexic type need precis japanes nlp basi treebank paper report motiv methodolog databas construct",0
NA,"chu ren huang nicoletta calzolari aldo gangemi alessandro lenci alessandro oltramari and laurent prvot eds ontology and the lexicon a natural language processing perspective studies in natural language processing","The relationship between ontologies and natural language lexicons is a hotly debated one. An ontology is a formalized system of concepts (potentially of a specific domain) and the relations these concepts entertain. A lexicon, on the other hand, is the language component that contains the conventionalized knowledge of natural language speakers about lexical items (mostly words, but also morphemes and idioms). Ontologies ‘operate’ on the conceptual level, lexicons on the linguistic level. Ontologies systematize and relate concepts, lexicons systematize and relate words and other lexical items. However, as semantic relations between lexical items reflect meaning relatedness and meaning is essentially conceptual, both notions appear to be very close to one another (and are often wrongly used interchangeably). The interplay of and mapping between ontologies and lexical resources is therefore a vital and challenging field of research, one which has gained additional momentum and importance...","Language Resources and Evaluation",2012,"No","chu ren huang nicoletta calzolari aldo gangemi alessandro lenci alessandro oltramari laurent prvot ed ontolog lexicon natur languag process perspect studi natur languag process relationship ontolog natur languag lexicon hot debat ontolog formal system concept potenti specif domain relat concept entertain lexicon hand languag compon convention knowledg natur languag speaker lexic item word morphem idiom ontolog oper conceptu level lexicon linguist level ontolog systemat relat concept lexicon systemat relat word lexic item semant relat lexic item reflect mean related mean essenti conceptu notion close wrong interchang interplay map ontolog lexic resourc vital challeng field research gain addit momentum import",0
"KeywordsKanji in web search Japanese web search queries Query processing Query substitution Query reformulation ","automatically generating related queries in japanese","Web searchers reformulate their queries, as they adapt to search engine behavior, learn more about a topic, or simply correct typing errors. Automatic query rewriting can help user web search, by augmenting a user’s query, or replacing the query with one likely to retrieve better results. One example of query-rewriting is spell-correction. We may also be interested in changing words to synonyms or other related terms. For Japanese, the opportunities for improving results are greater than for languages with a single character set, since documents may be written in multiple character sets, and a user may express the same meaning using different character sets. We give a description of the characteristics of Japanese search query logs and manual query reformulations carried out by Japanese web searchers. We use characteristics of Japanese query reformulations to extend previous work on automatic query rewriting in English, taking into account the Japanese writing system. We introduce several new features for building models resulting from this difference and discuss their impact on automatic query rewriting. We also examine enhancements in the form of rules which block conversion between some character sets, to address Japanese homophones. The precision/recall curves show significant improvement with the new feature set and blocking rules, and are often better than the English counterpart.","Language Resources and Evaluation",2006,"No","kanji web search japanes web search queri queri process queri substitut queri reformul automat generat relat queri japanes web searcher reformul queri adapt search engin behavior learn topic simpli correct type error automat queri rewrit user web search augment user queri replac queri retriev result queri rewrit spell correct interest chang word synonym relat term japanes opportun improv result greater languag singl charact set document written multipl charact set user express mean charact set give descript characterist japanes search queri log manual queri reformul carri japanes web searcher characterist japanes queri reformul extend previous work automat queri rewrit english take account japanes write system introduc featur build model result differ discuss impact automat queri rewrit examin enhanc form rule block convers charact set address japanes homophon precisionrecal curv show signific improv featur set block rule english counterpart",0
"KeywordsTechnology audit Human language technology Language resources BLaRK Language audit Language resource infrastructure Resource-scarce languages ","the south african human language technology audit","Human language technology (HLT) has been identified as a priority area by the South African government. However, despite efforts by government and the research and development (R&D) community, South Africa has not yet been able to maximise the opportunities of HLT and create a thriving HLT industry. One of the key challenges is the fact that there is insufficient codified knowledge about the current South African HLT components, their attributes and existing relationships. Hence a technology audit was conducted for the South African HLT landscape, to create a systematic and detailed inventory of the status of the HLT components across the eleven official languages. Based on the Basic Language Resource Kit (BLaRK) framework Krauwer (ELRA Newslett 3(2), 1998), we used various data collection methods (such as focus groups, questionnaires and personal consultations with HLT experts) to gather detailed information. The South African HLT landscape is analysed using a number of complementary approaches and based on the interpretations of the results, recommendations are made on how to accelerate HLT development in South Africa, as well as on how to conduct similar audits in other countries and contexts.","Language Resources and Evaluation",2011,"No","technolog audit human languag technolog languag resourc blark languag audit languag resourc infrastructur resourc scarc languag south african human languag technolog audit human languag technolog hlt identifi prioriti area south african govern effort govern research develop communiti south africa maximis opportun hlt creat thrive hlt industri key challeng fact insuffici codifi knowledg current south african hlt compon attribut exist relationship technolog audit conduct south african hlt landscap creat systemat detail inventori status hlt compon eleven offici languag base basic languag resourc kit blark framework krauwer elra newslett data collect method focus group questionnair person consult hlt expert gather detail inform south african hlt landscap analys number complementari approach base interpret result recommend made acceler hlt develop south africa conduct similar audit countri context",0
"KeywordsSemantic relations Nominals Classification SemEval ","classification of semantic relations between nominals","The NLP community has shown a renewed interest in deeper semantic analyses, among them automatic recognition of semantic relations in text. We present the development and evaluation of a semantic analysis task: automatic recognition of relations between pairs of nominals in a sentence. The task was part of SemEval-2007, the fourth edition of the semantic evaluation event previously known as SensEval. Apart from the observations we have made, the long-lasting effect of this task may be a framework for comparing approaches to the task. We introduce the problem of recognizing relations between nominals, and in particular the process of drafting and refining the definitions of the semantic relations. We show how we created the training and test data, list and briefly describe the 15 participating systems, discuss the results, and conclude with the lessons learned in the course of this exercise.","Language Resources and Evaluation",2009,"No","semant relat nomin classif semev classif semant relat nomin nlp communiti shown renew interest deeper semant analys automat recognit semant relat text present develop evalu semant analysi task automat recognit relat pair nomin sentenc task part semev fourth edit semant evalu event previous sensev observ made long last effect task framework compar approach task introduc problem recogn relat nomin process draft refin definit semant relat show creat train test data list briefli describ particip system discuss result conclud lesson learn exercis",0
"KeywordsSentiment analysis Polarity lexicon Polarity extraction Turkish WordNet ","sentiturknet a turkish polarity lexicon for sentiment analysis","Sentiment analysis aims to extract the sentiment polarity of given segment of text. Polarity resources that indicate the sentiment polarity of words are commonly used in different approaches. While English is the richest language in regard to having such resources, the majority of other languages, including Turkish, lack polarity resources. In this work we present the first comprehensive Turkish polarity resource, SentiTurkNet, where three polarity scores are assigned to each synset in the Turkish WordNet, indicating its positivity, negativity, and objectivity (neutrality) levels. Our method is general and applicable to other languages. Evaluation results for Turkish show that the polarity scores obtained through this method are more accurate compared to those obtained through direct translation (mapping) from SentiWordNet.","Language Resources and Evaluation",2016,"No","sentiment analysi polar lexicon polar extract turkish wordnet sentiturknet turkish polar lexicon sentiment analysi sentiment analysi aim extract sentiment polar segment text polar resourc sentiment polar word common approach english richest languag regard resourc major languag includ turkish lack polar resourc work present comprehens turkish polar resourc sentiturknet polar score assign synset turkish wordnet indic posit negat object neutral level method general applic languag evalu result turkish show polar score obtain method accur compar obtain direct translat map sentiwordnet",0
"Key Wordsconceptual analysis knowledge representation knowledge engineering semantic networks text analysis ","frame based representation of philosophical systems using a knowledge engineering tool","This article addresses the methodological problem of the non-linear representation of philosophical systems in a computerized knowledge base. It is a problem of knowledge representation as defined in the field of artificial intelligence. Instead of a purely theoretical discussion of the issue, we present selected results of a practical experiment which has in itself some theoretical significance. We show how one can represent different philosophies using CODE, a knowledge engineering system developed by artificial intelligence researchers. The hypothesis is that such a computer based representation of philosophical systems can give insight into their conceptual structure. We argue that computer aided text analysis can apply knowledge representation tools and techniques developed in artificial intelligence and we estimate how philosophers as well as knowledge engineers could gain from this cross-fertilization. This paper should be considered as an experiment report on the use of knowledge representation techniques in computer aided text analysis. It is part of a much broader project on the representation of conceptual structures in an expert system. However, we intentionally avoided technical issues related to either Computer Science or History of Philosophy to focus on the benefit to enhance traditional humanistic studies with tools and methods developed in AI on the one hand and the need to develop more appropriate tools on the other.","Computers and the Humanities",1993,"No","key wordsconceptu analysi knowledg represent knowledg engin semant network text analysi frame base represent philosoph system knowledg engin tool articl address methodolog problem linear represent philosoph system computer knowledg base problem knowledg represent defin field artifici intellig pure theoret discuss issu present select result practic experi theoret signific show repres philosophi code knowledg engin system develop artifici intellig research hypothesi comput base represent philosoph system give insight conceptu structur argu comput aid text analysi appli knowledg represent tool techniqu develop artifici intellig estim philosoph knowledg engin gain cross fertil paper consid experi report knowledg represent techniqu comput aid text analysi part broader project represent conceptu structur expert system intent avoid technic issu relat comput scienc histori philosophi focus benefit enhanc tradit humanist studi tool method develop ai hand develop tool",0
"Keywordsevaluation language resources production standards validation ","elra european language resources association background recent developments and future perspectives","The European Language Resources Association (ELRA) was founded in 1995 with the mission of providing language resources (LR) to European research institutions and companies. In this paper we describe the background, the mission and the major activities since then.","Language Resources and Evaluation",2005,"No","evalu languag resourc product standard valid elra european languag resourc associ background recent develop futur perspect european languag resourc associ elra found mission provid languag resourc lr european research institut compani paper describ background mission major activ",0
"adaptive narrative electronic literature media studies mutability ","mutability medium and character","Looking specifically at the genre ofadaptive narrative, this article explores thefuture of literature created for and withcomputer technology, focusing primarily on thetrope of mutability as it is played out withnew media. Some of the questions askedare: What can the medium of a work ofliterature, that is its material aspect, tellus about the text? About character? What canit possibly matter if narrative is recounted onpapyrus, retold on parchment and rag, and thenremediated in pixels? Isn't it the messagecarried by the medium we are most concernedwith, stable or unstable throughout the processof inscription, reinscription, encoding anddecoding, translation and remediation? Thispaper speculates about possibilities ratherthan attempts to answer these questions, butthe structuring and mean-making componentsconsidered here stand as examples of some wemay want to think about when developing futuretheories about literature – and all types ofwriting – generated by and for electronicenvironments.","Computers and the Humanities",2002,"No","adapt narrat electron literatur media studi mutabl mutabl medium charact specif genr ofadapt narrat articl explor thefutur literatur creat withcomput technolog focus primarili thetrop mutabl play withnew media question askedar medium work ofliteratur materi aspect tellus text charact canit possibl matter narrat recount onpapyrus retold parchment rag thenremedi pixel messagecarri medium concernedwith stabl unstabl processof inscript reinscript encod anddecod translat remedi thispap specul possibl ratherthan attempt answer question butth structur make componentsconsid stand exampl wemay develop futuretheori literatur type ofwrit generat electronicenviron",0
"KeywordsMachine Translation Target Language Lexical Item Indexing Scheme Semantic Component ","a survey of approaches and issues in machine aided translation systems",NA,"Computers and the Humanities",1979,"No","machin translat target languag lexic item index scheme semant compon survey approach issu machin aid translat system na",0
"KeywordsData Bank Computational Linguistic Common Procedure Usable Program Limited Effort ","the current state of music research and the computer","In our opening remarks we noted the emphasis on the tools of computer-oriented music research prevailing in most writing. It is not likely that this situation will change for some time. It is apparent that there is a wide variety of computer applications being tested in music research and equally apparent that there is only limited collaboration and limited effort at devising common procedures. It is to be hoped that the future will see the acceptance of one music representation as a lingua franca; the development of widely usable programs with a central clearing house for such programs; and the development of data banks of encoded music scores, thematic indices of various repertories, and other information useful to the music researcher.","Computers and the Humanities",1970,"No","data bank comput linguist common procedur usabl program limit effort current state music research comput open remark note emphasi tool comput orient music research prevail write situat chang time appar wide varieti comput applic test music research equal appar limit collabor limit effort devis common procedur hope futur accept music represent lingua franca develop wide usabl program central clear hous program develop data bank encod music score themat indic repertori inform music research",0
NA,"a bibliography of intelligent computer assisted language instruction",NA,"Computers and the Humanities",1989,"No","bibliographi intellig comput assist languag instruct na",0
"Key Wordsarchives electronic records electronic office systems historical research hypertext HYTIME linked documents ODA SGML tagging TEI textual records textual analysis ","electronically generated records and twentieth century history","The electronic generation of documents in modern offices will trasform the nature of archives, and also the techniques of historical research. Although considerable attention has been directed to developing research methodologies for social and economic history using computerized numeric data, almost no attention has been paid to the impact of machine readable textual records on historical writing. This article considers the advantages and disadvantages for the historian of the shift from paper records to electronic documents, and suggests a number of approaches to historical research made possible by the new technology.","Computers and the Humanities",1993,"No","key wordsarch electron record electron offic system histor research hypertext hytim link document oda sgml tag tei textual record textual analysi electron generat record twentieth centuri histori electron generat document modern offic trasform natur archiv techniqu histor research consider attent direct develop research methodolog social econom histori computer numer data attent paid impact machin readabl textual record histor write articl consid advantag disadvantag historian shift paper record electron document suggest number approach histor research made technolog",0
"Key wordscomputer-based learning foreign language hypermedia applications interactive video instructional technology Latin American culture Spanish language ","metacognitive learning techniques in the user interface advance organizers and captioning","The use of metacognitive strategies of learning and instruction such as content abstracts or previews, subtitles and captioning (on-screen foreign language subtitles) have been recurrent pedagogical tools for facilitating foreign language (L2) instruction. New technology has broadened their scope and multiplied the ways in which they can be used in L2 computer-based applications. A pilot test was carried out using a hypermedia instructional application for Spanish: “Operación Futuro.” The test addresses the question of how two types of metacognitive strategies, written and spoken Advance Organizers (AOs) and verbatim Captioning (CP) may facilitate L2 comprehension and recall.","Computers and the Humanities",1994,"No","key wordscomput base learn foreign languag hypermedia applic interact video instruct technolog latin american cultur spanish languag metacognit learn techniqu user interfac advanc organ caption metacognit strategi learn instruct content abstract preview subtitl caption screen foreign languag subtitl recurr pedagog tool facilit foreign languag l instruct technolog broaden scope multipli way l comput base applic pilot test carri hypermedia instruct applic spanish operaci futuro test address question type metacognit strategi written spoken advanc organ ao verbatim caption cp facilit l comprehens recal",0
"Key Wordsmachine-readable texts text analysis programs Shakespeare concordances indexes stylo-statistics ","the bard in bits electronic editions of shakespeare and programs to analyze them","Machine-readable texts in the humanities are in a period of rapid growth, as are programs for analyzing them, with concomitant problems of finding and choosing the most suitable of each. No one text-base, and no one search program, proves to be wholly suitable for all projects. Three of the currently available electronic editions of Shakespeare are discussed and compared, along with three commercial programs for text analysis on microcomputers.","Computers and the Humanities",1990,"No","key wordsmachin readabl text text analysi program shakespear concord index stylo statist bard bit electron edit shakespear program analyz machin readabl text human period rapid growth program analyz concomit problem find choos suitabl text base search program prove wholli suitabl project electron edit shakespear discuss compar commerci program text analysi microcomput",0
"KeywordsComputational Linguistic Humanity Computing ","the annals of humanities computing the index thomisticus",NA,"Computers and the Humanities",1980,"No","comput linguist human comput annal human comput index thomisticus na",0
"business strategies commercial electronic publishing development practicalities multimedia teaching tools SGML textbases ","electronic publishing at routledge","This article describes how an independent commercial academic publisher initiated its electronic publishing programme. It outlines the range of electronic activities under development and some of the issues addressed during the creation of electronic resources. Case studies of two early projects are included: a multimedia teaching too, A Right to Die? The Dax Cowart Case; and an SGML textbase, the Arden Shakespeare CD-ROM. In addition, the Routledge Encyclopedia of Philosophy is discussed as an example of the second generation of electronic projects at Routledge, highlighting lessons learned from previous projects and some of the issues relating to the production of a simultaneous print and electronic resource.","Computers and the Humanities",1998,"No","busi strategi commerci electron publish develop practic multimedia teach tool sgml textbas electron publish routledg articl describ independ commerci academ publish initi electron publish programm outlin rang electron activ develop issu address creation electron resourc case studi earli project includ multimedia teach die dax cowart case sgml textbas arden shakespear cd rom addit routledg encyclopedia philosophi discuss generat electron project routledg highlight lesson learn previous project issu relat product simultan print electron resourc",0
"Key wordscomputer-aided literature studies literature literary theory structuralism ","signs symbols and discourses a new direction for computer aided literature studies","Computer-aided literature studies have failed to have a significant impact on the field as a whole. This failure is traced to a concentration on how a text achieves its literary effect by the examination of subtle semantic or grammatical structures in single texts or the works of individual authors. Computer systems have proven to be very poorly suited to such refined analysis of complex language. Adopting such traditional objects of study has tended to discourage researchers from using the tool to ask questions to which it is better adapted, the examination of large amounts of simple linguistic features. Theoreticians such as Barthes, Foucault and Halliday show the importance of determining the linguistic and semantic characteristics of the language used by the author and her/his audience. Current technology, and databases like the TLG or ARTFL, facilitate such wide-spectrum analyses. Computer-aided methods are thus capable of opening up new areas of study, which can potentially transform the way in which literature is studied.","Computers and the Humanities",1993,"No","key wordscomput aid literatur studi literatur literari theori structur sign symbol discours direct comput aid literatur studi comput aid literatur studi fail signific impact field failur trace concentr text achiev literari effect examin subtl semant grammat structur singl text work individu author comput system proven poor suit refin analysi complex languag adopt tradit object studi tend discourag research tool question adapt examin larg amount simpl linguist featur theoretician barth foucault halliday show import determin linguist semant characterist languag author audienc current technolog databas tlg artfl facilit wide spectrum analys comput aid method capabl open area studi potenti transform literatur studi",0
"Key wordscomputer-assisted language learning education IT concepts ","a rationale for teacher education and call the holistic view and its implications","In a paper written in 1987 entitled “Computers and the Humanities Courses: Philosophical Bases and Approaches” Nancy Ide put forward two views on teacher education in humanities computing, the “Expert User's View” and the “Holistic View”.1 Ide's two views are derived from the collective opinions given by members of a workshop on teaching computing and humanities courses. In this article the degree to which Ide's two Views can be substantiated in Computer-Assisted Language Learning (CALL) is explored, through a review of the literature and through an international survey on CALL materials development conducted by the author in 1991 (Levy, 1994). On this basis, and given the scarcity of Holistic courses in CALL, a rationale for a CALL course with a holistic orientation is presented.","Computers and the Humanities",1996,"No","key wordscomput assist languag learn educ concept rational teacher educ call holist view implic paper written entitl comput human cours philosoph base approach nanci ide put forward view teacher educ human comput expert user view holist view ide view deriv collect opinion member workshop teach comput human cours articl degre ide view substanti comput assist languag learn call explor review literatur intern survey call materi develop conduct author levi basi scarciti holist cours call rational call holist orient present",0
"KeywordsComputational Linguistic Musical Computing ","problems of representation in musical computing",NA,"Computers and the Humanities",1977,"No","comput linguist music comput problem represent music comput na",0
"KeywordsComputational Linguistic Modern Data ","the medieval and early modern data bank",NA,"Computers and the Humanities",1992,"No","comput linguist modern data mediev earli modern data bank na",0
"authorship lexical richness Lewis Carroll pastiche ","authorship attribution and pastiche","This paper considers the question of authorship attribution techniques whenfaced with a pastiche. We ask whether the techniques can distinguish the real thing from the fake, or can the author fool the computer? If the latter, is this because the pastiche is good, or because the technique is faulty? Using a number of mainly vocabulary-based techniques, Gilbert Adair's pastiche of Lewis Carroll, Alice Through the Needle's Eye, is compared with the original `Alice' books. Standard measures of lexical richness, Yule's K andOrlov's Z both distinguish Adair from Carroll, though Z also distinguishesthe two originals. A principal component analysis based on word frequenciesfinds that the main differences are not due to authorship. A discriminantanalysis based on word usage and lexical richness successfully distinguishes thepastiche from the originals. Weighted cusum tests were also unable to distinguish the two authors in a majority of cases. As a cross-validation, wemade similar comparisons with control texts: another children's story from thesame era, and other work by Carroll and Adair. The implications of thesefindings are discussed.","Computers and the Humanities",2003,"No","authorship lexic rich lewi carrol pastich authorship attribut pastich paper consid question authorship attribut techniqu whenfac pastich techniqu distinguish real thing fake author fool comput pastich good techniqu faulti number vocabulari base techniqu gilbert adair pastich lewi carrol alic needl eye compar origin alic book standard measur lexic rich yule andorlov distinguish adair carrol distinguishesth origin princip compon analysi base word frequenciesfind main differ due authorship discriminantanalysi base word usag lexic rich success distinguish thepastich origin weight cusum test unabl distinguish author major case cross valid wemad similar comparison control text children stori thesam era work carrol adair implic thesefind discuss",0
"KeywordsComputational Linguistic ","cai in college english",NA,"Computers and the Humanities",1978,"No","comput linguist cai colleg english na",0
"ambiguity Arabic definite clause grammar heuristics parser single-parse syntax analysis ","identifying syntactic ambiguities in single parse arabic sentence","The aim of this paper is to describe a technique for identifying the sourcesof several types of syntactic ambiguity in Arabic Sentences with a singleparse only. Normally, any sentence with two or more structuralrepresentations is said to be syntactically ambiguous. However, Arabicsentences with only one structural representation may be ambiguous. Ourtechnique for identifying Syntactic Ambiguity in Single-Parse ArabicSentences (SASPAS) analyzes each sentence and verifies the conditionsthat govern the existence of certain types of syntactic ambiguities in Arabicsentences. SASPAS is integrated with the syntactic parser, which is basedon Definite Clause Grammar (DCG) formalism. The system accepts Arabicsentences in their original script.","Computers and the Humanities",2001,"No","ambigu arab definit claus grammar heurist parser singl pars syntax analysi identifi syntact ambigu singl pars arab sentenc aim paper describ techniqu identifi sourcesof type syntact ambigu arab sentenc singlepars sentenc structuralrepresent syntact ambigu arabicsent structur represent ambigu ourtechniqu identifi syntact ambigu singl pars arabicsent saspa analyz sentenc verifi conditionsthat govern exist type syntact ambigu arabicsent saspa integr syntact parser basedon definit claus grammar dcg formal system accept arabicsent origin script",0
"KeywordsComputer Programming Computational Linguistic Teaching Language ","flow a teaching language for computer programming in the humanities",NA,"Computers and the Humanities",1974,"No","comput program comput linguist teach languag flow teach languag comput program human na",0
"KeywordsComputational Linguistic ","the new philology an old discipline or a new science",NA,"Computers and the Humanities",1969,"No","comput linguist philolog disciplin scienc na",0
NA,"adding phonetic similarity data to a lexical database","As part of a project to construct an interactive program which would encourage children to play with language by building jokes, we developed a lexical database, starting from WordNet. To the existing information about part of speech, synonymy, hyponymy, etc., we have added phonetic representations and phonetic similarity ratings for pairs of words/phrases. [ABSTRACT FROM AUTHOR], Copyright of Language Resources & Evaluation is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","Language Resources and Evaluation",2008,"No","ad phonet similar data lexic databas part project construct interact program encourag children play languag build joke develop lexic databas start wordnet exist inform part speech synonymi hyponymi ad phonet represent phonet similar rate pair wordsphras abstract author copyright languag resourc evalu properti springer natur content copi email multipl site post listserv copyright holder express written permiss user print download email articl individu abstract abridg warranti accuraci copi user refer origin publish version materi full abstract copyright appli abstract",0
"LINGUISTICS, LANGUAGE & languages, DATABASES, INFORMATION storage & retrieval systems, ELECTRONIC data processing, COMPUTERS, CW, IR, lexical database, Reuters, SEMCOR, TC, WordNet, WSD","integrating linguistic resources in tc through wsd","Information access methods must be improved to overcome the information overload that most professionals face nowadays. Text classification tasks, like Text Categorization, help the users to access to the great amount of text they find in the Internet and their organizations. TC is the classification of documents into a predefined set of categories. Most approaches to automatic TC are based on the utilization of a training collection, which is a set of manually classified documents. Other linguistic resources that are emerging, like lexical databases, can also be used for classification tasks. This article describes an approach to TC based on the integration of a training collection (Reuters-21578) and a lexical database (WordNet 1.6) as knowledge sources. Lexical databases accumulate information on the lexical items of one or several languages. This information must be filtered in order to make an effective use of it in our model of TC. This filtering process is a Word Sense Disambig)","Computers and the Humanities",2001,"No","linguist languag languag databas inform storag retriev system electron data process comput cw ir lexic databas reuter semcor tc wordnet wsd integr linguist resourc tc wsd inform access method improv overcom inform overload profession face nowaday text classif task text categor user access great amount text find internet organ tc classif document predefin set categori approach automat tc base util train collect set manual classifi document linguist resourc emerg lexic databas classif task articl describ approach tc base integr train collect reuter lexic databas wordnet knowledg sourc lexic databas accumul inform lexic item languag inform filter order make effect model tc filter process word sens disambig",0
"LEXICON, LINGUISTICS, COMPUTATIONAL linguistics, MULTILINGUALISM, MONOLINGUALISM, DATABASES, Explanatory combinatorial lexicology, Graph model, Lexical database, Lexical function","lexical systems graph models of natural language lexicons","We introduce a new type of lexical structure called lexical system, an interoperable model that can feed both monolingual and multilingual language resources. We begin with a formal characterization of lexical systems as simple directed graphs, solely made up of nodes corresponding to lexical entities and links. To illustrate our approach, we present data borrowed from a lexical system that has been generated from the French DiCo database. We later explain how the compilation of the original dictionary-like database into a net-like one has been made possible. Finally, we discuss the potential of the proposed lexical structure for designing multilingual lexical resources. [ABSTRACT FROM AUTHOR], Copyright of Language Resources & Evaluation is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual )","Language Resources and Evaluation",2009,"No","lexicon linguist comput linguist multilingu monolingu databas explanatori combinatori lexicolog graph model lexic databas lexic function lexic system graph model natur languag lexicon introduc type lexic structur call lexic system interoper model feed monolingu multilingu languag resourc begin formal character lexic system simpl direct graph sole made node lexic entiti link illustr approach present data borrow lexic system generat french dico databas explain compil origin dictionari databas net made final discuss potenti propos lexic structur design multilingu lexic resourc abstract author copyright languag resourc evalu properti springer natur content copi email multipl site post listserv copyright holder express written permiss user print download email articl individu",0
NA,"wordnet then and now","We briefly discuss the origin and development of WordNet, a large lexical database for English. We outline its design and contents as well as its usefulness for Natural Language Processing. Finally, we discuss crosslinguistic WordNets and complementary lexical resources. [ABSTRACT FROM AUTHOR], Copyright of Language Resources & Evaluation is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)","Language Resources and Evaluation",2007,"No","wordnet briefli discuss origin develop wordnet larg lexic databas english outlin design content use natur languag process final discuss crosslinguist wordnet complementari lexic resourc abstract author copyright languag resourc evalu properti springer natur content copi email multipl site post listserv copyright holder express written permiss user print download email articl individu abstract abridg warranti accuraci copi user refer origin publish version materi full abstract copyright appli abstract",0
"WORDNET (Book)","reviews","Reviews the book `WordNet: An Electronic Lexical Database,' edited by Christiane Fellbaum.","Computers and the Humanities",1974,"No","wordnet book review review book wordnet electron lexic databas edit christian fellbaum",0
