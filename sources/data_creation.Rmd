---
title: "Dataset Creation"
author: "Erin M. Buchanan"
date: "`r Sys.Date()`"
output: html_document
---

## Source Search and Cleanup

This file takes the input data for the LAB2, updates the potential files and matches, and outputs new data to examine for addition to the lab. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Import .bib files from search data

```{r}
#install.packages("bib2df")
library(bib2df)

#import all bib files
lexical_DB_ebsco = bib2df("lexical_DB_ebsco.bib")
lexical_DB_PO = bib2df("lexical_DB_PO.bib")
lexical_norms_ebsco = bib2df("lexical_norms_ebsco.bib")
lexical_norms_PO = bib2df("lexical_norms_PO.bib")
linguistic_DB_ebsco = bib2df("linguistic_DB_ebsco.bib")
linguistic_DB_PO = bib2df("linguistic_DB_PO.bib")
linguistic_norms_ebsco = bib2df("linguistic_norms_ebsco.bib")
linguistic_norms_PO = bib2df("linguistic_norms_PO.bib")

#fix the ones without ISBN
lexical_DB_PO$ISBN = NA
lexical_norms_ebsco$ISBN = NA
lexical_norms_PO$ISBN = NA
linguistic_DB_ebsco = NA

#take out all the non plos ones from that search
lexical_DB_PO = subset(lexical_DB_PO, 
                       JOURNAL == "PLoS ONE")
lexical_norms_PO = subset(lexical_norms_PO, 
                       JOURNAL == "PLoS ONE")
linguistic_DB_PO = subset(linguistic_DB_PO, 
                       JOURNAL == "PLoS ONE")
linguistic_norms_PO = subset(linguistic_norms_PO, 
                       JOURNAL == "PLoS ONE")

#bind together
allbib = rbind(lexical_DB_ebsco, lexical_DB_PO,
               lexical_norms_ebsco, lexical_norms_PO,
               linguistic_DB_ebsco, linguistic_DB_PO,
               linguistic_norms_ebsco, linguistic_norms_PO)

#remove duplicates
allbib = unique(allbib)

#deal with year column
allbib$YEAR = substr(allbib$YEAR, 0, 4)
```

## Import BRM csv files from search data

```{r}
#import the files
lexical_DB_BRM  = read.csv("lexical_DB_BRM.csv", stringsAsFactors = F)
lexical_norms_BRM  = read.csv("lexical_norms_BRM.csv", stringsAsFactors = F)
linguistic_DB_BRM = read.csv("linguistic_DB_BRM.csv", stringsAsFactors = F)
linguistic_norms_BRM = read.csv("linguistic_norms_BRM.csv", stringsAsFactors = F)
corpus_BRM = read.csv("corpus_BRM.csv", stringsAsFactors = F)
norms_BRM = read.csv("norms_BRM.csv", stringsAsFactors = F)

#merge together
allBRM = rbind(lexical_DB_BRM, lexical_norms_BRM, 
               linguistic_DB_BRM, linguistic_norms_BRM,
               norms_BRM, corpus_BRM)

#remove duplicates
allBRM = unique(allBRM)
```

## Merge search data

```{r}
colnames(allBRM) = c("TITLE", "JOURNAL", 
                     "BOOKTITLE", "VOLUME", 
                     "NUMBER", "DOI", "AUTHOR", 
                     "YEAR", "URL", "CATEGORY")
allBRM$CATEGORY = toupper(allBRM$CATEGORY)

library(plyr)

alldata = rbind.fill(allbib, allBRM)

alldata$TITLE = tolower(alldata$TITLE)
alldata$TITLE = gsub("  ", " ", alldata$TITLE)

#remove duplicates
#duplicated(alldata$TITLE)
alldata = alldata[!duplicated(alldata$TITLE) , ]
```

## The lab_table yes data + merge with Mendeley yes data

```{r}
lab_data = read.csv("lab_table.csv", stringsAsFactors = F, encoding = "UTF-8")
mendeley_data = bib2df("mendeley.bib")

lab_data$ref_title = gsub("[^[:print:]]","", lab_data$ref_title)
lab_data$ref_title = tolower(lab_data$ref_title)

setdiff(mendeley_data$BIBTEXKEY, lab_data$bibtex)
#language goldmine not in lab ok
#toglia 2009 not in lab table
#jorgenson one is just being weird because of character sets

setdiff(lab_data$bibtex, mendeley_data$BIBTEXKEY)
#yep just the weird character issue 

#use the mendeley data it has the complete set with the abstracts
```

## Create a table with both together

```{r}
##add codes and take out extra columns
mendeley_data_red = mendeley_data[ , names(alldata)]
##add codes
alldata$code = "No"
mendeley_data_red$code = "Yes"


##remove all data ones that are in the mendeley data
##as much as we can, will have to filter out the last set by hand

##merge together
combo_data = rbind(mendeley_data_red, alldata)

##find duplicates
combo_data$TITLE = tolower(combo_data$TITLE)
combo_data$duplicate = !duplicated(combo_data$TITLE)

combo_data = subset(combo_data, duplicate == T)

library(dplyr)
combo_data = combo_data %>% mutate_all(as.character)

write.csv(as.data.frame(combo_data), "combined_data_unedited.csv", row.names = F)
```

## Import the new one 

```{r}
library(readxl)
edited_data <- read_xlsx("combined_data_edited.xlsx")

##exclude the duplicates
edited_data <- subset(edited_data, 
                      duplicate == "TRUE")
```

## Find Missing Abstracts, Keywords

```{r}
edited_data$KEYWORDS[edited_data$KEYWORDS == "NA"] = NA
summary(is.na(edited_data$KEYWORDS))

edited_data$ABSTRACT[edited_data$ABSTRACT == "NA"] = NA
summary(is.na(edited_data$ABSTRACT))

edited_data$TITLE[edited_data$TITLE == "NA"] = NA
summary(is.na(edited_data$TITLE))

#find missing abstracts from BRM
good_abstracts <- subset(edited_data, 
                         !is.na(ABSTRACT))

missing_abstracts_BRM <- subset(edited_data, 
                                is.na(ABSTRACT) & grepl("springer", edited_data$URL))

missing_abstracts_other <- subset(edited_data, 
                                is.na(ABSTRACT) & !grepl("springer", edited_data$URL))
```

### RVest for BRM

```{r}
library(rvest)

for (i in 78:nrow(missing_abstracts_BRM)){
  
  ##current url
  url_temp <- missing_abstracts_BRM$URL[i]
  
  #Reading the HTML code from the website 
  webpage <- read_html(url_temp)
  abstract <- html_nodes(webpage, ".Para")
  missing_abstracts_BRM$ABSTRACT[i] <- html_text(abstract)[1]
  
  keywords <- html_nodes(webpage, ".KeywordGroup")
  keywords <- html_text(keywords)
  
  if (length(keywords)>0){
  
  keywords <- gsub("These keywords were added by machine and not by the authors. This process is experimental and the keywords may be updated as the learning algorithm improves.", "", keywords)
  missing_abstracts_BRM$KEYWORDS[i] <- gsub("\t|\n", "", keywords)
  } else {missing_abstracts_BRM$KEYWORDS[i] <- NA}
  

  ##quick break 
  Sys.sleep(runif(1, 0, 10))
}

summary(is.na(missing_abstracts_BRM$ABSTRACT))
```

## Recombine the data

```{r}
all_data <- rbind(missing_abstracts_BRM, missing_abstracts_other, good_abstracts)
nrow(all_data)
table(all_data$code)

write.csv(all_data, "completed_clean_data.csv", row.names = F)
```



