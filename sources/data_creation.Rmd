---
title: "Dataset Creation"
author: "Erin M. Buchanan"
date: "`r Sys.Date()`"
output: html_document
---

## Source Search and Cleanup

This file takes the input data for the LAB2, updates the potential files and matches, and outputs new data to examine for addition to the lab. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(tm)
library(dplyr)
library(plyr)
library(bib2df)
library(expss)
library(rvest)
```

## Import .bib files from search data

```{r}
#import all bib files
lexical_DB_ebsco = bib2df("input_data/lexical_DB_ebsco.bib")
lexical_DB_PO = bib2df("input_data/lexical_DB_PO.bib")
lexical_norms_ebsco = bib2df("input_data/lexical_norms_ebsco.bib")
lexical_norms_PO = bib2df("input_data/lexical_norms_PO.bib")
linguistic_DB_ebsco = bib2df("input_data/linguistic_DB_ebsco.bib")
linguistic_DB_PO = bib2df("input_data/linguistic_DB_PO.bib")
linguistic_norms_ebsco = bib2df("input_data/linguistic_norms_ebsco.bib")
linguistic_norms_PO = bib2df("input_data/linguistic_norms_PO.bib")

#fix the ones without ISBN
lexical_DB_PO$ISBN = NA
lexical_norms_ebsco$ISBN = NA
lexical_norms_PO$ISBN = NA
linguistic_DB_ebsco = NA

#take out all the non plos ones from that search
lexical_DB_PO = subset(lexical_DB_PO, 
                       JOURNAL == "PLoS ONE")
lexical_norms_PO = subset(lexical_norms_PO, 
                       JOURNAL == "PLoS ONE")
linguistic_DB_PO = subset(linguistic_DB_PO, 
                       JOURNAL == "PLoS ONE")
linguistic_norms_PO = subset(linguistic_norms_PO, 
                       JOURNAL == "PLoS ONE")

#bind together
allbib = rbind(lexical_DB_ebsco, lexical_DB_PO,
               lexical_norms_ebsco, lexical_norms_PO,
               linguistic_DB_ebsco, linguistic_DB_PO,
               linguistic_norms_ebsco, linguistic_norms_PO)

#remove duplicates
allbib = unique(allbib)

#deal with year column
allbib$YEAR = substr(allbib$YEAR, 0, 4)
```

## Import BRM csv files from search data

```{r}
#import the files
lexical_DB_BRM  = read.csv("input_data/lexical_DB_BRM.csv", stringsAsFactors = F)
lexical_norms_BRM  = read.csv("input_data/lexical_norms_BRM.csv", stringsAsFactors = F)
linguistic_DB_BRM = read.csv("input_data/linguistic_DB_BRM.csv", stringsAsFactors = F)
linguistic_norms_BRM = read.csv("input_data/linguistic_norms_BRM.csv", stringsAsFactors = F)
corpus_BRM = read.csv("input_data/corpus_BRM.csv", stringsAsFactors = F)
norms_BRM = read.csv("input_data/norms_BRM.csv", stringsAsFactors = F)

#merge together
allBRM = rbind(lexical_DB_BRM, lexical_norms_BRM, 
               linguistic_DB_BRM, linguistic_norms_BRM,
               norms_BRM, corpus_BRM)

#remove duplicates
allBRM = unique(allBRM)
```

## Import LRE csv files from search data

```{r}
#import the files
lexical_DB_LRE  = read.csv("input_data/lexical_DB_LRE.csv", stringsAsFactors = F)
lexical_norms_LRE  = read.csv("input_data/lexical_norms_LRE.csv", stringsAsFactors = F)
linguistic_DB_LRE = read.csv("input_data/linguistic_DB_LRE.csv", stringsAsFactors = F)
linguistic_norms_LRE = read.csv("input_data/linguistic_norms_LRE.csv", stringsAsFactors = F)
corpus_LRE = read.csv("input_data/corpus_LRE.csv", stringsAsFactors = F)
norms_LRE = read.csv("input_data/norms_LRE.csv", stringsAsFactors = F)

#merge together
allLRE = rbind(lexical_DB_LRE, lexical_norms_LRE, 
               linguistic_DB_LRE, linguistic_norms_LRE,
               norms_LRE, corpus_LRE)

#remove duplicates
allLRE = unique(allLRE)
```

## Merge search data

```{r}
#fix colnames to merge
colnames(allBRM) = c("TITLE", "JOURNAL", 
                     "BOOKTITLE", "VOLUME", 
                     "NUMBER", "DOI", "AUTHOR", 
                     "YEAR", "URL", "CATEGORY")
allBRM$CATEGORY = toupper(allBRM$CATEGORY)

colnames(allLRE) = c("TITLE", "JOURNAL", 
                     "BOOKTITLE", "VOLUME", 
                     "NUMBER", "DOI", "AUTHOR", 
                     "YEAR", "URL", "CATEGORY")
allLRE$CATEGORY = toupper(allLRE$CATEGORY)

alldata = rbind.fill(allbib, allBRM, allLRE)

alldata$TITLE = tolower(alldata$TITLE)
alldata$TITLE = gsub("  ", " ", alldata$TITLE)

#remove duplicates
#duplicated(alldata$TITLE)
alldata = alldata[!duplicated(alldata$TITLE) , ]
```

## The lab_table included data + merge with Mendeley included data

```{r}
lab_data = read.csv("input_data/lab_table.csv", stringsAsFactors = F, encoding = "UTF-8")
mendeley_data = bib2df("input_data/mendeley.bib")

lab_data$ref_title = gsub("[^[:print:]]","", lab_data$ref_title)
lab_data$ref_title = tolower(lab_data$ref_title)

setdiff(mendeley_data$BIBTEXKEY, lab_data$bibtex)
#language goldmine not in lab ok
#toglia 2009 not in lab table
#jorgenson one is just being weird because of character sets

setdiff(lab_data$bibtex, mendeley_data$BIBTEXKEY)
#yep just the weird character issue 

#use the mendeley data it has the complete set with the abstracts
```

## Create a table with both together

```{r}
##add codes and take out extra columns
mendeley_data_red = mendeley_data[ , names(alldata)]
##add codes
alldata$code = "No"
mendeley_data_red$code = "Yes"

##remove all data ones that are in the mendeley data
##as much as we can, will have to filter out the last set by hand

##merge together
combo_data = rbind(mendeley_data_red, alldata)

##find duplicates
combo_data$TITLE <- iconv(combo_data$TITLE, "latin1", "ASCII", sub = "") #take out symbols
combo_data$TITLE <- gsub("-", " ", combo_data$TITLE) #take out dashes 
combo_data$TITLE <- removePunctuation(combo_data$TITLE) #take out rest of punct
combo_data$TITLE = tolower(combo_data$TITLE) #lower case
combo_data$TITLE <- gsub("  ", " ", combo_data$TITLE) #squeeze out double spaces
combo_data$duplicate = !duplicated(combo_data$TITLE) #figure out duplicates

nrow(subset(combo_data, duplicate == F & code == "Yes")) #make sure you aren't losing the Yes

reduce_titles = subset(combo_data, duplicate == T)
reduce_titles = reduce_titles %>% mutate_all(as.character)

##exclude ones with corrected at the beginning
no_correct <- subset(reduce_titles, 
                     !grepl("^correction|^erratum", reduce_titles$TITLE))
no_correct <- subset(no_correct, 
                     !is.na(TITLE))

##find any new weird repeats
flag_titles <- NA

##find ones to flag that have very similar titles
for (i in 1:length(no_correct$TITLE)){
  temp_dist <- stringdist(no_correct$TITLE[i], no_correct$TITLE)
  if (sum(temp_dist <= 5) > 1) {
    flag_titles <- rbind(flag_titles, no_correct[ temp_dist <=5, ])
  }
}

flag_titles_red <- subset(flag_titles,
                      !duplicated(flag_titles$TITLE))
write.csv(as.data.frame(flag_titles_red), "output_data/flagged_titles.csv", row.names = F)
##open this file and hand edit it!

##exclude previously flagged titles
exclude_these <- read.csv("output_data/exclude_titles.csv", stringsAsFactors = F)

final_excluded <- subset(no_correct, 
                         !(no_correct$TITLE %in% exclude_these$exclude_titles))
```

## Find Missing Abstracts, Keywords

```{r}
edited_data <- final_excluded

edited_data$KEYWORDS[edited_data$KEYWORDS == "NA"] = NA
summary(is.na(edited_data$KEYWORDS))

edited_data$ABSTRACT[edited_data$ABSTRACT == "NA"] = NA
summary(is.na(edited_data$ABSTRACT))

edited_data$TITLE[edited_data$TITLE == "NA"] = NA
summary(is.na(edited_data$TITLE))

#find missing abstracts from spring websites includes BRM and LRE
good_abstracts <- subset(edited_data, 
                         !is.na(ABSTRACT))
summary(is.na(good_abstracts$ABSTRACT))

missing_abstracts_BRM <- subset(edited_data, 
                                is.na(ABSTRACT) & grepl("springer", edited_data$URL))
summary(is.na(missing_abstracts_BRM$ABSTRACT))

missing_abstracts_other <- subset(edited_data, 
                                is.na(ABSTRACT) & !grepl("springer", edited_data$URL))
summary(is.na(missing_abstracts_other$ABSTRACT))

##fill in missing from previous dataset
last_run <- read.csv("completed_clean_data.csv", stringsAsFactors = F)
last_run$TITLE <- iconv(last_run$TITLE, "latin1", "ASCII", sub = "") #take out symbols
last_run$TITLE <- gsub("-", " ", last_run$TITLE) #take out dashes 
last_run$TITLE <- removePunctuation(last_run$TITLE) #take out rest of punct
last_run$TITLE = tolower(last_run$TITLE) #lower case
last_run$TITLE <- gsub("  ", " ", last_run$TITLE) #squeeze out double spaces

#match abstracts and keywords
missing_abstracts_BRM$ABSTRACT <- 
  vlookup(lookup_value = missing_abstracts_BRM$TITLE, #code to look up
          dict = last_run, #look up dataset
          result_column = "ABSTRACT", #value you want
          lookup_column = "TITLE") #how to match 

missing_abstracts_BRM$KEYWORDS <- 
  vlookup(lookup_value = missing_abstracts_BRM$TITLE, #code to look up
          dict = last_run, #look up dataset
          result_column = "KEYWORDS", #value you want
          lookup_column = "TITLE") #how to match         

good_abstracts_BRM <- subset(missing_abstracts_BRM,
                             !is.na(ABSTRACT))
missing_abstracts_BRM <- subset(missing_abstracts_BRM,
                                is.na(ABSTRACT))
```

### RVest for BRM, LRE, Computers and the Humanities

Note that Computers in the Humanities is the old LRE name.

```{r}
#loop over websites and get abstracts
for (i in 1:nrow(missing_abstracts_BRM)){
  
  ##current url
  url_temp <- missing_abstracts_BRM$URL[i]
  
  #Reading the HTML code from the website 
  webpage <- read_html(url_temp)
  abstract <- html_nodes(webpage, ".Para")
  missing_abstracts_BRM$ABSTRACT[i] <- html_text(abstract)[1]
  
  keywords <- html_nodes(webpage, ".KeywordGroup")
  keywords <- html_text(keywords)
  
  if (length(keywords)>0){
  
  keywords <- gsub("These keywords were added by machine and not by the authors. This process is experimental and the keywords may be updated as the learning algorithm improves.", "", keywords)
  missing_abstracts_BRM$KEYWORDS[i] <- gsub("\t|\n", "", keywords)
  } else {missing_abstracts_BRM$KEYWORDS[i] <- NA}
  

  ##quick break 
  Sys.sleep(runif(1, 0, 10))
}

summary(is.na(missing_abstracts_BRM$ABSTRACT))
```

## Recombine the data

```{r}
all_data <- rbind(missing_abstracts_BRM, 
                  missing_abstracts_other, 
                  good_abstracts,
                  good_abstracts_BRM)
nrow(all_data)
table(all_data$code)

write.csv(all_data, "completed_clean_data.csv", row.names = F)
```



