@article{2018-61201-03020181201,
Abstract = {We present the Chinese Lexical Database (CLD): a large-scale lexical database for simplified Chinese. The CLD provides a wealth of lexical information for 3913 one-character words, 34,233 two-character words, 7143 three-character words, and 3355 four-character words, and is publicly available through http://www.chineselexicaldatabase.com. For each of the 48,644 words in the CLD, we provide a wide range of categorical predictors, as well as an extensive set of frequency measures, complexity measures, neighborhood density measures, orthography-phonology consistency measures, and information-theoretic measures. We evaluate the explanatory power of the lexical variables in the CLD in the context of experimental data through analyses of lexical decision latencies for one-character, two-character, three-character and four-character words, as well as word naming latencies for one-character and two-character words. The results of these analyses are discussed. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
Author = {Sun, Ching Chu and  Hendrix, Peter and  Ma, Jianqiang and  Baayen, Rolf Harald},
ISSN = {1554-351X, 1554-3528},
Journal = {Behavior Research Methods},
Keywords = {Mandarin Chinese, simplified Chinese, Chinese lexical database, word naming, lexical decision, Databases, Language, Lexical Decision, Naming, Words (Phonetic Units)},
Number = {6},
Pages = {2606 - 2629},
Title = {Chinese lexical database (CLD): A large-scale lexical database for simplified Mandarin Chinese},
URL = {ching-chu.sun@uni-tuebingen.de, peter.hendrix@gmail.com, jianqiang.ma@uni-tuebingen.de, harald.baayen@uni-tuebingen.de},
Volume = {50},
Year = {2018},
}


@article{2018-61201-01020181201,
Abstract = {In this article, we present StimulStat—a lexical database for the Russian language in the form of a web application. The database contains more than 52,000 of the most frequent Russian lemmas and more than 1.7 million word forms derived from them. These lemmas and forms are characterized according to more than 70 properties that were demonstrated to be relevant for psycholinguistic research, including frequency, length, phonological and grammatical properties, orthographic and phonological neighborhood frequency and size, grammatical ambiguity, homonymy and polysemy. Some properties were retrieved from various dictionaries and are presented collectively in a searchable form for the first time, the others were computed specifically for the database. The database can be accessed freely at http://stimul.cognitivestudies.ru. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
Author = {Alexeeva, Svetlana and  Slioussar, Natalia and  Chernova, Daria},
ISSN = {1554-351X, 1554-3528},
Journal = {Behavior Research Methods},
Keywords = {lexical database, Russian, lemmas, forms, psycholinguistic research, Databases, Experimentation, Language, Psycholinguistics, Words (Phonetic Units)},
Number = {6},
Pages = {2305 - 2315},
Title = {StimulStat: A lexical database for Russian},
URL = {mail@s-alexeeva.ru, ORCID: 0000-0002-8540-6178, ORCID: 0000-0003-1706-6439},
Volume = {50},
Year = {2018},
}


@article{13243649620181017,
Abstract = {The Lesser Sunda Islands in eastern Indonesia cover a longitudinal distance of some 600 kilometres. They are the westernmost place where languages of the Austronesian family come into contact with a family of Papuan languages and constitute an area of high linguistic diversity. Despite its diversity, the Lesser Sundas are little studied and for most of the region, written historical records, as well as archaeological and ethnographic data are lacking. In such circumstances the study of relationships between languages through their lexicon is a unique tool for making inferences about human (pre-)history and tracing population movements. However, the lack of a collective body of lexical data has severely limited our understanding of the history of the languages and peoples in the Lesser Sundas. The LexiRumah database fills this gap by assembling lexicons of Lesser Sunda languages from published and unpublished sources, and making those lexicons available online in a consistent format. T)},
Author = {Kaiping, Gereon A. and  Klamer, Marian},
ISSN = {19326203},
Journal = {PLoS ONE},
Keywords = {LEXICAL phonology, AUSTRONESIAN languages, PRESERVATION of historic records, GEOMORPHOLOGY, LESSER Sunda Islands, Computer and information sciences, Data management, Earth sciences, Geomorphology, Grammar, Historical linguistics, Islands, Landforms, Languages, Lexicons, Linguistic morphology, Linguistics, Metadata, Phonology, Research Article, Semantics, Social sciences, Topography, Vocabulary},
Number = {10},
Pages = {1 - 29},
Title = {LexiRumah: An online lexical database of the Lesser Sunda Islands.},
Volume = {13},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=132436496&site=ehost-live&scope=site},
Year = {2018},
}


@article{2018-38143-01020180801,
Abstract = {In this article, we present Procura-PALavras (P-PAL), a Web-based interface for a new European Portuguese (EP) lexical database. Based on a contemporary printed corpus of over 227 million words, P-PAL provides a broad range of word attributes and statistics, including several measures of word frequency (e.g., raw counts, per-million word frequency, logarithmic Zipf scale), morpho-syntactic information (e.g., parts of speech [PoSs], grammatical gender and number, dominant PoS, and frequency and relative frequency of the dominant PoS), as well as several lexical and sublexical orthographic (e.g., number of letters; consonant–vowel orthographic structure; density and frequency of orthographic neighbors; orthographic Levenshtein distance; orthographic uniqueness point; orthographic syllabification; and trigram, bigram, and letter type and token frequencies), and phonological measures (e.g., pronunciation, number of phonemes, stress, density and frequency of phonological neighbors, transposed and phonographic neighbors, syllabification, and biphone and phone type and token frequencies) for ~53,000 lemmatized and ~208,000 nonlemmatized EP word forms. To obtain these metrics, researchers can choose between two word queries in the application: (i) analyze words previously selected for specific attributes and/or lexical and sublexical characteristics, or (ii) generate word lists that meet word requirements defined by the user in the menu of analyses. For the measures it provides and the flexibility it allows, P-PAL will be a key resource to support research in all cognitive areas that use EP verbal stimuli. P-PAL is freely available at http://p-pal.di.uminho.pt/tools. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
Author = {Soares, Ana Paula and  Iriarte, Álvaro and  de Almeida, José João and  Simões, Alberto and  Costa, Ana and  Machado, João and  França, Patrícia and  Comesaña, Montserrat and  Rauber, Andreia and  Rato, Anabela and  Perea, Manuel},
ISSN = {1554-351X, 1554-3528},
Journal = {Behavior Research Methods},
Keywords = {Lexical databases, Word frequency, Orthographic word statistics, Phonological word statistics, European Portuguese, Databases, Human Computer Interaction, Language, Phonology, Word Frequency, Foreign Languages, Grammar, Orthography, Statistics},
Number = {4},
Pages = {1461 - 1481},
Title = {Procura-PALavras (P-PAL): A Web-based interface for a new European Portuguese lexical database},
URL = {asoares@psi.uminho.pt},
Volume = {50},
Year = {2018},
}


@article{12177766920170301,
Abstract = {Similarity calculation between business process models has an important role in managing repository of business process model. One of its uses is to facilitate the searching process of models in the repository. Business process similarity is closely related to semantic string similarity. Semantic string similarity is usually performed by utilizing a lexical database such as WordNet to find the semantic meaning of the word. The activity name of the business process uses terms that specifically related to the business field. However, most of the terms in business domain are not available in WordNet. This case would decrease the semantic analysis quality of business process model. Therefore, this study would try to improve semantic analysis of business process model. We present a new lexical database called BBabelNet. B-BabelNet is a lexical database built by using the same method in BabelNet. We attempt to map the Wikipedia page to WordNet database but only focus on the word related to )},
Author = {Pamungkas, Endang Wahyu and  Sarno, Riyanarto and  Munif, Abdul},
ISSN = {16936930},
Journal = {Telkomnika},
Keywords = {BUSINESS databases, BUSINESS models, LEXICAL access, INSTITUTIONAL repositories, SEMANTICS, business domain, lexical database, semantic string similarity, similarity},
Number = {1},
Pages = {407 - 414},
Title = {B-BabelNet: Business-Specific Lexical Database for Improving Semantic Analysis of Business Process Models.},
Volume = {15},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=121777669&site=ehost-live&scope=site},
Year = {2017},
}


@article{12141222720170223,
Abstract = {Databases containing lexical properties on any given orthography are crucial for psycholinguistic research. In the last ten years, a number of lexical databases have been developed for Greek. However, these lack important part-of-speech information. Furthermore, the need for alternative procedures for calculating syllabic measurements and stress information, as well as combination of several metrics to investigate linguistic properties of the Greek language are highlighted. To address these issues, we present a new extensive lexical database of Modern Greek (GreekLex 2) with part-of-speech information for each word and accurate syllabification and orthographic information predictive of stress, as well as several measurements of word similarity and phonetic information. The addition of detailed statistical information about Greek part-of-speech, syllabification, and stress neighbourhood allowed novel analyses of stress distribution within different grammatical categories and syllabic l)},
Author = {Kyparissiadis, Antonios and  van Heuven, Walter J. B. and  Pitchford, Nicola J. and  Ledgeway, Timothy},
ISSN = {19326203},
Journal = {PLoS ONE},
Keywords = {PSYCHOLINGUISTICS, LEXICAL access, DATABASE design, STRESS concentration, PHONETIC transcriptions, SYLLABICATION, Biology and life sciences, Cognitive psychology, Cognitive science, DNA transcription, Gene expression, Genetics, Language, Linguistics, Mathematics, Neuroscience, Phonetics, Phonology, Physical sciences, Probability theory, Psycholinguistics, Psychology, Research Article, Social sciences, Statistical distributions, Syllables, Vowels},
Number = {2},
Pages = {1 - 20},
Title = {GreekLex 2: A comprehensive lexical database with part-of-speech, syllabic, phonological, and stress information.},
Volume = {12},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=121412227&site=ehost-live&scope=site},
Year = {2017},
}


@article{2017-45571-03020171001,
Abstract = {This article presents K-SPAN (Korean Surface Phonetics and Neighborhoods), a database of surface phonetic forms and several measures of phonological neighborhood density for 63,836 Korean words. Currently publicly available Korean corpora are limited by the fact that they only provide orthographic representations in Hangeul, which is problematic since phonetic forms in Korean cannot be reliably predicted from orthographic forms. We describe the method used to derive the surface phonetic forms from a publicly available orthographic corpus of Korean, and report on several statistics calculated using this database; namely, segment unigram frequencies, which are compared to previously reported results, along with segment-based and syllable-based neighborhood density statistics for three types of representation: an 'orthographic' form, which is a quasi-phonological representation, a 'conservative' form, which maintains all known contrasts, and a 'modern' form, which represents the pronunciation of contemporary Seoul Korean. These representations are rendered in an ASCII-encoded scheme, which allows users to query the corpus without having to read Korean orthography, and permits the calculation of a wide range of phonological measures. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
Author = {Holliday, Jeffrey J. and  Turnbull, Rory and  Eychenne, Julien},
ISSN = {1554-351X, 1554-3528},
Journal = {Behavior Research Methods},
Keywords = {Korean, Phonological neighborhood density, Lexicon, Lexical database, Databases, Methodology, Orthography, Statistical Analysis, Mental Lexicon, Cognitive Science, Language, Phonology},
Number = {5},
Pages = {1939 - 1950},
Title = {K-SPAN: A lexical database of Korean surface phonetic forms and phonological neighborhood density statistics},
URL = {holliday@korea.ac.kr, rory.turnbull@ens.fr, jeychenne@hufs.ac.kr},
Volume = {49},
Year = {2017},
}


@article{2017-19296-02920170401,
Abstract = {ASL-LEX is a lexical database that catalogues information about nearly 1,000 signs in American Sign Language (ASL). It includes the following information: subjective frequency ratings from 25–31 deaf signers, iconicity ratings from 21–37 hearing non-signers, videoclip duration, sign length (onset and offset), grammatical class, and whether the sign is initialized, a fingerspelled loan sign, or a compound. Information about English translations is available for a subset of signs (e.g., alternate translations, translation consistency). In addition, phonological properties (sign type, selected fingers, flexion, major and minor location, and movement) were coded and used to generate sub-lexical frequency and neighborhood density estimates. ASL-LEX is intended for use by researchers, educators, and students who are interested in the properties of the ASL lexicon. An interactive website where the database can be browsed and downloaded is available at http://asl-lex.org . (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
Author = {Caselli, Naomi K. and  Sehyr, Zed Sevcikova and  Cohen-Goldberg, Ariel M. and  Emmorey, Karen},
ISSN = {1554-351X, 1554-3528},
Journal = {Behavior Research Methods},
Keywords = {Sign language, Lexical database, Subjective frequency, Iconicity, Neighborhood density, Deaf, Language Proficiency, Lexical Decision, Sign Language},
Number = {2},
Pages = {784 - 801},
Title = {ASL-LEX: A lexical database of American Sign Language},
URL = {nkc@bu.edu, kemmorey@mail.sdsu.edu},
Volume = {49},
Year = {2017},
}


@article{12878085120180301,
Abstract = {In geographic information science, semantic relatedness is important for Geographic Information Retrieval (GIR), Linked Geospatial Data, geoparsing, and geo-semantics. But computing the semantic similarity/relatedness of geographic terminology is still an urgent issue to tackle. The thesaurus is a ubiquitous and sophisticated knowledge representation tool existing in various domains. In this article, we combined the generic lexical database (WordNet or HowNet) with the Thesaurus for Geographic Science and proposed a thesaurus-lexical relatedness measure (TLRM) to compute the semantic relatedness of geographic terminology. This measure quantified the relationship between terminologies, interlinked the discrete term trees by using the generic lexical database and realized the semantic relatedness computation of any two terminologies in the thesaurus. The TLRM was evaluated on a new relatedness baseline, namely, the Geo-Terminology Relatedness Dataset (GTRD) which was built by us, and th)},
Author = {Chen, Zugang and  Song, Jia and  Yang, Yaping},
ISSN = {22209964},
Journal = {ISPRS International Journal of Geo-Information},
Keywords = {GEOGRAPHIC information systems, INFORMATION retrieval, NATURAL language processing, geographic terminology, Geospatial Information Retrieval (GIR), lexical databases, semantic relatedness, thesaurus, thesaurus–lexical relatedness measure (TLRM), thesaurus-lexical relatedness measure (TLRM)},
Number = {3},
Pages = {98},
Title = {An Approach to Measuring Semantic Relatedness of Geographic Terminologies Using a Thesaurus and Lexical Database Sources.},
Volume = {7},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=128780851&site=ehost-live&scope=site},
Year = {2018},
}


@article{11079992920140702,
Abstract = {The Online Malay Language Corpus-based Lexical Database for Primary Schools discussed in this paper is a long-term research project by the School of Educational Studies, Universiti Sains Malaysia. Based on a corpus of Malay language textbooks used in primary schools, the project aims to develop a lexical database of Malay words commonly encountered by elementary school children in Malaysia. Available online, the database has an interactive interface that allows users to search in real-time primary linguistic features such as word frequency, word length, phoneme length, number and type of syllables, as well as word category. The database has proven to be a useful resource for both researchers and practitioners who use it to identify linguistically and culturally appropriate sets of word stimuli for material development, language assessment, language teaching, and language remediation. These applications facilitate and promote evidence-based teaching and research practices pertaining to)},
Author = {Lay Wah Lee and  Hui Min Low},
ISSN = {01274082},
Journal = {Kajian Malaysia: Journal of Malaysian Studies},
Keywords = {CORPORA (Linguistics), ONLINE databases, MALAY language, PRIMARY schools, TEACHING, assessment, Malay language, online lexical database, reading, teaching},
Pages = {151 - 166},
Title = {THE DEVELOPMENT AND APPLICATION OF AN ONLINE MALAY LANGUAGE CORPUS-BASED LEXICAL DATABASE.},
Volume = {32},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=110799929&site=ehost-live&scope=site},
Year = {2014},
}


@article{2016-16578-01020160301,
Abstract = {The LSE-Sign database is a free online tool for selecting Spanish Sign Language stimulus materials to be used in experiments. It contains 2,400 individual signs taken from a recent standardized LSE dictionary, and a further 2,700 related nonsigns. Each entry is coded for a wide range of grammatical, phonological, and articulatory information, including handshape, location, movement, and non-manual elements. The database is accessible via a graphically based search facility which is highly flexible both in terms of the search options available and the way the results are displayed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Gutierrez-Sigut, Eva and  Costello, Brendan and  Baus, Cristina and  Carreiras, Manuel},
ISSN = {1554-351X, 1554-3528},
Journal = {Behavior Research Methods},
Keywords = {Sign language, Lexical database, Spanish Sign Language (LSE lengua de signos española), Stimulus material, Databases, Grammar, Lexical Access, Phonology, Sign Language, Stimulus Parameters},
Number = {1},
Pages = {123 - 137},
Title = {LSE-Sign: A lexical database for Spanish Sign Language},
URL = {b.costello@bcbl.eu, ORCID: 0000-0002-1713-1483},
Volume = {48},
Year = {2016},
}


@article{2014-07832-02120140301,
Abstract = {In this article, we introduce ESCOLEX, the first European Portuguese children’s lexical database with grade-level-adjusted word frequency statistics. Computed from a 3.2-million-word corpus, ESCOLEX provides 48,381 word forms extracted from 171 elementary and middle school textbooks for 6- to 11-year-old children attending the first six grades in the Portuguese educational system. Like other children’s grade-level databases (e.g., Carroll, Davies, & Richman, 1971; Corral, Ferrero, & Goikoetxea, Behavior Research Methods, 41, 1009–1017, 2009; Lété, Sprenger-Charolles, & Colé, Behavior Research Methods, Instruments, & Computers, 36, 156–166, 2004; Zeno, Ivens, Millard, Duvvuri, 1995), ESCOLEX provides four frequency indices for each grade: overall word frequency (F), index of dispersion across the selected textbooks (D), estimated frequency per million words (U), and standard frequency index (SFI). It also provides a new measure, contextual diversity (CD). In addition, the number of letters in the word and its part(s) of speech, number of syllables, syllable structure, and adult frequencies taken from P-PAL (a European Portuguese corpus-based lexical database; Soares, Comesaña, Iriarte, Almeida, Simões, Costa, …, Machado, 2010; Soares, Iriarte, Almeida, Simões, Costa, França, …, Comesaña, in press) are provided. ESCOLEX will be a useful tool both for researchers interested in language processing and development and for professionals in need of verbal materials adjusted to children’s developmental stages. ESCOLEX can be downloaded along with this article or from http://p-pal.di.uminho.pt/about/databases. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Soares, Ana Paula and  Medeiros, José Carlos and  Simões, Alberto and  Machado, João and  Costa, Ana and  Iriarte, Álvaro and  de Almeida, José João and  Pinheiro, Ana P. and  Comesaña, Montserrat},
ISSN = {1554-351X, 1554-3528},
Journal = {Behavior Research Methods},
Keywords = {ESCOLEX, lexical database, textbooks, word frequency, language processing, grade level, teaching programs, Child, Computer-Assisted Instruction, Databases, Factual, Humans, Information Literacy, Language, Portugal, Reading, Schools, Textbooks as Topic, Vocabulary, Language Development, Lexical Access, Teaching Methods, Word Frequency, Databases, Grade Level, Textbooks},
Number = {1},
Pages = {240 - 253},
Title = {Escolex: A grade-level lexical database from European Portuguese elementary to middle school textbooks},
URL = {asoares@psi.uminho.pt, ORCID: 0000-0001-6961-2660, ORCID: 0000-0002-7981-3682, ORCID: 0000-0003-2547-7684},
Volume = {46},
Year = {2014},
}


@article{2015-13542-00420150401,
Abstract = {All words have properties linked to form, meaning and usage patterns which influence how easily they are accessed from the mental lexicon in language production, perception and comprehension. Examples of such properties are imageability, phonological and morphological complexity, word class, argument structure, frequency of use and age of acquisition. Due to linguistic and cultural variation the properties and the values associated with them differ across languages. Hence, for research as well as clinical purposes, language specific information on lexical properties is needed. To meet this need, an electronically searchable lexical database with more than 1600 Norwegian words coded for more than 12 different properties has been established. This article presents the content and structure of the database as well as the search options available in the interface. Finally, it briefly describes some of the ways in which the database can be used in research, clinical practice and teaching. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Lind, Marianne and  Simonsen, Hanne Gram and  Hansen, Pernille and  Holm, Elisabeth and  Mevik, Bjørn-Helge},
ISSN = {0269-9206, 1464-5076},
Journal = {Clinical Linguistics & Phonetics},
Keywords = {Age of acquisition, frequency, imageability, language processing, phonological neighbourhood density, Adult, Child, Child, Preschool, Databases, Factual, Humans, Infant, Language, Language Development, Language Disorders, Norway, Phonetics, Research, Semantics, Speech-Language Pathology, User-Computer Interface, Verbal Learning, Vocabulary, Clinicians, Language, Phonological Awareness, Comprehension, Databases},
Number = {4},
Pages = {276 - 290},
Title = {Norwegian Words: A lexical database for clinicians and researchers},
URL = {marianne.lind@statped.no},
Volume = {29},
Year = {2015},
}


@article{2015-47357-00120150701,
Abstract = {In this article we introduce childLex, an online database for children’s print language in German. childLex is based on a large corpus of children’s books and textbooks used in school that comprises ca. 8 million words. It includes linguistic norms on the lexical, super-lexical, and sub-lexical level. Such norms are urgently needed in psychological research for the design of age-adequate stimulus and training materials. Separate norms are available for three age groups: 6–8 years (1st and 2nd grade), 9–10 years (3rd and 4th grade), and 11–12 years (5th and 6th grade). Moreover, childLex is directly comparable to the DWDS corpus, a large corpus for adults’ print language. Here, we describe how childLex was collected and analyzed and compare frequency norms on the lexical, super-lexical, and sub-lexical level with frequency norms obtained from the DWDS corpus. Results show moderate levels of correspondence in the high-frequency range and on the sub-lexical level, but strong discrepancies for medium- and low-frequency elements on the lexical and super-lexical level. Finally, we illustrate which variables are available in childLex and how they can be retrieved using a web interface. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Schroeder, Sascha and  Würzner, Kay-Michael and  Heister, Julian and  Geyken, Alexander and  Kliegl, Reinhold},
ISSN = {0033-3042, 2190-6238},
Journal = {Psychologische Rundschau},
Keywords = {child language, linguistic database, lexical variables, Childhood Development, Databases, Linguistics, Lexical Access, Age Differences},
Number = {3},
Pages = {155 - 165},
Title = {childLex—Eine lexikalische Datenbank zur Schriftsprache für Kinder im Deutschen = childLex—A Lexical Database for Print Language for Children in German},
URL = {sascha.schroeder@mpib-berlin.mpg.de, ORCID: 0000-0002-0180-8488},
Volume = {66},
Year = {2015},
}


@article{11081100720151201,
Abstract = {This article introduces childLex, an online database of German read by children. childLex is based on a corpus of children's books and comprises 10 million words that were syntactically annotated and lemmatized. childLex reports linguistic norms for lexical, superlexical, and sublexical variables in three different age groups: 6-8 (grades 1-2), 9-10 (grades 3-4), and 11-12 years (grades 5-6). Here, we describe how childLex was collected and analyzed. In addition, we provide information about the distributions of word frequency, word length, and orthographic neighborhood size, as well as their intercorrelations. Finally, we explain how childLex can be accessed using a Web interface. [ABSTRACT FROM AUTHOR], Copyright of Behavior Research Methods is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for indivi)},
Author = {Schroeder, Sascha and  Würzner, Kay-Michael and  Heister, Julian and  Geyken, Alexander and  Kliegl, Reinhold},
ISSN = {1554351X},
Journal = {Behavior Research Methods},
Keywords = {LEXICAL grammar, ONLINE databases, CHILDREN'S books, READING, WORD frequency, Child language, Lexical database, Reading development},
Number = {4},
Pages = {1085 - 1094},
Title = {childLex: a lexical database of German read by children.},
Volume = {47},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=110811007&site=ehost-live&scope=site},
Year = {2015},
}


@article{2011-01460-00120110101,
Abstract = {The lexical database dlexDB supplies in form of an online database frequency-based norms of numerous processrelated word properties for psychological and linguistic research. These values include well known variables such as printed frequency of word form and lemma as documented also in CELEX (Baayen, Piepenbrock und Gulikers, 1995). In addition, we compute new values like frequencies based on syllables, and morphemes as well as frequencies of character chains, and multiple word combinations. The statistics are based on the Kernkorpus des Digitalen Wörterbuchs der deutschen Sprache (DWDS) with over 100 million running words. We illustrate the validity of these norms with new results about fixation durations in sentence reading. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Heister, Julian and  Würzner, Kay-Michael and  Bubenzer, Johannes and  Pohl, Edmund and  Hanneforth, Thomas and  Geyken, Alexander and  Kliegl, Reinhold},
ISSN = {0033-3042, 2190-6238},
Journal = {Psychologische Rundschau},
Keywords = {lexical database, corpus linguistics, dlexDB, reading, Databases, Psycholinguistics},
Number = {1},
Pages = {10 - 20},
Title = {dlexDB—Eine lexikalische Datenbank für die psychologische und linguistische Forschung = dlexDB—A lexical database for the psychological and linguistic research},
URL = {kliegl@uni-potsdam.de, ORCID: 0000-0002-0180-8488},
Volume = {62},
Year = {2011},
}


@article{2011-00095-01320100501,
Abstract = {In this article, we present a new lexical database for Modern Standard Arabic: Aralex. Based on a contemporary text corpus of 40 million words, Aralex provides information about (1) the token frequencies of roots and word patterns, (2) the type frequency, or family size, of roots and word patterns, and (3) the frequency of bigrams, trigrams in orthographic forms, roots, and word patterns. Aralex will be a useful tool for studying the cognitive processing of Arabic through the selection of stimuli on the basis of precise frequency counts. Researchers can use it as a source of information on natural language processing, and it may serve an educational purpose by providing basic vocabulary lists. Aralex is distributed under a GNU-like license, allowing people to interrogate it freely online or to download it from www.mrc-cbu.cam.ac.uk:8081/aralex.online/login.jsp. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Boudelaa, Sami and  Marslen-Wilson, William D.},
ISSN = {1554-351X, 1554-3528},
Journal = {Behavior Research Methods},
Keywords = {Aralex, lexical database, Modern Standard Arabic, cognitive processes, Arabs, Databases, Factual, Humans, Internet, Language, Psycholinguistics, Software, Vocabulary, Cognitive Processes, Databases, Language, Lexical Access, Words (Phonetic Units)},
Number = {2},
Pages = {481 - 487},
Title = {Aralex: A lexical database for Modern Standard Arabic},
URL = {sami.boudelaa@mrc-cbu.cam.ac.uk, ORCID: 0000-0003-0690-6308},
Volume = {42},
Year = {2010},
}


@article{EJ74767520070101,
Abstract = {Both classroom instruction and lexical database development stand to benefit from applied research on sign language, which takes into consideration American Sign Language rules, pedagogical issues, and teacher characteristics. In this study of technical science signs, teachers' experience with signing and, especially, knowledge of content, were found to be essential for the identification of signs appropriate for instruction. The results of this study also indicate a need for a systematic approach to examine both sign selection and its impact on learning by deaf students. Recommendations are made for the development of lexical databases and areas of research for optimizing the use of sign language in instruction.},
Author = {Lang, Harry G. and  Hupper, Mary LaPorta and  Monte, Denise A. and  Brown, Scott W. and  Babb, Ivar and  Scheifele, Pete M.},
ISSN = {1081-4159},
Journal = {Journal of Deaf Studies and Deaf Education},
Keywords = {Sign Language; Signs; Scientific Concepts; Lexicology; Databases; Science Instruction; Teaching Experience; Teacher Characteristics; Language Processing; Deafness; Students; Research},
Number = {1},
Pages = {65 - 79},
Title = {A Study of Technical Signs in Science: Implications for Lexical Database Development},
URL = {http://dx.doi.org/10.1093/deafed/enl018},
Volume = {12},
Year = {2007/01/01/},
}


@article{2008-10371-01520080801,
Abstract = {To conduct experimental investigations into the orthographic processing of Modern Greek, information is needed about the lexical properties known to influence visual word recognition. In this article we introduce GreekLex, a lexical database for Modern Greek, which presents collectively for the first time a series of orthographic measures that can be used for psycholinguistic research. GreekLex consists of 35,304 Modern Greek words ranging in length from 1 to 22 letters, and for each word includes the following statistical information: word length, word-form frequency, lemma frequency, neighborhood density and frequency, transposition neighbors, and addition and deletion neighbors. Furthermore, type and token frequency measures of single letters and bigrams derived from the database are also available. The complete database can be accessed and downloaded freely from www.psychology.nottingham.ac.uk/GreekLex. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Ktori, Maria and  van Heuven, Walter J. B. and  Pitchford, Nicola J.},
ISSN = {1554-351X, 1554-3528},
Journal = {Behavior Research Methods},
Keywords = {GreekLex, lexical database, modern Greek, experimental investigations, orthographic processing, visual word recognition, psycholinguistics, Databases, Factual, Greece, Humans, Language, Vocabulary, Cognitive Processes, Orthography, Psycholinguistics, Word Recognition, Lexical Access, Visual Perception},
Number = {3},
Pages = {773 - 783},
Title = {GreekLex: A lexical database of modern Greek},
URL = {lpxmk2@nottingham.ac.uk, ORCID: 0000-0003-3183-4449},
Volume = {40},
Year = {2008},
}


@article{2009-21653-00420091101,
Abstract = {The LEXIN database offers psycholinguistic indexes of the 13,184 different words (types) computed from 178,839 occurrences of these words (tokens) contained in a corpus of 134 beginning readers widely used in Spain. This database provides four statistical indicators: F (overall word frequency), D (index of dispersion across selected readers), U (estimated frequency per million words), and SFI (standard frequency index). It also gives information about the number of letters, syntactic category, and syllabic structure of the words included. To facilitate comparisons, LEXIN provides data from LEXESP’s (Sebastián-Gallés, Martí, Cuetos, & Carreiras, 2000), Alameda and Cuetos’s (1995), and Martínez and García’s (2004) Spanish adult psycholinguistic frequency databases. Access to the LEXIN database is facilitated by a computer program. The LEXIN program allows for the creation of word lists by letting the user specify searching criteria. LEXIN can be useful for researchers in cognitive psychology, particularly in the areas of psycholinguistics and education. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Corral, Silvia and  Ferrero, Marta and  Goikoetxea, Edurne},
ISSN = {0743-3808},
Journal = {Behavior Research Methods, Instruments & Computers},
Keywords = {lexical database, Spanish readers, kindergarten readers, first-grade readers, syntactic category, psycholinguistic indexes, LEXIN, Computer Software, Databases, Psycholinguistics, Reading Development, Vocabulary, Elementary School Students, Kindergarten Students, Phonology, Syntax, Words (Phonetic Units)},
Number = {4},
Pages = {1009 - 1017},
Title = {LEXIN: A lexical database from Spanish kindergarten and first-grade readers},
URL = {edurne.goikoetxea@deusto.es},
Volume = {41},
Year = {2009},
}


@article{2004-22444-01620040801,
Abstract = {In this article, we present a new lexical database for French: Lexique. In addition to classical word information such as gender, number, and grammatical category, Lexique includes a series of interesting new characteristics. First, word frequencies are based on two cues: a contemporary corpus of texts and the number of Web pages containing the word. Second, the database is split into a graphemic table with all the relevant frequencies, a table structured around lemmas (particularly interesting for the study of the inflectional family), and a table about surface frequency cues. Third, Lexique is distributed under a GNU-like license, allowing people to contribute to it. Finally, a metasearch engine, Open Lexique, has been developed so that new databases can be added very easily to the existing ones. Lexique can either be downloaded or interrogated freely from http://www.lexique.org. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {New, Boris and  Pallier, Christophe and  Brysbaert, Marc and  Ferrand, Ludovic},
ISSN = {0743-3808},
Journal = {Behavior Research Methods, Instruments & Computers},
Keywords = {Lexique, French lexical database, word frequency, Automatic Data Processing, Databases as Topic, Humans, Language, Psycholinguistics, Vocabulary, Databases, Word Frequency, Words (Phonetic Units)},
Number = {3},
Pages = {516 - 524},
Title = {Lexique 2: A new French lexical database},
URL = {boris.new@univ-paris5.fr},
Volume = {36},
Year = {2004},
}


@article{2003-04099-00520021001,
Abstract = {Several studies on auditory word recognition indicate that word processing is influenced by phonological similarity with other words. We describe a lexical database, VoColex, which provides several statistical indexes of phonological similarity between French words. Phonological similarity is computed according to two distinct principles. According to the first principle, phonologically similar words share initial phonemes with the target word. According to the second principle, phonological neighbours correspond to any words which can be derived from the target by a single phoneme change (substitution, addition, or deletion) whatever the position of the modified phoneme. The statistical data provided by VoCoLex allow the control and the empirical manipulation of various measures of phonological similarity, as well as quantitative descriptions of the auditory lexicon. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Dufour, Sophie and  Peereman, Ronald and  Pallier, Christophe and  Radeau, Monique},
ISSN = {0003-5033, 1955-2580},
Journal = {L'Année Psychologique},
Keywords = {phonological similarity, lexical database, French words, auditory word recognition, word processing, phoneme change, auditory lexicon, Phonemes, Word Processing, Word Recognition, Words (Phonetic Units)},
Number = {4},
Pages = {725 - 746},
Title = {VoCoLex: Une base de données lexicales sur les similaritiés phonologiques entre les mots français = VoCoLex: A lexical database on phonological similarity between French words},
URL = {Sophie@leadserv.u-bourgogne.fr},
Volume = {102},
Year = {2002},
}


@misc{3516684220080801,
Abstract = {As part of a project to construct an interactive program which would encourage children to play with language by building jokes, we developed a lexical database, starting from WordNet. To the existing information about part of speech, synonymy, hyponymy, etc., we have added phonetic representations and phonetic similarity ratings for pairs of words/phrases. [ABSTRACT FROM AUTHOR], Copyright of Language Resources & Evaluation is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
Author = {Manurung, Ruli and  Ritchie, Graeme and  Pain, Helen and  Waller, Annalu and  Black, Rolf and  O'Mara, Dave},
Title = {Adding phonetic similarity data to a lexical database.},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=35166842&site=ehost-live&scope=site},
Year = {2008},
}


@article{1261982519951101,
Abstract = {The article focuses on WordNet, an online lexical database designed for use under program control. WordNet database links English nouns, verbs, adjectives and adverbs to sets of synonyms that are in turn linked through semantic relations that determine word definitions. On WordNet, a form is represented by a string of ASCII characters and a sense is represented by the set of one or more synonyms that have that sense. It contains more than 118,000 different word forms and more than 90,000 different word senses. Approximately 17% of the words in WordNet are polysemous and approximately 40% have one or more synonyms. It lists the alternatives from which choices must be made. WordNet would be much more useful if incorporated the means for determining appropriate senses, allowing the program to evaluate the contexts in which words are used. Choosing between alternative senses of a polysemous word is a matter of distinguishing between different sets of linguistic contexts in which the word form can be used to express the world sense.},
Author = {Miller, George A.},
ISSN = {00010782},
Journal = {Communications of the ACM},
Keywords = {ONLINE databases, COMPUTER software, ASCII (Character set), WORD formation (Grammar), SYNONYMS, ENGLISH language},
Number = {11},
Pages = {39 - 41},
Title = {Word Net: A Lexical Database for English.},
Volume = {38},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=12619825&site=ehost-live&scope=site},
Year = {1995},
}


@article{1999-05841-02419990501,
Abstract = {During the last 20 years, psycholinguistic research has identified many variables that influence reading and spelling processes. In this article, the authors describe a new computerized lexical database, LEXOP, which provides quantitative descriptors about the relations between orthography and phonology for French monosyllabic words. Three main classes of variables are considered: consistency of print-to-sound and sound-to-print associations, frequency of orthography–phonology correspondences, and word neighborhood characteristics. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Peereman, Ronald and  Content, Alain},
ISSN = {0743-3808},
Journal = {Behavior Research Methods, Instruments & Computers},
Keywords = {computerized lexical database which provides quantitative descriptors about the relations between orthography & phonology for French monosyllabic words, Databases as Topic, Dictionaries as Topic, France, Language, Psycholinguistics, Statistics as Topic, Computer Applications, Databases, Human Computer Interaction, Orthography, Phonology, Computer Software, Words (Phonetic Units)},
Number = {2},
Pages = {376 - 379},
Title = {LEXOP: A lexical database providing orthography–phonology statistics for French monosyllabic words},
Volume = {31},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=psyh&AN=1999-05841-024&site=ehost-live&scope=site},
Year = {1999},
}


@article{2004-15015-01720040201,
Abstract = {This article presents MANULEX, a Web-accessible database that provides grade-level word frequency lists of nonlemmatized and lemmatized words (48,886 and 23,812 entries, respectively) computed from the 1.9 million words taken from 54 French elementary school readers. Word frequencies are provided for four levels: first grade (G1), second grade (G2), third to fifth grades (G3-5), and all grades (Gl-5). The frequencies were computed following the methods described by Carroll, Davies, and Richman (1971) and Zeno, Ivenz, Millard, and Duvvuri (1995), with four statistics at each level (F, overall word frequency; D, index of dispersion across the selected readers; U, estimated frequency per million words; and SFI, standard frequency index). The database also provides the number of letters in the word and syntactic category information. MANULEX is intended to be a useful tool for studying language development through the selection of stimuli based on precise frequency norms. Researchers in artificial intelligence can also use it as a source of information on natural language processing to simulate written language acquisition in children. Finally, it may serve an educational purpose by providing basic vocabulary lists. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Lété, Bernard and  Sprenger-Charolles, Liliane and  Colé, Pascale},
ISSN = {0743-3808},
Journal = {Behavior Research Methods, Instruments & Computers},
Keywords = {grade-level lexical database, elementary school readers, Web-accessible database, MANULEX, word frequency, nonlemmatized words, lemmatized words, Adolescent, Child, Databases, Factual, Humans, Information Dissemination, Information Storage and Retrieval, Internet, Language, Language Development, Psycholinguistics, Reading, Reference Values, Vocabulary, Databases, Elementary School Students, Lexical Access, Reading Ability, Word Frequency, Grade Level},
Number = {1},
Pages = {156 - 166},
Title = {MANULEX: A grade-level lexical database from French elementary school readers},
URL = {lete@inrp.fr},
Volume = {36},
Year = {2004},
}


@article{2000-14325-01120001101,
Abstract = {This article presents a computerized database of words for use in experimental research in cognitive psychology and psycholinguistics. The data are based on the oral vocabulary of 200 Spanish-speaking children aged from 11.16–49.16 mo. The database includes 15,428 Spanish words (tokens) and comprises 1,259 different words (types). It provides information about age of acquisition, orthography, grammar, semantics, and frequency. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Piñeiro, Armando and  Manzano, Mayra},
ISSN = {0743-3808},
Journal = {Behavior Research Methods, Instruments & Computers},
Keywords = {lexical database for Spanish-speaking children aged 1–4 yrs, Child, Child, Preschool, Cognitive Science, Cuba, Databases as Topic, Female, Humans, Infant, Language Development, Male, Psycholinguistics, Semantics, Databases, Verbal Stimuli, Words (Phonetic Units), Lexical Access},
Number = {4},
Pages = {616 - 628},
Title = {A lexical database for Spanish-speaking children},
Volume = {32},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=psyh&AN=2000-14325-011&site=ehost-live&scope=site},
Year = {2000},
}


@article{402780820001101,
Abstract = {Presents the lexical database for Spanish-speaking children.  Application of the computerized database to experimental research in cognitive psychology and psycholinguistics; Description and composition of the database; Analysis of the verbal utterances.},
Author = {Pineiro, Armando and  Manzano, Mayra},
ISSN = {07433808},
Journal = {Behavior Research Methods, Instruments, & Computers},
Keywords = {COMPUTERS in lexicology, COGNITIVE psychology, PSYCHOLINGUISTICS},
Number = {4},
Pages = {616},
Title = {A lexical database for Spanish-speaking children.},
Volume = {32},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=4027808&site=ehost-live&scope=site},
Year = {2000},
}


@misc{EJ46911319930101,
Abstract = {Important characteristics of lexical databases and their applications in information retrieval and natural language processing are explained. An ongoing project using various machine-readable sources to build a lexical database is described, and detailed designs of individual entries with examples are included. (Contains 66 references.) (EAM)},
Author = {Conlon, Sumali Pin-Ngern and  And Others},
Publisher = {Information Processing and Management},
Title = {Developing a Large Lexical Database for Information Retrieval, Parsing, and Text Generation Systems.},
Year = {1993/01/01/},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=eric&AN=EJ469113&site=ehost-live&scope=site},
}


@article{12768342920171201,
Abstract = {This study concerns the impact of the collocation/phraseme disambiguation component within the complex system of the rule-based morphological disambiguation of Czech. This system constitutes one of the two main disambiguation subsystems that are responsible for the morphological disambiguation of the corpora of synchronic Czech within the Czech National Corpus project. We will show that although the part of texts constituted by collocations/phrasemes (generally multiword expressions - MWEs) is relatively small and consequently the errorfree morphological disambiguation of MWEs covers only a small portion of textual material, such perfectly disambiguated fragments in sentences help to improve the disambiguation of the rest, non-MWE part of sentences. [ABSTRACT FROM AUTHOR], Copyright of Jazykovedný Casopis is the property of Sciendo and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, )},
Author = {HNÁTKOVÁ, MILENA and  PETKEVIČ, VLADIMÍR},
ISSN = {00215597},
Journal = {Jazykovedný Casopis},
Keywords = {MORPHOLOGY (Grammar), SENTENCES (Grammar), CZECH language, CORPORA (Linguistics), TERMS & phrases, Czech National Corpus, lexical database, morphological ambiguity, morphological analysis, morphological disambiguation, multiword expressions, process of disambiguation},
Number = {2},
Pages = {145 - 155},
Title = {MORPHOLOGICAL DISAMBIGUATION OF MULTIWORD EXPRESSIONS AND ITS IMPACT ON THE DISAMBIGUATION OF THEIR ENVIRONMENT IN A SENTENCE.},
Volume = {685},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=127683429&site=ehost-live&scope=site},
Year = {2017},
}


@article{12528142920170901,
Abstract = {In this paper we describe the data processing procedures and the preliminary results of the project Ob-Ugric database (OUDB), a web-based framework which aims at developing corpus-based descriptive resources of Khanty and Mansi dialects. Using established language documentation and annotation tools, OUDB provides interlinked corpus and lexicon data from digitized texts as well as recent fieldwork studies in an uniform IPA-transcription together with the corresponding audio recordings thus making these less described languages of the Ob-Ugric branch of the Finno-Ugric language family accessible for researchers as well as the language community and archiving the raw data for documentation, linguistic evaluation and possible future use in building resources for language technology applications. [ABSTRACT FROM AUTHOR], Copyright of Acta Linguistica Academica is the property of Akademiai Kiado and its content may not be copied or emailed to multiple sites or posted to a listserv without th)},
Author = {Wisiorek, Axel and  Schön, Zsófia},
ISSN = {25598201},
Journal = {Acta Linguistica Academica},
Keywords = {LEXICON, KHANTY language, MANSI language, DATABASES, PHONETIC transcriptions, CORPORA (Linguistics), annotated corpora, corpus-based lexical database, Khanty, language documentation, Mansi},
Number = {3},
Pages = {383 - 396},
Title = {Ob-Ugric database: Corpus and lexicon databases of Khanty and Mansi dialects.},
Volume = {64},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=125281429&site=ehost-live&scope=site},
Year = {2017},
}


@article{13644858920190501,
Abstract = {Topic Modeling encompasses a set of techniques for text clustering and tag recommendation with significant advantages such as unsupervised learning. Based on Latent Dirichlet Allocation (LDA) topic modeling, every single word is related to a set of topics with different weight. The weights are further estimated in order to determine the semantic relation between the words and the rest of the documents. Apparently the chief drawback of topic modeling techniques, specifically LDA, lies on their incapability in clustering short texts in which semantic relation between words is neglected. This issue is deemed more severe when analyzing social networks such as Twitter wherein short texts are the case. It is assumed that semantic relation between a document and the target short text helps obtain efficient clustering of short texts via topic modeling. Hence, the current paper proposes a method of topic modeling named Semantic Knowledge LDA based on semantic relations between the words in twe)},
Author = {Tajbakhsh, Mir Saman and  Bagherzadeh, Jamshid},
ISSN = {1088467X},
Journal = {Intelligent Data Analysis},
Keywords = {TAGS (Metadata), SEMANTICS, MICROBLOGS, DATA analysis, hashtag recommendation, LDA, semantic LDA, short text, Topic modeling, Twitter, TWITTER (Web resource)},
Number = {3},
Pages = {609 - 622},
Title = {Semantic knowledge LDA with topic vector for recommending hashtags: Twitter use case.},
Volume = {23},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=136448589&site=ehost-live&scope=site},
Year = {2019},
}


@article{12413061320170701,
Abstract = {Punctuated equilibrium theory (PET) suggests that the policy process is characterized by long periods of incremental change and short periods of punctuated change. The impetus for the latter is usually a focusing event that breaks open policy monopolies, allowing for major changes in legislative decision making. While a burgeoning body of literature, a shortcoming in the PET literature is that it has yet to explain why focusing events and subsequent breakdowns in policy monopolies sometimes fail to result in punctuated policy. We integrate theories on cultural change with punctuated equilibrium to explain why focusing events do not always result in the dramatic policy changes that we might expect. Specifically, we use the context of national energy policy and the lexical database, Google Ngram Viewer, to trace punctuating energy-related events and the occurrence or lack thereof subsequent policy change from 1952 to 2000. [ABSTRACT FROM AUTHOR], Copyright of Review of Policy Research i)},
Author = {Fowler, Luke and  Neaves, Tonya T. and  Terman, Jessica N. and  Cosby, Arthur G.},
ISSN = {1541132X},
Journal = {Review of Policy Research},
Keywords = {ENERGY policy, PUNCTUATED equilibrium (Social science), DECISION making in political science, SOCIAL change, POLICY sciences, UNITED States, energy policy, Ngram, punctuated equilibrium theory},
Number = {4},
Pages = {559 - 577},
Title = {Cultural Penetration and Punctuated Policy Change: Explaining the Evolution of U.S. Energy Policy.},
Volume = {34},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=124130613&site=ehost-live&scope=site},
Year = {2017},
}


@article{2017-36245-00120171201,
Abstract = {Plagiarism takes place when we use any person’s work without giving due acknowledgment. There are several fields where the text similarity is involved like web document retrieval, information mining, and searching related articles. Several approaches have been introduced for detecting plagiarism in the text documents based on the syntactic structure of the text, string similarity, fingerprinting, semantic meaning underlying the text, etc. The basic limitation of plagiarism detection systems these days is that they fail to detect tough cases of plagiarism. The proposed plagiarism detection approach is the hybrid of semantic and syntactic similarity between the text documents. This novel approach exploits linguistic information sources non-linearly using the lexical database for finding the relatedness between text documents. The proposed approach uses semantic knowledge to perform cognitive-inspired computing. The framework is capable of detecting intelligent plagiarism cases like a verbatim copy, paraphrasing, rewording in a sentence, and sentence transformation. The approach has been evaluated on the standard PAN-PC-11 dataset. The experiments show that our technique has outperformed other strong baseline techniques in terms of precision, recall, F-measure, and plagiarism detection (PlagDet) score. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
Author = {Sahi, Mansi and  Gupta, Vishal},
ISSN = {1866-9956, 1866-9964},
Journal = {Cognitive Computation},
Keywords = {Information sources, Semantic relatedness, Lexical database, Databases, Semantics, Websites},
Number = {6},
Pages = {852 - 867},
Title = {A novel technique for detecting plagiarism in documents exploiting information sources},
URL = {vishal_gupta100@yahoo.co.in},
Volume = {9},
Year = {2017},
}


@article{12686726420170601,
Abstract = {The African wordnet (AWN) provides South African indigenous languages with a platform to access a machine-readable lexical database organised by meaning. The creation of the African wordnet was based on the Princeton wordnet. As in the case of the Princeton wordnet, the African wordnet groups African language words into sets of synonyms along with short definitions and usage examples, as well as records relations between synonyms. This article examines a number of synsets in order to identify the word-formation processes used by various linguists in constructing the AWN. Since the English Princeton wordnet was used as the basis for the lexical database in the creation of the African wordnet, various word-formation strategies had to be used to account for lexical items that are not lexicalised in the African languages. Access to the created synsets was gained via a web browser, which is an automated text analysis application. [ABSTRACT FROM AUTHOR], Copyright of Southern African Lingui)},
Author = {Madonsela, Stanley},
ISSN = {16073614},
Journal = {Southern African Linguistics & Applied Language Studies},
Keywords = {WORD formation (Grammar), LEXICAL access, BROWSERS (Computer programs), AUTOMATION, AFRICAN languages},
Number = {2},
Pages = {201 - 210},
Title = {Word-formation strategies and processes in the creation of synsets for the African wordnet.},
Volume = {35},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=126867264&site=ehost-live&scope=site},
Year = {2017},
}


@article{11801408620160906,
Abstract = {Background: Curious parallels between the processes of species and language evolution have been observed by many researchers. Retracing the evolution of Indo-European (IE) languages remains one of the most intriguing intellectual challenges in historical linguistics. Most of the IE language studies use the traditional phylogenetic tree model to represent the evolution of natural languages, thus not taking into account reticulate evolutionary events, such as language hybridization and word borrowing which can be associated with species hybridization and horizontal gene transfer, respectively. More recently, implicit evolutionary networks, such as split graphs and minimal lateral networks, have been used to account for reticulate evolution in linguistics. Results: Striking parallels existing between the evolution of species and natural languages allowed us to apply three computational biology methods for reconstruction of phylogenetic networks to model the evolution of IE languages. We )},
Author = {Willems, Matthieu and  Laforest, Louise and  Makarenkov, Vladimir and  Lord, Etienne and  Lapointe, François-Joseph and  Labelle, Gilbert and  Di Sciullo, Anna Maria},
ISSN = {14712148},
Journal = {BMC Evolutionary Biology},
Keywords = {HYBRIDIZATION, LANGUAGE & history, INDO-European languages -- History, PHYLOGENY, LANGUAGE & languages, CHARTS, diagrams, etc., Historical linguistics, Phylogenetic networks, Phylogenetic trees, Reticulate evolution},
Pages = {1 - 18},
Title = {Using hybridization networks to retrace the evolution of Indo-European languages.},
Volume = {16},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=118014086&site=ehost-live&scope=site},
Year = {2016},
}


@article{8977532320120701,
Abstract = {This paper is an invitation to discuss open questions in the domain of morphology, such as the adequacy of the principle of atomicity for words, the place of morphology in the architecture of grammar, and the necessity to mingle historical and synchronic approaches to increase the knowledge of words. (English) [ABSTRACT FROM AUTHOR], Partindo de referências a trabalhos anteriores e de algumas novas reflexões, faz-se um convite à discussão de questões em aberto no domínio da investigação em morfologia, como as que dizem respeito à adequação do princípio da atomicidade das palavras, ao lugar da morfologia na gramática e à necessidade de cruzamento das abordagens histórica e sincrônica no conhecimento das palavras. (Portuguese) [ABSTRACT FROM AUTHOR], Copyright of Revista de Estudos da Linguagem is the property of Revista de Estudos da Linguagem and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission)},
Author = {Villalva, Alina},
ISSN = {01040588},
Journal = {Revista de Estudos da Linguagem},
Keywords = {MORPHOLOGY (Grammar), GRAMMAR, LINGUISTIC usage, COMPARATIVE grammar, VOCABULARY, LISBON (Portugal), Historical morphology, Lexical database, Morphology, Word definition, Bases de dados lexicais, Definição de palavra, Morfologia, Morfologia histórica},
Number = {2},
Pages = {125 - 139},
Title = {Palavras, que as há.},
Volume = {20},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=89775323&site=ehost-live&scope=site},
Year = {2012},
}


@article{12825858320150101,
Abstract = {The aim of this paper is to identify the types of recategorization that arise in the recursive formation of Old English nouns and adjectives by means of prefixation and suffixation. The first step of this analysis is to isolate the recursive adjectival and nominal formations, for which the lexical database of Old English Nerthus (www.nerthusproject.com) is used. Out of a total of nearly 7,500 affixed nouns and adjectives, there are 388 recursive formations. The main conclusion of this article is that recursivity in the formation of adjectives and nouns crucially depends on the noun as source category and the adjective as path category. As a general rule, the derivation proceeds as follows: noun > adjective > noun / adjective. [ABSTRACT FROM AUTHOR], Copyright of RaeL: Revista Electronica de Linguistica Aplicada is the property of Asociacion Espanola de Linguistica Aplicada and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright hold)},
Author = {VEA, RAQUEL},
ISSN = {18859089},
Journal = {RaeL: Revista Electronica de Linguistica Aplicada},
Keywords = {ENGLISH language, VOCABULARY, NOUNS (Grammar), SUFFIXES & prefixes (Grammar), AFFIXES (Grammar), affixation, Old English, recategorization, recursivity},
Number = {14},
Pages = {67 - 81},
Title = {RECATEGORIZATION IN THE RECURSIVE FORMATION OF OLD ENGLISH NOUNS AND ADJECTIVES.},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=128258583&site=ehost-live&scope=site},
Year = {2015},
}


@article{13107618820180401,
Abstract = {A great number of modern behavioral studies postulate the idea that the emotional content of verbal stimuli affects the speed of visual word recognition in different cognitive tasks, but emotional factors (e.g., emotional valence) are still not verified in current models of word recognition. Several studies were conducted on the material of English, but there is lack of experimental studies on the material of other languages. The present study aimed to reveal a word’s emotional valence effect in automatic lexical processing using Russian lexical database ENRuN generated by D.V. Lyusin and T.A. Sysoeva. Two lexical decision tasks were conducted in 92 Russian speakers (N=44 in yes/no LDT; N=48 in go/no-go LDT). The experimental blocks were equal in two studies and comprised 120 Russian emotional nouns retrieved from the ENRuN database that were either positive (N=40), negative (N=40) or neutral (N=40), and 120 non-words. Using mixed-effects models a significant effect of a word’s emotio)},
Author = {Власов, М. С. and  Сычев, О. А.},
ISSN = {19986645},
Journal = {Tomsk State University Journal of Philology},
Keywords = {emotional valence, negative affect, positive affect, visual word recognition, визуальное распознавание слова, негативный аффект, позитивный аффект, эмоциональная окраска слов},
Pages = {18 - 52},
Title = {ВЗАИМОДЕЙСТВИЕ ЭМОЦИОНАЛЬНЫХ И ЛИНГВИСТИЧЕСКИХ ФАКТОРОВ В ПРОЦЕССЕ ПЕРЕРАБОТКИ ЛЕКСИЧЕСКОЙ ИНФОРМАЦИИ (НА МАТЕРИАЛЕ ИМЕН СУЩЕСТВИТЕЛЬНЫХ РУССКОГО ЯЗЫКА)},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=131076188&site=ehost-live&scope=site},
Year = {2018},
}


@article{12771519620180201,
Abstract = {This paper reports on a study characterizing design processes and the potential of design spaces through measuring the information entropy of empirical data derived from protocol studies. The sequential segments in a protocol analysis can be related to each other by examining their semantic content producing a design session's linkograph, which defines the design space for a design session. From a linkograph, it is possible to compute the probabilities of the connectivity of each segment for its forelinks and its backlinks, together with the probabilities of distance among links. A linkograph's entropy is a measure of the information in the design session. It is claimed that the entropy of the linkograph measures the potential of the design space being generated as the design proceeds chronologically. We present an approach to the automated construction of linkographs by connecting segments using the lexical database WordNet and measure its entropy. A case study of two design sessions)},
Author = {Kan, Jeff W.T. and  Gero, John S.},
ISSN = {08900604},
Journal = {AI EDAM},
Keywords = {TECHNOLOGICAL innovations, ENTROPY (Information theory), COMPUTER network protocols, SEMANTIC Web, DATABASES, Design Processes, Entropy, Information Analysis, Protocol Studies},
Number = {1},
Pages = {32 - 43},
Title = {Characterizing innovative processes in design spaces through measuring the information entropy of empirical data from protocol studies.},
Volume = {32},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=127715196&site=ehost-live&scope=site},
Year = {2018},
}


@article{EJ114680220170701,
Abstract = {Cognitive mechanisms for sign language lexical access are fairly unknown. This study investigated whether phonological similarity facilitates lexical retrieval in sign languages using measures from a new lexical database for American Sign Language. Additionally, it aimed to determine which similarity metric best fits the present data in order to inform theories of how phonological similarity is constructed within the lexicon and to aid in the operationalization of phonological similarity in sign language. Sign repetition latencies and accuracy were obtained when native signers were asked to reproduce a sign displayed on a computer screen. Results indicated that, as predicted, phonological similarity facilitated repetition latencies and accuracy as long as there were no strict constraints on the type of sublexical features that overlapped. The data converged to suggest that one similarity measure, MaxD, defined as the overlap of any 4 sublexical features, likely best represents mechanisms of phonological similarity in the mental lexicon. Together, these data suggest that lexical access in sign language is facilitated by phonologically similar lexical representations in memory and the optimal operationalization is defined as liberal constraints on overlap of 4 out of 5 sublexical features--similar to the majority of extant definitions in the literature.},
Author = {Williams, Joshua T. and  Stone, Adam and  Newman, Sharlene D.},
ISSN = {1081-4159},
Journal = {Journal of Deaf Studies and Deaf Education},
Keywords = {American Sign Language; Phonology; Lexicology; Repetition; Accuracy; Prediction; Data Analysis; Memory; Language Processing},
Number = {3},
Pages = {303 - 315},
Title = {Operationalization of Sign Language Phonological Similarity and Its Effects on Lexical Access},
URL = {http://dx.doi.org/10.1093/deafed/enx014},
Volume = {22},
Year = {2017/07/01/},
}


@article{5317072720090701,
Abstract = {In Natural Language Processing (NLP) domain, a lexical database, i.e. a systematic stock of words of a natural language, is a crucial component of a wide variety of NLP applications. In this paper, we present the interlingua used in the construction of the bilingual lexical database called REBECA, one of the few lexical resources that encompasses Brazilian Portuguese (BP) language. The interlingua is the core feature of REBECA and it is composed of a set of concepts which allows the alignment of the English and BP monolingual databases. Precisely, we present: (i) the composition; (ii) the structure, (iv) the formalism, and (v) the editing tool used to develop REBECA. . As a result, we obtained a formal and hierarchical interlingua, which allowed a proper link between the monolingual databases. (English) [ABSTRACT FROM AUTHOR], No Processamento Automático das Línguas Naturais (PLN), as bases de dados lexicais desempenham papel central em diversos sistemas que processam língua natural. )},
Author = {Di Felippo, Ariani and  Dias-da-Silva, Bento Carlos},
ISSN = {14152533},
Journal = {Revista Veredas},
Keywords = {NATURAL language processing, ELECTRONIC data processing, COMPUTER science, DATABASES, ARTIFICIAL intelligence, INFORMATION storage & retrieval systems, PROGRAMMING languages, Concept, Interlingua, Lexical database, Lexical-conceptual alignment, Natural language processing, Alinhamento léxico-conceitual, Base de dados lexicais, Conceito, Interlíngua, Processamento automático das línguas naturais},
Number = {2},
Pages = {50 - 67},
Title = {A interlíngua da base lexical bilíngue REBECA.},
Volume = {13},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=53170727&site=ehost-live&scope=site},
Year = {2009},
}


@article{11206161820151101,
Abstract = {Semantic similarity between words can be applied in many fields including computational linguistics, artificial intelligence, and information retrieval. In this paper, we present weighted method for measuring a semantic similarity between words in a document. This method uses edge distance and depth of WordNet. The method calculates a semantic similarity between words on the basis of document information. Document information uses word term frequencies(TF) and word concept frequencies(CF). Each word weight value is calculated by TF and CF in the document. The method includes the edge distance between words, the depth of subsumer, and the word weight in the document. We compared out scheme with the other method by experiments. As the result, the proposed method outperforms other similarity measures. In the document, the word weight value is calculated by the proposed method. Other methods which based simple shortest distance or depth had difficult to represent the information or merge )},
Author = {강석훈 and  박종민},
ISSN = {19754701},
Journal = {Journal of the Korea Academia-Industrial Cooperation Society},
Keywords = {Corpus statistics, Information content, Lexical database, Semantic similarity, WordNet, Corpus statistics, Information content, Lexical database, Semantic similarity, WordNet},
Number = {11},
Pages = {7718 - 7728},
Title = {워드넷을 이용한 문서내에서 단어 사이의 의미적 유사도 측정.},
Volume = {16},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=112061618&site=ehost-live&scope=site},
Year = {2015},
}


@article{EJ113900720170401,
Abstract = {This article analyses how a set of psycholinguistic factors may account for children's lexical development. Age of acquisition is compared to a measure of lexical development based on vocabulary size rather than age, and robust regression models are used to assess the individual and joint effects of word class, frequency, imageability and phonological neighbourhood density on Norwegian children's early lexical development. The Norwegian Communicative Development Inventories (CDI) norms were used to calculate each CDI word's age of acquisition and vocabulary size of acquisition. Lexical properties were downloaded from the lexical database Norwegian Words, supplemented with data on frequency in adult and child-directed speech. Age of acquisition correlated highly with vocabulary size of acquisition, but the new measure was more evenly distributed and more sensitive to lexical effects. Frequency in child-directed speech was the most important predictor of lexical development, followed by imageability, which seems to account for the dominance of nominals over predicates in Norwegian.},
Author = {Hansen, Pernille},
ISSN = {0142-7237},
Journal = {First Language},
Keywords = {Psycholinguistics; Native Language; Norwegian; Language Acquisition; Vocabulary Development; Age Differences; Correlation; Word Frequency; Phonology; Regression (Statistics); Adults; Children; Databases; Form Classes (Languages); Measures (Individuals); Foreign Countries; Statistical Analysis, Norway},
Number = {2},
Pages = {205 - 225},
Title = {What Makes a Word Easy to Acquire? The Effects of Word Class, Frequency, Imageability and Phonological Neighbourhood Density on Lexical Development},
URL = {http://dx.doi.org/10.1177/0142723716679956},
Volume = {37},
Year = {2017/04/01/},
}


@article{8511118820120101,
Abstract = {The aim of this journal article is to measure the productivity of two Old English verbal suffixes that display a low index of productivity, namely -ettan and -læcan. The main sources of this research are the lexical database of Old English Nerthus and the Dictionary of Old English Corpus. Methodologically, two indexes of productivity are measured, productivity in the narrow sense P and global productivity P* (Baayen 1989, 1992, 1993), as well as frequency, which is calculated in terms of type and token (Bauer 2005). Two conclusions are reached. Firstly, the indexes of productivity P and global productivity P* of-ettan are higher than the ones of -læcan. After a comparison with other Old English affixes, it turns out that -ettan is relatively productive while the suffix --læcan is practically unproductive. Secondly, the indexes of productivity and the frequencies of type and token of these affixes coincide and are in accordance with Bauer's (2004) generalization that a low type frequen)},
Author = {VILLALTA, GEMA MAÍZ},
ISSN = {02132028},
Journal = {Revista Española de Lingüística Aplicada (RESLA (Revista Espanola de Linguistica Aplicada))},
Keywords = {ENGLISH language, RESEARCH, OLD English language, SUFFIXES & prefixes (Grammar), PRODUCTIVITY (Linguistics), FREQUENCY (Linguistics), AFFIXES (Grammar), TYPE & token (Linguistics), corpus, frequency, lexical database of Old English Nerthus, Old English, productivity, word-formation, base de datos léxica Nerthus, Corpus, formación de palabras, frecuencia, Inglés antiguo, productividad},
Pages = {117 - 131},
Title = {LOW PRODUCTIVITY INDEXES: THE OLD ENGLISH VERBAL SUFFIXES -ETTAN AND -LÆCAN.},
Volume = {25},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=85111188&site=ehost-live&scope=site},
Year = {2012},
}


@article{13414567320180101,
Abstract = {Although rarely studied, the persistence of semantic identity in the WordNet lexical database is crucial for the interoperability of all the resources that use WordNet data. The present study investigates the stability of the two primary entities of the WordNet database (the word senses and the synonym sets), by following their respective identifiers (the sense keys and the synset offsets) across all the versions released between 1995 and 2012, while also considering drifts of identical definitions and semantic relations. Contrary to expectations, 94.4% of the WordNet 1.5 synsets still persisted in the latest 2012 version, compared to only 89.1% of the corresponding sense keys. Meanwhile, the splits and merges between synonym sets remained few and simple. These results are presented in tables that allow to estimate the lexicographic effort needed for updating WordNet-based resources to newer WordNet versions. We discuss the specific challenges faced by both the dominant synset-based m)},
Author = {KAFE, ERIC},
ISSN = {20807147},
Journal = {Cognitive Studies / Études Cognitives},
Keywords = {PERSISTENCE, key violations, mappings, semantic identifiers, sense keys, synsets, wordnets},
Number = {18},
Pages = {1 - 20},
Title = {PERSISTENT SEMANTIC IDENTITY IN WORDNET.},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=134145673&site=ehost-live&scope=site},
Year = {2018},
}


@masterthesis{2016-31156-07220160101,
Abstract = {Our understanding of the mental lexicon, the way meaning is extracted from word forms, is almost entirely built on data from spoken languages. While there is much work demonstrating that in many ways the linguistic structure and psychological mechanisms for processing signed language and spoken language processing are the same, less is known about the signed language mental lexicon. In this dissertation, I examine the structure of the American Sign Language mental lexicon, and the ways meaning can be extracted from the manual/visual signal. In the third chapter of this dissertation I ask whether a single cognitive architecture might explain diverse behavioral patterns in signed and spoken language. Chen and Mirman (2012) presented a computational model of word processing that unified opposite effects of neighborhood density in speech production, perception, and written word recognition. Carreiras et al. (2008) demonstrate that neighborhood density effects in Spanish Sign Language (LSE) also vary depending on whether the neighbors share the same handshape or location. We present a spreading activation architecture that borrows the principles proposed by Chen and Mirman (2012), and show that if this architecture is elaborated to incorporate relatively minor facts about either 1) the time course of sign perception or 2) the frequency of sub-lexical units in sign languages, it produces data that match the experimental findings from sign languages. This work serves as a proof of concept that a single cognitive architecture could underlie both sign and word recognition. In the second chapter I present ASL-LEX, a lexical database for ASL that catalogues more than forty properties about almost 1,000 signs. The database includes, for example, information about each sign's iconicity, phonological make-up, and neighborhood density. I use this information to better understand the structure of the ASL lexicon, the distribution of each of these properties, and the relationships between these properties. This lexical database is the largest and most comprehensive database of ASL, and can be used by researchers to develop experiments and by educators to identify and support vocabulary development. In the fourth chapter, I use ASL-LEX to develop a tightly-controlled study of sign perception. I ask whether neighborhood density and sub-lexical frequency play a role in sign perception, and if the mechanisms of sign perception are affected by early language experience. Eighty deaf participants with varying early language backgrounds completed a lexical decision task. I find that neighborhood density inhibits sign perception in people with low early ASL exposure, but has no effect in people with high early ASL exposure. Location frequency inhibits sign perception in all people, but the effect is stronger in people with low early ASL exposure. This suggests that impoverished access to ASL early in life has lasting consequences for sign perception. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Caselli, Naomi K.},
Keywords = {language, sign language, lexicon, word forms, Language, Sign Language, Word Recognition, Mental Lexicon},
School = {ProQuest Information & Learning},
Title = {Language deprivation and the American sign language lexicon},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=psyh&AN=2016-31156-072&site=ehost-live&scope=site},
Year = {2016},
}


@article{2395012920070201,
Abstract = {Abstract: This paper addresses the issue of how semantic information can be automatically assigned to compound terms, i.e. both a definition and a set of semantic relations. This is particularly crucial when elaborating multilingual databases and when developing cross-language information retrieval systems. The paper shows how morphosemantics can contribute in the constitution of multilingual lexical networks in biomedical corpora. It presents a system capable of labelling terms with morphologically related words, i.e. providing them with a definition, and grouping them according to synonymy, hyponymy and proximity relations. The approach requires the interaction of three techniques: (1) a language-specific morphosemantic parser, (2) a multilingual table defining basic relations between word roots and (3) a set of language-independent rules to draw up the list of related terms. This approach has been fully implemented for French, on an about 29,000 terms biomedical lexicon, resulting )},
Author = {Namer, Fiammetta and  Baud, Robert},
ISSN = {13865056},
Journal = {International Journal of Medical Informatics},
Keywords = {MEDICAL informatics, SEMANTICS, COMPUTERS in medicine, COMPUTER networks, SYNONYMS, Biomedical lexical database, Language, Morphosemantics for French, Multilingualism, Natural language processing, Neoclassical compounds, Semantic relations, Semantics},
Number = {2/3},
Pages = {226 - 233},
Title = {Defining and relating biomedical terms: Towards a cross-language morphosemantics-based system},
Volume = {76},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=23950129&site=ehost-live&scope=site},
Year = {2007},
}


@article{9639668520140401,
Abstract = {English, German, and Dutch show very similar word stress patterns, in that word stress is not fixed to a certain position within a word, but realized within the final three syllables. There is, however, no consensus on the actual stress-assigning algorithms and the role of quantity (e.g., Kiparsky ; Wiese ; Hayes ; Giegerich , ; Trommelen and Zonneveld , ). Existing studies are methodologically problematic since they largely depend on convenience samples of existing words and do not test their claims with new words. Using mixed effects regression and classification trees as analytical tools, this paper presents the results of a production experiment with pseudowords and an analysis of large random samples as found in the CELEX lexical database. It is shown that stress assignment is sensitive to syllabic weight in all three languages, though in slightly different ways. The implications of these results for the metrical structure of the three languages are discussed. [ABSTRACT FROM AUTH)},
Author = {Domahs, Ulrike and  Plag, Ingo and  Carroll, Rebecca},
ISSN = {13834924},
Journal = {Journal of Comparative Germanic Linguistics},
Keywords = {ACCENTS & accentuation, GERMAN language, DUTCH language, ENGLISH language, LEXICAL grammar, corpus analysis, Germanic word stress, metrical prosody, pseudoword production task, quantity-sensitivity},
Number = {1},
Pages = {59 - 96},
Title = {Word stress assignment in German, English and Dutch: Quantity-sensitivity and extrametricality revisited.},
Volume = {17},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=96396685&site=ehost-live&scope=site},
Year = {2014},
}


@article{9082173020131001,
Abstract = {Volunteered geographic information (VGI) is generated by heterogenous ‘information communities’ that co-operate to produce reusable units of geographic knowledge. A consensual lexicon is a key factor to enable this open production model. Lexical definitions help demarcate the boundaries of terms, forming a thin semantic ground on which knowledge can travel. In VGI, lexical definitions often appear to be inconsistent, circular, noisy and highly idiosyncratic. Computing the semantic similarity of these ‘volunteered lexical definitions’ has a wide range of applications in GIScience, including information retrieval, data mining and information integration. This article describes a knowledge-based approach to quantify the semantic similarity of lexical definitions. Grounded in the recursive intuition that similar terms are described using similar terms, the approach relies on paraphrase-detection techniques and the lexical database WordNet. The cognitive plausibility of the approach is eva)},
Author = {Ballatore, Andrea and  Wilson, David C. and  Bertolotto, Michela},
ISSN = {13658816},
Journal = {International Journal of Geographical Information Science},
Keywords = {GEOGRAPHIC information systems, GEOSPATIAL data, INFORMATION resources management, DATABASE searching, INFORMATION retrieval, DATA mining, crowdsourcing, geo-semantics, lexical definitions, OpenStreetMap, semantic similarity, volunteered geographic information, WordNet},
Number = {10},
Pages = {2099 - 2118},
Title = {Computing the semantic similarity of geographic terms using volunteered lexical definitions.},
Volume = {27},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=90821730&site=ehost-live&scope=site},
Year = {2013},
}


@article{1689880120010501,
Abstract = {Information access methods must be improved to overcome the information overload that most professionals face nowadays. Text classification tasks, like Text Categorization, help the users to access to the great amount of text they find in the Internet and their organizations. TC is the classification of documents into a predefined set of categories. Most approaches to automatic TC are based on the utilization of a training collection, which is a set of manually classified documents. Other linguistic resources that are emerging, like lexical databases, can also be used for classification tasks. This article describes an approach to TC based on the integration of a training collection (Reuters-21578) and a lexical database (WordNet 1.6) as knowledge sources. Lexical databases accumulate information on the lexical items of one or several languages. This information must be filtered in order to make an effective use of it in our model of TC. This filtering process is a Word Sense Disambig)},
Author = {Ureña-López, L. Alfonso and  Buenaga, Manuel and  Gómez, José M.},
ISSN = {00104817},
Journal = {Computers & the Humanities},
Keywords = {LINGUISTICS, LANGUAGE & languages, DATABASES, INFORMATION storage & retrieval systems, ELECTRONIC data processing, COMPUTERS, CW, IR, lexical database, Reuters, SEMCOR, TC, WordNet, WSD},
Number = {2},
Pages = {215 - 230},
Title = {Integrating Linguistic Resources in TC through WSD.},
Volume = {35},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=16898801&site=ehost-live&scope=site},
Year = {2001},
}


@masterthesis{ED58225920170101,
Abstract = {Kenyan Sign Language (KSL) is a thriving national sign language used by tens of thousands of signers in Kenya, and which emerged out of two deaf schools in western Kenya in the early 1960s. In this thesis, I provide a thorough description and analysis of the basic phonological components of the KSL lexicon used in the southwestern region of Kenya (formerly south Nyanza Province). This phonological grammar of (SoNy)KSL makes contributions in three domains. In the descriptive domain, it provides a thorough report of the basic units in the main phonological parameters; i.e., Handshape (Ch. 4), Location (Ch. 5), and Movement (Ch 6, 7), as well as the evidence for the distinctiveness of each unit. The description for Movement and Location are particularly noteworthy because those parameters have received less attention in sign linguistics in general compared to Handshape. In the methodological domain, the grammar is based on a KSL Lexical Database built for this project, in which over 50 phonetic characteristics of 1,880 non-compound signs were coded. This database is currently one of only a few such richly coded lexical databases of sign languages. In addition, this grammar employs a rigorous approach to determining lexical contrast, which has yielded a separate dataset of 461 minimal pairs (Ch. 3). This dataset is unique in sign linguistics and reveals patterns of lexical contrast that were not previously known--and which have generated new hypotheses about how lexical contrast may be constrained by degrees of visual similarity. Finally, this thesis makes a theoretical contribution by comparing how different models of sign phonology can account for sign types in KSL. By evaluating the explanatory power of the main theories of sign phonology on the basis of specific descriptive data, this thesis gives unique insights into the theoretical validity of these models. It also proposes modifications in some cases, especially with regard to how the Dependency Model (DPM) can account for the representation of movement features and their relationship to the timing tier. In addition, a new movement feature, [dispersed], is described and its implementation worked out in the DPM. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]},
Author = {Morgan, Hope E.},
Keywords = {Foreign Countries; Phonology; Sign Language; Dialects; Hearing Impairments; Deafness; Geographic Regions; Grammar; Motion; Nonverbal Communication; Phonetics; Databases; Coding; Linguistic Theory, Kenya},
School = {ProQuest LLC},
Title = {The Phonology of Kenyan Sign Language (Southwestern Dialect)},
URL = {http://gateway.proquest.com/openurl?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&res_dat=xri:pqm&rft_dat=xri:pqdiss:10635950},
Year = {2017/01/01/},
}


@article{12350481920170801,
Abstract = {We describe and discuss the design and prototype of the Distributed Model Integration Framework (DMIF) that links models deployed on different hardware and software platforms. We used distributed computing and service-oriented development approaches to address the different aspects of interoperability. Reusable web service wrappers were developed for technical interoperability models created in NetLogo and GAMS modeling languages. We investigated automated semantic mapping of text-based input-output data and attribute names of components using word overlap semantic matching algorithms and using an openly available lexical database. We also incorporated automated unit conversion in semantic mediation by using openly available ontologies. DMIF helps to avoid significant amount of reinvention by framework developers, and opens up the modeling process for many stakeholders who are not prepared to deal with the technical difficulties associated with installing, configuring, and running var)},
Author = {Belete, Getachew F. and  Voinov, Alexey and  Morales, Javier},
ISSN = {13648152},
Journal = {Environmental Modelling & Software},
Keywords = {DISTRIBUTED computing, ELECTRONIC data processing, ALGORITHMS, SEMANTICS, INFORMATION theory, Integrated modeling, Semantic mediation, Service oriented architecture, Web services, Wrapping},
Pages = {112 - 126},
Title = {Designing the Distributed Model Integration Framework – DMIF.},
Volume = {94},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=123504819&site=ehost-live&scope=site},
Year = {2017},
}


@article{8890844920130601,
Abstract = {This paper presents a new method of analysis by which structural similarities between brain data and linguistic data can be assessed at the semantic level. It shows how to measure the strength of these structural similarities and so determine the relatively better fit of the brain data with one semantic model over another. The first model is derived from WordNet, a lexical database of English compiled by language experts. The second is given by the corpus-based statistical technique of latent semantic analysis (LSA), which detects relations between words that are latent or hidden in text. The brain data are drawn from experiments in which statements about the geography of Europe were presented auditorily to participants who were asked to determine their truth or falsity while electroencephalographic (EEG) recordings were made. The theoretical framework for the analysis of the brain and semantic data derives from axiomatizations of theories such as the theory of differences in utility )},
Author = {Crangle, Colleen E. and  Perreau-Guimaraes, Marcos and  Suppes, Patrick},
ISSN = {19326203},
Journal = {PLoS ONE},
Keywords = {BRAIN physiology, NEUROLINGUISTICS, SEMANTICS, DATA analysis, LANGUAGE & languages, ELECTROENCEPHALOGRAPHY, COGNITIVE psychology, Cognitive psychology, Communications, Computer science, Computing methods, Human intelligence, Linguistics, Natural language processing, Neurolinguistics, Neuropsychology, Psycholinguistics, Psychology, Research Article, Semantics, Social and behavioral sciences},
Number = {6},
Pages = {1 - 16},
Title = {Structural Similarities between Brain and Linguistic Data Provide Evidence of Semantic Relations in the Brain.},
Volume = {8},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=88908449&site=ehost-live&scope=site},
Year = {2013},
}


@masterthesis{ED54606420120101,
Abstract = {Learning vocabulary and understanding texts present difficulty for language learners due to, among other things, the high degree of lexical ambiguity. By developing an intelligent tutoring system, this dissertation examines whether automatically providing enriched sense-specific information is effective for vocabulary learning and reading comprehension of second language learners. The system developed in this study contributes to an extended understanding of how NLP techniques can be applied more effectively in an educational environment. The system allows learners to upload texts and click on any content word in order to obtain sense-appropriate lexical information for unfamiliar or unknown words during reading. The system consists of three components: (1) the system manager controls the interaction among each learner, the NLP server, and the lexical database; (2) the NLP server converts a raw input text to a linguistically-analyzed text; (3) the lexical database is used to provide a sense-appropriate definition and example sentences of a word to the learner. To obtain the sense-appropriate information, the system first performs word sense disambiguation (WSD) on the input text. Pointing to appropriate examples tuned for language learners, however, is complicated by the fact that the database of examples is from one repository (COBUILD), while automatic WSD systems generally rely on senses from another (WordNet). The lexical database, then, is indexed by WordNet senses, each of which points to an appropriate corresponding COBUILD sense. The fact that every sense inventory has its own standards of sense distinction poses a serious problem in integrating these inventories into one. To redirect an input WordNet sense to a corresponding COBUILD sense, thus, a word sense alignment algorithm was developed, following a heuristic of favoring flatter alignment structures. With this system, an empirical study was conducted with 60 intermediate learners of English as a second language to examine whether this system can lead learners to improve their vocabulary acquisition and reading comprehension. The findings show that learners demonstrated higher performance when receiving sense-specific information. Furthermore, the qualitative examination of the effect of automatic system errors show that, although learners showed learning regardless of the appropriateness of lexical information, they still showed relatively greater learning when given appropriate lexical information. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]},
Author = {Eom, Soojeong},
Keywords = {Intelligent Tutoring Systems; Vocabulary Development; Reading Comprehension; Second Language Learning; Teaching Methods; Educational Technology; Access to Information; Interaction; Databases; English (Second Language); Program Effectiveness; Qualitative Research; Technology Uses in Education},
School = {ProQuest LLC},
Title = {Automatic Presentation of Sense-Specific Lexical Information in an Intelligent Learning System},
URL = {http://gateway.proquest.com/openurl?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&res_dat=xri:pqm&rft_dat=xri:pqdiss:3524085},
Year = {2012/01/01/},
}


@article{EJ109158720160301,
Abstract = {What determines consonant doubling in English? This question is pursued by using a large lexical database to establish systematic correlations between spelling, phonology and morphology. The main insights are: Consonant doubling is most regular at morpheme boundaries. It can be described in graphemic terms alone, i.e. without reference to phonology. In monomorphemic words, consonant doubling depends mostly on the word ending. Certain endings correlate with double consonants (e.g. (er) as in (summer), while others correlate with single consonants (e.g. <it> as in <visit>). What is more, it is the graphemic form of the word ending that determines the presence or absence of double consonants: The word endings <-ic> and <-ick>, for example, are homophonous, but the former almost always occurs with single consonants (e.g. <panic>), the latter with double consonants (e.g. <derrick>). That makes graphemic word endings peculiar entities: Like suffixes, they are recurring and they have distributional properties--but unlike suffixes, they have no morphosyntactic or semantic function.},
Author = {Berg, Kristian},
ISSN = {0922-4777},
Journal = {Reading and Writing: An Interdisciplinary Journal},
Keywords = {English; Phonemes; Correlation; Morphology (Languages); Syntax; Semantics; Morphemes; Phonology; Databases; Phoneme Grapheme Correspondence; Suprasegmentals; Intonation},
Number = {3},
Pages = {453 - 474},
Title = {Double Consonants in English: Graphemic, Morphological, Prosodic and Etymological Determinants},
URL = {http://dx.doi.org/10.1007/s11145-015-9610-z},
Volume = {29},
Year = {2016/03/01/},
}


@article{3409591320081001,
Abstract = {Considering the popularity of the Internet, an automatic interactive feedback system for E-learning websites is becoming increasingly desirable. However, computers still have problems understanding natural languages, especially the Chinese language, firstly because the Chinese language has no space to segment lexical entries (its segmentation method is more difficult than that of English) and secondly because of the lack of a complete grammar in the Chinese language, making parsing more difficult and complicated. Building an automated Chinese feedback system for special application domains could solve these problems. This paper proposes an interactive feedback mechanism in a virtual campus that can parse, understand and respond to Chinese sentences. This mechanism utilizes a specific lexical database according to the particular application. In this way, a virtual campus website can implement a special application domain that chooses the proper response in a user friendly, accurate and)},
Author = {Jui-Fa Chen and  Wei-Chuan Lin and  Chih-Yu Jian and  Ching-Chung Hung},
ISSN = {15393100},
Journal = {International Journal of Distance Education Technologies},
Keywords = {INTERNET in education, ONLINE education, NATURAL language processing, CHINESE language, COMPARATIVE grammar, LEXICAL grammar, SIMULATION methods & models, grammar, interactive feedback, lexical database, natural language, segmentation method},
Number = {4},
Pages = {62 - 90},
Title = {A Chinese Interactive Feedback System for a Virtual Campus.},
Volume = {6},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=34095913&site=ehost-live&scope=site},
Year = {2008},
}


@article{13123916320161201,
Abstract = {In this paper we focus on collocations, which have been studied in computational linguistics since they constitute a key factor when processing natural languages. For instance, they usually represent a challenge in automatic translation because the association of two terms is not easily computed. We proposed that the parser should be provided with a lexical database in order to make more effective the identification of collocations during the parsing process. We assessed this claim by using a corpus of 6'000 sentences retrieved from the British magazine The Economist Espresso. The corpus was parsed twice, first with the collocation detection component turned on and then with it turned off, and to make the comparison the Fips tagger was used. The results showed an improvement of the quality when the parser has access to collocation knowledge. (English) [ABSTRACT FROM AUTHOR], En este artículo el análisis se centra en colocaciones, las cuales han sido estudiadas en repetidas ocasiones e)},
Author = {Wehrli, Eric},
ISSN = {03780473},
Journal = {Káñina},
Keywords = {COLLOCATION (Linguistics), SENTENCES (Grammar), PARSING (Grammar), COMPUTATIONAL linguistics, LEXICAL access, collocations, computational linguistics, multiword expressions, natural language processing, sentence parsing, análisis sintáctico de oraciones, colocaciones, lingüística computacional, procesamiento de lenguaje natural, unidades léxicas pluriverbales},
Number = {4},
Pages = {49 - 58},
Title = {MEASURING THE IMPACT OF COLLOCATIONAL KNOWLEDGE ON SENTENCE PARSING.},
Volume = {40},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=131239163&site=ehost-live&scope=site},
Year = {2016},
}


@article{2015-13542-00120150401,
Abstract = {The editorial discusses articles in this issue of Clinical Linguistics & Phonetics. The papers included in this issue cover the main areas of the speech and language sciences: phonetics, phonology, lexis, syntax and semantics. The work reported here underlines the importance of considering head stabilization in ultrasound studies. Lili Yeh, Bill Wells, Joy Stackhouse and Marcin Szczerbinski investigated phonological awareness in children acquiring Mandarin, especially of constituent parts of the syllable. However, as well as looking at the development of awareness, the authors were able to use their data to compare the merits of different models of the syllable in Mandarin. Marianne Lind and colleagues Hanna Simonsen, Pernille Hansen, Elisabeth Holm, and Bjørn- Helge Mevik provide a lexical database for clinicians and clinical linguists working with Norwegian. Jodi Tommerdahl and Cynthia Kilpatrick describe a study of child directed speech. Their particular area of interest is test–retest reliability, and they looked at frequency of morphosyntactic productions in 17 mothers talking to their children. Yalda Kazemi, Thomas Klee and Helen Stringer examined language sample measures for Persian-speaking children and their diagnostic accuracy in distinguishing language impaired from normally developing speakers. The final paper in this issue is by Seung-yun Yan and Diane Sidtis and explores hemispheric specialization for common and proper nouns. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Ball, Martin J. and  Müller, Nicole},
ISSN = {0269-9206, 1464-5076},
Journal = {Clinical Linguistics & Phonetics},
Keywords = {language sciences, diagnostic accuracy, lexical database, phonological awareness, Child, Congresses as Topic, Humans, Linguistics, Phonetics, Societies, Scientific, Language, Lexical Access, Phonological Awareness, Sciences},
Number = {4},
Pages = {247 - 248},
Title = {Editorial},
URL = {martin.j.ball@liu.se},
Volume = {29},
Year = {2015},
}


@article{11467775520160601,
Abstract = {This article addresses the question of the types of morphological relatedness that the lexical class of the adjective presents in Old English. After an exhaustive analysis of the derivational paradigms of the language based on data retrieved from the lexical database of Old English Nerthus, the following conclusions are reached. Two types of morphological relatedness are identifiable, namely explicit and implicit. Short distance and long distance relations overlap with explicit and implicit morphological relatedness. These relations involve four types of units, to wit lexical primes (the bases of lexical paradigms), derived adjectives (the input to recursive processes of word-formation), target adjectives (the output of processes that cannot be inputted to a recursive process) and morphologically unrelated adjectives (which are neither the input nor the output of a process of word-formation). [ABSTRACT FROM AUTHOR], Copyright of Studia Neophilologica is the property of Routledge and i)},
Author = {Novo Urraca, Carmen},
ISSN = {00393274},
Journal = {Studia Neophilologica},
Keywords = {MORPHOLOGY (Grammar), ADJECTIVES (Grammar), OLD English language, WORD formation (Grammar), LINGUISTIC typology, derivational morphology, lexical paradigm, morphological relatedness, recursivity},
Number = {1},
Pages = {43 - 55},
Title = {Morphological Relatedness and the Typology of Adjectival Formations in Old English.},
Volume = {88},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=114677755&site=ehost-live&scope=site},
Year = {2016},
}


@article{4867942020100301,
Abstract = {This research represents the first attempt to produce a working system for the automatic processing of texts of Bahasa Melayu 'Malay'. At the heart of the system is an integrated relational lexical database called MALEX, which draws on the experience of working on English and other languages, but which is specifically tailored to the conditions of Malay. The development of the database is from the beginning entirely data driven, and is based on the analysis of a corpus of naturally produced Malay texts. In designing procedures which access the database, properties of the text are consistently and rigorously distinguished from properties of the lexicon and of the grammar. The system is currently used to provide information for a range of applications, for grammatical tagging, stemming and lemmatisation, parsing, and for generating phonological representations. It is hoped and intended that the design features of MALEX will be transferable, and provide a model for the development of wor)},
Author = {Don, Zuraidah Mohd},
ISSN = {14060922},
Journal = {TRAMES: A Journal of the Humanities & Social Sciences},
Keywords = {MALAY language, TEXT processing (Computer science), DATABASES, CORPORA (Linguistics), LEXICON, WRITTEN communication, LINGUISTICS, corpus, lexicon, Malay, part of speech, text},
Number = {1},
Pages = {90 - 103},
Title = {PROCESSING NATURAL MALAY TEXTS: A DATA-DRIVEN APPROACH.},
Volume = {14},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=48679420&site=ehost-live&scope=site},
Year = {2010},
}


@article{12492347720150101,
Abstract = {Natural Language Processing (NLP) includes a lot more than a computer recognizing a list of words. The goal of the thesis will be to develop a scalable framework for Albanian Language Processing: to build an extendable lexical database; define contemporary Albanian Language Corpus (ALC); study the morphologic and linguistic structure of AL; and to build an algorithm for part-of-speech tagging (POST) and to use it for tagging the documents/text in AL. With our framework approach for applying NLP tools and methodologies for processing of AL we aim to provide a foundation for further studies on the computational linguistics of AL: providing the capability to extend the model with new modules and enriching the rules and definition already set during this paper. [ABSTRACT FROM AUTHOR], Copyright of Proceedings of the Annual South-East European Doctoral Student Conference is the property of South-East European Research Centre and its content may not be copied or emailed to multiple sites or)},
Author = {Voca, Blerim and  Kadriu, Arbana},
ISSN = {17913578},
Journal = {Proceedings of the Annual South-East European Doctoral Student Conference},
Keywords = {NATURAL language processing, ALBANIAN language, TAGS (Metadata), COMPUTER algorithms, COMPUTATIONAL linguistics, Albanian Language (AL), Albanian Language Corpus (ALC), Natural Language Processing (NLP), Part-of-Speech tagging (POST)},
Pages = {304 - 309},
Title = {A scheme for Albanian Language Processing.},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=124923477&site=ehost-live&scope=site},
Year = {2015},
}


@masterthesis{2013-99130-55520130101,
Abstract = {Learning vocabulary and understanding texts present difficulty for language learners due to, among other things, the high degree of lexical ambiguity. By developing an intelligent tutoring system, this dissertation examines whether automatically providing enriched sense-specific information is effective for vocabulary learning and reading comprehension of second language learners. The system developed in this study contributes to an extended understanding of how NLP techniques can be applied more effectively in an educational environment. The system allows learners to upload texts and click on any content word in order to obtain sense-appropriate lexical information for unfamiliar or unknown words during reading. The system consists of three components: (1) the system manager controls the interaction among each learner, the NLP server, and the lexical database; (2) the NLP server converts a raw input text to a linguistically-analyzed text; (3) the lexical database is used to provide a sense-appropriate definition and example sentences of a word to the learner. To obtain the sense-appropriate information, the system first performs word sense disambiguation (WSD) on the input text. Pointing to appropriate examples tuned for language learners, however, is complicated by the fact that the database of examples is from one repository (COBUILD), while automatic WSD systems generally rely on senses from another (WordNet). The lexical database, then, is indexed by WordNet senses, each of which points to an appropriate corresponding COBUILD sense. The fact that every sense inventory has its own standards of sense distinction poses a serious problem in integrating these inventories into one. To redirect an input WordNet sense to a corresponding COBUILD sense, thus, a word sense alignment algorithm was developed, following a heuristic of favoring flatter alignment structures. With this system, an empirical study was conducted with 60 intermediate learners of English as a second language to examine whether this system can lead learners to improve their vocabulary acquisition and reading comprehension. The findings show that learners demonstrated higher performance when receiving sense-specific information. Furthermore, the qualitative examination of the effect of automatic system errors show that, although learners showed learning regardless of the appropriateness of lexical information, they still showed relatively greater learning when given appropriate lexical information. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Eom, Soojeong},
Keywords = {automatic presentation, lexical information, intelligent learning system, multimedia, vocabulary, Foreign Language Learning, Intelligent Tutoring Systems, Vocabulary, Multimedia, Neurolinguistic Programming},
School = {ProQuest Information & Learning},
Title = {Automatic presentation of sense-specific lexical information in an intelligent learning system},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=psyh&AN=2013-99130-555&site=ehost-live&scope=site},
Year = {2013},
}


@article{2018-20002-00620170101,
Abstract = {The CELEX lexical database (Baayen, Piepenbrock & van Rijn 1995) was developed in the 1990s, providing a database of the syntactic, morphological, phonological and orthographic forms of between 50,000 and 125,000 words of Dutch, English and German. This database was used as the basis for the development of the PolyLex lexicons, which included syntactic, morphological and phonological information for around 3,000 words of Dutch, English and German. Orthographic information was subsequently added in the PolyOrth project. The PolyOrth project was based on the assumption that the underlying, lexical phonological forms could be used to derive the surface orthographic forms by means of a combination of phoneme-grapheme mappings and sets of autonomous spelling rules for each language. One of the complications encountered during the project was the fact that the phonological forms in CELEX were not always genuinely underlying forms which made deriving the orthographic forms tricky. This paper discusses the nature and status of underlying phonological forms, their relation to orthography and the issues of finding this information in databases. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
Author = {Cahill, Lynne},
ISSN = {1387-6732, 1570-6001},
Journal = {Written Language and Literacy},
Keywords = {phoneme-grapheme mappings, lexical databases, lexicons, underlying phonology, lexical phonology, post-lexical phonology, Databases, Handwriting, Phonemes, Phonology, Linguistics, Lexical Access},
Number = {1},
Pages = {104 - 127},
Title = {What are the 'phonemes' in phoneme-grapheme mappings? A perspective on the use of databases for lexicon development},
URL = {L.J.Cahill@sussex.ac.uk},
Volume = {20},
Year = {2017},
}


@inbook{2016-61176-01620170101,
Abstract = {WordNet, a large lexical database of English, was conceived as a model of human semantic organization. Evidence from timing experiments, association norms, and distributional properties of words supported a semantic network model in which words are interlinked via a small number of lexical and conceptual relations. Its large coverage and unique structure, which allows automatic systems to detect and quantify semantic relatedness among words, soon made WordNet an invaluable tool for natural language processing tasks. Information retrieval, document summarization, and machine translation crucially require word sense discrimination and disambiguation. Wordnets have been built in dozens of languages and for specific technical sublanguages, and the number of applications in research, language technology and pedagogy has grown. Although WordNet’s central focus has shifted from its psycholinguistic origins, its design, based on theories about the structure of the human mental lexicon, is validated as a sound approach to representing the meanings of words. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
Author = {Fellbaum, Christiane and  Chipman, Susan E. F.},
Booktitle = {The Oxford handbook of cognitive science.},
ISBN = {978-0-19-984219-3, 978-0-19-984417-3},
Pages = {301 - 313},
Publisher = {Oxford University Press},
Title = {WordNet: An electronic lexical resource},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=psyh&AN=2016-61176-016&site=ehost-live&scope=site},
Year = {2017},
}


@article{11051862420150706,
Abstract = {It is the linguistic ontology that is in the focus of this investigation. This notion is one of the basic in the computational linguistics domain vocabulary and it is understood as an informational resource. Nevertheless this notion still attracts philosophy experts and mostly they interpret it through the notion of Existence. This article deals with different understanding of the notion «ontology» (both philosophical and linguistic) in the context of cognitive technologies. The cognitive approach is aimed for accurate understanding of the notion technology within the framework of language functioning. It means (implies) the ability to perceive, process, keep, transfer information (knowledge) that is realized through different means of mental activity. This original definition of cognitive technology stipulates for viewing linguistic ontology as one of the cognitive technologies. That's why linguistic ontologies are observed as informational data base in this article. For better compr)},
Author = {Татаринова, Л. В.},
ISSN = {20720831},
Journal = {In the World of Scientific Discoveries / V Mire Nauchnykh Otkrytiy},
Keywords = {cognitive, conceptualization, domain vocabulary, knowledge representation, language, language formal models, ontology, technology, когнитивный, концептуализация, онтология, предметная область, репрезентация знаний, технология, формальные модели языка, язык, cognitive, conceptualization, domain vocabulary, knowledge representation, language, language formal models, ontology, technology},
Number = {7.5},
Pages = {2093 - 2111},
Title = {ЛИНГВИСТИЧЕСКАЯ ОНТОЛОГИЯ КАК КОГНИТИВНАЯ ТЕХНОЛОГИЯ},
Volume = {67},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=110518624&site=ehost-live&scope=site},
Year = {2015},
}


@article{10851967620150701,
Abstract = {Within the Spanish educational context, one of the most loyal and indisputable allies for teaching and learning foreign languages is the textbook. Several studies have been conducted on particular aspects of textbooks (Aguirre Lora, 2001; Escaño, 2013; Autor, 2012; Jiménez Catalán, 2003; Nodarse González & Mons Obermayer, 2013), but few of them carry out a holistic, diachronic and multimodal analysis. This study verifies the adequacy of textbooks for teaching and learning English as a foreign language to adults in a non-immersion context. Specifically, it analyzes the lexical level of the selected samples, the skills demanded by the activities, and the distribution of the lessons' elements according to their semantics. The analytical tools used in the study include the lexical database English Vocabulary Profile, Bloom's taxonomy (1956) and the principles of visual composition developed by Kress and van Leeuwen (2006). The main conclusions that can be drawn from this document are: the)},
Author = {Romero, Rocío González},
ISSN = {16987799},
Journal = {Foro de Educación},
Keywords = {adult education, English, multimodality, teaching foreign languages, teaching materials, educación de personas adultas, enseñanza de lenguas extranjeras, inglés, material didáctico, multimodalidad},
Number = {19},
Pages = {343 - 356},
Title = {Análisis holístico, diacrónico y multimodal de libros de texto de inglés como lengua extranjera: Una nueva forma de mejorar la comprensión.},
Volume = {13},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=108519676&site=ehost-live&scope=site},
Year = {2015},
}


@article{10151449020150401,
Abstract = {We examined the potential advantage of the lexical databases using subtitles and present SUBTLEX-PT, a new lexical database for 132,710 Portuguese words obtained from a 78 million corpus based on film and television series subtitles, offering word frequency and contextual diversity measures. Additionally we validated SUBTLEX-PT with a lexical decision study involving 1920 Portuguese words (and 1920 nonwords) with different lengths in letters (M = 6.89,SD = 2.10) and syllables (M = 2.99,SD = 0.94). Multiple regression analyses on latency and accuracy data were conducted to compare the proportion of variance explained by the Portuguese subtitle word frequency measures with that accounted by the recent written-word frequency database (Procura-PALavras; P-PAL; Soares, Iriarte, et al., 2014). As its international counterparts, SUBTLEX-PT explains approximately 15% more of the variance in the lexical decision performance of young adults than the P-PAL database. Moreover, in line with recent)},
Author = {Soares, Ana Paula and  Machado, João and  Costa, Ana and  Iriarte, Álvaro and  Simões, Alberto and  de Almeida, José João and  Comesaña, Montserrat and  Perea, Manuel},
ISSN = {17470218},
Journal = {Quarterly Journal of Experimental Psychology},
Keywords = {WORD frequency, DATABASES, PORTUGUESE language, MULTIPLE regression analysis, LEXICOLOGY, Contextual diversity, Portuguese, Subtitles, Word frequency},
Number = {4},
Pages = {680 - 696},
Title = {On the advantages of word frequency and contextual diversity measures extracted from subtitles: The case of Portuguese.},
Volume = {68},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=101514490&site=ehost-live&scope=site},
Year = {2015},
}


@article{10922168120150301,
Abstract = {This paper predicts the mutual intelligibility of 15 Chinese dialects from multiple objective distance measures. Empirical mutual intelligibility measures were obtained from functional intelligibility tests at the sentence level from 15 listeners for each of 15 Chinese dialects. We computed various proximity measures on the basis of shared phonemes and tones in the sound inventories of the 15 dialects. Next, Levenshtein (string-edit) distance measures were computed on the 764 common syllabic units (zi in Pinyin, i.e., a meaningful character or morpheme with a complete transcription of segments and tone) shared by the same 15 Chinese dialects in the Dialect Sound Database of Modern Chinese (compiled by the Chinese Academy of Social Sciences). Unweighted and perceptually weighted Levenshtein distance measures were computed. We also included objective similarity measures of phonological correspondence, based on the Zihui character list and of lexical affinity, based on the Cihui cross-di)},
Author = {Chaoju Tang and  van Heuven, Vincent J.},
ISSN = {00243949},
Journal = {Linguistics},
Keywords = {MUTUAL intelligibility of modern languages, CHINESE language, DIALECTS, LINGUISTICS, EMPIRICAL research, Chinese dialects, functional test, Levenshtein distance, lexical affinity, mutual intelligibility, opinion test, phonological affinity, tonal distance, word recognition},
Number = {2},
Pages = {285 - 312},
Title = {Predicting mutual intelligibility of Chinese dialects from multiple objective linguistic distance measures.},
Volume = {53},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=109221681&site=ehost-live&scope=site},
Year = {2015},
}


@article{4421396220080301,
Abstract = {In this paper the role of concept characteristics in lexical dialectometric research is examined in three consecutive logical steps. First, a regression analysis of data taken from a large lexical database of Limburgish dialects in Belgium and The Netherlands is conducted to illustrate that concept characteristics such as concept salience, concept vagueness and negative affect contribute to the lexical heterogeneity in the dialect data. Next, it is shown that the relationship between concept characteristics and lexical heterogeneity influences the results of conventional lexical dialectometric measurements. Finally, a dialectometric procedure is proposed which downplays this undesired influence, thus making it possible to obtain a clearer picture of the ‘truly’ regional variation. More specifically, a lexical dialectometric method is proposed in which concept characteristics form the basis of a weighting schema that determines to which extent concept specific dissimilarities can contr)},
Author = {Speelman, Dirk and  Geeraerts, Dirk},
ISSN = {17538548},
Journal = {International Journal of Humanities & Arts Computing: A Journal of Digital Humanities},
Keywords = {LEXICAL phonology, DIALECT research, COMPARATIVE linguistics, LINGUISTIC geography, DUTCH language -- Dialects, ANALYSIS of variance, NETHERLANDS, BENELUX countries},
Number = {1-2},
Pages = {221 - 242},
Title = {The Role of Concept Characteristics in Lexical Dialectometry.},
Volume = {2},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=44213962&site=ehost-live&scope=site},
Year = {2008},
}


@article{9713187420140901,
Abstract = {In this article, we present the first open-access lexical database that provides phonological representations for 120,000 Italian word forms. Each of these also includes syllable boundaries and stress markings and a comprehensive range of lexical statistics. Using data derived from this lexicon, we have also generated a set of derived databases and provided estimates of positional frequency use for Italian phonemes, syllables, syllable onsets and codas, and character and phoneme bigrams. These databases are freely available from phonitalia.org. This article describes the methods, content, and summarizing statistics for these databases. In a first application of this database, we also demonstrate how the distribution of phonological substitution errors made by Italian aphasic patients is related to phoneme frequency. [ABSTRACT FROM AUTHOR], Copyright of Behavior Research Methods is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted )},
Author = {Goslin, Jeremy and  Galluzzi, Claudia and  Romani, Cristina},
ISSN = {1554351X},
Journal = {Behavior Research Methods},
Keywords = {ITALIAN language, PHONEME (Linguistics), FOREIGN language education digital resources, PHONETICS, LEXICOLOGY, Aphasic errors, Lexical statistics, Phonological lexicon},
Number = {3},
Pages = {872 - 886},
Title = {PhonItalia: a phonological lexicon for Italian.},
Volume = {46},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=97131874&site=ehost-live&scope=site},
Year = {2014},
}


@masterthesis{ED55282220130101,
Abstract = {This dissertation addresses semantic search of Web services using natural language processing. We first survey various existing approaches, focusing on the fact that the expensive costs of current semantic annotation frameworks result in limited use of semantic search for large scale applications. We then propose a vector space model based service searching framework to combine traditional frequency weighted term-document matrix and syntactical information extracted from a lexical database and a dependency grammar parser. In particular, instead of using terms as the rows in a term-document matrix, we propose using synsets in WordNet to distinguish different meanings of a word under different contexts as well as clustering different words with the similar meaning. Also based on the characteristics of Web services descriptions, we propose an approach to identifying semantically important terms to adjust the weights. Our experiments show that our approach achieves its goal well. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]},
Author = {Hao, Ke},
Keywords = {Semantics; Online Searching; Natural Language Processing; Internet},
School = {ProQuest LLC},
Title = {Semantic Search of Web Services},
URL = {http://gateway.proquest.com/openurl?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&res_dat=xri:pqm&rft_dat=xri:pqdiss:3557251},
Year = {2013/01/01/},
}


@article{9271634320140401,
Abstract = {Highlights: [•] Italian Sign Language MultiWordNet integrating (LMWN) is presented. [•] LMWN integrates MultiWordNet (MWN) lexical database with Italian Sign language (LIS). [•] Italian synsets are linked to LIS signs to extract bilingual information from the collected lexicon. [•] Semantic relationships of LIS Signs with MWN are displayed; words existing in DataBase are highlighted. [•] Paper concludes that relatively small size of lexicon can help to cover a significant portion of MWN. [ABSTRACT FROM AUTHOR], Copyright of Expert Systems with Applications is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the mater)},
Author = {Shoaib, U. and  Ahmad, N. and  Prinetto, P. and  Tiotto, G.},
ISSN = {09574174},
Journal = {Expert Systems with Applications},
Keywords = {ITALIAN Sign Language, LEXICAL access, DATABASE management, SEMANTICS, INFORMATION technology, INFORMATION theory, Animation, Annotation, Avatar, Bilingual lexical resources, Language resources, MultiWordNet, Sign language, Sign language dictionaries},
Number = {5},
Pages = {2300 - 2308},
Title = {Integrating MultiWordNet with Italian Sign Language lexical resources.},
Volume = {41},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=92716343&site=ehost-live&scope=site},
Year = {2014},
}


@article{9301301020131201,
Abstract = {The aim of the article is to identify the exponent for the semantic prime TOUCH in Old English. Therefore, this research contributes to the frame of the Natural Semantic Metalanguage Research Programme (NSMRP) by applying it to the study of a historical language. Throughout such an application several descriptive and methodological questions arise. On the descriptive side, it is necessary to propose a cluster of semantic, morphological, textual and syntactic criteria that allow for the identification of the prime at stake, given that the nature of the object of study is not compatible with the translation into the native language generally adopted by the NSMRP. The analysis focuses on the category actions, events, movement and contact, and relies on data retrieved from theHistorical Thesaurus of the Oxford English Dictionary, theDictionary of Old English Corpusand the lexical database of Old EnglishNerthus. Although the cluster of criteria evinces a clear candidate for semantic prime )},
Author = {Mateo Mendaza, Raquel},
ISSN = {07268602},
Journal = {Australian Journal of Linguistics},
Keywords = {OLD English language, SEMANTICS, METALANGUAGE, TRANSLATING & interpreting, MORPHOLOGY (Grammar), WORD formation (Grammar), Hypernyms, Natural Semantic Metalanguage, Old English, Semantic Primes, Semantics},
Number = {4},
Pages = {449 - 466},
Title = {The Old English Exponent for the Semantic Prime TOUCH. Descriptive and Methodological Questions.},
Volume = {33},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=93013010&site=ehost-live&scope=site},
Year = {2013},
}


@article{EJ97661820120701,
Abstract = {The objective of our study is to introduce a fully automated, computational linguistic technique to quantify semantic relations between words generated on a standard semantic verbal fluency test and to determine its cognitive and clinical correlates. Cognitive differences between patients with Alzheimer's disease and mild cognitive impairment are evident in their performance on the semantic verbal fluency test. In addition to the semantic verbal fluency test score, several other performance characteristics sensitive to disease status and predictive of future cognitive decline have been defined in terms of words generated from semantically related categories (clustering) and shifting between categories (switching). However, the traditional assessment of clustering and switching has been performed manually in a qualitative fashion resulting in subjective scoring with limited reproducibility and scalability. Our approach uses word definitions and hierarchical relations between the words in WordNet[R], a large electronic lexical database, to quantify the degree of semantic similarity and relatedness between words. We investigated the novel semantic fluency indices of mean cumulative similarity and relatedness between all pairs of words regardless of their order, and mean sequential similarity and relatedness between pairs of adjacent words in a sample of patients with clinically diagnosed probable (n=55) or possible (n=27) Alzheimer's disease or mild cognitive impairment (n=31). The semantic fluency indices differed significantly between the diagnostic groups, and were strongly associated with neuropsychological tests of executive function, as well as the rate of global cognitive decline. Our results suggest that word meanings and relations between words shared across individuals and computationally modeled via WordNet and large text corpora provide the necessary context to account for the variability in language-based behavior and relate it to cognitive dysfunction observed in mild cognitive impairment and Alzheimer's disease. (Contains 3 tables and 4 figures.)},
Author = {Pakhomov, Serguei V. S. and  Hemmy, Laura S. and  Lim, Kelvin O.},
ISSN = {0028-3932},
Journal = {Neuropsychologia},
Keywords = {Semantics; Alzheimers Disease; Diseases; Patients; Scores; Semiotics; Cognitive Processes; Neurological Impairments; Computation; Cluster Grouping; Correlation; Language Fluency; Word Recognition},
Number = {9},
Pages = {2165 - 2175},
Title = {Automated Semantic Indices Related to Cognitive Function and Rate of Cognitive Decline},
URL = {http://dx.doi.org/10.1016/j.neuropsychologia.2012.05.016},
Volume = {50},
Year = {2012/07/01/},
}


@article{2177851220060801,
Abstract = {Sentence similarity measures play an increasingly important role in text-related research and applications in areas such as text mining, Web page retrieval, and dialogue systems. Existing methods for computing sentence similarity have been adopted from approaches used for long text documents. These methods process sentences in a very high-dimensional space and are consequently inefficient, require human input, and are not adaptable to some application domains. This paper focuses directly on computing the similarity between very short texts of sentence length. It presents an algorithm that takes account of semantic information and word order information implied in the sentences. The semantic similarity of two sentences is calculated using information from a structured lexical database and from corpus statistics. The use of a lexical database enables our method to model human common sense knowledge and the incorporation of corpus statistics allows our method to be adaptable to different)},
Author = {Yuhua Li and  McLean, David and  Bandar, Zuhair A. and  O'Shea, James D. and  Crockett, Keeley},
ISSN = {10414347},
Journal = {IEEE Transactions on Knowledge & Data Engineering},
Keywords = {SEMANTIC networks (Information theory), ARTIFICIAL intelligence, INFORMATION theory, COMMUNICATION, NATURAL language processing, ELECTRONIC data processing, HUMAN-computer interaction, ERGONOMICS, COMPUTER science, corpus, natural language processing, semantic nets, Sentence similarity, word similarity},
Number = {8},
Pages = {1138 - 1150},
Title = {Sentence Similarity Based on Semantic Nets and Corpus Statistics.},
Volume = {18},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=21778512&site=ehost-live&scope=site},
Year = {2006},
}


@article{2015-09766-00420150301,
Abstract = {This paper predicts the mutual intelligibility of 15 Chinese dialects from multiple objective distance measures. Empirical mutual intelligibility measures were obtained from functional intelligibility tests at the sentence level from 15 listeners for each of 15 Chinese dialects. We computed various proximity measures on the basis of shared phonemes and tones in the sound inventories of the 15 dialects. Next, Levenshtein (string-edit) distance measures were computed on the 764 common syllabic units (zi in Pinyin, i.e., a meaningful character or morpheme with a complete transcription of segments and tone) shared by the same 15 Chinese dialects in the Dialect Sound Database of Modern Chinese (compiled by the Chinese Academy of Social Sciences). Unweighted and perceptually weighted Levenshtein distance measures were computed. We also included objective similarity measures of phonological correspondence, based on the Zihui character list and of lexical affinity, based on the Cihui cross-dialect lexical database with all cognate and non-cognate expressions of 905 core concepts) that have been published by Cheng (1997). The best single predictor of mutual intelligibility between a pair of dialects was the percentage of cognates shared between them (r² = .548). Including all predictors afforded a highly accurate prediction of mutual intelligibility (R² = .874). A very reasonable prediction is afforded if we just add the lexical frequency of finals (syllable rhymes) shared by a pair of dialects (R² = .611). (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Tang, Chaoju and  van Heuven, Vincent J.},
ISSN = {0024-3949, 1613-396X},
Journal = {Linguistics},
Keywords = {mutual intelligibility, opinion test, functional test, word recognition, Chinese dialects, Levenshtein distance, tonal distance, lexical affinity, phonological affinity, Dialect, Linguistics, Phonology, Speech Perception, Word Recognition},
Number = {2},
Pages = {285 - 312},
Title = {Predicting mutual intelligibility of Chinese dialects from multiple objective linguistic distance measures},
URL = {1006946669@qq.com, v.j.j.p.van.heuven@hum.leidenuniv.nl},
Volume = {53},
Year = {2015},
}


@article{8668244620130301,
Abstract = {The Sensor Web is a growing phenomenon where an increasing number of sensors are collecting data in the physical world, to be made available over the Internet. To help realize the Sensor Web, the Open Geospatial Consortium (OGC) has developed open standards to standardize the communication protocols for sharing sensor data. Spatial Data Infrastructures (SDIs) are systems that have been developed to access, process, and visualize geospatial data from heterogeneous sources, and SDIs can be designed specifically for the Sensor Web. However, there are problems with interoperability associated with a lack of standardized naming, even with data collected using the same open standard. The objective of this research is to automatically group similar sensor data layers. We propose a methodology to automatically group similar sensor data layers based on the phenomenon they measure. Our methodology is based on a unique bottom-up approach that uses text processing, approximate string matching, an)},
Author = {Knoechel, Ben and  Chih-Yuan Huang and  Liang, Steve H. L.},
ISSN = {22209964},
Journal = {ISPRS International Journal of Geo-Information},
Keywords = {GEOGRAPHIC information systems, SPATIAL data infrastructures, INFORMATION storage & retrieval systems, ELECTRONIC data processing, data interoperability, data mining, GIS, information retrieval, OGC, SOS, OGC Geospatial Consortium Inc.},
Number = {1},
Pages = {1 - 26},
Title = {A Bottom-Up Approach for Automatically Grouping Sensor Data Layers by their Observed Property.},
Volume = {2},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=86682446&site=ehost-live&scope=site},
Year = {2013},
}


@article{8365283520130101,
Abstract = {Abstract: The substantial increase of social networks and their combination with mobile devices make rigorous analysis of the outcomes of such system of paramount importance for intelligence gathering and decision making purposes. Since the introduction of Twitter system in 2006, tweeting emerged as an efficient open social network that attracted interest from various research/commercial and military communities. This paper investigates the current software architecture of Twitter system and put forward a new architecture dedicated for semantic and spatial analysis of Twitter data. Especially, Twitter Streaming API was used as a basis for tweet collection data stored in MySQL like database. While Lucene system together with WordNet lexical database linked to advanced natural language processing and PostGIS platform were used to ensure semantic and spatial analysis of the collected data. A functional diversity approach was implemented to enforce fault tolerance for the data collection )},
Author = {Oussalah, M. and  Bhat, F. and  Challis, K. and  Schnier, T.},
ISSN = {09507051},
Journal = {Knowledge-Based Systems},
Keywords = {SOFTWARE architecture, SEARCH algorithms, SOCIAL networks, MOBILE communication systems, DECISION making, DATABASES, Data mining, Semantic analysis, Social network, Software architecture, Tweet, TWITTER (Web resource)},
Pages = {105 - 120},
Title = {A software architecture for Twitter collection, search and geolocation services},
Volume = {37},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=83652835&site=ehost-live&scope=site},
Year = {2013},
}


@article{2033834520060301,
Abstract = {WordNet, a lexical database for English that is extensively used by computational linguists, has not previously distinguished hyponyms that are classes from hyponyms that are instances. This note describes an attempt to draw that distinction and proposes a simple way to incorporate the results into future versions of WordNet. [ABSTRACT FROM AUTHOR], Copyright of Computational Linguistics is the property of MIT Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
Author = {Miller, George A. and  Hristea, Florentina},
ISSN = {08912017},
Journal = {Computational Linguistics},
Keywords = {LEXICAL access, ONLINE information services, SEMANTICS, COMPUTATIONAL linguistics, LEXICOLOGY},
Number = {1},
Pages = {1 - 3},
Title = {WordNet Nouns: Classes and Instances.},
Volume = {32},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=20338345&site=ehost-live&scope=site},
Year = {2006},
}


@article{2009-00421-00820090101,
Abstract = {We report work on adding semantic role labels to the Chinese Treebank, a corpus already annotated with phrase structures. The work involves locating all verbs and their nominalizations in the corpus, and semi-automatically adding semantic role labels to their arguments, which are constituents in a parse tree. Although the same procedure is followed, different issues arise in the annotation of verbs and nominalized predicates. For verbs, identifying their arguments is generally straightforward given their syntactic structure in the Chinese Treebank as they tend to occupy well-defined syntactic positions. Our discussion focuses on the syntactic variations in the realization of the arguments as well as our approach to annotating dislocated and discontinuous arguments. In comparison, identifying the arguments for nominalized predicates is more challenging and we discuss criteria and procedures for distinguishing arguments from non-arguments. In particular we focus on the role of support verbs as well as the relevance of event/result distinctions in the annotation of the predicate-argument structure of nominalized predicates. We also present our approach to taking advantage of the syntactic structure in the Chinese Treebank to bootstrap the predicate-argument structure annotation of verbs. Finally, we discuss the creation of a lexical database of frame files and its role in guiding predicate argument annotation. Procedures for ensuring annotation consistency and inter-annotator agreement evaluation results are also presented. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Xue, Nianwen and  Martha, Palmer},
ISSN = {1351-3249, 1469-8110},
Journal = {Natural Language Engineering},
Keywords = {Chinese Treebank, semantic roles, phrase structures, verbs, lexical database, Databases, Lexical Access, Phrases, Semantics, Verbs},
Number = {1},
Pages = {143 - 172},
Title = {Adding semantic roles to the Chinese Treebank},
URL = {Nianwen.Xue@Colorado.EDU},
Volume = {15},
Year = {2009},
}


@article{2014-08495-00920140401,
Abstract = {Cross-language comparisons can provide important constraints on our understanding of how people read aloud. French is an interesting case because it differs from most other writing systems in that it uses a large number of multi-letter vowel graphemes and consonants that are systematically silent (i.e., do not map to any lexical phonology; e.g., trop ). Here, we developed a French version of the Connectionist Dual Process Model of Reading Aloud (CDP++) that can handle multisyllabic stimuli (up to three syllables) and has a large-scale lexicon of more than 100,000 words. We tested the model on extant data and an additional experiment examining the reading aloud of nonwords with potentially silent letters. The results from the extant data showed that the model was able to capture a number of important psycholinguistic effects in the literature and explained between 52% and 67% of the item-specific variance in two large databases. The results of the silent-letter experiment showed that, contrary to what would be predicted on the basis of lexical database statistics, people generally pronounce 'silent' consonants in nonwords. We show that the French CDP++ model faithfully predicted this effect because it implements a linear mapping between orthography and phonology. These findings highlight the theoretical and practical significance of using computational models to help determine the processes and representations that underlie skilled reading. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
Author = {Perry, Conrad and  Ziegler, Johannes C. and  Zorzi, Marco},
ISSN = {0749-596X, 1096-0821},
Journal = {Journal of Memory and Language},
Keywords = {reading aloud, dual process models, lexical phonology, letters, psycholinguistic effects, Lexical Access, Oral Reading, Phonology, Dual Process Models, Letters (Alphabet), Psycholinguistics},
Pages = {98 - 115},
Title = {When silent letters say more than a thousand words: An implementation and evaluation of CDP++ in French},
URL = {ConradPerry@gmail.com, ORCID: 0000-0002-4651-6390},
Volume = {72},
Year = {2014},
}


@article{2018-60750-00120181112,
Abstract = {Presents a study which aims to investigate SPALEX, a Spanish lexical decision database by focusing on native Spanish speakers at a global scale and with a vast amount of words, to provide a useful tool for researchers exploring the acquisition and processing of this language in native and foreign contexts. SPALEX contains data from a Spanish crowd-sourced lexical decision mega study. The authors collected the data through an online platform from May 12th, 2014 to December 19th, 2017. The majority of the data was acquired during the first month of the experiment, when an advertising campaign was done in order to attract the public’s attention. Participants also had the option of publishing their results via social networks, which led to attract more participants in a snow-ball sampling fashion. Additionally, the database contains information on participants that voluntarily provided information about their gender, age, country of origin, education level, handedness, native language, and best foreign language. In each experimental session, participants responded to 70 words and 30 non-words presented randomly and without repetition. Accuracy in SPALEX is expressed as 1 for correct answers and 0 for incorrect answers. Based on participants’ responses, the authors calculated percentage known, a measure of the percentage of participants that know a particular word. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
Author = {Aguasvivas, Jose Armando and  Carreiras, Manuel and  Brysbaert, Marc and  Mandera, Paweł and  Keuleers, Emmanuel and  Duñabeitia, Jon Andoni},
ISSN = {1664-1078},
Journal = {Frontiers in Psychology},
Keywords = {megastudies, lexical decision, vocabulary knowledge, online assessments, lexical database, Data Collection, Databases, Linguistics, Lexical Decision, Vocabulary},
Title = {SPALEX: A Spanish lexical decision database from a massive online data collection},
URL = {jdunabeitia@nebrija.es},
Volume = {9},
Year = {2018},
}


@article{2008-15532-00520080101,
Abstract = {Considering the popularity of the Internet, an automatic interactive feedback system for E-learning websites is becoming increasingly desirable. However, computers still have problems understanding natural languages, especially the Chinese language, firstly because the Chinese language has no space to segment lexical entries (its segmentation method is more difficult than that of English) and secondly because of the lack of a complete grammar in the Chinese language, making parsing more difficult and complicated. Building an automated Chinese feedback system for special application domains could solve these problems. This paper proposes an interactive feedback mechanism in a virtual campus that can parse, understand and respond to Chinese sentences. This mechanism utilizes a specific lexical database according to the particular application. In this way, a virtual campus website can implement a special application domain that chooses the proper response in a user friendly, accurate and timely manner. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Chen, Jui-Fa and  Lin, Wei-Chuan and  Jian, Chih-Yu and  Hung, Ching-Chung},
ISSN = {1539-3100, 1539-3119},
Journal = {International Journal of Distance Education Technologies},
Keywords = {chinese interactive feedback system, e-learning websites, virtual campus, specific lexical database, user friendly, Campuses, Databases, Systems Design, Virtual Reality, Websites, Computers, Feedback, Internet},
Number = {4},
Pages = {62 - 90},
Title = {A Chinese interactive feedback system for a virtual campus},
Volume = {6},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=psyh&AN=2008-15532-005&site=ehost-live&scope=site},
Year = {2008},
}


@article{6681545520111101,
Abstract = {Our survey shows that the techniques used in data extraction from deep webs need to be improved to achieve the efficiency and accuracy of automatic wrappers. Further investigations indicate that the development of a lightweight ontological technique using existing lexical database for English (WordNet) is able to check the similarity of data records and detect the correct data region with higher precision using the semantic properties of these data records. The advantages of this method are that it can extract three types of data records, namely, single-section data records, multiple-section data records, and loosely structured data records, and it also provides options for aligning iterative and disjunctive data items. Experimental results show that our technique is robust and performs better than the existing state-of-the-art wrappers. Tests also show that our wrapper is able to extract data records from multilingual web pages and that it is domain independent. [ABSTRACT FROM PUBLIS)},
Author = {Hong, Jer Lang},
ISSN = {10946977},
Journal = {IEEE Transactions on Systems, Man & Cybernetics: Part C - Applications & Reviews},
Keywords = {DATA extraction, WRAPPING machines, SEMANTICS, WEBSITES, ELECTRONIC data processing, Automatic wrapper, deep web, ontology},
Number = {6},
Pages = {854 - 868},
Title = {Data Extraction for Deep Web Using WordNet.},
Volume = {41},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=66815455&site=ehost-live&scope=site},
Year = {2011},
}


@article{6193135220110801,
Abstract = {In this paper, I investigate the phonological similarity of different elements of the phonological pole of multi-word units. I discuss two case studies on slightly different levels of abstractness. The first case study investigates lexically fully-specified V-NPDirObj idioms such as kick the bucket and lose one's cool; the idioms investigated are taken from the Collins Cobuild Dictionary of Idioms (Harper Collins, 2002). The second case study investigates the lexically less specified way-construction, which is exemplified by He fought his way through the crowd (cf. Goldberg, Constructions: A Construction Grammar approach to argument structure, The University of Chicago Press, 1995: Ch. 9), on the basis of data from the British National Corpus 1.0. I show that both patterns exhibit a strong phonological within-pole relation, namely a strong preference for having their slots filled with phonologically similar elements, where phonological similarity is manifested in alliteration patterns)},
Author = {Gries, Stefan Th.},
ISSN = {09365907},
Journal = {Cognitive Linguistics},
Keywords = {PHONOLOGY (Grammar), IDIOMS, CONSTRUCTION grammar, ALLITERATION, SEMANTICS, CELEX (Information retrieval system), DATABASES, alliteration, CELEX, corpus data, Semantic and phonological constituents, semantic and phonological poles},
Number = {3},
Pages = {491 - 510},
Title = {Phonological similarity in multi-word units.},
Volume = {22},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=61931352&site=ehost-live&scope=site},
Year = {2011},
}


@article{8268305720130110,
Abstract = {Abstract: Recommender systems have been developed in variety of domains, including asynchronous discussion group which is one of the most interesting ones. Due to the information overload and its varieties in discussion groups, it is difficult to draw out the relevant information. Therefore, recommender systems play an important role in filtering and customizing the desired information. Nowadays, collaborative and content-based filtering are the most adopted techniques being utilized in recommender systems. The collaborative filtering technique recommends items based on liked-mind users’ opinions and users’ preferences. Alternatively, the aim of the content-based filtering technique is the identification of items which are similar to those a user has preferred in past. To overcome the drawbacks of the aforementioned techniques, a hybrid recommender system combines two or more recommendation techniques to obtain more accuracy. The most important achievement of this study is to present )},
Author = {Kardan, Ahmad A. and  Ebrahimi, Mahnaz},
ISSN = {00200255},
Journal = {Information Sciences},
Keywords = {RECOMMENDER systems, INFORMATION filtering systems, ASSOCIATION rule mining, DATA mining, INFORMATION theory, COMPARATIVE studies, CONTENT mining, ELECTRONIC discussion groups, Association rules mining, Asynchronous discussion group, Collaborative filtering, collaborative filtering ( CF ), computer supported collaborative learning ( CSCL ), Content-based filtering, content-based filtering ( CBF ), Hybrid Filtering ( HF ), Hybrid recommender system, Mean Absolute Error ( MAE ), Recommender system, Word Sense Disambiguation ( Abbreviations: WSD )},
Pages = {93 - 110},
Title = {A novel approach to hybrid recommendation systems based on association rules mining for content recommendation in asynchronous discussion groups},
Volume = {219},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=82683057&site=ehost-live&scope=site},
Year = {2013},
}


@article{2014-44358-00220140101,
Abstract = {The architecture of writing systems metaphor has special relevance for understanding the structural nature of the Japanese writing system, and, more specifically, for appreciating how the 2,136 kanji of the jō-yō-kanji- hyō/ ‘List of characters for general use’ function as the core building blocks in the orthographic representation of a considerable proportion of the Japanese lexicon. In seeking to illuminate the multiple layers of internal structure within Japanese kanji, the Japanese lexicon, and the Japanese writing system, the paper draws on insights and observations gained from an ongoing project to construct a large-scale Japanese lexical database system. Reflecting structural distinctions within the database, the paper consists of three main sections addressing the different structural levels of kanji components, jōyō kanji, and the lexicon. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Joyce, Terry and  Masuda, Hisashi and  Ogawa, Taeko},
ISSN = {1387-6732, 1570-6001},
Journal = {Written Language and Literacy},
Keywords = {Japanese writing system, building blocks, jōyō kanji, components, orthographic structure, database, Metaphor, Orthography, Writing Skills, Written Language, Databases},
Number = {2},
Pages = {173 - 194},
Title = {Jōyō kanji as core building blocks of the Japanese writing system: Some observations from database construction},
URL = {terry@tama.ac.jp},
Volume = {17},
Year = {2014},
}


@article{6092541620110601,
Abstract = {Abstract: The category system in Wikipedia can be taken as a conceptual network. We label the semantic relations between categories using methods based on connectivity in the network and lexico-syntactic matching. The result is a large scale taxonomy. For evaluation we propose a method which (1) manually determines the quality of our taxonomy, and (2) automatically compares its coverage with ResearchCyc, one of the largest manually created ontologies, and the lexical database WordNet. Additionally, we perform an extrinsic evaluation by computing semantic similarity between words in benchmarking datasets. The results show that the taxonomy compares favorably in quality and coverage with broad-coverage manually created resources. [Copyright &y& Elsevier], Copyright of Artificial Intelligence is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may )},
Author = {Ponzetto, Simone Paolo and  Strube, Michael},
ISSN = {00043702},
Journal = {Artificial Intelligence},
Keywords = {TAXONOMY, KNOWLEDGE acquisition (Expert systems), NATURAL language processing, BENCHMARKING (Management), SET theory, LEXICOLOGY, SEMANTIC computing, DATABASES, Knowledge acquisition, Lexical semantics, Natural language processing},
Number = {9/10},
Pages = {1737 - 1756},
Title = {Taxonomy induction based on a collaboratively built knowledge repository},
Volume = {175},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=60925416&site=ehost-live&scope=site},
Year = {2011},
}


@article{5874537620110401,
Abstract = {Abstract: Sentence and short-text semantic similarity measures are becoming an important part of many natural language processing tasks, such as text summarization and conversational agents. This paper presents SyMSS, a new method for computing short-text and sentence semantic similarity. The method is based on the notion that the meaning of a sentence is made up of not only the meanings of its individual words, but also the structural way the words are combined. Thus, SyMSS captures and combines syntactic and semantic information to compute the semantic similarity of two sentences. Semantic information is obtained from a lexical database. Syntactic information is obtained through a deep parsing process that finds the phrases in each sentence. With this information, the proposed method measures the semantic similarity between concepts that play the same syntactic role. Psychological plausibility is added to the method by using previous findings about how humans weight different syntac)},
Author = {Oliva, Jesús and  Serrano, José Ignacio and  del Castillo, María Dolores and  Iglesias, Ángel},
ISSN = {0169023X},
Journal = {Data & Knowledge Engineering},
Keywords = {SEMANTIC integration (Computer systems), DATABASE management, SYNTAX (Grammar), RANKING, PARSING (Computer grammar), PLAUSIBILITY (Logic), INTUITION, NATURAL language processing, Linguistic tools for IS modeling, Natural language processing (NLP), Semantic similarity, Sentence similarity, Text DBs},
Number = {4},
Pages = {390 - 405},
Title = {SyMSS: A syntax-based measure for short-text semantic similarity},
Volume = {70},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=58745376&site=ehost-live&scope=site},
Year = {2011},
}


@article{5572781620101201,
Abstract = {In this paper, we propose automatic categorization and summarization of documentaries using subtitles of videos. We propose two methods for video categorization. The first makes unsupervised categorization by applying natural language processing techniques on video subtitles and uses the WordNet lexical database and WordNet domains. The second has the same extraction steps but uses a learning module to categorize. Experiments with documentary videos give promising results in discovering the correct categories of videos. We also propose a video summarization method using the subtitles of videos and text summarization techniques. Significant sentences in the subtitles of a video are identified using these techniques and a video summary is then composed by finding the video parts corresponding to these summary sentences. [ABSTRACT FROM PUBLISHER], Copyright of Journal of Information Science is the property of Sage Publications, Ltd. and its content may not be copied or emailed to multipl)},
Author = {Demirtas, Kezban and  Cicekli, Nihan Kesim and  Cicekli, Ilyas},
ISSN = {01655515},
Journal = {Journal of Information Science},
Keywords = {VIDEOS, NATURAL language processing, DATABASES, SEMANTICS, MULTIMEDIA communications, text summarization, video categorization, video summarization, WordNet domains},
Number = {6},
Pages = {671 - 689},
Title = {Automatic categorization and summarization of documentaries.},
Volume = {36},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=55727816&site=ehost-live&scope=site},
Year = {2010},
}


@article{1997-42754-01119971201,
Abstract = {A lexical modeling methodology was employed to examine how the distribution of phonemic patterns in the lexicon constrains lexical equivalence under conditions of reduced phonetic distinctiveness experienced by speechreaders. The technique involved selection of a phonemically transcribed machine-readable lexical database; definition of transcription rules based on measures of phonetic similarity; application of the transcription rules to a lexical database and formation of lexical equivalence classes; and computation of 3 metrics to examine the transcribed lexicon. The metric percent words unique demonstrated that distribution of words in the language preserves lexical uniqueness across a wide range in the number of potentially available phonemic distinctions. Expected class size demonstrated that if at least 12 phonemic equivalence classes were available, any given word would be highly similar to only a few other words. Percent information extracted provided evidence that high-frequency words tend not to reside in the same lexical equivalence classes as other high-frequency words. The steepness of the functions obtained for each metric shows that small increments in the number of visually perceptible phonemic distinctions can result in substantial changes in lexical uniqueness. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Auer, Edward T. Jr. and  Bernstein, Lynne E.},
ISSN = {0001-4966},
Journal = {Journal of the Acoustical Society of America},
Keywords = {reduced phonetic distinctiveness, phonemic patterns in lexicon & lexical uniqueness using computational model, speechreaders, Automatic Data Processing, Humans, Lipreading, Models, Theoretical, Phonetics, Speech Perception, Vocabulary, Lipreading, Mathematical Modeling, Phonemes, Speech Perception, Words (Phonetic Units)},
Number = {6},
Pages = {3704 - 3710},
Title = {Speechreading and the structure of the lexicon: Computationally modeling the effects of reduced phonetic distinctiveness on lexical uniqueness},
Volume = {102},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=psyh&AN=1997-42754-011&site=ehost-live&scope=site},
Year = {1997},
}


@article{EJ83963420090301,
Abstract = {Multilingual lexicons are needed in various applications, such as cross-lingual information retrieval, machine translation, and some others. Often, these applications suffer from the ambiguity of dictionary items, especially when an intermediate natural language is involved in the process of the dictionary construction, since this language adds its ambiguity to the ambiguity of working languages. This paper aims to propose a new method for producing multilingual dictionaries without the risk of introducing additional ambiguity. As a disambiguated intermediate language we use the so-called Universal Words. A set of more than 200,000 unambiguous Universal Words have been constructed automatically on the basis of the well-known English lexical database WordNet. This approach is being used for the construction of a five language-dictionary in the field of cultural heritage within the framework of the PATRILEX project sponsored by the Spanish Research Council.},
Author = {Boguslavsky, Igor and  Cardenosa, Jesus and  Gallardo, Carolina},
ISSN = {0142-6001},
Journal = {Applied Linguistics},
Keywords = {Translation; Figurative Language; Multilingualism; Dictionaries; Cultural Background; Information Retrieval; Natural Language Processing; Databases; Computational Linguistics},
Number = {1},
Pages = {70 - 92},
Title = {A Novel Approach to Creating Disambiguated Multilingual Dictionaries},
URL = {http://dx.doi.org/10.1093/applin/amn036},
Volume = {30},
Year = {2009/03/01/},
}


@masterthesis{ED51768320090101,
Abstract = {In this study I introduce BioFrameNet, an extension of the Berkeley FrameNet lexical database to the domain of molecular biology. I examine the syntactic and semantic combinatorial possibilities exhibited in the lexical items used in this domain in order to get a better understanding of the grammatical properties of the language used in scientific writings on molecular biology.    The particular data considered is a collection of Gene References in Function (GRIF) texts that describe various types of intracellular protein transport events, a collection that had previously been annotated for an ontologically grounded knowledge base. GRIF texts use long, complex noun phrases, with the omission of many items, resulting in a dense, telegraphic style of writing. This introduces an additional level of complexity to language used in scientific writings of this domain.    In providing a frame semantic analysis and cataloging of the grammatical structures used in the scientific language of molecular biology, we see how well a FrameNet approach can handle language of this domain. Extending FrameNet to this domain serves as a testing ground for some of FrameNet's principles and claims, as it becomes evident how well a FrameNet approach handles language in a significantly different field than has been previously examined. I show how domain ontologies and knowledge bases, sources of definitions and classifications of biological phenomena based entirely on their biological properties, can be used in conjunction with lexical resources. At the same time, I also illustrate the overlap of grammatical properties across separate domain ontology classes, demonstrating that although the biology defined and classified in these classes is different, language used to describe and discuss them is not. Finally, I also explore the possibility that BioFrameNet can be used with tools that carry out Natural Language Processing tasks such as automatic semantic role labeling. Therefore, this work is at the intersection of theoretical frame semantics and practical applications and will potentially provide benefit to linguists, BioNLP engineers, and biologists.    [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]},
Author = {Dolbey, Andrew Eric},
Keywords = {Semantics; Nouns; Grammar; Molecular Biology; Citations (References); Language Processing; Natural Language Processing; Semiotics; Science Education; Universities; Engineering; Reader Text Relationship},
School = {ProQuest LLC},
Title = {BioFrameNet: A FrameNet Extension to the Domain of Molecular Biology},
URL = {http://gateway.proquest.com/openurl?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&res_dat=xri:pqdiss&rft_dat=xri:pqdiss:3411048},
Year = {2009/01/01/},
}


@article{5282620720101015,
Abstract = {Abstract: A novel modeling method for a collection of short text snippets is presented in this paper to measure the similarity between pairs of snippets. The method takes account of both the semantic and statistical information within the short text snippets, and consists of three steps. Given a set of raw short text snippets, it first establishes the initial similarity between words by using a lexical database. The method then iteratively calculates both word similarity and short text similarity. Finally, a proximity matrix is constructed based on word similarity and used to convert the raw text snippets into vectors. Word similarity and text clustering experiments show that the proposed short text modeling method improves the performance of existing text-related information retrieval (IR) techniques. [ABSTRACT FROM AUTHOR], Copyright of Information Sciences is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without)},
Author = {Wenyin, Liu and  Quan, Xiaojun and  Feng, Min and  Qiu, Bite},
ISSN = {00200255},
Journal = {Information Sciences},
Keywords = {MATHEMATICAL models, SEMANTIC computing, STATISTICS, INFORMATION processing, ITERATIVE methods (Mathematics), DATABASES, MATRICES, INFORMATION retrieval, NUMERICAL analysis, Information retrieval, Query expansion, Question answering, Short text similarity, Text mining, Text similarity},
Number = {20},
Pages = {4031 - 4041},
Title = {A short text modeling method combining semantic and statistical information},
Volume = {180},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=52826207&site=ehost-live&scope=site},
Year = {2010},
}


@article{4519761820090601,
Abstract = {The systematic presentation of collocations is increasingly recognized as a very useful addition to specialized reference works. However, few dictionaries or terminological databases actually include this kind of data. More surprisingly still, no method has been designed yet to allow efficient access to and retrieval of specific specialized collocations from electronic reference tools. This article presents two new search paths for accessing and extracting collocations from an English-French specialized lexical database. The paths have been designed according to two specific user-defined situations: (1) translation from L1 to L2; and (2) text production in L2. We exploit a formal semantic encoding of collocations based on Lexical Functions (LFs). LFs allow us to establish an equivalence relationship between collocations that convey the same meaning in different languages without having to link the collocations formally. They also allow us to extract sets of collocations associated wit)},
Author = {L' Homme, Marie-Claude and  Leroyer, Patrick},
ISSN = {09299971},
Journal = {Terminology},
Keywords = {COLLOCATION (Linguistics), SEMANTICS, LEXICOLOGY, TERMS & phrases, LINGUISTICS, TRANSLATING & interpreting, DATABASES, INFORMATION retrieval, collocation, equivalent, lexical function, lexical relation, lexicographic functions, search paths, specialized lexicography, terminology},
Number = {2},
Pages = {258 - 283},
Title = {Combining the semantics of collocations with situation-driven search paths in specialized dictionaries.},
Volume = {15},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=45197618&site=ehost-live&scope=site},
Year = {2009},
}


@article{3664994320090201,
Abstract = {We introduce a new type of lexical structure called lexical system, an interoperable model that can feed both monolingual and multilingual language resources. We begin with a formal characterization of lexical systems as simple directed graphs, solely made up of nodes corresponding to lexical entities and links. To illustrate our approach, we present data borrowed from a lexical system that has been generated from the French DiCo database. We later explain how the compilation of the original dictionary-like database into a net-like one has been made possible. Finally, we discuss the potential of the proposed lexical structure for designing multilingual lexical resources. [ABSTRACT FROM AUTHOR], Copyright of Language Resources & Evaluation is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual )},
Author = {Polguère, Alain},
ISSN = {1574020X},
Journal = {Language Resources & Evaluation},
Keywords = {LEXICON, LINGUISTICS, COMPUTATIONAL linguistics, MULTILINGUALISM, MONOLINGUALISM, DATABASES, Explanatory combinatorial lexicology, Graph model, Lexical database, Lexical function},
Number = {1},
Pages = {41 - 55},
Title = {Lexical systems: graph models of natural language lexicons.},
Volume = {43},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=36649943&site=ehost-live&scope=site},
Year = {2009},
}


@article{3483897420081101,
Abstract = {With the advent of the Internet, billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of nonparametric methods, we explore this world with the aid of a large data set of 79,302,017 images collected from the Web. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution, the images in the data set are stored as 32 × 32 color images. Each image is loosely labeled with one of the 75,062 nonabstract nouns in English, as listed in the Wordnet lexical database. Hence, the image database gives comprehensive coverage of all object categories and scenes. The semantic information from Wordnet can be used in conjunction with the nearest neighbor methods to perform object classification over a range of semantic levels, minimizing the effects of labeling noise. For certain classes that are particularly prevalent in the data set, such as people, we are able to)},
Author = {Torralba, Antonio and  Fergus, Rob and  Freeman, William T.},
ISSN = {01628828},
Journal = {IEEE Transactions on Pattern Analysis & Machine Intelligence},
Keywords = {IMAGE retrieval, ANNOTATIONS, COMPUTER vision, NONPARAMETRIC statistics, NEAREST neighbor analysis (Statistics), PATTERN recognition systems, SAMPLING (Process), Internet images, large data sets, nearest neighbor methods, Object recognition, tiny images},
Number = {11},
Pages = {1958 - 1970},
Title = {80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition.},
Volume = {30},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=34838974&site=ehost-live&scope=site},
Year = {2008},
}


@article{EJ85091520060901,
Abstract = {The complexity of Chinese orthography has hindered the progress of research in Chinese to the same level of sophistication of that in alphabetic languages such as English. Also, there has been no publicly available resource concerning the decomposition of Chinese characters, which is essential in any attempt to model the cognitive processes of Chinese character recognition. Here we report our construction and analysis of a Chinese lexical database containing the most frequent phonetic compounds decomposed into semantic and phonetic radicals according to Chinese etymology. Each radical was further decomposed into basic stroke patterns according to a Chinese transcription system, Cangjie (Chu, 1979 Laboratory of chu Bong-Foo Retrieved August 25, 2004, from http://www.cbflabs.com/). Other information such as pronunciation and character frequency were also incorporated. We examine the distribution of different types of character, the information skew in phonetic compounds, the relations between subcharacter orthographic units and the pronunciation of the entire character, and the processing implications of these phenomena in terms of universal psycholinguistic principles.},
Author = {Hsiao, Janet Hui-wen and  Shillcock, Richard},
ISSN = {0090-6905},
Journal = {Journal of Psycholinguistic Research},
Keywords = {Phonetics; Etymology; Semantics; Romanization; Personality; Character Recognition; Databases; Chinese; Cognitive Processes; Language Processing; Pronunciation; Psycholinguistics},
Number = {5},
Pages = {405 - 426},
Title = {Analysis of a Chinese Phonetic Compound Database: Implications for Orthographic Processing},
URL = {http://dx.doi.org/10.1007/s10936-006-9022-y},
Volume = {35},
Year = {2006/09/01/},
}


@article{EJ73024020040301,
Abstract = {This study reports two experiments assessing the spelling performance of French first graders after 3 months and after 9 months of literacy instruction. The participants were asked to spell high and low frequency irregular words (Experiment 1) and pseudowords, some of which had lexical neighbours (Experiment 2). The lexical database which children had been exposed to was strictly controlled. Both a frequency effect in word spelling accuracy and an analogy effect in pseudoword spelling were obtained after only 3 months of reading instruction. The results suggest that children establish specific orthographic knowledge from the very beginning of literacy acquisition.},
Author = {Martinet, Catherine and  Valdois, Sylviane and  Fayol, Michel},
ISSN = {0010-0277},
Journal = {Cognition},
Keywords = {Grade 1; Reading Instruction; Literacy Education; Spelling; Lexicology; Word Frequency; Foreign Countries; Case Studies; Orthographic Symbols, France, France},
Number = {2},
Pages = {11 - 22},
Title = {Lexical Orthographic Knowledge Develops from the Beginning of Literacy Acquisition},
URL = {http://dx.doi.org/10.1016/j.cognition.2003.09.002},
Volume = {91},
Year = {2004/03/01/},
}


@article{3330340120080501,
Abstract = {This study addresses the extent to which the location of primary stress in Dutch, German, and English monomorphemic words is affected by the syllables preceding the three final syllables. We present analyses of the monomorphemic words in the CELEX lexical database, which showed that penultimate primary stress is less frequent in Dutch and English trisyllabic than quadrisyllabic words. In addition, we discuss paper-and-pencil experiments in which native speakers assigned primary stress to pseudowords. These experiments provided evidence that in all three languages penultimate stress is more likely in quadrisyllabic than in trisyllabic words. We explain this length effect with the preferences in these languages for word-initial stress and for alternating patterns of stressed and unstressed syllables. The experimental data also showed important intra- and interspeaker variation, and they thus form a challenging test case for theories of language variation. [ABSTRACT FROM AUTHOR], Copyrig)},
Author = {Ernestus, Mirjam and  Neijt, Anneke},
ISSN = {00243949},
Journal = {Linguistics},
Keywords = {CELEX (Information retrieval system), LEXICAL grammar, SYLLABLE (Grammar), INTERLANGUAGE (Language learning), ENGLISH language, DUTCH language, GERMAN language},
Number = {3},
Pages = {507 - 540},
Title = {Word length and the location of primary word stress in Dutch, German, and English.},
Volume = {46},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=33303401&site=ehost-live&scope=site},
Year = {2008},
}


@article{2886561020080229,
Abstract = {AbstractÂ Â We address the challenge of semantic gap reduction for image retrieval through an improved support vector machines (SVM)-based active relevance feedback framework, together with a hybrid visual and conceptual content representation and retrieval. We introduce a new feature vector based on projecting the keywords associated to an image on a set of âkey conceptsâ with the help of an external lexical database. We then put forward two improvements of SVM-based relevance feedback method. First, to optimize the transfer of information between the user and the system, we introduce a new active learning selection criterion that minimizes redundancy between the candidate images shown to the user. Second, as most image classes span a wide range of scales in the description space, we argue that the insensitivity of the SVM to the scale of the data is desirable in this context and we show how to obtain it by using specific kernel functions. Experimental evaluations show that the j)},
Author = {Marin  Ferecatu and  Nozha  Boujemaa and  Michel  Crucianu},
ISSN = {09424962},
Journal = {Multimedia Systems},
Keywords = {MULTIMEDIA systems, INFORMATION retrieval, DATABASES, COMPUTER files},
Number = {5/6},
Pages = {309 - 322},
Title = {Semantic interactive image retrieval combining visual and conceptual content description.},
Volume = {13},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=28865610&site=ehost-live&scope=site},
Year = {2008},
}


@article{2726303320070801,
Abstract = {Many recent studies have demonstrated the influence of sublexical frequency measures on language processing, or called for controlling sublexical measures when selecting stimulus material for psycholinguistic studies (Aichert & Ziegler, 2005). The present study discusses which measures should be controlled for in what kind of study, and presents orthographic and phonological syllable, dual unit (bigram and biphoneme) and single unit (letter and phoneme) type and token frequency measures derived from the lemma and word form corpora of the CELEX lexical database (Baayen, Piepenbrock, & Gulikers, 1995). Additionally, we present the SUBLEX software as an adaptive tool for calculating sublexical frequency measures and discuss possible future applications. The measures and the software can be downloaded at www.psychonomic.org. [ABSTRACT FROM AUTHOR], Copyright of Behavior Research Methods is the property of Springer Nature and its content may not be copied or emailed to multiple sites or po)},
Author = {Hofmann, Markus J. and  Stenneken, Prisca and  Conrad, Markus and  Jacobs, Arthur M.},
ISSN = {1554351X},
Journal = {Behavior Research Methods},
Keywords = {LEXICAL phonology, WORD frequency, GERMAN language, ORTHOGRAPHY & spelling, PHONOLOGY (Grammar), SYLLABLE (Grammar)},
Number = {3},
Pages = {620 - 629},
Title = {Sublexical frequency measures for orthographic and phonological units in German.},
Volume = {39},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=27263033&site=ehost-live&scope=site},
Year = {2007},
}


@misc{2736287220070501,
Abstract = {We briefly discuss the origin and development of WordNet, a large lexical database for English. We outline its design and contents as well as its usefulness for Natural Language Processing. Finally, we discuss crosslinguistic WordNets and complementary lexical resources. [ABSTRACT FROM AUTHOR], Copyright of Language Resources & Evaluation is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
Author = {Miller, George and  Fellbaum, Christiane},
Title = {WordNet then and now.},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=27362872&site=ehost-live&scope=site},
Year = {2007},
}


@article{1734447220050701,
Abstract = {Agent technologies represent a promising approach for the integration of interorganizational capabilities across distributed, networked environments. However, knowledge sharing interoperability problems can arise when agents incorporating differing ontologies try to synchronize their internal information. Moreover, in practice, agents may not have a common or global consensus ontology that will facilitate knowledge sharing and integration of functional capabilities. We propose a method to enable agents to develop a local consensus ontology during operation time as needed. By identifying similarities in the ontologies of their peer agents, a set of agents can discover new concepts/relations and integrate them into a local consensus ontology on demand. We evaluate this method, both syntactically and semantically, when forming local consensus ontologies with and without the use of a lexical database. We also report on the effects when several factors, such as the similarity measure, the )},
Author = {Williams, Andrew B. and  Padmanabhan, Anand and  Brian Blake, M.},
ISSN = {10414347},
Journal = {IEEE Transactions on Knowledge & Data Engineering},
Keywords = {ONTOLOGY, COMPUTER service industry, INTERNET industry, KNOWLEDGE management, METAPHYSICS, WEB services, agent-mediated electronic commerce, Ontologies in agent-based information systems},
Number = {7},
Pages = {969 - 981},
Title = {Experimentation with Local Consensus Ontologies with Implications for Automated Service Composition.},
Volume = {17},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=17344472&site=ehost-live&scope=site},
Year = {2005},
}


@article{1560192720040801,
Abstract = {On the basis of calculations using the latest lexical database produced by Amano and Kondo (2000), the fourth edition of a Web-accessible database of characteristics of the 1,945 basic Japanese kanji was produced by including the mathematical concepts of entropy, redundancy, and symmetry and by replacing selected indexes found in previous editions (Tamaoka, Kirsner, Yanase, Miyaoka, & Kawakami, 2002). The kanji database in the fourth edition introduces seven new figures for kanji characteristics: (1) printed frequency, (2) lexical productivity, (3) accumulative lexical productivity, (4) symmetry for lexical productivity, (5) entropy, (6) redundancy, and (7) numbers of meanings for On-readings and Kun-readings. The file of the fourth edition of the kanji database may be downloaded from the Psychonomic Society Web archive, http://www.psychonomics.org/archive/. [ABSTRACT FROM AUTHOR], Copyright of Behavior Research Methods, Instruments, & Computers is the property of Springer Nature and )},
Author = {Tamaoka, Katsuo and  Makioka, Shogo},
ISSN = {07433808},
Journal = {Behavior Research Methods, Instruments, & Computers},
Keywords = {LEXICAL grammar, DATABASES, ELECTRONIC information resources, LEXICOLOGY, LANGUAGE & languages, KANJI},
Number = {3},
Pages = {548 - 558},
Title = {New figures for a Web-accessible database of the 1,945 basic Japanese kanji, fourth edition.},
Volume = {36},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=15601927&site=ehost-live&scope=site},
Year = {2004},
}


@article{1560191620040801,
Abstract = {WordNet, an electronic dictionary (or lexical database), is a valuable resource for computational and cognitive scientists. Recent work on the computing of semantic distances among nodes (synsets) in WordNet has made it possible to build a large database of semantic distances for use in selecting word pairs for psychological research. The database now contains nearly 50,000 pairs of words that have values for semantic distance, associative strength, and similarity based on co-occurrence. Semantic distance was found to correlate weakly with these other measures but to correlate more strongly with another measure of semantic relatedness, featural similarity. Hierarchical clustering analysis suggested that the knowledge structure underlying semantic distance is similar in gross form to that underlying featural similarity. In experiments in which semantic similarity ratings were used, human participants were able to discriminate semantic distance. Thus, semantic distance as derived from W)},
Author = {Maki, Willams S. and  McKinley, Lauren N. and  Thompson, Amber G.},
ISSN = {07433808},
Journal = {Behavior Research Methods, Instruments, & Computers},
Keywords = {ENCYCLOPEDIAS & dictionaries, ELECTRONIC information resources, COMPARATIVE linguistics, WORD (Linguistics), LANGUAGE & languages, SEMANTICS},
Number = {3},
Pages = {421 - 431},
Title = {Semantic distance norms computed from an electronic dictionary (WordNet).},
Volume = {36},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=15601916&site=ehost-live&scope=site},
Year = {2004},
}


@article{1385398920040701,
Abstract = {This paper presents a suite of methods and results for the semantic disambiguation of WordNet glosses. WordNet is a resource widely used in natural language processing and artificial intelligence. Intended and designed as a lexical database, WordNet exhibits some deficiencies when used as a knowledge base. By semantically disambiguating the words in the glosses, we add pointers from each word to its concept or synset, and this increases the connectivity between the WordNet concepts by approximately an order of magnitude. We show how lexical chains and other applications can be built on this richly connected WordNet. The semantic disambiguation of the WordNet glosses is performed using automatic methods based on a set of heuristics. The precision of the semantic annotation is improved by using voting between the disambiguation system described here and another WSD system. The entire WordNet 2.0 has been disambiguated with an overall precision of 86% and is available at . [Copyright &y&)},
Author = {Moldovan, Dan and  Novischi, Adrian},
ISSN = {08852308},
Journal = {Computer Speech & Language},
Keywords = {HUMAN-computer interaction, NEURAL computers, LOGIC machines, SELF-organizing systems},
Number = {3},
Pages = {301 - 317},
Title = {Word sense disambiguation of WordNet glosses},
Volume = {18},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=13853989&site=ehost-live&scope=site},
Year = {2004},
}


@article{886526320021201,
Abstract = {Yamazaki, Ellis, Morrison, and Lambon Ralph in 1997 demonstrated that written and spoken age-of-acquisitions had a stronger effect on the naming latency of single Kanji words than any other variable including familiarity. The present study was designed to reanalyze Yamazaki, et al.'s data, using the ratings of written and spoken age-of-acquisitions and visual and auditory familiarities taken from the NTT lexical database. This analysis showed that visual familiarity exerted a stronger independent effect on naming latency than two types of age-of-acquisitions. [ABSTRACT FROM AUTHOR], Copyright of Perceptual & Motor Skills is the property of Sage Publications Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to th)},
Author = {Shibahara, Naoki and  Kondo, Tadahisa},
ISSN = {00315125},
Journal = {Perceptual & Motor Skills},
Keywords = {VERBAL learning, VISUAL learning, AUDITORY perception, VISUAL perception},
Number = {3},
Pages = {741},
Title = {VARIABLES AFFECTING NAMING LATENCY FOR JAPANESE KANJI: A RE-ANALYSIS OF YAMAZAKI, ET AL. (1997).},
Volume = {95},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=8865263&site=ehost-live&scope=site},
Year = {2002},
}


@article{405082420001201,
Abstract = {Structural ambiguity, particularly attachment of prepositional phrases, is a serious type of global ambiguity in Natural Language. The disambiguation becomes crucial when a syntactic analyzer must make the correct decision among at least two equally grammatical parse-trees for the same sentence. This paper attempts to find answers to the problem of how attachment ambiguity can be resolved by utilizing Machine Learning (ML) techniques. ML is founded on the assumption that the performance in cognitive tasks is based on the similarity of new situations (testing) to stored representations of earlier experiences (training). Therefore, a large amount of training data is an important prerequisite for providing a solution to the problem. A combination of unsupervised and restricted supervised acquisition of such data will be reported. Training is performed both on a subset of the content of the Gothenburg Lexical Database (GLDB), and on instances of large corpora annotated with coarse-grained)},
Author = {Kokkinakis, Dimitrios},
ISSN = {03325865},
Journal = {Nordic Journal of Linguistics (Taylor & Francis)},
Keywords = {AMBIGUITY, MACHINE learning},
Number = {2},
Pages = {191 - 213},
Title = {PP-Attachment Disambiguation for Swedish: Combining Unsupervised and Supervised Training Data.},
Volume = {23},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=4050824&site=ehost-live&scope=site},
Year = {2000},
}


@misc{ED25831219850501,
Abstract = {The results of three projects concerned with auditory word recognition and the structure of the lexicon are reported in this paper. The first project described was designed to test experimentally several specific predictions derived from MACS, a simulation model of the Cohort Theory of word recognition. The second project description provides the results of analyses of the structure and distribution of words in the lexicon using a large lexical database. In this discussion, statistics about similarity spaces for high and low frequency words are applied to previously published data on the intelligibility of words presented in noise, and differences in identification are shown to be related to structural factors about the specific words and the distribution of similar words in  their neighborhoods. Finally, the third project description reports efforts at developing a new theory of word recognition known as the Phonetic Refinement Theory, which was designed to incorporate some of the detailed acoustic-phonetic and phonotactic knowledge that listeners have about the internal structure of words and the organization of words in the lexicon, and about how they use this knowledge in word recognition. (Author/HOD)},
Author = {Pisoni, David B. and  And Others and  Indiana Univ., Bloomington.},
Title = {Speech Perception, Word Recognition and the Structure of the Lexicon. Research on Speech Perception Progress Report No. 10.},
Year = {1985/05/01/},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=eric&AN=ED258312&site=ehost-live&scope=site},
}


@article{1437295019980701,
Abstract = {The article focuses on thematic roles and semantic invariants of lexical derivations. This paper is related to the project called "Semantic Dictionary Viewed As a Lexical Database." In fact, semantics allows to make useful predictions about different aspects of linguistic behaviour of a word--such as, morphological and syntactic combinability restrictions; differences in the aspectual potential of a verb; referential status of arguments; prosody and word order, etc. The language that has been studied here is Russian. In this paper, examples have been translated into English, or comparable English examples have been used, wherever possible.},
Author = {Paducheva, Elena V.},
ISSN = {01654004},
Journal = {Folia Linguistica},
Keywords = {SEMANTICS, MORPHOLOGY (Grammar), LANGUAGE & languages, VERSIFICATION, VERBS, RUSSIAN language},
Number = {3/4},
Pages = {349 - 363},
Title = {Thematic Roles and the Quest for Semantic Invariants of Lexical Derivations.},
Volume = {32},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=14372950&site=ehost-live&scope=site},
Year = {1998},
}


@article{11086978920151201,
Abstract = {At present, there is no comprehensive psycholinguistic database containing Swedish words with ratings of word properties such as e.g. imageability, although researchers carrying out psycholinguistic studies in Swedish face the need to be able to control for and systematically vary such properties. The present study addressed this issue by investigating the possibility of transferring English word ratings to Swedish. Imageability, familiarity and age of acquisition (AoA) ratings were obtained for a sample of Swedish words (N = 99). These ratings were then compared with the corresponding English ratings from the Medical Research Council (MRC) Psycholinguistic Database (Coltheart 1981) using Spearman correlation. Swedish and English word ratings were found to be highly correlated for imageability and AoA, and moderately correlated for familiarity. Following these results, we suggest that, in general, ratings of these variables can be reliably transferred between the two languages, althou)},
Author = {Blomberg, Frida and  Öberg, Carl},
ISSN = {03325865},
Journal = {Nordic Journal of Linguistics (Cambridge University Press)},
Keywords = {SWEDISH language, ENGLISH language, PSYCHOLINGUISTICS, DATABASES, age of acquisition, English, familiarity, imageability, lexical database construction, psycholinguistics, Swedish, word ratings, MEDICAL Research Council (Great Britain)},
Number = {3},
Pages = {351 - 364},
Title = {Swedish and English word ratings of imageability, familiarity and age of acquisition are highly correlated.},
Volume = {38},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=110869789&site=ehost-live&scope=site},
Year = {2015},
}


@article{2008-16607-00620081001,
Abstract = {The semantic annotation of texts with senses from a computational lexicon is a complex and often subjective task. As a matter of fact, the fine granularity of the WordNet sense inventory [Fellbaum, Christiane (ed.). 1998. WordNet: An Electronic Lexical Database MIT Press], a de facto standard within the research community, is one of the main causes of a low inter-tagger agreement ranging between 70% and 80% and the disappointing performance of automated fine-grained disambiguation systems (around 65% state of the art in the Senseval-3 English all-words task). In order to improve the performance of both manual and automated sense taggers, either we change the sense inventory (e.g. adopting a new dictionary or clustering WordNet senses) or we aim at resolving the disagreements between annotators by dealing with the fineness of sense distinctions. The former approach is not viable in the short term, as wide-coverage resources are not publicly available and no large-scale reliable clustering of WordNet senses has been released to date. The latter approach requires the ability to distinguish between subtle or misleading sense distinctions. In this paper, we propose the use of structural semantic interconnections—a specific kind of lexical chains—for the adjudication of disagreed sense assignments to words in context. The approach relies on the exploitation of the lexicon structure as a support to smooth possible divergencies between sense annotators and foster coherent choices. We perform a twofold experimental evaluation of the approach applied to manual annotations from the SemCor corpus, and automatic annotations from the Senseval-3 English all-words competition. Both sets of experiments and results are entirely novel: structural adjudication allows to improve the state-of-the-art performance in all-words disambiguation by 3.3 points (achieving a 68.5% Fl-score) and attains figures around 80% precision and 60% recall in the adjudication of disagreements from human annotators. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Navigli, Roberto},
ISSN = {1351-3249, 1469-8110},
Journal = {Natural Language Engineering},
Keywords = {structural approach, automatic adjudication, word sense disagreements, semantic annotation, text structure, computational lexicon, Connotations, Lexical Access, Semantics, Text Structure, Word Meaning, Adjudication},
Number = {4},
Pages = {547 - 573},
Title = {A structural approach to the automatic adjudication of word sense disagreements},
URL = {navigli@di.uniroma1.it},
Volume = {14},
Year = {2008},
}


@article{2008-04225-00720080301,
Abstract = {Semantic intrusions are inappropriate responses frequently observed in patients with Alzheimer's disease. They belong to the same category as the words to be remembered, but their prototypic value remains largely unexplored. The prototype is the most representative word in a particular lexical category. The prototypic value is measured according to different criteria: written and oral lexical frequency, frequency of use, degree of typicality, degree of familiarity and rank of quotation. The objective of the study was to evaluate the prototypic value of intrusions produced by 17 Alzheimer's patients with mild to severe dementia, during the cued recall of the Grober & Buschke procedure (RL/RI 16 items). The prototypic value was compared to the categorial norms provided by 1) 17 control subjects and 2) the lexical database 'Lexique 3'. The results show that intrusions had a significantly higher prototypic value than targeted items. The prototypic value increased with the progression of the disease, and according to the evaluation criteria used. Thus with the criteria 'frequency of use', 'degree of typicality' and 'degree of familiarity,' the prototypic value increased exponentially with the severity of dementia. In contrast, in spite of the development of the pathology, the prototypic value decreased when assessed by the criteria of 'rank of quotation', and 'lexical frequency' (oral and written). In conclusion, the qualitative analysis of the prototypic value of intrusion errors in Alzheimers opens up new clinical and methodological considerations. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Canolle, Marianne and  Messaoudi, Mouloud and  Ayoub, Bronia and  Descours, Irène and  Bocquet, Patrick and  Gely-Nargeot, Marie-Christine and  Touchon, Jacques},
ISSN = {1760-1703},
Journal = {Psychologie & NeuroPsychiatrie du Vieillissement},
Keywords = {prototypic value, semantic intrusion errors, Alzheimers disease, dementia, cued recall, Aged, Aged, 80 and over, Alzheimer Disease, Databases, Factual, Female, Humans, Male, Middle Aged, Semantics, Speech, Alzheimer's Disease, Dementia, Errors, Semantics, Cued Recall},
Number = {1},
Pages = {67 - 79},
Title = {Valeur prototypique des intrusions sémantiques dans la maladie d'Alzheimer = Prototypic value of semantic intrusion errors in Alzheimer's disease},
URL = {marianne.canolle@ccl.aphp.fr},
Volume = {6},
Year = {2008},
}


@article{2008-08100-00120070701,
Abstract = {Collaborative and content-based filtering are the recommendation techniques most widely adopted to date. Traditional collaborative approaches compute a similarity value between the current user and each other user by taking into account their rating style, that is the set of ratings given on the same items. Based on the ratings of the most similar users, commonly referred to as neighbors, collaborative algorithms compute recommendations for the current user. The problem with this approach is that the similarity value is only computable if users have common rated items. The main contribution of this work is a possible solution to overcome this limitation. We propose a new content-collaborative hybrid recommender which computes similarities between users relying on their content-based profiles, in which user preferences are stored, instead of comparing their rating styles. In more detail, user profiles are clustered to discover current user neighbors. Content-based user profiles play a key role in the proposed hybrid recommender. Traditional keyword-based approaches to user profiling are unable to capture the semantics of user interests. A distinctive feature of our work is the integration of linguistic knowledge in the process of learning semantic user profiles representing user interests in a more effective way, compared to classical keyword-based profiles, due to a sense-based indexing. Semantic profiles are obtained by integrating machine learning algorithms for text categorization, namely a naïve Bayes approach and a relevance feedback method, with a word sense disambiguation strategy based exclusively on the lexical knowledge stored in the WordNet lexical database. Experiments carried out on a content-based extension of the EachMovie dataset show an improvement of the accuracy of sense-based profiles with respect to keyword-based ones, when coping with the task of classifying movies as interesting (or not) for the current user. An experimental session has been also performed in order to evaluate the proposed hybrid recommender system. The results highlight the improvement in the predictive accuracy of collaborative recommendations obtained by selecting like-minded users according to user profiles. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Degemmis, Marco and  Lops, Pasquale and  Semeraro, Giovanni},
ISSN = {0924-1868, 1573-1391},
Journal = {User Modeling and User-Adapted Interaction},
Keywords = {content-collaborative recommender, WordNet-based user profiles, neighborhood formation, content-based filtering, Algorithms, Collaboration, Linguistics, Machine Learning, Semantics},
Number = {3},
Pages = {217 - 255},
Title = {A content-collaborative recommender that exploits WordNet-based user profiles for neighborhood formation},
URL = {degemmis@di.uniba.it, lops@di.uniba.it, semeraro@di.uniba.it, ORCID: 0000-0001-6883-1853},
Volume = {17},
Year = {2007},
}


@article{2006-11337-00320060101,
Abstract = {The Dutch spelling system, like other European spelling systems, represents a certain balance between preserving the spelling of morphemes (the morphological principle) and obeying letter-to-sound regularities (the phonological principle). We present experimental results with artificial learners that show a competition effect between the two principles: adhering more to one principle leads to more violations of the other. The artificial learners, memory-based learning algorithms, are trained (1) to convert written words to their phonemic counterparts and (2) to analyze written words on their morphological composition, based on data extracted from the CELEX lexical database. As an exception to the competition effect we show that introducing the schwa as a letter in the spelling system causes both morphology and phonology to be learnt better by the artificial learners. In general we argue that artificial learning studies are a tool in obtaining objective measurements on a spelling system that may be of help in spelling reform processes. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {van den Bosch, Antal},
ISSN = {1387-6732, 1570-6001},
Journal = {Written Language and Literacy},
Keywords = {Dutch spelling space, phonology, morphology, phonemes, artificial learners, algorithms, learning, Learning, Morphology, Phonology, Spelling, Algorithms},
Number = {1},
Pages = {25 - 44},
Title = {Spelling space: A computational test bed for phonological and morphological changes in Dutch spelling},
URL = {Antal.vdnBosch@uvt.nl},
Volume = {9},
Year = {2006},
}


@article{202084219990701,
Abstract = {Reviews the book `WordNet: An Electronic Lexical Database,' edited by Christiane Fellbaum.},
Author = {Godby, Carol Jean},
ISSN = {00242519},
Journal = {Library Quarterly},
Keywords = {WORDNET (Book)},
Number = {3},
Pages = {406},
Title = {Reviews.},
Volume = {69},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=2020842&site=ehost-live&scope=site},
Year = {1999},
}


@article{2003-03926-04320030201,
Abstract = {Presents an errata to an original article by N. Shibahara and T. V. Kondo entitled 'Variables affecting naming for Japanese Kanji: A reanalysis of Yamazaki, et al. (1997)' which appeared in Perceptual and Motor Skills, (2002), 95, pp. 741-745. The first footnote is incomplete. The complete information is provided. (The following abstract of this article originally appeared in record [rid]2003-04086-006[/rid].) M. Yamazaki, A. W. Ellis, C. M. Morrison, and M. A. L. Lambon Ralph in 1997 (see record [rid]1997-05966-008[/rid]) demonstrated that written and spoken age-of-acquisitions had a stronger effect on the naming latency of single Kanji words than any other variable including familiarity. The present study was designed to reanalyze M. Yamazaki, et al.'s data, using the ratings of written and spoken age-of-acquisitions and visual and auditory familiarities taken from the Nippon Telephone and Telegram Corporation lexical database. This analysis showed that visual familiarity exerted a stronger independent effect on naming latency than two types of age-of-acquisitions. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Shibahara, N. and  Kondo, T. V.},
ISSN = {0031-5125, 1558-688X},
Journal = {Perceptual and Motor Skills},
Keywords = {naming latency, visual familiarity, Japanese Kanji, age of acquisition, auditory familiarity, writing development, speech development, Naming, Speech Development, Word Recognition, Words (Phonetic Units), Written Language},
Number = {1},
Title = {Variables affecting naming latency for Japanese Kanji: A reanalysis of Yamazaki, et al (1997): Erratum},
URL = {n.shibahara@ucl.ac.uk},
Volume = {96},
Year = {2003},
}


@article{2001-11497-00120010901,
Abstract = {Results of the noun–verb pair comprehension and production tests from the Test Battery for Auslan Morphology and Syntax (A. Schembri et al., 2000) are presented, reanalyzed, and compared to data from 2 other cases dealing with noun–verb pairs: the Auslan lexical database and a comparison of Auslan and American Sign Language (ASL) signs. The data confirm the existence of formationally related noun–verb pairs in Auslan in which the verb displays a single movement and the noun displays a repeated movement. The data also suggest that the best exemplars of noun–verb pairs of this type in Auslan form a distinct set of iconic (mimetic) signs archetypically based on inherently reversible actions (such as opening and shutting). This strong iconic link perhaps explains why the derivational process appears to be of limited productivity, though it does appear to have 'spread' to a number of signs that appear to have no such iconicity. There appears to be considerable variability in the use of the derivational markings, particularly in connected discourse, even for signs of the 'open and shut' variety. Overall, the derivational process is apparently still closely linked to an iconic base, is incipient in the grammar of Auslan, and is best described as only partially grammaticalized. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Johnston, Trevor},
ISSN = {1081-4159, 1465-7325},
Journal = {Journal of Deaf Studies and Deaf Education},
Keywords = {verb-noun pair comprehension & production, Auslan, American Sign Language, iconicity of signs, grammaticalized signs, Grammar, Sign Language, Nouns, Verbs},
Number = {4},
Pages = {235 - 257},
Title = {Nouns and verbs in Australian sign language: An open and shut case?},
URL = {rctaj@alinga.newcastle.edu.au},
Volume = {6},
Year = {2001},
}


@article{1997-06868-00119970101,
Abstract = {Studied implementational choices in pronunciation by analogy (PbA) for English to assess its ultimate suitability - both as a model of the human process of reading aloud, and as a component of a text-to-speech (TTS) system. The variables studied were the specific lexical database used as the basis of the analogy process, the way of ranking/scoring candidate pronunciations, and the effect of manual vs automatic alignment of letters and phonemes. When tested with short (monosyllabic) pseudowords, the lowest error rate achieved was 14.3%. This suggests that the current PbA systems are at best poor models of pseudoword pronunciation by humans. When tested with lexical words temporarily removed from the dictionary, the best performance obtained was 93.5% phonemes correct for a 16,280-word dictionary. This was superior to the 25.7% words correct obtained using a set of popular letter-to-sound rules, indicating considerable scope for analogy methods to be exploited in future TTS systems. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Damper, R. I. and  Eastmond, J. F. G.},
ISSN = {0023-8309, 1756-6053},
Journal = {Language and Speech},
Keywords = {basic implementational choices, performance of pronunciation by analogy as model of human process of reading aloud & component of text-to-speech systems, Analogy, Automated Speech Recognition, Models, Oral Reading, Pronunciation, Computer Simulation, Lexical Access, Reasoning},
Number = {1},
Pages = {1 - 23},
Title = {Pronunciation by analogy: Impact of implementational choices on performance},
Volume = {40},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=psyh&AN=1997-06868-001&site=ehost-live&scope=site},
Year = {1997},
}


@article{1994-31765-00119931201,
Abstract = {Describes a synthesis-by-analogy system which is a model of novel-word pronunciation by humans. It uses analogy in both orthographic and phonological domains and is applied to the pronunciation of novel words in British English and German. A major part of this cross-language study concerned the impact of implementational choices on performance, where this was defined as the ability of the system to produce pronunciations in line with those given by humans. The size and content of the lexical database on which any analogy system must be based were also considered. The better performing implementations produced useful results for both British English and German. However, best results for each of the 2 languages were obtained from different implementations. The system described is also a psychological model of reading aloud. (German & French abstracts) (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
Author = {Sullivan, K. P. H. and  Damper, R. I.},
ISSN = {0167-6393, 1872-7182},
Journal = {Speech Communication},
Keywords = {implementation choice, synthesis by analogy computer system modeling of novel word pronunciation in British English vs German, conference presentation, Computer Simulation, Pronunciation, Synthetic Speech, Words (Phonetic Units)},
Number = {3-4},
Pages = {441 - 452},
Title = {Novel-word pronunciation: A cross-language study},
Volume = {13},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=psyh&AN=1994-31765-001&site=ehost-live&scope=site},
Year = {1993},
}


@article{2017-35600-00320160101,
Abstract = {It has been frequently noted in the literature that content words need to consist of at least three letters; this observation is commonly dubbed 'three letter rule.' However, a survey of the CELEX database (Baayen et al. 1995) shows that there are (nearly) no content words in English and German that begin with two or more consonant letters and end in a single vowel letter. Words such as [bruː] are not spelt < bru > but < brew > with an additional letter. These findings cannot be accounted for by the three letter rule but they are explicable within a supra-segmental theory of graphematics that includes graphematic feet and graphematic weight: a well-formed graphematic word consists of at least one graphematic foot that in turn consists of at least one heavy graphematic syllable. This paper offers a data-based survey in order to answer the question whether there is a suprasegmental minimality constraint for monosyllabic graphematic words in English and German. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
Author = {Evertz, Martin},
ISSN = {1387-6732, 1570-6001},
Journal = {Written Language and Literacy},
Keywords = {Graphematic hierarchy, graphematic foot, graphematic syllable, graphematic weight, minimal words, lexical database, CELEX, English, German, Language, Lexical Access, Word Associations, Foreign Language Translation, Letters (Alphabet), Vowels},
Number = {2},
Pages = {189 - 211},
Title = {Minimal graphematic words in English and German: Lexical evidence for a theory of graphematic feet},
URL = {martin.evertz@uni-koeln.de},
Volume = {19},
Year = {2016},
}


@article{1992-22350-00119911201,
Abstract = {Discusses principles of lexical semantics developed in the course of building an on-line lexical database. The approach is relational rather than componential. The fundamental semantic relation is synonymy, which is required in order to define the lexicalized concepts that words can be used to express. Other semantic relations between these concepts are then described. No single set of semantic relations or organizational structure is adequate for the entire lexicon: nouns, adjectives, and verbs each have their own semantic relations and their own organization determined by the role they must play in the construction of linguistic messages. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Miller, George A. and  Fellbaum, Christiane},
ISSN = {0010-0277, 1873-7838},
Journal = {Cognition},
Keywords = {principles of lexical semantic memory & semantic relations & networks of English, Attention, Cognition, Communication, Concept Formation, Databases, Bibliographic, Humans, Online Systems, Psycholinguistics, Reading, Semantics, Linguistics, Semantic Memory},
Number = {1-3},
Pages = {197 - 229},
Title = {Semantic networks of English},
Volume = {41},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=psyh&AN=1992-22350-001&site=ehost-live&scope=site},
Year = {1991},
}


@article{3688155720090301,
Abstract = {Multilingual lexicons are needed in various applications, such as cross-lingual information retrieval, machine translation, and some others. Often, these applications suffer from the ambiguity of dictionary items, especially when an intermediate natural language is involved in the process of the dictionary construction, since this language adds its ambiguity to the ambiguity of working languages. This paper aims to propose a new method for producing multilingual dictionaries without the risk of introducing additional ambiguity. As a disambiguated intermediate language we use the so-called Universal Words. A set of more than 200,000 unambiguous Universal Words have been constructed automatically on the basis of the well-known English lexical database WordNet. This approach is being used for the construction of a five language-dictionary in the field of cultural heritage within the framework of the PATRILEX project sponsored by the Spanish Research Council. [ABSTRACT FROM AUTHOR], Copyr)},
Author = {Igor  Boguslavsky and  Jesús  Cardeñosa and  Carolina  Gallardo},
ISSN = {01426001},
Journal = {Applied Linguistics},
Keywords = {APPLIED linguistics, POLYGLOT dictionaries, ENCYCLOPEDIAS & dictionaries, LANGUAGE & languages -- Dictionaries},
Number = {1},
Pages = {70 - 70},
Title = {A Novel Approach to Creating Disambiguated Multilingual Dictionaries.},
Volume = {30},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=36881557&site=ehost-live&scope=site},
Year = {2009},
}


@masterthesis{2002-95003-10820020201,
Abstract = {This thesis investigates the perceptual categories associated with contrasting pitch accents in Tokyo Japanese. Various aspects of pitch movements throughout the words are systematically varied in order to determine what aspects of the pitch contour affect categorization of words on the basis of accent. In addition, this thesis also investigates the effect on categorization of loss of pitch due to lack of voicing in various parts of the word. The model determined by these studies reveals two more-or-less orthogonal perceptual dimensions; pitch alignment with speech segments determines accent location and the amount of pitch drop determines accent presence. This study also investigates how to quantify the distinctive function of pitch accent in a way which incorporates the frequency of the contrasting items, as well as the peculiar category structure of accents. This model was applied in the analysis of a large-scale lexical database, revealing many irregularities in the distribution and use of accents. Comparing this quantification of the lexical use of accent with the perceptual experiments shows that accent-location detection is functionally more fundamental than accent-presence detection in short, 2-mora words. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Kitahara, Mafuyu},
Keywords = {category structure, pitch accent, Tokyo Japanese, Speech Characteristics, Speech Pitch},
School = {ProQuest Information & Learning},
Title = {Category structure and function of pitch accent in Tokyo Japanese},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=psyh&AN=2002-95003-108&site=ehost-live&scope=site},
Year = {2002},
}


@article{3114184720031001,
Abstract = {The article discusses a study on exploration of dictionary corpus of two-handed signs in American Sign Language (ASL), which tests the generalizability of two conditions, Symmetry Condition and Dominance Condition. It examines the full range of two-handed signs. The study takes into consideration that many predicates of motion that occur in ordinary conversation use two hands moving independently. It also notes that sometimes a sign has two distinct parts, where each part is of a different type. According to the authors, the conditions uncovered in the study might be morpheme structure constraints or follow from the physiological limitations of hands in motion. They conclude that signs group in a similar way but with some differences once the conditions are properly understood.},
Author = {Napoli, Donna Jo and  Wu, Jeff},
ISSN = {13879316},
Journal = {Sign Language & Linguistics},
Keywords = {AMERICAN Sign Language, CORPORA (Linguistics), SIGN language, MORPHEMICS, CONVERSATION, SIGNS & symbols, MORPHOLOGY (Grammar), LINGUISTIC analysis, PSYCHOLOGY of movement, American Sign Language, handshape changes, handshapes, lexical database, morphology, movement, physiological constraints, symmetry},
Number = {2},
Pages = {123 - 205},
Title = {Morpheme structure constraints on two-handed signs in American Sign Language.},
Volume = {6},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=31141847&site=ehost-live&scope=site},
Year = {2003},
}


@article{368085120000901,
Abstract = {Reviews the book 'WordNet: An electronic lexical database,' edited by Christiane Fellbaum.},
Author = {Kilgarriff, Adam},
ISSN = {00978507},
Journal = {Language},
Keywords = {WORDNET (Book)},
Number = {3},
Pages = {706},
Title = {WordNet (Book Review).},
Volume = {76},
URL = {http://search.ebscohost.com.proxy.missouristate.edu/login.aspx?direct=true&db=a9h&AN=3680851&site=ehost-live&scope=site},
Year = {2000},
}


@article{2007-06890-00220070101,
Abstract = {Age of acquisition of a word and familiarity are important factors for the lexical processing, in production as in perception. To help developing research on the mechanisms underlying the lexical processing in French, a lexical data base was built for a corpus of 1225 monosyllabic and disyllabic French words. This article describes the method used to collect the data, the rough information obtained with the survey, explains the method that was used to process the rough information, describes the content of the lexical data base CHACQFAM, obtained after the rough data has been processed, and describes the validation procedure of its content. CHACQFAM is made freely available to researchers in an electronic format, from the website 'http://psycholinguistique.unige.ch/'. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
Author = {Lachaud, Christian Michel},
ISSN = {0003-5033, 1955-2580},
Journal = {L'Année Psychologique},
Keywords = {age of acquisition, familiarity, monosyllabic French words, bisyllabic French words, lexical processing, lexical database, Databases, Familiarity, Learning, Syllables, Words (Phonetic Units), Cognitive Processes},
Number = {1},
Pages = {39 - 63},
Title = {CHACQFAM: Une base de données renseignant l'âge d'acquisition estimé et la familiarité pour 1225 mots monosyllabiques et bisyllabiques du français = CHACQFAM: A lexical data base for the estimated age of acquisition and familiarity of 1225 monosyllabic and bisyllabic French words},
URL = {Christian.Lachaud@pse.unige.ch},
Volume = {107},
Year = {2007},
}

