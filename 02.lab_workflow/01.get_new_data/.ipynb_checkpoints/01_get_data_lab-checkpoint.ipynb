{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0d51c97-d210-4144-8a37-0ec1565e8dfe",
   "metadata": {},
   "source": [
    "# Step 1 Get New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28404902-93fc-4036-ac57-dd9732b24ffd",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac43c52a-a27b-47e9-9cb4-f2def55a006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import math, requests, pandas as pd, re\n",
    "import textwrap\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717aa7aa-18ae-4883-87b5-7e6e2143ad54",
   "metadata": {},
   "source": [
    "## OpenAlex\n",
    "\n",
    "We will use OpenAlex to mimic the searches from the original LAB within EBSCO given the overlap in content and moving to a reproducible pipeline. This open resource includes the journals from previously investigated searches including Behavior Research Methods, Language Resources and Evaluation, and PLoS One. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fde4f06a-6028-4b1d-9881-4ae3bb796006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total=1641, per_page=200, pages=9\n",
      "page 9/9… collected 1641\n",
      "Done. Rows fetched: 1641\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>doi</th>\n",
       "      <th>venue</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>openalex_id</th>\n",
       "      <th>is_oa</th>\n",
       "      <th>cited_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>On the predictive validity of various corpus-b...</td>\n",
       "      <td>2018</td>\n",
       "      <td>10.3758/s13428-017-1001-8</td>\n",
       "      <td>Behavior Research Methods</td>\n",
       "      <td>Xiaocong Chen; Yanping Dong; Xiufen Yu</td>\n",
       "      <td>None</td>\n",
       "      <td>[Lexical diversity, Computer science, Lemma (b...</td>\n",
       "      <td>https://openalex.org/W2784175655</td>\n",
       "      <td>True</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Predicting Lexical Norms: A Comparison between...</td>\n",
       "      <td>2018</td>\n",
       "      <td>10.5334/joc.50</td>\n",
       "      <td>Journal of Cognition</td>\n",
       "      <td>Hendrik Vankrunkelsven; Steven Verheyen; Gert ...</td>\n",
       "      <td>In two studies we compare a distributional sem...</td>\n",
       "      <td>[Concreteness, Word Association, Word (group t...</td>\n",
       "      <td>https://openalex.org/W2902591385</td>\n",
       "      <td>True</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Psycholinguistic norms for more than 300 lexic...</td>\n",
       "      <td>2021</td>\n",
       "      <td>10.3758/s13428-020-01524-y</td>\n",
       "      <td>Behavior Research Methods</td>\n",
       "      <td>Patrick C. Trettenbrein; Nina-Kristin Pendzich...</td>\n",
       "      <td>Sign language offers a unique perspective on t...</td>\n",
       "      <td>[Iconicity, Age of Acquisition, German, Comput...</td>\n",
       "      <td>https://openalex.org/W2980432777</td>\n",
       "      <td>True</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Norms of conceptual familiarity for 3,596 Fren...</td>\n",
       "      <td>2018</td>\n",
       "      <td>10.3758/s13428-018-1106-8</td>\n",
       "      <td>Behavior Research Methods</td>\n",
       "      <td>Georges Chedid; Maximiliano A. Wilson; Christo...</td>\n",
       "      <td>None</td>\n",
       "      <td>[Noun, Linguistics, Lexical decision task, Psy...</td>\n",
       "      <td>https://openalex.org/W2888587255</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Norm It! : Lexical Normalization for Italian a...</td>\n",
       "      <td>2020</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>Rob van der Goot; Alan Ramponi; Tommaso Casell...</td>\n",
       "      <td>None</td>\n",
       "      <td>[Computer science, Normalization (sociology), ...</td>\n",
       "      <td>https://openalex.org/W3026608847</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  year  \\\n",
       "0  On the predictive validity of various corpus-b...  2018   \n",
       "1  Predicting Lexical Norms: A Comparison between...  2018   \n",
       "2  Psycholinguistic norms for more than 300 lexic...  2021   \n",
       "3  Norms of conceptual familiarity for 3,596 Fren...  2018   \n",
       "4  Norm It! : Lexical Normalization for Italian a...  2020   \n",
       "\n",
       "                          doi                      venue  \\\n",
       "0   10.3758/s13428-017-1001-8  Behavior Research Methods   \n",
       "1              10.5334/joc.50       Journal of Cognition   \n",
       "2  10.3758/s13428-020-01524-y  Behavior Research Methods   \n",
       "3   10.3758/s13428-018-1106-8  Behavior Research Methods   \n",
       "4                                                   None   \n",
       "\n",
       "                                             authors  \\\n",
       "0             Xiaocong Chen; Yanping Dong; Xiufen Yu   \n",
       "1  Hendrik Vankrunkelsven; Steven Verheyen; Gert ...   \n",
       "2  Patrick C. Trettenbrein; Nina-Kristin Pendzich...   \n",
       "3  Georges Chedid; Maximiliano A. Wilson; Christo...   \n",
       "4  Rob van der Goot; Alan Ramponi; Tommaso Casell...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0                                               None   \n",
       "1  In two studies we compare a distributional sem...   \n",
       "2  Sign language offers a unique perspective on t...   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [Lexical diversity, Computer science, Lemma (b...   \n",
       "1  [Concreteness, Word Association, Word (group t...   \n",
       "2  [Iconicity, Age of Acquisition, German, Comput...   \n",
       "3  [Noun, Linguistics, Lexical decision task, Psy...   \n",
       "4  [Computer science, Normalization (sociology), ...   \n",
       "\n",
       "                        openalex_id  is_oa  cited_by  \n",
       "0  https://openalex.org/W2784175655   True        50  \n",
       "1  https://openalex.org/W2902591385   True        57  \n",
       "2  https://openalex.org/W2980432777   True        18  \n",
       "3  https://openalex.org/W2888587255   True        20  \n",
       "4  https://openalex.org/W3026608847  False        14  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = (\n",
    "    \"https://api.openalex.org/works?\"\n",
    "    \"filter=title_and_abstract.search:lexical+database+OR+lexical+norms+OR+linguistic+database+OR+linguistic+norms,\"\n",
    "    \"publication_year:2018-2025,\"\n",
    "    \"type:types/article|types/dataset|types/preprint|types/supplementary-materials|types/report|types/book-chapter\"\n",
    "    \"&sort=relevance_score:desc\"\n",
    "    \"&per_page=200\"     # bump page size to reduce calls\n",
    ")\n",
    "\n",
    "def decode_abstract(inv):\n",
    "    if not isinstance(inv, dict) or not inv: return None\n",
    "    pos2tok = {p:t for t,ps in inv.items() for p in ps}\n",
    "    txt = \" \".join(pos2tok.get(i,\"\") for i in range(max(pos2tok)+1))\n",
    "    txt = re.sub(r\"\\s+([,.!?;:])\", r\"\\1\", txt)\n",
    "    return re.sub(r\"\\s{2,}\", \" \", txt).strip() or None\n",
    "\n",
    "# probe for total\n",
    "probe = requests.get(base_url + \"&page=1\", timeout=30)\n",
    "probe.raise_for_status()\n",
    "meta = probe.json()[\"meta\"]\n",
    "total, per_page = meta[\"count\"], meta[\"per_page\"]\n",
    "pages = math.ceil(total / per_page)\n",
    "print(f\"total={total}, per_page={per_page}, pages={pages}\")\n",
    "\n",
    "rows = []\n",
    "for p in range(1, pages+1):\n",
    "    r = requests.get(base_url + f\"&page={p}\", timeout=60)\n",
    "    r.raise_for_status()\n",
    "    for w in r.json().get(\"results\", []):\n",
    "        rows.append({\n",
    "            \"title\": w.get(\"title\"),\n",
    "            \"year\": w.get(\"publication_year\"),\n",
    "            \"doi\": (w.get(\"doi\") or \"\").replace(\"https://doi.org/\", \"\"),\n",
    "            \"venue\": ((w.get(\"primary_location\") or {}).get(\"source\") or {}).get(\"display_name\"),\n",
    "            \"authors\": \"; \".join(a[\"author\"][\"display_name\"] for a in w.get(\"authorships\", [])),\n",
    "            \"abstract\": decode_abstract(w.get(\"abstract_inverted_index\")),\n",
    "            # OpenAlex doesn’t store author-entered keywords; concepts are the closest proxy\n",
    "            \"keywords\": [c[\"display_name\"] for c in w.get(\"concepts\", [])],\n",
    "            \"openalex_id\": w.get(\"id\"),\n",
    "            \"is_oa\": (w.get(\"open_access\") or {}).get(\"is_oa\"),\n",
    "            \"cited_by\": w.get(\"cited_by_count\", 0),\n",
    "        })\n",
    "    print(f\"page {p}/{pages}… collected {len(rows)}\", end=\"\\r\")\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"\\nDone. Rows fetched:\", len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3c9885-0772-446a-8459-b15242722a29",
   "metadata": {},
   "source": [
    "## Examine Abstracts\n",
    "\n",
    "We need to examine if all articles have abstracts for being able to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9203be1a-4e9c-483c-b6f4-2d45b61d1819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 1641\n",
      "With abstract: 1623 (98.9%)\n",
      "Missing abstract: 18\n",
      "\n",
      "Examples missing abstracts:\n",
      "• 2018 | On the predictive validity of various corpus-based frequency norms in L2 English lexical processing | Behavior Research Methods | DOI: 10.3758/s13428-017-1001-8\n",
      "• 2018 | Norms of conceptual familiarity for 3,596 French nouns and their contribution in lexical decision | Behavior Research Methods | DOI: 10.3758/s13428-018-1106-8\n",
      "• 2020 | Norm It! : Lexical Normalization for Italian and Its Downstream Effects for Dependency Parsing |  | DOI: —\n",
      "• 2022 | Translation norms for Malay and English words: The effects of word class, semantic variability, lexical characteristics, | Behavior Research Methods | DOI: 10.3758/s13428-022-01977-3\n",
      "• 2019 | LEXICAL QUANTOR GENESIS VS LANGUAGE NORM DYNAMICS |  | DOI: 10.36059/978-966-397-124-7/39-56\n",
      "• 2023 | Lexical Norms in Business, Informal and Internet Communication | Studies in systems, decision and control | DOI: 10.1007/978-3-031-27506-7_4\n",
      "• 2019 | The Hebrew Web MB-CDI-WG Norms: Lexical Development of Hebrew-speaking Toddlers |  | DOI: 10.17605/osf.io/vu2na\n",
      "• 2021 | 1aSC6 - Words matter: Analyzing lexical effects on recognition and effort using normed word stimuli |  | DOI: 10.26226/morressier.606f15dd30a2e980041f238c\n",
      "• 2018 | Les normes lexicales émotionnelles | HAL (Le Centre pour la Communication Scientifique Directe) | DOI: —\n",
      "• 2023 | THE STUDY OF LEXICAL NORMS AND THEIR SPECIFIC FEATURES | Zenodo (CERN European Organization for Nuclear Research) | DOI: 10.5281/zenodo.7503056\n",
      "• 2023 | THE STUDY OF LEXICAL NORMS AND THEIR SPECIFIC FEATURES | Zenodo (CERN European Organization for Nuclear Research) | DOI: 10.5281/zenodo.7508859\n",
      "• 2021 | LEXICAL NORMS OF THE GOLDEN AND SILVER AGES OF RUSSIAN POETRY | Libri Magistri | DOI: 10.52172/2587-6945_2021_17_3_13\n",
      "• 2019 | A study of lexical and semantic deviation from the norm of tāleb āmoli sonnets |  | DOI: —\n",
      "• 2022 | Variation and dynamics of lexical norms in the regional usus of the 18th century Russian language | Slavic historical lexicology and lexicography | DOI: 10.30842/26583755202202\n",
      "• 2024 | Language in the Church: Orthodox Religious Terminology in Polish and the Role of Translations in Establishing Lexical No |  | DOI: 10.47743/phss-2024-0012\n",
      "• 2019 | Norms of concept familiarity and emotional valence for 3,596 French nouns and their contribution in lexical decision | HAL (Le Centre pour la Communication Scientifique Directe) | DOI: —\n",
      "• 2019 | Psycholinguistic norms for more than 300 lexical manual signs in German Sign Language (DGS) | Theoretical Issues Sign Language Research | DOI: —\n",
      "• 2024 | A SET OF TASKS FOR MASTERING THE LEXICAL NORMS OF PROFESSIONAL SPEECH: AN EXAMPLE OF A PRACTICAL LESSON IN A MEDICAL HIG | Humanities science current issues | DOI: 10.24919/2308-4863/74-2-59\n"
     ]
    }
   ],
   "source": [
    "def summarize_abstracts(df: pd.DataFrame, n_show: int = 5):\n",
    "    # treat empty strings/whitespace as missing\n",
    "    has_abs = df[\"abstract\"].astype(\"string\").str.strip().ne(\"\").fillna(False)\n",
    "\n",
    "    total = len(df)\n",
    "    with_abs = int(has_abs.sum())\n",
    "    without_abs = total - with_abs\n",
    "    pct = (with_abs / total * 100) if total else 0.0\n",
    "\n",
    "    print(f\"Total rows: {total}\")\n",
    "    print(f\"With abstract: {with_abs} ({pct:.1f}%)\")\n",
    "    print(f\"Missing abstract: {without_abs}\")\n",
    "\n",
    "    if without_abs:\n",
    "        # show a few examples that are missing\n",
    "        missing = df.loc[~has_abs, [\"title\", \"year\", \"doi\", \"venue\", \"openalex_id\"]].head(n_show)\n",
    "        print(\"\\nExamples missing abstracts:\")\n",
    "        for _, r in missing.iterrows():\n",
    "            print(\"•\", r[\"year\"], \"|\", (r[\"title\"] or \"\")[:120].rstrip(), \"|\", r[\"venue\"] or \"\", \"| DOI:\", r[\"doi\"] or \"—\")\n",
    "\n",
    "    return has_abs\n",
    "\n",
    "# Run the summary on your df\n",
    "has_abs_mask = summarize_abstracts(df, n_show=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f546cb6d-e3f6-423b-b174-c41ca38c4007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- polite headers (some APIs appreciate a contact) ----\n",
    "CONTACT_EMAIL = \"ebuchanan@harrisburgu.edu\"  # set yours\n",
    "HEADERS = {\"Accept\": \"application/json\", \"User-Agent\": f\"LAB-abstract-enricher ({CONTACT_EMAIL})\"}\n",
    "\n",
    "def _clean_doi(doi: str) -> str:\n",
    "    if not doi: return \"\"\n",
    "    doi = doi.strip()\n",
    "    return re.sub(r\"^https?://(dx\\.)?doi\\.org/\", \"\", doi, flags=re.I)\n",
    "\n",
    "def safe_request(url, params=None, headers=None):\n",
    "    try:\n",
    "        r = requests.get(url, params=params, headers=headers, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        return r.json(), None\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        return None, f\"HTTP {r.status_code}: {r.text[:200]}\"\n",
    "    except Exception as e:\n",
    "        return None, f\"Other error: {str(e)}\"\n",
    "\n",
    "def fetch_crossref_abstract(doi):\n",
    "    doi = _clean_doi(doi)\n",
    "    url = f\"https://api.crossref.org/works/{doi}\"\n",
    "    js, err = safe_request(url, headers=HEADERS)\n",
    "    if err: return None, \"ERROR\", err\n",
    "    abs_ = (js.get(\"message\") or {}).get(\"abstract\")\n",
    "    if abs_:\n",
    "        # strip tags\n",
    "        abs_ = re.sub(r\"<[^>]+>\", \"\", abs_)\n",
    "        return abs_.strip(), \"Crossref\", None\n",
    "    return None, \"MISSING\", None\n",
    "\n",
    "def fetch_europepmc_abstract(doi):\n",
    "    url = \"https://www.ebi.ac.uk/europepmc/webservices/rest/search\"\n",
    "    params = {\"query\": f\"DOI:{doi}\", \"format\": \"json\", \"pageSize\": 1}\n",
    "    js, err = safe_request(url, params=params, headers=HEADERS)\n",
    "    if err: return None, \"ERROR\", err\n",
    "    res = js.get(\"resultList\", {}).get(\"result\", [])\n",
    "    if res and res[0].get(\"abstractText\"):\n",
    "        return res[0][\"abstractText\"], \"EuropePMC\", None\n",
    "    return None, \"MISSING\", None\n",
    "\n",
    "def get_abstract_by_doi(doi):\n",
    "    doi = _clean_doi(doi)\n",
    "    if not doi: return None, \"MISSING\", None\n",
    "    # try Crossref then Europe PMC\n",
    "    for fetcher in (fetch_crossref_abstract, fetch_europepmc_abstract):\n",
    "        abs_, status, err = fetcher(doi)\n",
    "        if status == \"ERROR\":  # API error\n",
    "            return None, status, err\n",
    "        if status != \"MISSING\":  # success\n",
    "            return abs_, status, None\n",
    "    return None, \"MISSING\", None\n",
    "\n",
    "def enrich_missing_abstracts(df, doi_col=\"doi\", abs_col=\"abstract\", sleep=0.3):\n",
    "    \"\"\"\n",
    "    For rows where df[abs_col] is empty, try to fetch an abstract by DOI.\n",
    "    Prints status for each attempt; only writes into df[abs_col] on success.\n",
    "    Expects get_abstract_by_doi() -> (abstract, status, err).\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import pandas as pd\n",
    "\n",
    "    if abs_col not in df.columns:\n",
    "        df[abs_col] = None\n",
    "\n",
    "    mask_missing = df[abs_col].isna() | (df[abs_col].astype(str).str.strip() == \"\")\n",
    "    idxs = df.index[mask_missing].tolist()\n",
    "\n",
    "    for i in idxs:\n",
    "        doi = str(df.at[i, doi_col] or \"\").strip()\n",
    "        if not doi:\n",
    "            print(f\"[Row {i}] No DOI, skipping.\")\n",
    "            continue\n",
    "\n",
    "        abs_, status, err = get_abstract_by_doi(doi)\n",
    "\n",
    "        if status == \"ERROR\":\n",
    "            print(f\"[Row {i}] DOI {doi}: API error -> {err}\")\n",
    "        elif status == \"MISSING\":\n",
    "            print(f\"[Row {i}] DOI {doi}: No abstract found.\")\n",
    "        else:\n",
    "            print(f\"[Row {i}] DOI {doi}: Abstract found via {status}.\")\n",
    "            df.at[i, abs_col] = abs_\n",
    "\n",
    "        time.sleep(sleep)  # be polite to APIs\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a2abead-a329-445b-bc75-3eed702e0e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing before: 18 of 1641\n"
     ]
    }
   ],
   "source": [
    "# df = <your dataframe from OpenAlex>\n",
    "# Summarize before:\n",
    "missing_before = df[\"abstract\"].isna() | (df[\"abstract\"].astype(str).str.strip() == \"\")\n",
    "print(\"Missing before:\", int(missing_before.sum()), \"of\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9100628c-fe24-43e9-8f43-9c43b0dd4e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 0] DOI 10.3758/s13428-017-1001-8: No abstract found.\n",
      "[Row 3] DOI 10.3758/s13428-018-1106-8: No abstract found.\n",
      "[Row 4] No DOI, skipping.\n",
      "[Row 8] DOI 10.3758/s13428-022-01977-3: No abstract found.\n",
      "[Row 11] DOI 10.36059/978-966-397-124-7/39-56: No abstract found.\n",
      "[Row 14] DOI 10.1007/978-3-031-27506-7_4: No abstract found.\n",
      "[Row 16] DOI 10.17605/osf.io/vu2na: API error -> HTTP 404: Resource not found.\n",
      "[Row 18] DOI 10.26226/morressier.606f15dd30a2e980041f238c: No abstract found.\n",
      "[Row 47] No DOI, skipping.\n",
      "[Row 53] DOI 10.5281/zenodo.7503056: API error -> HTTP 404: Resource not found.\n",
      "[Row 57] DOI 10.5281/zenodo.7508859: API error -> HTTP 404: Resource not found.\n",
      "[Row 73] DOI 10.52172/2587-6945_2021_17_3_13: No abstract found.\n",
      "[Row 89] No DOI, skipping.\n",
      "[Row 101] DOI 10.30842/26583755202202: No abstract found.\n",
      "[Row 102] DOI 10.47743/phss-2024-0012: No abstract found.\n",
      "[Row 104] No DOI, skipping.\n",
      "[Row 110] No DOI, skipping.\n",
      "[Row 114] DOI 10.24919/2308-4863/74-2-59: No abstract found.\n"
     ]
    }
   ],
   "source": [
    "df = enrich_missing_abstracts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "481e2e0e-26d1-445a-b0d2-365386d1e0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing after: 18 of 1641\n"
     ]
    }
   ],
   "source": [
    "missing_after = df[\"abstract\"].isna() | (df[\"abstract\"].astype(str).str.strip() == \"\")\n",
    "print(\"Missing after:\", int(missing_after.sum()), \"of\", len(df))\n",
    "\n",
    "# Peek at newly-filled examples\n",
    "# df.loc[df[\"abstract_source\"].notna(), [\"title\", \"doi\", \"abstract_source\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3e56ba-10b8-43db-b40d-c80fcc7e76a0",
   "metadata": {},
   "source": [
    "## Write to CSV\n",
    "\n",
    "Write the output for the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c022ef73-9a17-4b05-8ec4-850e075e1618",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('new_data_to_classify.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Jupyter Env)",
   "language": "python",
   "name": "py311-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
