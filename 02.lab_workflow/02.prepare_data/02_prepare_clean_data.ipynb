{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "260064f0-59e8-46cb-ab42-8e500fbfae63",
   "metadata": {},
   "source": [
    "# Prepare the Data for Classification\n",
    "\n",
    "In this file, we de-duplicate the `new_data_to_classify.csv` dataset by using the `both_lab_table.csv`, so articles already processed are ignored. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda19f45-a6cd-43e2-92dc-32e6da958bde",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22e4b0f9-c2c9-49e9-9fdf-7eaa4556cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, unicodedata, pandas as pd\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e0ae9d-8034-4d9d-9f49-d61b91ad2143",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "493f9f4b-0326-49e3-b2bc-d8b06584ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv(\"../01.get_new_data/new_data_to_classify.csv\")\n",
    "## remember you need to update this to read from the final final \n",
    "df_old = pd.read_csv(\"both_lab_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc800a6a-7b88-43f4-b68f-6f7922919ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original new fetch: 1641\n",
      "Already in previous data: 97\n",
      "New unique rows after exclusion: 1544\n"
     ]
    }
   ],
   "source": [
    "# normalize DOIs in both dataframes\n",
    "def clean_doi(x):\n",
    "    if pd.isna(x): return \"\"\n",
    "    x = str(x).strip().lower()\n",
    "    x = re.sub(r\"^https?://(dx\\.)?doi\\.org/\", \"\", x)\n",
    "    return x\n",
    "\n",
    "df_old[\"doi_clean\"] = df_old[\"DOI\"].apply(clean_doi)\n",
    "df_new[\"doi_clean\"] = df_new[\"doi\"].apply(clean_doi)\n",
    "\n",
    "# --- find overlap ---\n",
    "already = set(df_old[\"doi_clean\"].dropna().unique())\n",
    "mask_new = ~df_new[\"doi_clean\"].isin(already)\n",
    "\n",
    "df_new_only = df_new[mask_new].reset_index(drop=True)\n",
    "\n",
    "print(\"Original new fetch:\", len(df_new))\n",
    "print(\"Already in previous data:\", len(df_new) - mask_new.sum())\n",
    "print(\"New unique rows after exclusion:\", len(df_new_only))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff035c2-81d2-4b90-b0ca-7728c047c73c",
   "metadata": {},
   "source": [
    "## Create Prediction Information\n",
    "\n",
    "Clean up the data for prediction purposes including the following steps:\n",
    "\n",
    "- Paste together keywords, title, and abstract.\n",
    "- Lowercase\n",
    "- Convert to ASCII\n",
    "- Remove any \"keywords\" or NA values\n",
    "- Expand contractions\n",
    "- Remove punctuation\n",
    "- Remove stopwords\n",
    "- Remove extra white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fec8b850-3175-431d-ac2c-87ae4fde4f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concreteness Word Association Word (group theory) Natural language processing Computer science Association (psychology) Valence (chemistry) Word lists by frequency Lexical decision task Age of Acquisition Artificial intelligence Psychology Cognitive psychology Linguistics Cognition Philosophy Physics Quantum mechanics Neuroscience Psychotherapist Sentence Predicting Lexical Norms: A Comparison between a Word Association Model and Text-Based Word Co-occurrence Models In two studies we compare a d\n"
     ]
    }
   ],
   "source": [
    "# convert stringified lists into real lists\n",
    "def coerce_keywords(val):\n",
    "    if isinstance(val, list):\n",
    "        return val\n",
    "    if isinstance(val, str) and val.startswith(\"[\") and val.endswith(\"]\"):\n",
    "        try:\n",
    "            return ast.literal_eval(val)\n",
    "        except Exception:\n",
    "            return [val]\n",
    "    if pd.isna(val):\n",
    "        return []\n",
    "    return [val]\n",
    "\n",
    "df_new_only[\"keywords\"] = df_new_only[\"keywords\"].apply(coerce_keywords)\n",
    "\n",
    "# now rebuild text\n",
    "def make_text(row):\n",
    "    parts = []\n",
    "    if row[\"keywords\"]:\n",
    "        parts.append(\" \".join(row[\"keywords\"]))\n",
    "    if pd.notna(row.get(\"title\")):\n",
    "        parts.append(str(row[\"title\"]))\n",
    "    if pd.notna(row.get(\"abstract\")):\n",
    "        parts.append(str(row[\"abstract\"]))\n",
    "    return \" \".join(parts)\n",
    "\n",
    "df_new_only[\"text\"] = df_new_only.apply(make_text, axis=1)\n",
    "\n",
    "print(df_new_only[\"text\"].iloc[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62fe4a42-3af9-49d8-a707-37255411478a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved -> complete_processed_data_no_stem.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['concreteness word association word group theory natural language processing computer science association psychology valence chemistry word lists frequency lexical decision task age acquisition artificial intelligence psychology cognitive psychology linguistics cognition philosophy physics quantum mechanics neuroscience psychotherapist sentence predicting lexical norms comparison word association model text based word occurrence models studies compare distributional semantic model derived word occurrences word association based model ability predict properties affect lexical processing focus age acquisition concreteness affective variables valence arousal dominance variables shown fundamental word meaning studies use model based data obtained continued free word association task predict variables study directly compare model word occurrence model based syntactic dependency relations model better predicting variables scrutiny dutch study replicate findings english compare results reported literature studies word association based model fit predict diverse word properties especially case predicting affective word properties association model superior distributional model',\n",
       " 'iconicity age acquisition german computer science linguistics sign language spoken language psychology american sign language natural language processing cognition philosophy neuroscience psycholinguistic norms lexical signs german sign language dgs sign language offers unique perspective human faculty language illustrating linguistic abilities bound speech writing studies spoken written language processing lexical variables example age acquisition play important role information available german sign language deutsche gebardensprache dgs present set norms frequency age acquisition iconicity lexical dgs signs derived subjective ratings deaf signers provide additional norms iconicity transparency set signs derived ratings hearing non signers addition empirical norming data dataset includes machine readable information sign correspondence german english annotations lexico semantic phonological properties handed vs handed place articulation likely lexical class animacy verb type potential homonymy potential dialectal variation finally include information sign onset offset stimulus clips automated motion tracking data norms stimulus clips data code used analysis available open science framework hope prove useful researchers https doi org osf io mz j',\n",
       " 'linguistics german turkish lexical item language transfer multilingualism context archaeology foreign language psychology normative meaning existential sociology computer science language education history comprehension approach philosophy archaeology epistemology psychotherapist multilingual lexical transfer challenges monolingual educational norms quite abstract foreign language learners frequently use words previously acquired language s target language especially languages related ringbom hakan lexical transfer l production jasone cenoz britta hufeisen amp ulrike jessner eds cross linguistic influence language acquisition psycholinguistic perspectives clevedon multilingual matters insertions referred lexical transfer commonly divided transfer form transfer meaning bardel camilla lexical cross linguistic influence language development hagen peukert ed transfer effects multilingual language development amsterdam john benjamins ringbom hakan lexical transfer l production jasone cenoz britta hufeisen amp ulrike jessner eds cross linguistic influence language acquisition psycholinguistic perspectives clevedon multilingual matters lexical transfer challenges monolingual habitus prevailing foreign language classes requires students rely exclusively target language inhibit influences english classes students avoid use different languages ideally produce monolingual english output context current study investigates use lexical transfer instances short english texts written bilingual russian turkish german monolingual german secondary school students initially attending year longitudinal perspective assesses students increasingly adhere imposed normative rules ii influence background variables language background mono vs bilingual type school higher vs lower academic track gender female vs male age measurement points period years exert use lexical transfer instances apart gender factors impact lexical transfer statistically significant way evoking different norm based explanations']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- helpers ---\n",
    "def to_ascii(s: str) -> str:\n",
    "    # latin1 -> ascii-ish: normalize then drop non-ASCII\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", str(s))\n",
    "    return s.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "\n",
    "# minimal contraction expander (no installs needed)\n",
    "_CONTRACTIONS = {\n",
    "    \"can't\":\"cannot\",\"won't\":\"will not\",\"n't\":\" not\",\n",
    "    \"'re\":\" are\",\"'s\":\" is\",\"'d\":\" would\",\"'ll\":\" will\",\"'t\":\" not\",\n",
    "    \"'ve\":\" have\",\"'m\":\" am\",\"’re\":\" are\",\"’s\":\" is\",\"’d\":\" would\",\n",
    "    \"’ll\":\" will\",\"’t\":\" not\",\"’ve\":\" have\",\"’m\":\" am\"\n",
    "}\n",
    "_contr_pat = re.compile(\"|\".join(map(re.escape, sorted(_CONTRACTIONS, key=len, reverse=True))))\n",
    "\n",
    "def expand_contractions(text: str) -> str:\n",
    "    return _contr_pat.sub(lambda m: _CONTRACTIONS[m.group(0)], text)\n",
    "\n",
    "STOPWORDS = set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    # tokenise on whitespace after basic cleanup\n",
    "    tokens = text.split()\n",
    "    return \" \".join(t for t in tokens if t not in STOPWORDS)\n",
    "\n",
    "def clean_text_series(s: pd.Series) -> pd.Series:\n",
    "    s = s.fillna(\"\").astype(str)\n",
    "\n",
    "    # 1) convert to ascii\n",
    "    s = s.apply(to_ascii)\n",
    "\n",
    "    # 2) remove leading \"Keywords\" (case-insensitive, optional colon)\n",
    "    s = s.str.replace(r\"^\\s*keywords[:\\-]?\\s*\", \" \", case=False, regex=True)\n",
    "\n",
    "    # 3) remove standalone \"NA\" (start or word boundary)\n",
    "    s = s.str.replace(r\"(^|\\s)NA\\b\", \" \", case=False, regex=True)\n",
    "\n",
    "    # 4) lowercase\n",
    "    s = s.str.lower()\n",
    "\n",
    "    # 5) expand contractions\n",
    "    s = s.apply(expand_contractions)\n",
    "\n",
    "    # 6) remove punctuation\n",
    "    s = s.str.replace(r\"[^\\w\\s]\", \" \", regex=True)\n",
    "\n",
    "    # 7) remove digits\n",
    "    s = s.str.replace(r\"\\d+\", \" \", regex=True)\n",
    "\n",
    "    # 8) remove stopwords\n",
    "    s = s.apply(remove_stopwords)\n",
    "\n",
    "    # 9) collapse extra whitespace\n",
    "    s = s.str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "    return s\n",
    "\n",
    "# run the cleaner\n",
    "df_new_only[\"text\"] = clean_text_series(df_new_only[\"text\"])\n",
    "\n",
    "df_new_only.to_csv(\"complete_processed_data_no_stem.csv\", index=False)\n",
    "\n",
    "print(\"saved -> complete_processed_data_no_stem.csv\")\n",
    "df_new_only[\"text\"].head(3).to_list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Jupyter Env)",
   "language": "python",
   "name": "py311-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
